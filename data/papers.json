[
  {
    "id": "http://arxiv.org/abs/2512.16019v1",
    "title": "Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios",
    "authors": [
      "Zhang",
      "Tsoi",
      "Nagib"
    ],
    "summary": "Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.",
    "pdf_url": "https://arxiv.org/pdf/2512.16019v1",
    "github_url": null,
    "published": "2025-12-17T23:06:36+00:00",
    "updated": "2025-12-17T23:06:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15047v1",
    "title": "HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles",
    "authors": [
      "Wang",
      "Feng",
      "Fang"
    ],
    "summary": "3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.",
    "pdf_url": "https://arxiv.org/pdf/2512.15047v1",
    "github_url": null,
    "published": "2025-12-17T03:22:27+00:00",
    "updated": "2025-12-17T03:22:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.14691v2",
    "title": "MMGR: Multi-Modal Generative Reasoning",
    "authors": [
      "Cai",
      "Qiu",
      "Ma"
    ],
    "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.",
    "pdf_url": "https://arxiv.org/pdf/2512.14691v2",
    "github_url": null,
    "published": "2025-12-16T18:58:04+00:00",
    "updated": "2025-12-17T18:42:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.14222v2",
    "title": "History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation",
    "authors": [
      "Ding",
      "Gao",
      "Pan"
    ],
    "summary": "Aerial Vision-and-Language Navigation (AVLN) requires Unmanned Aerial Vehicle (UAV) agents to localize targets in large-scale urban environments based on linguistic instructions. While successful navigation demands both global environmental reasoning and local scene comprehension, existing UAV agents typically adopt mono-granularity frameworks that struggle to balance these two aspects. To address this limitation, this work proposes a History-Enhanced Two-Stage Transformer (HETT) framework, which integrates the two aspects through a coarse-to-fine navigation pipeline. Specifically, HETT first predicts coarse-grained target positions by fusing spatial landmarks and historical context, then refines actions via fine-grained visual analysis. In addition, a historical grid map is designed to dynamically aggregate visual features into a structured spatial memory, enhancing comprehensive scene awareness. Additionally, the CityNav dataset annotations are manually refined to enhance data quality. Experiments on the refined CityNav dataset show that HETT delivers significant performance gains, while extensive ablation studies further verify the effectiveness of each component.",
    "pdf_url": "https://arxiv.org/pdf/2512.14222v2",
    "github_url": null,
    "published": "2025-12-16T09:16:07+00:00",
    "updated": "2025-12-17T02:51:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.13215v1",
    "title": "Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment",
    "authors": [
      "Qu",
      "Li",
      "Zhong"
    ],
    "summary": "Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).",
    "pdf_url": "https://arxiv.org/pdf/2512.13215v1",
    "github_url": null,
    "published": "2025-12-15T11:28:04+00:00",
    "updated": "2025-12-15T11:28:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12622v1",
    "title": "D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation",
    "authors": [
      "Wang",
      "Lee",
      "Dai"
    ],
    "summary": "Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.",
    "pdf_url": "https://arxiv.org/pdf/2512.12622v1",
    "github_url": null,
    "published": "2025-12-14T09:53:15+00:00",
    "updated": "2025-12-14T09:53:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11736v1",
    "title": "Bench-Push: Benchmarking Pushing-based Navigation and Manipulation Tasks for Mobile Robots",
    "authors": [
      "Zhong",
      "Caro",
      "Ramesh"
    ],
    "summary": "Mobile robots are increasingly deployed in cluttered environments with movable objects, posing challenges for traditional methods that prohibit interaction. In such settings, the mobile robot must go beyond traditional obstacle avoidance, leveraging pushing or nudging strategies to accomplish its goals. While research in pushing-based robotics is growing, evaluations rely on ad hoc setups, limiting reproducibility and cross-comparison. To address this, we present Bench-Push, the first unified benchmark for pushing-based mobile robot navigation and manipulation tasks. Bench-Push includes multiple components: 1) a comprehensive range of simulated environments that capture the fundamental challenges in pushing-based tasks, including navigating a maze with movable obstacles, autonomous ship navigation in ice-covered waters, box delivery, and area clearing, each with varying levels of complexity; 2) novel evaluation metrics to capture efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-Push to evaluate example implementations of established baselines across environments. Bench-Push is open-sourced as a Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.",
    "pdf_url": "https://arxiv.org/pdf/2512.11736v1",
    "github_url": "https://github.com/IvanIZ/BenchNPIN",
    "published": "2025-12-12T17:25:32+00:00",
    "updated": "2025-12-12T17:25:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10360v1",
    "title": "CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "He",
      "Li"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps. While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks. To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR). RSMP adopts a causal-learning-based dual-branch architecture to enhance generalization, while RLMR leverages panoramic visual prompting with chain-of-thought reasoning to support interpretable spatial understanding and navigation. We further introduce an uncertainty-aware collaboration mechanism (UCM) that adaptively fuses decisions from both models. For obstacle avoidance, in simulation, we replace the rule-based controller with a fully learnable point-goal policy, and in real-world deployment, we design a LiDAR-based clustering module for generating navigable waypoints and pair it with an online SLAM-based local controller. CLASH achieves state-of-the-art (SoTA) results (ranking 1-st) on the VLN-CE leaderboard, significantly improving SR and SPL on the test-unseen set over the previous SoTA methods. Real-world experiments demonstrate CLASH's strong robustness, validating its effectiveness in both simulation and deployment scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2512.10360v1",
    "github_url": null,
    "published": "2025-12-11T07:20:06+00:00",
    "updated": "2025-12-11T07:20:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10322v1",
    "title": "User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation",
    "authors": [
      "Yu",
      "Li",
      "Mahmood"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.",
    "pdf_url": "https://arxiv.org/pdf/2512.10322v1",
    "github_url": null,
    "published": "2025-12-11T06:11:45+00:00",
    "updated": "2025-12-11T06:11:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10310v1",
    "title": "Efficient-VLN: A Training-Efficient Vision-Language Navigation Model",
    "authors": [
      "Zheng",
      "Huang",
      "Li"
    ],
    "summary": "Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.",
    "pdf_url": "https://arxiv.org/pdf/2512.10310v1",
    "github_url": null,
    "published": "2025-12-11T05:57:48+00:00",
    "updated": "2025-12-11T05:57:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10046v1",
    "title": "SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration",
    "authors": [
      "Zhuang",
      "Ren",
      "Ye"
    ],
    "summary": "Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.10046v1",
    "github_url": null,
    "published": "2025-12-10T20:04:08+00:00",
    "updated": "2025-12-10T20:04:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09607v1",
    "title": "UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories",
    "authors": [
      "Mei",
      "Yang",
      "Guo"
    ],
    "summary": "Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.",
    "pdf_url": "https://arxiv.org/pdf/2512.09607v1",
    "github_url": null,
    "published": "2025-12-10T12:54:04+00:00",
    "updated": "2025-12-10T12:54:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08639v1",
    "title": "Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning",
    "authors": [
      "Xu",
      "Liu",
      "Luomei"
    ],
    "summary": "Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.",
    "pdf_url": "https://arxiv.org/pdf/2512.08639v1",
    "github_url": null,
    "published": "2025-12-09T14:25:24+00:00",
    "updated": "2025-12-09T14:25:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08186v1",
    "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
    "authors": [
      "Wei",
      "Wan",
      "Peng"
    ],
    "summary": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, \"grounds slowly\" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, \"moves fast\" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.08186v1",
    "github_url": null,
    "published": "2025-12-09T02:29:36+00:00",
    "updated": "2025-12-09T02:29:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03958v2",
    "title": "MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation",
    "authors": [
      "Zhao",
      "Lyu",
      "Chen"
    ],
    "summary": "Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extended Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction. Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception. To bridge this gap, we present the method of Agricultural Vision-and-Language Navigation with Monocular Depth Estimation (MDE-AgriVLN), in which we propose the MDE module generating depth features from RGB images, to assist the decision-maker on multimodal reasoning. When evaluated on the A2A benchmark, our MDE-AgriVLN method successfully increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, demonstrating the state-of-the-art performance in the agricultural VLN domain. Code: https://github.com/AlexTraveling/MDE-AgriVLN.",
    "pdf_url": "https://arxiv.org/pdf/2512.03958v2",
    "github_url": "https://github.com/AlexTraveling/MDE-AgriVLN",
    "published": "2025-12-03T16:52:07+00:00",
    "updated": "2025-12-15T05:12:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03639v1",
    "title": "Context-Triggered Contingency Games for Strategic Multi-Agent Interaction",
    "authors": [
      "Schweppe",
      "Schmuck"
    ],
    "summary": "We address the challenge of reliable and efficient interaction in autonomous multi-agent systems, where agents must balance long-term strategic objectives with short-term dynamic adaptation. We propose context-triggered contingency games, a novel integration of strategic games derived from temporal logic specifications with dynamic contingency games solved in real time. Our two-layered architecture leverages strategy templates to guarantee satisfaction of high-level objectives, while a new factor-graph-based solver enables scalable, real-time model predictive control of dynamic interactions. The resulting framework ensures both safety and progress in uncertain, interactive environments. We validate our approach through simulations and hardware experiments in autonomous driving and robotic navigation, demonstrating efficient, reliable, and adaptive multi-agent interaction.",
    "pdf_url": "https://arxiv.org/pdf/2512.03639v1",
    "github_url": null,
    "published": "2025-12-03T10:19:36+00:00",
    "updated": "2025-12-03T10:19:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02631v1",
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "authors": [
      "Wang",
      "Lin",
      "Yang"
    ],
    "summary": "Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.",
    "pdf_url": "https://arxiv.org/pdf/2512.02631v1",
    "github_url": null,
    "published": "2025-12-02T10:40:46+00:00",
    "updated": "2025-12-02T10:40:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02458v1",
    "title": "Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration",
    "authors": [
      "Cai",
      "Du",
      "Wang"
    ],
    "summary": "Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.",
    "pdf_url": "https://arxiv.org/pdf/2512.02458v1",
    "github_url": null,
    "published": "2025-12-02T06:35:30+00:00",
    "updated": "2025-12-02T06:35:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01952v1",
    "title": "GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment",
    "authors": [
      "He",
      "Patrikar",
      "Kim"
    ],
    "summary": "Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.01952v1",
    "github_url": null,
    "published": "2025-12-01T18:03:29+00:00",
    "updated": "2025-12-01T18:03:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01550v1",
    "title": "NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction",
    "authors": [
      "Liu",
      "Xie",
      "Luo"
    ],
    "summary": "Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.",
    "pdf_url": "https://arxiv.org/pdf/2512.01550v1",
    "github_url": null,
    "published": "2025-12-01T11:24:16+00:00",
    "updated": "2025-12-01T11:24:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00226v1",
    "title": "DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation",
    "authors": [
      "Wang",
      "Zhang"
    ],
    "summary": "3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language tasks.In this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.",
    "pdf_url": "https://arxiv.org/pdf/2512.00226v1",
    "github_url": null,
    "published": "2025-11-28T22:02:10+00:00",
    "updated": "2025-11-28T22:02:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21135v1",
    "title": "SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation",
    "authors": [
      "Chen",
      "Guo",
      "Chu"
    ],
    "summary": "Embodied navigation that adheres to social norms remains an open research challenge. Our \\textbf{SocialNav} is a foundational model for socially-aware navigation with a hierarchical \"brain-action\" architecture, capable of understanding high-level social norms and generating low-level, socially compliant trajectories. To enable such dual capabilities, we construct the SocNav Dataset, a large-scale collection of 7 million samples, comprising (1) a Cognitive Activation Dataset providing social reasoning signals such as chain-of-thought explanations and social traversability prediction, and (2) an Expert Trajectories Pyramid aggregating diverse navigation demonstrations from internet videos, simulated environments, and real-world robots. A multi-stage training pipeline is proposed to gradually inject and refine navigation intelligence: we first inject general navigation skills and social norms understanding into the model via imitation learning, and then refine such skills through a deliberately designed Socially-Aware Flow Exploration GRPO (SAFE-GRPO), the first flow-based reinforcement learning framework for embodied navigation that explicitly rewards socially compliant behaviors. SocialNav achieves +38% success rate and +46% social compliance rate compared to the state-of-the-art method, demonstrating strong gains in both navigation performance and social compliance. Our project page: https://amap-eai.github.io/SocialNav/",
    "pdf_url": "https://arxiv.org/pdf/2511.21135v1",
    "github_url": null,
    "published": "2025-11-26T07:36:01+00:00",
    "updated": "2025-11-26T07:36:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20620v1",
    "title": "Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI",
    "authors": [
      "Liu",
      "Li",
      "Deng"
    ],
    "summary": "Reproducible closed-loop evaluation remains a major bottleneck in Embodied AI such as visual navigation. A promising path forward is high-fidelity simulation that combines photorealistic sensor rendering with geometrically grounded interaction in complex, open-world urban environments. Although recent video-3DGS methods ease open-world scene capturing, they are still unsuitable for benchmarking due to large visual and geometric sim-to-real gaps. To address these challenges, we introduce Wanderland, a real-to-sim framework that features multi-sensor capture, reliable reconstruction, accurate geometry, and robust view synthesis. Using this pipeline, we curate a diverse dataset of indoor-outdoor urban scenes and systematically demonstrate how image-only pipelines scale poorly, how geometry quality impacts novel view synthesis, and how all of these adversely affect navigation policy learning and evaluation reliability. Beyond serving as a trusted testbed for embodied navigation, Wanderland's rich raw sensor data further allows benchmarking of 3D reconstruction and novel view synthesis models. Our work establishes a new foundation for reproducible research in open-world embodied AI. Project website is at https://ai4ce.github.io/wanderland/.",
    "pdf_url": "https://arxiv.org/pdf/2511.20620v1",
    "github_url": null,
    "published": "2025-11-25T18:43:55+00:00",
    "updated": "2025-11-25T18:43:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18845v1",
    "title": "UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model",
    "authors": [
      "Huang",
      "Tang",
      "Zhan"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires agents to autonomously navigate complex environments via visual images and natural language instruction--remains highly challenging. Recent research on enhancing language-guided navigation reasoning using pre-trained large language models (LLMs) has shown promising prospects. However, the reasoning of such methods is limited to the linguistic modality, lacking visual reasoning capabilities. Moreover, existing reasoning modules are optimized separately from navigation policies, leading to incompatibility and potential conflicts in optimization objectives. To tackle these challenges, we introduce UNeMo, a novel framework designed for the collaborative optimization of visual state reasoning and navigational decision-making. It introduces a Multimodal World Model (MWM) that takes visual features, language instructions, and navigational actions as inputs to jointly predict subsequent visual states, enabling cross-modal reasoning. Via a Hierarchical Prediction-Feedback (HPN) mechanism, MWM collaborates with navigation policies: the first layer generates actions using current vision-and-language features; MWM then infers post-action visual states to guide the second layer's fine-grained decisions. This forms a dynamic bidirectional promotion mechanism where MWM reasoning optimizes navigation policies, while policy decisions feedback to improve MWM's reasoning accuracy. Experiments on R2R and REVERIE datasets show UNeMo outperforms state-of-the-art methods by 2.1% and 0.7% in navigation accuracy for unseen scenes, validating its effectiveness.",
    "pdf_url": "https://arxiv.org/pdf/2511.18845v1",
    "github_url": null,
    "published": "2025-11-24T07:31:58+00:00",
    "updated": "2025-11-24T07:31:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17889v1",
    "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots",
    "authors": [
      "Huang",
      "Li",
      "Yang"
    ],
    "summary": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.",
    "pdf_url": "https://arxiv.org/pdf/2511.17889v1",
    "github_url": "https://github.com/AIGeeksGroup/MobileVLA-R1",
    "published": "2025-11-22T02:34:10+00:00",
    "updated": "2025-11-22T02:34:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17225v1",
    "title": "TP-MDDN: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making",
    "authors": [
      "Li",
      "Huang",
      "He"
    ],
    "summary": "In daily life, people often move through spaces to find objects that meet their needs, posing a key challenge in embodied AI. Traditional Demand-Driven Navigation (DDN) handles one need at a time but does not reflect the complexity of real-world tasks involving multiple needs and personal choices. To bridge this gap, we introduce Task-Preferenced Multi-Demand-Driven Navigation (TP-MDDN), a new benchmark for long-horizon navigation involving multiple sub-demands with explicit task preferences. To solve TP-MDDN, we propose AWMSystem, an autonomous decision-making system composed of three key modules: BreakLLM (instruction decomposition), LocateLLM (goal selection), and StatusMLLM (task monitoring). For spatial memory, we design MASMap, which combines 3D point cloud accumulation with 2D semantic mapping for accurate and efficient environmental understanding. Our Dual-Tempo action generation framework integrates zero-shot planning with policy-based fine control, and is further supported by an Adaptive Error Corrector that handles failure cases in real time. Experiments demonstrate that our approach outperforms state-of-the-art baselines in both perception accuracy and navigation robustness.",
    "pdf_url": "https://arxiv.org/pdf/2511.17225v1",
    "github_url": null,
    "published": "2025-11-21T13:12:13+00:00",
    "updated": "2025-11-21T13:12:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16567v2",
    "title": "POMA-3D: The Point Map Way to 3D Scene Understanding",
    "authors": [
      "Mao",
      "Luo",
      "Huang"
    ],
    "summary": "In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/",
    "pdf_url": "https://arxiv.org/pdf/2511.16567v2",
    "github_url": null,
    "published": "2025-11-20T17:22:51+00:00",
    "updated": "2025-11-21T11:07:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14161v2",
    "title": "RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action",
    "authors": [
      "Sun",
      "Zhang",
      "Pang"
    ],
    "summary": "Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an \"Action (Object, Container)\" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.",
    "pdf_url": "https://arxiv.org/pdf/2511.14161v2",
    "github_url": null,
    "published": "2025-11-18T05:54:05+00:00",
    "updated": "2025-11-19T04:44:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14131v1",
    "title": "Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation",
    "authors": [
      "Zhong",
      "Zhang",
      "Zhang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.",
    "pdf_url": "https://arxiv.org/pdf/2511.14131v1",
    "github_url": null,
    "published": "2025-11-18T04:32:00+00:00",
    "updated": "2025-11-18T04:32:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13524v1",
    "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
    "authors": [
      "Peng",
      "Pan",
      "He"
    ],
    "summary": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
    "pdf_url": "https://arxiv.org/pdf/2511.13524v1",
    "github_url": null,
    "published": "2025-11-17T15:58:46+00:00",
    "updated": "2025-11-17T15:58:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13207v1",
    "title": "PIGEON: VLM-Driven Object Navigation via Points of Interest Selection",
    "authors": [
      "Peng",
      "Zhang",
      "Chi"
    ],
    "summary": "Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.",
    "pdf_url": "https://arxiv.org/pdf/2511.13207v1",
    "github_url": null,
    "published": "2025-11-17T10:19:13+00:00",
    "updated": "2025-11-17T10:19:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13132v1",
    "title": "Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack",
    "authors": [
      "Li",
      "Tang",
      "Huang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.",
    "pdf_url": "https://arxiv.org/pdf/2511.13132v1",
    "github_url": null,
    "published": "2025-11-17T08:39:29+00:00",
    "updated": "2025-11-17T08:39:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17588v1",
    "title": "Synthesis of mass-spring networks from high-level code descriptions",
    "authors": [
      "Omidvar",
      "Serra-Garcia"
    ],
    "summary": "Structural nonlinearity can be harnessed to program complex functionalities in robotic devices. However, it remains a challenge to design nonlinear systems that will accomplish a specific, desired task. The responses that we typically describe as intelligent -- such a robot navigating a maze -- require a large number of degrees of freedom and cannot be captured by traditional optimization objective functions. In this work, we explore a code-based synthesis approach to design mass-spring systems with embodied intelligence. The approach starts from a source code, written in a \\emph{mechanical description language}, that details the system boundary, sensor and actuator locations, and desired behavior. A synthesizer software then automatically generates a mass-spring network that performs the described function from the source code description. We exemplify this methodology by designing mass-spring systems realizing a maze-navigating robot and a programmable lock. Remarkably, mechanical description languages can be combined with large-language models, to translate a natural-language description of a task into a functional device.",
    "pdf_url": "https://arxiv.org/pdf/2511.17588v1",
    "github_url": null,
    "published": "2025-11-16T17:57:42+00:00",
    "updated": "2025-11-16T17:57:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12436v1",
    "title": "RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation",
    "authors": [
      "Hao",
      "Tang",
      "Zhang"
    ],
    "summary": "Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.",
    "pdf_url": "https://arxiv.org/pdf/2511.12436v1",
    "github_url": null,
    "published": "2025-11-16T03:35:50+00:00",
    "updated": "2025-11-16T03:35:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00041v1",
    "title": "VISTAv2: World Imagination for Indoor Vision-and-Language Navigation",
    "authors": [
      "Huang",
      "Jiang",
      "Gao"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires agents to follow language instructions while acting in continuous real-world spaces. Prior image imagination based VLN work shows benefits for discrete panoramas but lacks online, action-conditioned predictions and does not produce explicit planning values; moreover, many methods replace the planner with long-horizon objectives that are brittle and slow. To bridge this gap, we propose VISTAv2, a generative world model that rolls out egocentric future views conditioned on past observations, candidate action sequences, and instructions, and projects them into an online value map for planning. Unlike prior approaches, VISTAv2 does not replace the planner. The online value map is fused at score level with the base objective, providing reachability and risk-aware guidance. Concretely, we employ an action-aware Conditional Diffusion Transformer video predictor to synthesize short-horizon futures, align them with the natural language instruction via a vision-language scorer, and fuse multiple rollouts in a differentiable imagination-to-value head to output an imagined egocentric value map. For efficiency, rollouts occur in VAE latent space with a distilled sampler and sparse decoding, enabling inference on a single consumer GPU. Evaluated on MP3D and RoboTHOR, VISTAv2 improves over strong baselines, and ablations show that action-conditioned imagination, instruction-guided value fusion, and the online value-map planner are all critical, suggesting that VISTAv2 offers a practical and interpretable route to robust VLN.",
    "pdf_url": "https://arxiv.org/pdf/2512.00041v1",
    "github_url": null,
    "published": "2025-11-14T10:20:22+00:00",
    "updated": "2025-11-14T10:20:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10585v1",
    "title": "Textual understanding boost in the WikiRace",
    "authors": [
      "Ebrahimi",
      "Fuhrman",
      "Nguyen"
    ],
    "summary": "The WikiRace game, where players navigate between Wikipedia articles using only hyperlinks, serves as a compelling benchmark for goal-directed search in complex information networks. This paper presents a systematic evaluation of navigation strategies for this task, comparing agents guided by graph-theoretic structure (betweenness centrality), semantic meaning (language model embeddings), and hybrid approaches. Through rigorous benchmarking on a large Wikipedia subgraph, we demonstrate that a purely greedy agent guided by the semantic similarity of article titles is overwhelmingly effective. This strategy, when combined with a simple loop-avoidance mechanism, achieved a perfect success rate and navigated the network with an efficiency an order of magnitude better than structural or hybrid methods. Our findings highlight the critical limitations of purely structural heuristics for goal-directed search and underscore the transformative potential of large language models to act as powerful, zero-shot semantic navigators in complex information spaces.",
    "pdf_url": "https://arxiv.org/pdf/2511.10585v1",
    "github_url": null,
    "published": "2025-11-13T18:25:43+00:00",
    "updated": "2025-11-13T18:25:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10376v2",
    "title": "MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation",
    "authors": [
      "Huang",
      "Zhao",
      "Wang"
    ],
    "summary": "Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relation",
    "pdf_url": "https://arxiv.org/pdf/2511.10376v2",
    "github_url": null,
    "published": "2025-11-13T14:51:21+00:00",
    "updated": "2025-11-14T12:20:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08942v1",
    "title": "Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",
    "authors": [
      "Habibpour",
      "Afghah"
    ],
    "summary": "While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.",
    "pdf_url": "https://arxiv.org/pdf/2511.08942v1",
    "github_url": null,
    "published": "2025-11-12T03:38:50+00:00",
    "updated": "2025-11-12T03:38:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08935v1",
    "title": "Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation",
    "authors": [
      "Wang",
      "Chen",
      "Chen"
    ],
    "summary": "Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.",
    "pdf_url": "https://arxiv.org/pdf/2511.08935v1",
    "github_url": null,
    "published": "2025-11-12T03:23:09+00:00",
    "updated": "2025-11-12T03:23:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07811v1",
    "title": "Virtual Traffic Lights for Multi-Robot Navigation: Decentralized Planning with Centralized Conflict Resolution",
    "authors": [
      "Gupta",
      "Nguyen",
      "Phan"
    ],
    "summary": "We present a hybrid multi-robot coordination framework that combines decentralized path planning with centralized conflict resolution. In our approach, each robot autonomously plans its path and shares this information with a centralized node. The centralized system detects potential conflicts and allows only one of the conflicting robots to proceed at a time, instructing others to stop outside the conflicting area to avoid deadlocks. Unlike traditional centralized planning methods, our system does not dictate robot paths but instead provides stop commands, functioning as a virtual traffic light. In simulation experiments with multiple robots, our approach increased the success rate of robots reaching their goals while reducing deadlocks. Furthermore, we successfully validated the system in real-world experiments with two quadruped robots and separately with wheeled Duckiebots.",
    "pdf_url": "https://arxiv.org/pdf/2511.07811v1",
    "github_url": null,
    "published": "2025-11-11T04:07:44+00:00",
    "updated": "2025-11-11T04:07:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06840v1",
    "title": "PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory",
    "authors": [
      "Jin",
      "Wu",
      "Chen"
    ],
    "summary": "Zero-shot object navigation (ZSON) in unseen environments remains a challenging problem for household robots, requiring strong perceptual understanding and decision-making capabilities. While recent methods leverage metric maps and Large Language Models (LLMs), they often depend on depth sensors or prebuilt maps, limiting the spatial reasoning ability of Multimodal Large Language Models (MLLMs). Mapless ZSON approaches have emerged to address this, but they typically make short-sighted decisions, leading to local deadlocks due to a lack of historical context. We propose PanoNav, a fully RGB-only, mapless ZSON framework that integrates a Panoramic Scene Parsing module to unlock the spatial parsing potential of MLLMs from panoramic RGB inputs, and a Memory-guided Decision-Making mechanism enhanced by a Dynamic Bounded Memory Queue to incorporate exploration history and avoid local deadlocks. Experiments on the public navigation benchmark show that PanoNav significantly outperforms representative baselines in both SR and SPL metrics.",
    "pdf_url": "https://arxiv.org/pdf/2511.06840v1",
    "github_url": null,
    "published": "2025-11-10T08:31:32+00:00",
    "updated": "2025-11-10T08:31:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06182v2",
    "title": "OpenVLN: Open-world Aerial Vision-Language Navigation",
    "authors": [
      "Lin",
      "Sun",
      "Liu"
    ],
    "summary": "Vision-language models (VLMs) have been widely-applied in ground-based vision-language navigation (VLN). However, the vast complexity of outdoor aerial environments compounds data acquisition challenges and imposes long-horizon trajectory planning requirements on Unmanned Aerial Vehicles (UAVs), introducing novel complexities for aerial VLN. To address these challenges, we propose a data-efficient Open-world aerial Vision-Language Navigation (i.e., OpenVLN) framework, which could execute language-guided flight with limited data constraints and enhance long-horizon trajectory planning capabilities in complex aerial environments. Specifically, we reconfigure a reinforcement learning framework to optimize the VLM for UAV navigation tasks, which can efficiently fine-tune VLM by using rule-based policies under limited training data. Concurrently, we introduce a long-horizon planner for trajectory synthesis that dynamically generates precise UAV actions via value-based rewards. To the end, we conduct sufficient navigation experiments on the TravelUAV benchmark with dataset scaling across diverse reward settings. Our method demonstrates consistent performance gains of up to 4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success weighted by Path Length over baseline methods, validating its deployment efficacy for long-horizon UAV navigation in complex aerial environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.06182v2",
    "github_url": null,
    "published": "2025-11-09T01:50:53+00:00",
    "updated": "2025-11-21T00:57:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05889v1",
    "title": "From Words to Safety: Language-Conditioned Safety Filtering for Robot Navigation",
    "authors": [
      "Feng",
      "Zhang",
      "Bansal"
    ],
    "summary": "As robots become increasingly integrated into open-world, human-centered environments, their ability to interpret natural language instructions and adhere to safety constraints is critical for effective and trustworthy interaction. Existing approaches often focus on mapping language to reward functions instead of safety specifications or address only narrow constraint classes (e.g., obstacle avoidance), limiting their robustness and applicability. We propose a modular framework for language-conditioned safety in robot navigation. Our framework is composed of three core components: (1) a large language model (LLM)-based module that translates free-form instructions into structured safety specifications, (2) a perception module that grounds these specifications by maintaining object-level 3D representations of the environment, and (3) a model predictive control (MPC)-based safety filter that enforces both semantic and geometric constraints in real time. We evaluate the effectiveness of the proposed framework through both simulation studies and hardware experiments, demonstrating that it robustly interprets and enforces diverse language-specified constraints across a wide range of environments and scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2511.05889v1",
    "github_url": null,
    "published": "2025-11-08T07:07:11+00:00",
    "updated": "2025-11-08T07:07:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00027v1",
    "title": "A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation",
    "authors": [
      "Yakolli",
      "Gautam",
      "Das"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is a multi-modal, cooperative task requiring agents to interpret human instructions, navigate 3D environments, and communicate effectively under ambiguity. This paper presents a comprehensive review of recent VLN advancements in robotics and outlines promising directions to improve multi-robot coordination. Despite progress, current models struggle with bidirectional communication, ambiguity resolution, and collaborative decision-making in the multi-agent systems. We review approximately 200 relevant articles to provide an in-depth understanding of the current landscape. Through this survey, we aim to provide a thorough resource that inspires further research at the intersection of VLN and robotics. We advocate that the future VLN systems should support proactive clarification, real-time feedback, and contextual reasoning through advanced natural language understanding (NLU) techniques. Additionally, decentralized decision-making frameworks with dynamic role assignment are essential for scalable, efficient multi-robot collaboration. These innovations can significantly enhance human-robot interaction (HRI) and enable real-world deployment in domains such as healthcare, logistics, and disaster response.",
    "pdf_url": "https://arxiv.org/pdf/2512.00027v1",
    "github_url": null,
    "published": "2025-11-06T07:52:56+00:00",
    "updated": "2025-11-06T07:52:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00933v1",
    "title": "Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation",
    "authors": [
      "Shi",
      "Li",
      "Qiao"
    ],
    "summary": "Recent advances in Vision-and-Language Navigation in Continuous Environments (VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve zero-shot navigation. However, existing methods often rely on panoramic observations and two-stage pipelines involving waypoint predictors, which introduce significant latency and limit real-world applicability. In this work, we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that eliminates the need for panoramic views and waypoint predictors. Our approach uses only three frontal RGB-D images combined with natural language instructions, enabling MLLMs to directly predict actions. To enhance decision robustness, we introduce an Uncertainty-Aware Reasoning module that integrates (i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past Bidirectional Reasoning mechanism for globally coherent planning. Experiments on both simulated and real-robot environments demonstrate that our method significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines. These results demonstrate the practicality and effectiveness of Fast-SmartWay for real-world zero-shot embodied navigation.",
    "pdf_url": "https://arxiv.org/pdf/2511.00933v1",
    "github_url": null,
    "published": "2025-11-02T13:21:54+00:00",
    "updated": "2025-11-02T13:21:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27324v1",
    "title": "Generative Semantic Coding for Ultra-Low Bitrate Visual Communication and Analysis",
    "authors": [
      "Chen",
      "Wang",
      "Zhu"
    ],
    "summary": "We consider the problem of ultra-low bit rate visual communication for remote vision analysis, human interactions and control in challenging scenarios with very low communication bandwidth, such as deep space exploration, battlefield intelligence, and robot navigation in complex environments. In this paper, we ask the following important question: can we accurately reconstruct the visual scene using only a very small portion of the bit rate in existing coding methods while not sacrificing the accuracy of vision analysis and performance of human interactions? Existing text-to-image generation models offer a new approach for ultra-low bitrate image description. However, they can only achieve a semantic-level approximation of the visual scene, which is far insufficient for the purpose of visual communication and remote vision analysis and human interactions. To address this important issue, we propose to seamlessly integrate image generation with deep image compression, using joint text and coding latent to guide the rectified flow models for precise generation of the visual scene. The semantic text description and coding latent are both encoded and transmitted to the decoder at a very small bit rate. Experimental results demonstrate that our method can achieve the same image reconstruction quality and vision analysis accuracy as existing methods while using much less bandwidth. The code will be released upon paper acceptance.",
    "pdf_url": "https://arxiv.org/pdf/2510.27324v1",
    "github_url": null,
    "published": "2025-10-31T09:49:42+00:00",
    "updated": "2025-10-31T09:49:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26909v2",
    "title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models",
    "authors": [
      "Windecker",
      "Patel",
      "Reuss"
    ],
    "summary": "Vision-language models demonstrate unprecedented performance and generalization across a wide range of tasks and scenarios. Integrating these foundation models into robotic navigation systems opens pathways toward building general-purpose robots. Yet, evaluating these models' navigation capabilities remains constrained by costly real-world trials, overly simplified simulations, and limited benchmarks. We introduce NaviTrace, a high-quality Visual Question Answering benchmark where a model receives an instruction and embodiment type (human, legged robot, wheeled robot, bicycle) and must output a 2D navigation trace in image space. Across 1000 scenarios and more than 3000 expert traces, we systematically evaluate eight state-of-the-art VLMs using a newly introduced semantic-aware trace score. This metric combines Dynamic Time Warping distance, goal endpoint error, and embodiment-conditioned penalties derived from per-pixel semantics and correlates with human preferences. Our evaluation reveals consistent gap to human performance caused by poor spatial grounding and goal localization. NaviTrace establishes a scalable and reproducible benchmark for real-world robotic navigation. The benchmark and leaderboard can be found at https://leggedrobotics.github.io/navitrace_webpage/.",
    "pdf_url": "https://arxiv.org/pdf/2510.26909v2",
    "github_url": null,
    "published": "2025-10-30T18:16:32+00:00",
    "updated": "2025-11-04T21:17:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26782v2",
    "title": "Clone Deterministic 3D Worlds",
    "authors": [
      "Xia",
      "Lu",
      "Li"
    ],
    "summary": "A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future physical state of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. However, existing world models often focus on random generation of open worlds, but neglect the need for high-fidelity modeling of deterministic scenarios (such as fixed-map mazes and static space robot navigation). In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone a deterministic 3D world. 1) Through diagnostic experiment, we quantitatively demonstrate that high-fidelity cloning is feasible and the primary bottleneck for long-horizon fidelity is the geometric structure of the latent representation, not the dynamics model itself. 2) Building on this insight, we show that applying temporal contrastive learning principle as a geometric regularization can effectively curate a latent space that better reflects the underlying physical state manifold, demonstrating that contrastive constraints can serve as a powerful inductive bias for stable world modeling; we call this approach Geometrically-Regularized World Models (GRWM). At its core is a lightweight geometric regularization module that can be seamlessly integrated into standard autoencoders, reshaping their latent space to provide a stable foundation for effective dynamics modeling. By focusing on representation quality, GRWM offers a simple yet powerful pipeline for improving world model fidelity.",
    "pdf_url": "https://arxiv.org/pdf/2510.26782v2",
    "github_url": null,
    "published": "2025-10-30T17:56:43+00:00",
    "updated": "2025-11-18T04:52:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25760v2",
    "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks",
    "authors": [
      "Zheng",
      "Dongfang",
      "Jiang"
    ],
    "summary": "Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.",
    "pdf_url": "https://arxiv.org/pdf/2510.25760v2",
    "github_url": "https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning",
    "published": "2025-10-29T17:55:43+00:00",
    "updated": "2025-11-02T09:49:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25191v1",
    "title": "SoraNav: Adaptive UAV Task-Centric Navigation via Zeroshot VLM Reasoning",
    "authors": [
      "Song",
      "Yadav",
      "Guo"
    ],
    "summary": "Interpreting visual observations and natural language instructions for complex task execution remains a key challenge in robotics and AI. Despite recent advances, language-driven navigation is still difficult, particularly for UAVs in small-scale 3D environments. Existing Vision-Language Navigation (VLN) approaches are mostly designed for ground robots and struggle to generalize to aerial tasks that require full 3D spatial reasoning. The emergence of large Vision-Language Models (VLMs), such as GPT and Claude, enables zero-shot semantic reasoning from visual and textual inputs. However, these models lack spatial grounding and are not directly applicable to navigation. To address these limitations, SoraNav is introduced, an adaptive UAV navigation framework that integrates zero-shot VLM reasoning with geometry-aware decision-making. Geometric priors are incorporated into image annotations to constrain the VLM action space and improve decision quality. A hybrid switching strategy leverages navigation history to alternate between VLM reasoning and geometry-based exploration, mitigating dead-ends and redundant revisits. A PX4-based hardware-software platform, comprising both a digital twin and a physical micro-UAV, enables reproducible evaluation. Experimental results show that in 2.5D scenarios, our method improves Success Rate (SR) by 25.7% and Success weighted by Path Length (SPL) by 17%. In 3D scenarios, it improves SR by 29.5% and SPL by 18.5% relative to the baseline.",
    "pdf_url": "https://arxiv.org/pdf/2510.25191v1",
    "github_url": null,
    "published": "2025-10-29T05:46:29+00:00",
    "updated": "2025-10-29T05:46:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00033v1",
    "title": "STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization",
    "authors": [
      "He",
      "Gao",
      "Li"
    ],
    "summary": "The Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) task requires agents to navigate previously unseen 3D environments using natural language instructions, without any scene-specific training. A critical challenge in this setting lies in ensuring agents' actions align with both spatial structure and task intent over long-horizon execution. Existing methods often fail to achieve robust navigation due to a lack of structured decision-making and insufficient integration of feedback from previous actions. To address these challenges, we propose STRIDER (Instruction-Aligned Structural Decision Space Optimization), a novel framework that systematically optimizes the agent's decision space by integrating spatial layout priors and dynamic task feedback. Our approach introduces two key innovations: 1) a Structured Waypoint Generator that constrains the action space through spatial structure, and 2) a Task-Alignment Regulator that adjusts behavior based on task progress, ensuring semantic alignment throughout navigation. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms strong SOTA across key metrics; in particular, it improves Success Rate (SR) from 29% to 35%, a relative gain of 20.7%. Such results highlight the importance of spatially constrained decision-making and feedback-guided execution in improving navigation fidelity for zero-shot VLN-CE.",
    "pdf_url": "https://arxiv.org/pdf/2511.00033v1",
    "github_url": null,
    "published": "2025-10-27T04:37:21+00:00",
    "updated": "2025-10-27T04:37:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21307v2",
    "title": "Towards Physically Executable 3D Gaussian for Embodied Navigation",
    "authors": [
      "Miao",
      "Wei",
      "Ge"
    ],
    "summary": "3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic real-time rendering capabilities, is regarded as an effective tool for narrowing the sim-to-real gap. However, it lacks fine-grained semantics and physical executability for Visual-Language Navigation (VLN). To address this, we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments for 3D Navigation), a new paradigm that upgrades 3DGS into an executable, semantically and physically aligned environment. It comprises two components: (1) Object-Centric Semantic Grounding, which adds object-level fine-grained annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds collision objects into 3DGS and constructs rich physical interfaces. We release InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data. Experiments show that 3DGS scene data is more difficult to converge, while exhibiting strong generalizability, improving baseline performance by 31% on the VLN-CE Unseen task. Our data and code are available at: https://sage-3d.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2510.21307v2",
    "github_url": null,
    "published": "2025-10-24T10:05:00+00:00",
    "updated": "2025-12-15T10:05:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20818v1",
    "title": "VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation",
    "authors": [
      "Castro",
      "Rajagopal",
      "Gorbatov"
    ],
    "summary": "A fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment (e.g., quadrupeds can walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that decouples semantic planning from embodiment grounding: a generalist planner learns from diverse, open-world data, while a specialist affordance model learns the robot's physical constraints and capabilities in safe, low-cost simulation. We enabled this separation by carefully designing an interface that lets a high-level planner propose candidate paths directly in image space that the affordance model then evaluates and re-ranks. Our real-world experiments show that VAMOS achieves higher success rates in both indoor and complex outdoor navigation than state-of-the-art model-based and end-to-end learning methods. We also show that our hierarchical design enables cross-embodied navigation across legged and wheeled robots and is easily steerable using natural language. Real-world ablations confirm that the specialist model is key to embodiment grounding, enabling a single high-level planner to be deployed across physically distinct wheeled and legged robots. Finally, this model significantly enhances single-robot reliability, achieving 3X higher success rates by rejecting physically infeasible plans. Website: https://vamos-vla.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.20818v1",
    "github_url": null,
    "published": "2025-10-23T17:59:45+00:00",
    "updated": "2025-10-23T17:59:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20685v2",
    "title": "C-NAV: Towards Self-Evolving Continual Object Navigation in Open World",
    "authors": [
      "Yu",
      "Zhu",
      "Liu"
    ],
    "summary": "Embodied agents are expected to perform object navigation in dynamic, open-world environments. However, existing approaches typically rely on static trajectories and a fixed set of object categories during training, overlooking the real-world requirement for continual adaptation to evolving scenarios. To facilitate related studies, we introduce the continual object navigation benchmark, which requires agents to acquire navigation skills for new object categories while avoiding catastrophic forgetting of previously learned knowledge. To tackle this challenge, we propose C-Nav, a continual visual navigation framework that integrates two key innovations: (1) A dual-path anti-forgetting mechanism, which comprises feature distillation that aligns multi-modal inputs into a consistent representation space to ensure representation consistency, and feature replay that retains temporal features within the action decoder to ensure policy consistency. (2) An adaptive sampling strategy that selects diverse and informative experiences, thereby reducing redundancy and minimizing memory overhead. Extensive experiments across multiple model architectures demonstrate that C-Nav consistently outperforms existing approaches, achieving superior performance even compared to baselines with full trajectory retention, while significantly lowering memory requirements. The code will be publicly available at https://bigtree765.github.io/C-Nav-project.",
    "pdf_url": "https://arxiv.org/pdf/2510.20685v2",
    "github_url": null,
    "published": "2025-10-23T15:57:43+00:00",
    "updated": "2025-10-30T08:58:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19732v2",
    "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning",
    "authors": [
      "Gupta",
      "Yadav",
      "Kira"
    ],
    "summary": "To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo's effectiveness on a gridworld meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints. Our code is available at: https://github.com/gunshi/memo.",
    "pdf_url": "https://arxiv.org/pdf/2510.19732v2",
    "github_url": "https://github.com/gunshi/memo",
    "published": "2025-10-22T16:24:47+00:00",
    "updated": "2025-11-27T02:24:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19655v1",
    "title": "LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments",
    "authors": [
      "Ding",
      "Xu",
      "Fang"
    ],
    "summary": "Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires an agent to navigate unseen environments based on natural language instructions without any prior training. Current methods face a critical trade-off: either rely on environment-specific waypoint predictors that limit scene generalization, or underutilize the reasoning capabilities of large models during navigation. We introduce LaViRA, a simple yet effective zero-shot framework that addresses this dilemma by decomposing action into a coarse-to-fine hierarchy: Language Action for high-level planning, Vision Action for perceptual grounding, and Robot Action for robust navigation. This modular decomposition allows us to leverage the distinct strengths of different scales of Multimodal Large Language Models (MLLMs) at each stage, creating a system that is powerful in its reasoning, grounding and practical control. LaViRA significantly outperforms existing state-of-the-art methods on the VLN-CE benchmark, demonstrating superior generalization capabilities in unseen environments, while maintaining transparency and efficiency for real-world deployment.",
    "pdf_url": "https://arxiv.org/pdf/2510.19655v1",
    "github_url": null,
    "published": "2025-10-22T14:58:16+00:00",
    "updated": "2025-10-22T14:58:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21809v1",
    "title": "Embodied Navigation with Auxiliary Task of Action Description Prediction",
    "authors": [
      "Kondoh",
      "Kanezaki"
    ],
    "summary": "The field of multimodal robot navigation in indoor environments has garnered significant attention in recent years. However, as tasks and methods become more advanced, the action decision systems tend to become more complex and operate as black-boxes. For a reliable system, the ability to explain or describe its decisions is crucial; however, there tends to be a trade-off in that explainable systems can not outperform non-explainable systems in terms of performance. In this paper, we propose incorporating the task of describing actions in language into the reinforcement learning of navigation as an auxiliary task. Existing studies have found it difficult to incorporate describing actions into reinforcement learning due to the absence of ground-truth data. We address this issue by leveraging knowledge distillation from pre-trained description generation models, such as vision-language models. We comprehensively evaluate our approach across various navigation tasks, demonstrating that it can describe actions while attaining high navigation performance. Furthermore, it achieves state-of-the-art performance in the particularly challenging multimodal navigation task of semantic audio-visual navigation.",
    "pdf_url": "https://arxiv.org/pdf/2510.21809v1",
    "github_url": null,
    "published": "2025-10-21T09:12:22+00:00",
    "updated": "2025-10-21T09:12:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18211v1",
    "title": "Distributed Stochastic Search for Multi-Agent Model Predictive Control",
    "authors": [
      "Yoon",
      "Saravanos",
      "Theodorou"
    ],
    "summary": "Many real-world multi-agent systems exhibit nonlinear dynamics and complex inter-agent interactions. As these systems increase in scale, the main challenges arise from achieving scalability and handling nonconvexity. To address these challenges, this paper presents a distributed sampling-based optimization framework for multi-agent model predictive control (MPC). We first introduce stochastic search, a generalized sampling-based optimization method, as an effective approach to solving nonconvex MPC problems because of its exploration capabilities. Nevertheless, optimizing the multi-agent systems in a centralized fashion is not scalable as the computational complexity grows intractably as the number of agents increases. To achieve scalability, we formulate a distributed MPC problem and employ the alternating direction method of multipliers (ADMM) to leverage the distributed approach. In multi-robot navigation simulations, the proposed method shows a remarkable capability to navigate through nonconvex environments, outperforming a distributed optimization baseline using the interior point optimizer (IPOPT). In a 64-agent multi-car formation task with a challenging configuration, our method achieves 100% task completion with zero collisions, whereas distributed IPOPT fails to find a feasible solution.",
    "pdf_url": "https://arxiv.org/pdf/2510.18211v1",
    "github_url": null,
    "published": "2025-10-21T01:28:04+00:00",
    "updated": "2025-10-21T01:28:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16518v1",
    "title": "DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation",
    "authors": [
      "Ortega-Peimbert",
      "Busch",
      "Homberger"
    ],
    "summary": "Advances in open-vocabulary semantic mapping and object navigation have enabled robots to perform an informed search of their environment for an arbitrary object. However, such zero-shot object navigation is typically designed for simple queries with an object name like \"television\" or \"blue rug\". Here, we consider more complex free-text queries with spatial relationships, such as \"find the remote on the table\" while still leveraging robustness of a semantic map. We present DIV-Nav, a real-time navigation system that efficiently addresses this problem through a series of relaxations: i) Decomposing natural language instructions with complex spatial constraints into simpler object-level queries on a semantic map, ii) computing the Intersection of individual semantic belief maps to identify regions where all objects co-exist, and iii) Validating the discovered objects against the original, complex spatial constrains via a LVLM. We further investigate how to adapt the frontier exploration objectives of online semantic mapping to such spatial search queries to more effectively guide the search process. We validate our system through extensive experiments on the MultiON benchmark and real-world deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More details and videos are available at https://anonsub42.github.io/reponame/",
    "pdf_url": "https://arxiv.org/pdf/2510.16518v1",
    "github_url": null,
    "published": "2025-10-18T14:22:32+00:00",
    "updated": "2025-10-18T14:22:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16457v1",
    "title": "NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation",
    "authors": [
      "Xu",
      "Gong",
      "MU"
    ],
    "summary": "In this work we concentrate on the task of goal-oriented Vision-and-Language Navigation (VLN). Existing methods often make decisions based on historical information, overlooking the future implications and long-term outcomes of the actions. In contrast, we aim to develop a foresighted agent. Specifically, we draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory data, in order to learn the general knowledge regarding the layout and object relations within indoor scenes. This model can generate a Q-feature, analogous to the Q-value in traditional Q-network, for each candidate action, which describes the potential future information that may be observed after taking the specific action. Subsequently, a cross-modal future encoder integrates the task-agnostic Q-feature with navigation instructions to produce a set of action scores reflecting future prospects. These scores, when combined with the original scores based on history, facilitate an A*-style searching strategy to effectively explore the regions that are more likely to lead to the destination. Extensive experiments conducted on widely used goal-oriented VLN datasets validate the effectiveness of the proposed method.",
    "pdf_url": "https://arxiv.org/pdf/2510.16457v1",
    "github_url": null,
    "published": "2025-10-18T11:29:33+00:00",
    "updated": "2025-10-18T11:29:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15018v1",
    "title": "UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos",
    "authors": [
      "Liu",
      "He",
      "Ricci"
    ],
    "summary": "Urban embodied AI agents, ranging from delivery robots to quadrupeds, are increasingly populating our cities, navigating chaotic streets to provide last-mile connectivity. Training such agents requires diverse, high-fidelity urban environments to scale, yet existing human-crafted or procedurally generated simulation scenes either lack scalability or fail to capture real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim system that converts crowd-sourced city-tour videos into physics-aware, interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a repository of 100k+ annotated urban 3D assets with semantic and physical attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene layouts from video and instantiates metric-scale 3D simulations using retrieved assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed scenes from 24 countries, along with a curated benchmark of 10 artist-designed test scenes. Experiments show that UrbanVerse scenes preserve real-world semantics and layouts, achieving human-evaluated realism comparable to manually crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit scaling power laws and strong generalization, improving success by +6.3% in simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior methods, accomplishing a 300 m real-world mission with only two interventions.",
    "pdf_url": "https://arxiv.org/pdf/2510.15018v1",
    "github_url": null,
    "published": "2025-10-16T17:42:34+00:00",
    "updated": "2025-10-16T17:42:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14904v2",
    "title": "MaskCaptioner: Learning to Jointly Segment and Caption Object Trajectories in Videos",
    "authors": [
      "Fiastre",
      "Yang",
      "Schmid"
    ],
    "summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting, tracking, and captioning object trajectories in a video, requiring the ability to understand spatio-temporal details and describe them in natural language. Due to the complexity of the task and the high cost associated with manual annotation, previous approaches resort to disjoint training strategies, potentially leading to suboptimal performance. To circumvent this issue, we propose to generate captions about spatio-temporally localized entities leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an end-to-end model capable of jointly detecting, segmenting, tracking and captioning object trajectories. Moreover, with pretraining on LVISCap and LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are available at https://www.gabriel.fiastre.fr/maskcaptioner/.",
    "pdf_url": "https://arxiv.org/pdf/2510.14904v2",
    "github_url": null,
    "published": "2025-10-16T17:20:22+00:00",
    "updated": "2025-10-30T15:39:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14357v1",
    "title": "SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation",
    "authors": [
      "Zhao",
      "Lyu",
      "Li"
    ],
    "summary": "Agricultural robots are emerging as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily rely on manual operation or fixed rail systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling robots to navigate to the target positions following the natural language instructions. In practical agricultural scenarios, navigation instructions often repeatedly occur, yet AgriVLN treat each instruction as an independent episode, overlooking the potential of past experiences to provide spatial context for subsequent ones. To bridge this gap, we propose the method of Spatial Understanding Memory for Agricultural Vision-and-Language Navigation (SUM-AgriVLN), in which the SUM module employs spatial understanding and save spatial memory through 3D reconstruction and representation. When evaluated on the A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47 to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m, demonstrating the state-of-the-art performance in the agricultural domain. Code: https://github.com/AlexTraveling/SUM-AgriVLN.",
    "pdf_url": "https://arxiv.org/pdf/2510.14357v1",
    "github_url": "https://github.com/AlexTraveling/SUM-AgriVLN",
    "published": "2025-10-16T06:53:32+00:00",
    "updated": "2025-10-16T06:53:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12919v1",
    "title": "Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation",
    "authors": [
      "Khan",
      "Ibuki",
      "Chatterjee"
    ],
    "summary": "Level set methods underpin modern safety techniques such as control barrier functions (CBFs), while also serving as implicit surface representations for geometric shapes via distance fields. Inspired by these two paradigms, we propose a unified framework where the implicit surface itself acts as a CBF. We leverage Gaussian process (GP) implicit surface (GPIS) to represent the safety boundaries, using safety samples which are derived from sensor measurements to condition the GP. The GP posterior mean defines the implicit safety surface (safety belief), while the posterior variance provides a robust safety margin. Although GPs have favorable properties such as uncertainty estimation and analytical tractability, they scale cubically with data. To alleviate this issue, we develop a sparse solution called sparse Gaussian CBFs. To the best of our knowledge, GPIS have not been explicitly used to synthesize CBFs. We validate the approach on collision avoidance tasks in two settings: a simulated 7-DOF manipulator operating around the Stanford bunny, and a quadrotor navigating in 3D around a physical chair. In both cases, Gaussian CBFs (with and without sparsity) enable safe interaction and collision-free execution of trajectories that would otherwise intersect the objects.",
    "pdf_url": "https://arxiv.org/pdf/2510.12919v1",
    "github_url": null,
    "published": "2025-10-14T18:45:59+00:00",
    "updated": "2025-10-14T18:45:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11760v1",
    "title": "Audio-Guided Visual Perception for Audio-Visual Navigation",
    "authors": [
      "Wang",
      "Yu",
      "Sun"
    ],
    "summary": "Audio-Visual Embodied Navigation aims to enable agents to autonomously navigate to sound sources in unknown 3D environments using auditory cues. While current AVN methods excel on in-distribution sound sources, they exhibit poor cross-source generalization: navigation success rates plummet and search paths become excessively long when agents encounter unheard sounds or unseen environments. This limitation stems from the lack of explicit alignment mechanisms between auditory signals and corresponding visual regions. Policies tend to memorize spurious \\enquote{acoustic fingerprint-scenario} correlations during training, leading to blind exploration when exposed to novel sound sources. To address this, we propose the AGVP framework, which transforms sound from policy-memorable acoustic fingerprint cues into spatial guidance. The framework first extracts global auditory context via audio self-attention, then uses this context as queries to guide visual feature attention, highlighting sound-source-related regions at the feature level. Subsequent temporal modeling and policy optimization are then performed. This design, centered on interpretable cross-modal alignment and region reweighting, reduces dependency on specific acoustic fingerprints. Experimental results demonstrate that AGVP improves both navigation efficiency and robustness while achieving superior cross-scenario generalization on previously unheard sounds.",
    "pdf_url": "https://arxiv.org/pdf/2510.11760v1",
    "github_url": null,
    "published": "2025-10-13T05:06:45+00:00",
    "updated": "2025-10-13T05:06:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10823v1",
    "title": "The Irrational Machine: Neurosis and the Limits of Algorithmic Safety",
    "authors": [
      "Howard"
    ],
    "summary": "We present a framework for characterizing neurosis in embodied AI: behaviors that are internally coherent yet misaligned with reality, arising from interactions among planning, uncertainty handling, and aversive memory. In a grid navigation stack we catalogue recurrent modalities including flip-flop, plan churn, perseveration loops, paralysis and hypervigilance, futile search, belief incoherence, tie break thrashing, corridor thrashing, optimality compulsion, metric mismatch, policy oscillation, and limited-visibility variants. For each we give lightweight online detectors and reusable escape policies (short commitments, a margin to switch, smoothing, principled arbitration). We then show that durable phobic avoidance can persist even under full visibility when learned aversive costs dominate local choice, producing long detours despite globally safe routes. Using First/Second/Third Law as engineering shorthand for safety latency, command compliance, and resource efficiency, we argue that local fixes are insufficient; global failures can remain. To surface them, we propose genetic-programming based destructive testing that evolves worlds and perturbations to maximize law pressure and neurosis scores, yielding adversarial curricula and counterfactual traces that expose where architectural revision, not merely symptom-level patches, is required.",
    "pdf_url": "https://arxiv.org/pdf/2510.10823v1",
    "github_url": null,
    "published": "2025-10-12T22:22:17+00:00",
    "updated": "2025-10-12T22:22:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08713v1",
    "title": "Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation",
    "authors": [
      "Dong",
      "Wu",
      "Chen"
    ],
    "summary": "Enabling embodied agents to effectively imagine future states is critical for robust and generalizable visual navigation. Current state-of-the-art approaches, however, adopt modular architectures that separate navigation planning from visual world modeling, leading to state-action misalignment and limited adaptability in novel or dynamic scenarios. To overcome this fundamental limitation, we propose UniWM, a unified, memory-augmented world model integrating egocentric visual foresight and planning within a single multimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly grounds action decisions in visually imagined outcomes, ensuring tight alignment between prediction and control. A hierarchical memory mechanism further integrates detailed short-term perceptual cues with longer-term trajectory context, enabling stable, coherent reasoning over extended horizons. Extensive experiments across four challenging benchmarks (Go Stanford, ReCon, SCAND, HuRoN) demonstrate that UniWM substantially improves navigation success rates by up to 30%, significantly reduces trajectory errors compared to strong baselines, and exhibits impressive zero-shot generalization on the unseen TartanDrive dataset. These results highlight UniWM as a principled step toward unified, imagination-driven embodied navigation.",
    "pdf_url": "https://arxiv.org/pdf/2510.08713v1",
    "github_url": null,
    "published": "2025-10-09T18:18:11+00:00",
    "updated": "2025-10-09T18:18:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08553v1",
    "title": "Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation",
    "authors": [
      "Xu",
      "Pan",
      "Liu"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.",
    "pdf_url": "https://arxiv.org/pdf/2510.08553v1",
    "github_url": "https://github.com/xyz9911/Memoir",
    "published": "2025-10-09T17:58:01+00:00",
    "updated": "2025-10-09T17:58:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08173v1",
    "title": "NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions",
    "authors": [
      "Yang",
      "Long",
      "Yu"
    ],
    "summary": "Instruction-following navigation is a key step toward embodied intelligence. Prior benchmarks mainly focus on semantic understanding but overlook systematically evaluating navigation agents' spatial perception and reasoning capabilities. In this work, we introduce the NavSpace benchmark, which contains six task categories and 1,228 trajectory-instruction pairs designed to probe the spatial intelligence of navigation agents. On this benchmark, we comprehensively evaluate 22 navigation agents, including state-of-the-art navigation models and multimodal large language models. The evaluation results lift the veil on spatial intelligence in embodied navigation. Furthermore, we propose SNav, a new spatially intelligent navigation model. SNav outperforms existing navigation agents on NavSpace and real robot tests, establishing a strong baseline for future work.",
    "pdf_url": "https://arxiv.org/pdf/2510.08173v1",
    "github_url": null,
    "published": "2025-10-09T12:59:19+00:00",
    "updated": "2025-10-09T12:59:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.05684v2",
    "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI",
    "authors": [
      "Choi",
      "Jung",
      "Seong"
    ],
    "summary": "Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/",
    "pdf_url": "https://arxiv.org/pdf/2510.05684v2",
    "github_url": null,
    "published": "2025-10-07T08:40:33+00:00",
    "updated": "2025-12-17T19:23:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.00604v1",
    "title": "Disentangling Foreground and Background for vision-Language Navigation via Online Augmentation",
    "authors": [
      "Xu",
      "Zhang",
      "Li"
    ],
    "summary": "Following language instructions, vision-language navigation (VLN) agents are tasked with navigating unseen environments. While augmenting multifaceted visual representations has propelled advancements in VLN, the significance of foreground and background in visual observations remains underexplored. Intuitively, foreground regions provide semantic cues, whereas the background encompasses spatial connectivity information. Inspired on this insight, we propose a Consensus-driven Online Feature Augmentation strategy (COFA) with alternative foreground and background features to facilitate the navigable generalization. Specifically, we first leverage semantically-enhanced landmark identification to disentangle foreground and background as candidate augmented features. Subsequently, a consensus-driven online augmentation strategy encourages the agent to consolidate two-stage voting results on feature preferences according to diverse instructions and navigational locations. Experiments on REVERIE and R2R demonstrate that our online foreground-background augmentation boosts the generalization of baseline and attains state-of-the-art performance.",
    "pdf_url": "https://arxiv.org/pdf/2510.00604v1",
    "github_url": null,
    "published": "2025-10-01T07:32:36+00:00",
    "updated": "2025-10-01T07:32:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.00466v1",
    "title": "Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation",
    "authors": [
      "Su",
      "Fu",
      "Zhou"
    ],
    "summary": "Offline reinforcement learning (RL) has emerged as a promising framework for addressing robot social navigation challenges. However, inherent uncertainties in pedestrian behavior and limited environmental interaction during training often lead to suboptimal exploration and distributional shifts between offline training and online deployment. To overcome these limitations, this paper proposes a novel offline-to-online fine-tuning RL algorithm for robot social navigation by integrating Return-to-Go (RTG) prediction into a causal Transformer architecture. Our algorithm features a spatiotem-poral fusion model designed to precisely estimate RTG values in real-time by jointly encoding temporal pedestrian motion patterns and spatial crowd dynamics. This RTG prediction framework mitigates distribution shift by aligning offline policy training with online environmental interactions. Furthermore, a hybrid offline-online experience sampling mechanism is built to stabilize policy updates during fine-tuning, ensuring balanced integration of pre-trained knowledge and real-time adaptation. Extensive experiments in simulated social navigation environments demonstrate that our method achieves a higher success rate and lower collision rate compared to state-of-the-art baselines. These results underscore the efficacy of our algorithm in enhancing navigation policy robustness and adaptability. This work paves the way for more reliable and adaptive robotic navigation systems in real-world applications.",
    "pdf_url": "https://arxiv.org/pdf/2510.00466v1",
    "github_url": null,
    "published": "2025-10-01T03:37:02+00:00",
    "updated": "2025-10-01T03:37:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.00441v3",
    "title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation",
    "authors": [
      "Pan",
      "Xu",
      "Liu"
    ],
    "summary": "Visual navigation is a fundamental problem in embodied AI, yet practical deployments demand long-horizon planning capabilities to address multi-objective tasks. A major bottleneck is data scarcity: policies learned from limited data often overfit and fail to generalize OOD. Existing neural network-based agents typically increase architectural complexity that paradoxically become counterproductive in the small-sample regime. This paper introduce NeuRO, a integrated learning-to-optimize framework that tightly couples perception networks with downstream task-level robust optimization. Specifically, NeuRO addresses core difficulties in this integration: (i) it transforms noisy visual predictions under data scarcity into convex uncertainty sets using Partially Input Convex Neural Networks (PICNNs) with conformal calibration, which directly parameterize the optimization constraints; and (ii) it reformulates planning under partial observability as a robust optimization problem, enabling uncertainty-aware policies that transfer across environments. Extensive experiments on both unordered and sequential multi-object navigation tasks demonstrate that NeuRO establishes SoTA performance, particularly in generalization to unseen environments. Our work thus presents a significant advancement for developing robust, generalizable autonomous agents.",
    "pdf_url": "https://arxiv.org/pdf/2510.00441v3",
    "github_url": null,
    "published": "2025-10-01T02:48:28+00:00",
    "updated": "2025-10-21T17:55:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.25966v1",
    "title": "MUVLA: Learning to Explore Object Navigation via Map Understanding",
    "authors": [
      "Han",
      "Jia",
      "Zhang"
    ],
    "summary": "In this paper, we present MUVLA, a Map Understanding Vision-Language-Action model tailored for object navigation. It leverages semantic map abstractions to unify and structure historical information, encoding spatial context in a compact and consistent form. MUVLA takes the current and history observations, as well as the semantic map, as inputs and predicts the action sequence based on the description of goal object. Furthermore, it amplifies supervision through reward-guided return modeling based on dense short-horizon progress signals, enabling the model to develop a detailed understanding of action value for reward maximization. MUVLA employs a three-stage training pipeline: learning map-level spatial understanding, imitating behaviors from mixed-quality demonstrations, and reward amplification. This strategy allows MUVLA to unify diverse demonstrations into a robust spatial representation and generate more rational exploration strategies. Experiments on HM3D and Gibson benchmarks demonstrate that MUVLA achieves great generalization and learns effective exploration behaviors even from low-quality or partially successful trajectories.",
    "pdf_url": "https://arxiv.org/pdf/2509.25966v1",
    "github_url": null,
    "published": "2025-09-30T09:02:58+00:00",
    "updated": "2025-09-30T09:02:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.25687v2",
    "title": "OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation",
    "authors": [
      "Xue",
      "Hu",
      "Luo"
    ],
    "summary": "Embodied navigation presents a core challenge for intelligent robots, requiring the comprehension of visual environments, natural language instructions, and autonomous exploration. Existing models often fall short in offering a unified solution across diverse navigation paradigms, resulting in low success rates and limited generalization. We introduce OmniNav, a unified framework addressing instruct-goal, object-goal, point-goal navigation, and frontier-based exploration within a single architecture. Our approach features a lightweight, low-latency policy that accurately predicts continuous-space waypoints (coordinates and orientations). This policy surpasses action-chunk methods in precision and supports real-world deployment at control frequencies up to 5 Hz. Architecturally, OmniNav employs a fast-slow system design: a fast module generates waypoints using short-horizon visual context and subtasks, while a slow module performs deliberative planning with long-horizon observations and candidate frontiers to select subsequent subgoals and subtasks. This collaboration enhances path efficiency and maintains trajectory coherence, particularly in exploration and memory-intensive scenarios. Crucially, we identify that the primary bottleneck isn't merely navigation policy learning, but a robust understanding of general instructions and objects. To boost generalization, OmniNav integrates large-scale, general-purpose training datasets, including those for image captioning and visual recognition, into a joint multi-task regimen. This significantly improves success rates and robustness. Extensive experiments confirm OmniNav's state-of-the-art performance across various navigation benchmarks, with real-world deployment further validating its efficacy. OmniNav provides practical insights for embodied navigation, charting a scalable path towards versatile, highly generalizable robotic intelligence.",
    "pdf_url": "https://arxiv.org/pdf/2509.25687v2",
    "github_url": null,
    "published": "2025-09-30T02:44:28+00:00",
    "updated": "2025-10-09T08:47:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.25655v1",
    "title": "Landmark-Guided Knowledge for Vision-and-Language Navigation",
    "authors": [
      "Yang",
      "Zhu",
      "Yu"
    ],
    "summary": "Vision-and-language navigation is one of the core tasks in embodied intelligence, requiring an agent to autonomously navigate in an unfamiliar environment based on natural language instructions. However, existing methods often fail to match instructions with environmental information in complex scenarios, one reason being the lack of common-sense reasoning ability. This paper proposes a vision-and-language navigation method called Landmark-Guided Knowledge (LGK), which introduces an external knowledge base to assist navigation, addressing the misjudgment issues caused by insufficient common sense in traditional methods. Specifically, we first construct a knowledge base containing 630,000 language descriptions and use knowledge Matching to align environmental subviews with the knowledge base, extracting relevant descriptive knowledge. Next, we design a Knowledge-Guided by Landmark (KGL) mechanism, which guides the agent to focus on the most relevant parts of the knowledge by leveraging landmark information in the instructions, thereby reducing the data bias that may arise from incorporating external knowledge. Finally, we propose Knowledge-Guided Dynamic Augmentation (KGDA), which effectively integrates language, knowledge, vision, and historical information. Experimental results demonstrate that the LGK method outperforms existing state-of-the-art methods on the R2R and REVERIE vision-and-language navigation datasets, particularly in terms of navigation error, success rate, and path efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2509.25655v1",
    "github_url": null,
    "published": "2025-09-30T01:54:27+00:00",
    "updated": "2025-09-30T01:54:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.25139v1",
    "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs",
    "authors": [
      "Zhang",
      "Ma",
      "Wang"
    ],
    "summary": "Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agent's contextual understanding by incorporating textual descriptions from multiple perspectives that facilitate analogical reasoning across images. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance.",
    "pdf_url": "https://arxiv.org/pdf/2509.25139v1",
    "github_url": null,
    "published": "2025-09-29T17:51:01+00:00",
    "updated": "2025-09-29T17:51:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.24907v2",
    "title": "Real-time Recognition of Human Interactions from a Single RGB-D Camera for Socially-Aware Robot Navigation",
    "authors": [
      "Nguyen",
      "Nguyen",
      "Nu"
    ],
    "summary": "{Recognizing human interactions is essential for social robots as it enables them to navigate safely and naturally in shared environments. Conventional robotic systems however often focus on obstacle avoidance, neglecting social cues necessary for seamless human-robot interaction. To address this gap, we propose a framework to recognize human group interactions for socially aware navigation. Our method utilizes color and depth frames from a monocular RGB-D camera to estimate 3D human keypoints and positions. Principal component analysis (PCA) is then used to determine dominant interaction directions. The shoelace formula is finally applied to compute interest points and engagement areas. Extensive experiments have been conducted to evaluate the validity of the proposed method. The results show that our method is capable of recognizing group interactions across different scenarios with varying numbers of individuals. It also achieves high-speed performance, processing each frame in approximately 4 ms on a single-board computer used in robotic systems. The method is implemented as a ROS 2 package making it simple to integrate into existing navigation systems. Source code is available at https://github.com/thanhlong103/social-interaction-detector",
    "pdf_url": "https://arxiv.org/pdf/2509.24907v2",
    "github_url": "https://github.com/thanhlong103/social-interaction-detector",
    "published": "2025-09-29T15:14:56+00:00",
    "updated": "2025-10-17T14:01:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.24763v1",
    "title": "SSR-ZSON: Zero-Shot Object Navigation via Spatial-Semantic Relations within a Hierarchical Exploration Framework",
    "authors": [
      "Meng",
      "Li",
      "Mao"
    ],
    "summary": "Zero-shot object navigation in unknown environments presents significant challenges, mainly due to two key limitations: insufficient semantic guidance leads to inefficient exploration, while limited spatial memory resulting from environmental structure causes entrapment in local regions. To address these issues, we propose SSR-ZSON, a spatial-semantic relative zero-shot object navigation method based on the TARE hierarchical exploration framework, integrating a viewpoint generation strategy balancing spatial coverage and semantic density with an LLM-based global guidance mechanism. The performance improvement of the proposed method is due to two key innovations. First, the viewpoint generation strategy prioritizes areas of high semantic density within traversable sub-regions to maximize spatial coverage and minimize invalid exploration. Second, coupled with an LLM-based global guidance mechanism, it assesses semantic associations to direct navigation toward high-value spaces, preventing local entrapment and ensuring efficient exploration. Deployed on hybrid Habitat-Gazebo simulations and physical platforms, SSR-ZSON achieves real-time operation and superior performance. On Matterport3D and Habitat-Matterport3D datasets, it improves the Success Rate(SR) by 18.5\\% and 11.2\\%, and the Success weighted by Path Length(SPL) by 0.181 and 0.140, respectively, over state-of-the-art methods.",
    "pdf_url": "https://arxiv.org/pdf/2509.24763v1",
    "github_url": null,
    "published": "2025-09-29T13:28:24+00:00",
    "updated": "2025-09-29T13:28:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.24528v3",
    "title": "CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D",
    "authors": [
      "Mirzaei",
      "Amoie",
      "Ekhterachian"
    ],
    "summary": "3D scene understanding is fundamental for embodied AI and robotics, supporting reliable perception for interaction and navigation. Recent approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning embedding vectors to 2D class-agnostic masks generated via vision-language models (VLMs) and projecting these into 3D. However, these methods often produce fragmented masks and inaccurate semantic assignments due to the direct use of raw masks, limiting their effectiveness in complex environments. To address this, we leverage SemanticSAM with progressive granularity refinement to generate more accurate and numerous object-level masks, mitigating the over-segmentation commonly observed in mask generation models such as vanilla SAM, and improving downstream 3D semantic segmentation. To further enhance semantic context, we employ a context-aware CLIP encoding strategy that integrates multiple contextual views of each mask using empirically determined weighting, providing much richer visual context. We evaluate our approach on multiple 3D scene understanding tasks, including 3D semantic segmentation and object retrieval from language queries, across several benchmark datasets. Experimental results demonstrate significant improvements over existing methods, highlighting the effectiveness of our approach.",
    "pdf_url": "https://arxiv.org/pdf/2509.24528v3",
    "github_url": null,
    "published": "2025-09-29T09:43:00+00:00",
    "updated": "2025-12-07T23:06:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.24387v1",
    "title": "AdaNav: Adaptive Reasoning with Uncertainty for Vision-Language Navigation",
    "authors": [
      "Ding",
      "Wei",
      "Yang"
    ],
    "summary": "Vision Language Navigation (VLN) requires agents to follow natural language instructions by grounding them in sequential visual observations over long horizons. Explicit reasoning could enhance temporal consistency and perception action alignment, but reasoning at fixed steps often leads to suboptimal performance and unnecessary computation. To address this, we propose AdaNav, an uncertainty-based adaptive reasoning framework for VLN. At its core is the Uncertainty Adaptive Reasoning Block (UAR), a lightweight plugin that dynamically triggers reasoning. We introduce Action Entropy as a policy prior for UAR and progressively refine it through a Heuristics to RL training method, enabling agents to learn difficulty aware reasoning policies under the strict data limitations of embodied tasks. Results show that with only 6K training samples, AdaNav achieves substantial gains over closed source models trained on million scale data, improving success rate by 20% on R2R val-unseen, 11.7% on RxR-CE, and 11.4% in real world scenes. The code is available at https://github.com/xinding-sys/AdaNav.",
    "pdf_url": "https://arxiv.org/pdf/2509.24387v1",
    "github_url": "https://github.com/xinding-sys/AdaNav",
    "published": "2025-09-29T07:36:45+00:00",
    "updated": "2025-09-29T07:36:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.24321v1",
    "title": "SONAR: Semantic-Object Navigation with Aggregated Reasoning through a Cross-Modal Inference Paradigm",
    "authors": [
      "Wang",
      "Sun",
      "Chi"
    ],
    "summary": "Understanding human instructions and accomplishing Vision-Language Navigation tasks in unknown environments is essential for robots. However, existing modular approaches heavily rely on the quality of training data and often exhibit poor generalization. Vision-Language Model based methods, while demonstrating strong generalization capabilities, tend to perform unsatisfactorily when semantic cues are weak. To address these issues, this paper proposes SONAR, an aggregated reasoning approach through a cross modal paradigm. The proposed method integrates a semantic map based target prediction module with a Vision-Language Model based value map module, enabling more robust navigation in unknown environments with varying levels of semantic cues, and effectively balancing generalization ability with scene adaptability. In terms of target localization, we propose a strategy that integrates multi-scale semantic maps with confidence maps, aiming to mitigate false detections of target objects. We conducted an evaluation of the SONAR within the Gazebo simulator, leveraging the most challenging Matterport 3D (MP3D) dataset as the experimental benchmark. Experimental results demonstrate that SONAR achieves a success rate of 38.4% and an SPL of 17.7%.",
    "pdf_url": "https://arxiv.org/pdf/2509.24321v1",
    "github_url": null,
    "published": "2025-09-29T06:09:28+00:00",
    "updated": "2025-09-29T06:09:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.23563v1",
    "title": "RAVEN: Resilient Aerial Navigation via Open-Set Semantic Memory and Behavior Adaptation",
    "authors": [
      "Kim",
      "Alama",
      "Kurdydyk"
    ],
    "summary": "Aerial outdoor semantic navigation requires robots to explore large, unstructured environments to locate target objects. Recent advances in semantic navigation have demonstrated open-set object-goal navigation in indoor settings, but these methods remain limited by constrained spatial ranges and structured layouts, making them unsuitable for long-range outdoor search. While outdoor semantic navigation approaches exist, they either rely on reactive policies based on current observations, which tend to produce short-sighted behaviors, or precompute scene graphs offline for navigation, limiting adaptability to online deployment. We present RAVEN, a 3D memory-based, behavior tree framework for aerial semantic navigation in unstructured outdoor environments. It (1) uses a spatially consistent semantic voxel-ray map as persistent memory, enabling long-horizon planning and avoiding purely reactive behaviors, (2) combines short-range voxel search and long-range ray search to scale to large environments, (3) leverages a large vision-language model to suggest auxiliary cues, mitigating sparsity of outdoor targets. These components are coordinated by a behavior tree, which adaptively switches behaviors for robust operation. We evaluate RAVEN in 10 photorealistic outdoor simulation environments over 100 semantic tasks, encompassing single-object search, multi-class, multi-instance navigation and sequential task changes. Results show RAVEN outperforms baselines by 85.25% in simulation and demonstrate its real-world applicability through deployment on an aerial robot in outdoor field tests.",
    "pdf_url": "https://arxiv.org/pdf/2509.23563v1",
    "github_url": null,
    "published": "2025-09-28T01:43:25+00:00",
    "updated": "2025-09-28T01:43:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.22653v1",
    "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation",
    "authors": [
      "Hu",
      "Lin",
      "Lee"
    ],
    "summary": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harnesses VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, outperforming the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs. Project page: https://spf-web.pages.dev",
    "pdf_url": "https://arxiv.org/pdf/2509.22653v1",
    "github_url": null,
    "published": "2025-09-26T17:59:59+00:00",
    "updated": "2025-09-26T17:59:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.22548v1",
    "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation",
    "authors": [
      "Zeng",
      "Qi",
      "Chang"
    ],
    "summary": "Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2509.22548v1",
    "github_url": null,
    "published": "2025-09-26T16:29:37+00:00",
    "updated": "2025-09-26T16:29:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.21930v1",
    "title": "DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation",
    "authors": [
      "Wang",
      "Chen"
    ],
    "summary": "Visual navigation is essential for robotics and embodied AI. However, existing foundation models, particularly those with transformer decoders, suffer from high computational overhead and lack interpretability, limiting their deployment in resource-tight scenarios. To address this, we propose DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer selection based on scene complexity. It employs a trainable hard feature selector for sparse operations, enhancing efficiency and interpretability. Additionally, we integrate feature selection into an early-exit mechanism, with Bayesian Optimization determining optimal exit thresholds to reduce computational cost. Extensive experiments in real-world-based datasets and simulated environments demonstrate the effectiveness of DynaNav. Compared to ViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time, and 32.8% lower memory usage, while improving navigation performance across four public datasets.",
    "pdf_url": "https://arxiv.org/pdf/2509.21930v1",
    "github_url": null,
    "published": "2025-09-26T06:15:31+00:00",
    "updated": "2025-09-26T06:15:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.21657v2",
    "title": "FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction",
    "authors": [
      "Dai",
      "Jiang",
      "Wang"
    ],
    "summary": "High-quality 3D world models are pivotal for embodied intelligence and Artificial General Intelligence (AGI), underpinning applications such as AR/VR content creation and robotic navigation. Despite the established strong imaginative priors, current video foundation models lack explicit 3D grounding capabilities, thus being limited in both spatial consistency and their utility for downstream 3D reasoning tasks. In this work, we present FantasyWorld, a geometry-enhanced framework that augments frozen video foundation models with a trainable geometric branch, enabling joint modeling of video latents and an implicit 3D field in a single forward pass. Our approach introduces cross-branch supervision, where geometry cues guide video generation and video priors regularize 3D prediction, thus yielding consistent and generalizable 3D-aware video representations. Notably, the resulting latents from the geometric branch can potentially serve as versatile representations for downstream 3D tasks such as novel view synthesis and navigation, without requiring per-scene optimization or fine-tuning. Extensive experiments show that FantasyWorld effectively bridges video imagination and 3D perception, outperforming recent geometry-consistent baselines in multi-view coherence and style consistency. Ablation studies further confirm that these gains stem from the unified backbone and cross-branch information exchange.",
    "pdf_url": "https://arxiv.org/pdf/2509.21657v2",
    "github_url": null,
    "published": "2025-09-25T22:24:23+00:00",
    "updated": "2025-10-31T08:16:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.20499v1",
    "title": "Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting",
    "authors": [
      "Li",
      "Li",
      "Wang"
    ],
    "summary": "With the rapid progress of foundation models and robotics, vision-language navigation (VLN) has emerged as a key task for embodied agents with broad practical applications. We address VLN in continuous environments, a particularly challenging setting where an agent must jointly interpret natural language instructions, perceive its surroundings, and plan low-level actions. We propose a zero-shot framework that integrates a simplified yet effective waypoint predictor with a multimodal large language model (MLLM). The predictor operates on an abstract obstacle map, producing linearly reachable waypoints, which are incorporated into a dynamically updated topological graph with explicit visitation records. The graph and visitation information are encoded into the prompt, enabling reasoning over both spatial structure and exploration history to encourage exploration and equip MLLM with local path planning for error correction. Extensive experiments on R2R-CE and RxR-CE show that our method achieves state-of-the-art zero-shot performance, with success rates of 41% and 36%, respectively, outperforming prior state-of-the-art methods.",
    "pdf_url": "https://arxiv.org/pdf/2509.20499v1",
    "github_url": null,
    "published": "2025-09-24T19:21:39+00:00",
    "updated": "2025-09-24T19:21:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.19843v1",
    "title": "PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents",
    "authors": [
      "Ziliotto",
      "Akkara",
      "Daniele"
    ],
    "summary": "Recent advances in Embodied AI have enabled agents to perform increasingly complex tasks and adapt to diverse environments. However, deploying such agents in realistic human-centered scenarios, such as domestic households, remains challenging, particularly due to the difficulty of modeling individual human preferences and behaviors. In this work, we introduce PersONAL (PERSonalized Object Navigation And Localization, a comprehensive benchmark designed to study personalization in Embodied AI. Agents must identify, retrieve, and navigate to objects associated with specific users, responding to natural-language queries such as \"find Lily's backpack\". PersONAL comprises over 2,000 high-quality episodes across 30+ photorealistic homes from the HM3D dataset. Each episode includes a natural-language scene description with explicit associations between objects and their owners, requiring agents to reason over user-specific semantics. The benchmark supports two evaluation modes: (1) active navigation in unseen environments, and (2) object grounding in previously mapped scenes. Experiments with state-of-the-art baselines reveal a substantial gap to human performance, highlighting the need for embodied agents capable of perceiving, reasoning, and memorizing over personalized information; paving the way towards real-world assistive robot.",
    "pdf_url": "https://arxiv.org/pdf/2509.19843v1",
    "github_url": null,
    "published": "2025-09-24T07:39:16+00:00",
    "updated": "2025-09-24T07:39:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.19480v1",
    "title": "OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation",
    "authors": [
      "Hirose",
      "Glossop",
      "Shah"
    ],
    "summary": "Humans can flexibly interpret and compose different goal specifications, such as language instructions, spatial coordinates, or visual references, when navigating to a destination. In contrast, most existing robotic navigation policies are trained on a single modality, limiting their adaptability to real-world scenarios where different forms of goal specification are natural and complementary. In this work, we present a training framework for robotic foundation models that enables omni-modal goal conditioning for vision-based navigation. Our approach leverages a high-capacity vision-language-action (VLA) backbone and trains with three primary goal modalities: 2D poses, egocentric images, and natural language, as well as their combinations, through a randomized modality fusion strategy. This design not only expands the pool of usable datasets but also encourages the policy to develop richer geometric, semantic, and visual representations. The resulting model, OmniVLA, achieves strong generalization to unseen environments, robustness to scarce modalities, and the ability to follow novel natural language instructions. We demonstrate that OmniVLA outperforms specialist baselines across modalities and offers a flexible foundation for fine-tuning to new modalities and tasks. We believe OmniVLA provides a step toward broadly generalizable and flexible navigation policies, and a scalable path for building omni-modal robotic foundation models. We present videos showcasing OmniVLA performance and will release its checkpoints and training code on our project page.",
    "pdf_url": "https://arxiv.org/pdf/2509.19480v1",
    "github_url": null,
    "published": "2025-09-23T18:40:29+00:00",
    "updated": "2025-09-23T18:40:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.20401v2",
    "title": "SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment",
    "authors": [
      "Singh",
      "Sarkar",
      "Armeni"
    ],
    "summary": "Aligning 3D scene graphs is a crucial initial step for several applications in robot navigation and embodied perception. Current methods in 3D scene graph alignment often rely on single-modality point cloud data and struggle with incomplete or noisy input. We introduce SGAligner++, a cross-modal, language-aided framework for 3D scene graph alignment. Our method addresses the challenge of aligning partially overlapping scene observations across heterogeneous modalities by learning a unified joint embedding space, enabling accurate alignment even under low-overlap conditions and sensor noise. By employing lightweight unimodal encoders and attention-based fusion, SGAligner++ enhances scene understanding for tasks such as visual localization, 3D reconstruction, and navigation, while ensuring scalability and minimal computational overhead. Extensive evaluations on real-world datasets demonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40% on noisy real-world reconstructions, while enabling cross-modal generalization.",
    "pdf_url": "https://arxiv.org/pdf/2509.20401v2",
    "github_url": null,
    "published": "2025-09-23T18:31:29+00:00",
    "updated": "2025-10-16T15:30:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.19105v2",
    "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation",
    "authors": [
      "Prajapati",
      "Trivedi",
      "Hanson"
    ],
    "summary": "Successful navigation in outdoor environments requires accurate prediction of the physical interactions between the robot and the terrain. Many prior methods rely on geometric or semantic labels to classify traversable surfaces. However, such labels cannot distinguish visually similar surfaces that differ in material properties. Spectral sensors enable inference of material composition from surface reflectance measured across multiple wavelength bands. Although spectral sensing is gaining traction in robotics, widespread deployment remains constrained by the need for custom hardware integration, high sensor costs, and compute-intensive processing pipelines. In this paper, we present the RGB Image to Spectral Signature Neural Network (RS-Net), a deep neural network designed to bridge the gap between the accessibility of RGB sensing and the rich material information provided by spectral data. RS-Net predicts spectral signatures from RGB patches, which we map to terrain labels and friction coefficients. The resulting terrain classifications are integrated into a sampling-based motion planner for a wheeled robot operating in outdoor environments. Likewise, the friction estimates are incorporated into a contact-force-based MPC for a quadruped robot navigating slippery surfaces. Overall, our framework learns the task-relevant physical properties offline during training and thereafter relies solely on RGB sensing at run time.",
    "pdf_url": "https://arxiv.org/pdf/2509.19105v2",
    "github_url": null,
    "published": "2025-09-23T14:49:48+00:00",
    "updated": "2025-11-28T00:09:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.19002v2",
    "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction",
    "authors": [
      "Wang",
      "Murata",
      "Zhang"
    ],
    "summary": "Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.",
    "pdf_url": "https://arxiv.org/pdf/2509.19002v2",
    "github_url": null,
    "published": "2025-09-23T13:46:31+00:00",
    "updated": "2025-11-15T10:09:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.21377v1",
    "title": "Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation",
    "authors": [
      "Yu",
      "Zhang",
      "Zhu"
    ],
    "summary": "Audiovisual embodied navigation enables robots to locate audio sources by dynamically integrating visual observations from onboard sensors with the auditory signals emitted by the target. The core challenge lies in effectively leveraging multimodal cues to guide navigation. While prior works have explored basic fusion of visual and audio data, they often overlook deeper perceptual context. To address this, we propose the Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target architecture coupled with a refined Transformer mechanism to filter and selectively fuse cross-modal information. Extensive experiments on the Replica and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art performance, outperforming existing methods in success rate (SR), path efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits strong scalability and generalizability, paving the way for advanced multimodal fusion strategies in robotic navigation. The code and videos are available at   https://github.com/zzzmmm-svg/DMTF.",
    "pdf_url": "https://arxiv.org/pdf/2509.21377v1",
    "github_url": "https://github.com/zzzmmm-svg/DMTF",
    "published": "2025-09-23T09:31:00+00:00",
    "updated": "2025-09-23T09:31:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.18592v1",
    "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
    "authors": [
      "Bhatt",
      "Yang",
      "Siva"
    ],
    "summary": "Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task-location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments. Codebase, datasets, and videos for VLN-Zero are available at: https://vln-zero.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2509.18592v1",
    "github_url": null,
    "published": "2025-09-23T03:23:03+00:00",
    "updated": "2025-09-23T03:23:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.18384v1",
    "title": "AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback",
    "authors": [
      "Yang",
      "Hong",
      "Perin"
    ],
    "summary": "Large language models (LLMs) can translate natural language instructions into executable action plans for robotics, autonomous driving, and other domains. Yet, deploying LLM-driven planning in the physical world demands strict adherence to safety and regulatory constraints, which current models often violate due to hallucination or weak alignment. Traditional data-driven alignment methods, such as Direct Preference Optimization (DPO), require costly human labeling, while recent formal-feedback approaches still depend on resource-intensive fine-tuning. In this paper, we propose LAD-VF, a fine-tuning-free framework that leverages formal verification feedback for automated prompt engineering. By introducing a formal-verification-informed text loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts rather than model parameters. This yields three key benefits: (i) scalable adaptation without fine-tuning; (ii) compatibility with modular LLM architectures; and (iii) interpretable refinement via auditable prompts. Experiments in robot navigation and manipulation tasks demonstrate that LAD-VF substantially enhances specification compliance, improving success rates from 60% to over 90%. Our method thus presents a scalable and interpretable pathway toward trustworthy, formally-verified LLM-driven control systems.",
    "pdf_url": "https://arxiv.org/pdf/2509.18384v1",
    "github_url": null,
    "published": "2025-09-22T20:14:32+00:00",
    "updated": "2025-09-22T20:14:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.17500v1",
    "title": "SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge",
    "authors": [
      "Xie",
      "Zhang",
      "Liu"
    ],
    "summary": "Large-scale Video Object Segmentation (LSVOS) addresses the challenge of accurately tracking and segmenting objects in long video sequences, where difficulties stem from object reappearance, small-scale targets, heavy occlusions, and crowded scenes. Existing approaches predominantly adopt SAM2-based frameworks with various memory mechanisms for complex video mask generation. In this report, we proposed Segment Anything with Memory Strengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE track of ICCV 2025, which integrates the strengths of stateof-the-art VOS models into an effective paradigm. To handle visually similar instances and long-term object disappearance in MOSE, we incorporate a long-term memorymodule for reliable object re-identification. Additionly, we adopt SAM2Long as a post-processing strategy to reduce error accumulation and enhance segmentation stability in long video sequences. Our method achieved a final performance of 0.8427 in terms of J &F in the test-set leaderboard.",
    "pdf_url": "https://arxiv.org/pdf/2509.17500v1",
    "github_url": null,
    "published": "2025-09-22T08:30:34+00:00",
    "updated": "2025-09-22T08:30:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.17430v2",
    "title": "EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device",
    "authors": [
      "Chhablani",
      "Ye",
      "Irshad"
    ],
    "summary": "The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20% and 40% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87-0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: https://gchhablani.github.io/embodied-splat.",
    "pdf_url": "https://arxiv.org/pdf/2509.17430v2",
    "github_url": null,
    "published": "2025-09-22T07:22:31+00:00",
    "updated": "2025-09-23T03:58:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.17204v2",
    "title": "Ratatouille: Imitation Learning Ingredients for Real-world Social Robot Navigation",
    "authors": [
      "Han",
      "Vanniasinghe",
      "Sahak"
    ],
    "summary": "Scaling Reinforcement Learning to in-the-wild social robot navigation is both data-intensive and unsafe, since policies must learn through direct interaction and inevitably encounter collisions. Offline Imitation learning (IL) avoids these risks by collecting expert demonstrations safely, training entirely offline, and deploying policies zero-shot. However, we find that naively applying Behaviour Cloning (BC) to social navigation is insufficient; achieving strong performance requires careful architectural and training choices. We present Ratatouille, a pipeline and model architecture that, without changing the data, reduces collisions per meter by 6 times and improves success rate by 3 times compared to naive BC. We validate our approach in both simulation and the real world, where we collected over 11 hours of data on a dense university campus. We further demonstrate qualitative results in a public food court. Our findings highlight that thoughtful IL design, rather than additional data, can substantially improve safety and reliability in real-world social navigation. Video: https://youtu.be/tOdLTXsaYLQ. Code will be released after acceptance.",
    "pdf_url": "https://arxiv.org/pdf/2509.17204v2",
    "github_url": null,
    "published": "2025-09-21T19:17:39+00:00",
    "updated": "2025-09-23T14:11:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.18200v1",
    "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought",
    "authors": [
      "Huang"
    ],
    "summary": "Conversational agents must translate egocentric utterances (e.g., \"on my right\") into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation.",
    "pdf_url": "https://arxiv.org/pdf/2509.18200v1",
    "github_url": null,
    "published": "2025-09-20T05:25:32+00:00",
    "updated": "2025-09-20T05:25:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.16445v1",
    "title": "FiLM-Nav: Efficient and Generalizable Navigation via VLM Fine-tuning",
    "authors": [
      "Yokoyama",
      "Ha"
    ],
    "summary": "Enabling robotic assistants to navigate complex environments and locate objects described in free-form language is a critical capability for real-world deployment. While foundation models, particularly Vision-Language Models (VLMs), offer powerful semantic understanding, effectively adapting their web-scale knowledge for embodied decision-making remains a key challenge. We present FiLM-Nav (Fine-tuned Language Model for Navigation), an approach that directly fine-tunes pre-trained VLM as the navigation policy. In contrast to methods that use foundation models primarily in a zero-shot manner or for map annotation, FiLM-Nav learns to select the next best exploration frontier by conditioning directly on raw visual trajectory history and the navigation goal. Leveraging targeted simulated embodied experience allows the VLM to ground its powerful pre-trained representations in the specific dynamics and visual patterns relevant to goal-driven navigation. Critically, fine-tuning on a diverse data mixture combining ObjectNav, OVON, ImageNav, and an auxiliary spatial reasoning task proves essential for achieving robustness and broad generalization. FiLM-Nav sets a new state-of-the-art in both SPL and success rate on HM3D ObjectNav among open-vocabulary methods, and sets a state-of-the-art SPL on the challenging HM3D-OVON benchmark, demonstrating strong generalization to unseen object categories. Our work validates that directly fine-tuning VLMs on diverse simulated embodied data is a highly effective pathway towards generalizable and efficient semantic navigation capabilities.",
    "pdf_url": "https://arxiv.org/pdf/2509.16445v1",
    "github_url": null,
    "published": "2025-09-19T21:51:42+00:00",
    "updated": "2025-09-19T21:51:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.15180v1",
    "title": "Parallel Simulation of Contact and Actuation for Soft Growing Robots",
    "authors": [
      "Gao",
      "Chen",
      "Bhovad"
    ],
    "summary": "Soft growing robots, commonly referred to as vine robots, have demonstrated remarkable ability to interact safely and robustly with unstructured and dynamic environments. It is therefore natural to exploit contact with the environment for planning and design optimization tasks. Previous research has focused on planning under contact for passively deforming robots with pre-formed bends. However, adding active steering to these soft growing robots is necessary for successful navigation in more complex environments. To this end, we develop a unified modeling framework that integrates vine robot growth, bending, actuation, and obstacle contact. We extend the beam moment model to include the effects of actuation on kinematics under growth and then use these models to develop a fast parallel simulation framework. We validate our model and simulator with real robot experiments. To showcase the capabilities of our framework, we apply our model in a design optimization task to find designs for vine robots navigating through cluttered environments, identifying designs that minimize the number of required actuators by exploiting environmental contacts. We show the robustness of the designs to environmental and manufacturing uncertainties. Finally, we fabricate an optimized design and successfully deploy it in an obstacle-rich environment.",
    "pdf_url": "https://arxiv.org/pdf/2509.15180v1",
    "github_url": null,
    "published": "2025-09-18T17:38:17+00:00",
    "updated": "2025-09-18T17:38:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.15273v2",
    "title": "Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI",
    "authors": [
      "Ni",
      "Zhang",
      "Li"
    ],
    "summary": "Embodied AI development significantly lags behind large foundation models due to three critical challenges: (1) lack of systematic understanding of core capabilities needed for Embodied AI, making research lack clear objectives; (2) absence of unified and standardized evaluation systems, rendering cross-benchmark evaluation infeasible; and (3) underdeveloped automated and scalable acquisition methods for embodied data, creating critical bottlenecks for model scaling. To address these obstacles, we present Embodied Arena, a comprehensive, unified, and evolving evaluation platform for Embodied AI. Our platform establishes a systematic embodied capability taxonomy spanning three levels (perception, reasoning, task execution), seven core capabilities, and 25 fine-grained dimensions, enabling unified evaluation with systematic research objectives. We introduce a standardized evaluation system built upon unified infrastructure supporting flexible integration of 22 diverse benchmarks across three domains (2D/3D Embodied Q&A, Navigation, Task Planning) and 30+ advanced models from 20+ worldwide institutes. Additionally, we develop a novel LLM-driven automated generation pipeline ensuring scalable embodied evaluation data with continuous evolution for diversity and comprehensiveness. Embodied Arena publishes three real-time leaderboards (Embodied Q&A, Navigation, Task Planning) with dual perspectives (benchmark view and capability view), providing comprehensive overviews of advanced model capabilities. Especially, we present nine findings summarized from the evaluation results on the leaderboards of Embodied Arena. This helps to establish clear research veins and pinpoint critical research problems, thereby driving forward progress in the field of Embodied AI.",
    "pdf_url": "https://arxiv.org/pdf/2509.15273v2",
    "github_url": null,
    "published": "2025-09-18T11:53:37+00:00",
    "updated": "2025-09-23T16:30:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.15250v2",
    "title": "Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning",
    "authors": [
      "Qin",
      "Burns",
      "Plummer"
    ],
    "summary": "Large models achieve strong performance on Vision-and-Language Navigation (VLN) tasks, but are costly to run in resource-limited environments. Token pruning offers appealing tradeoffs for efficiency with minimal performance loss by reducing model input size, but prior work overlooks VLN-specific challenges. For example, information loss from pruning can effectively increase computational cost due to longer walks. Thus, the inability to identify uninformative tokens undermines the supposed efficiency gains from pruning. To address this, we propose Navigation-Aware Pruning (NAP), which uses navigation-specific traits to simplify the pruning process by pre-filtering tokens into foreground and background. For example, image views are filtered based on whether the agent can navigate in that direction. We also extract navigation-relevant instructions using a Large Language Model. After filtering, we focus pruning on background tokens, minimizing information loss. To further help avoid increases in navigation length, we discourage backtracking by removing low-importance navigation nodes. Experiments on standard VLN benchmarks show NAP significantly outperforms prior work, preserving higher success rates while saving more than 50% FLOPS.",
    "pdf_url": "https://arxiv.org/pdf/2509.15250v2",
    "github_url": null,
    "published": "2025-09-18T01:05:37+00:00",
    "updated": "2025-09-22T01:18:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.13733v3",
    "title": "FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph",
    "authors": [
      "Zhou",
      "Xiao",
      "Liu"
    ],
    "summary": "Visual-Language Navigation (VLN) is a fundamental challenge in robotic systems, with broad applications for the deployment of embodied agents in real-world environments. Despite recent advances, existing approaches are limited in long-range spatial reasoning, often exhibiting low success rates and high inference latency, particularly in long-range navigation tasks. To address these limitations, we propose FSR-VLN, a vision-language navigation system that combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation supporting progressive retrieval, from coarse room-level localization to fine-grained goal view and object identification. Building on HMSG, FSR first performs fast matching to efficiently select candidate rooms, views, and objects, then applies VLM-driven refinement for final goal selection. We evaluated FSR-VLN across four comprehensive indoor datasets collected by humanoid robots, utilizing 87 instructions that encompass a diverse range of object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all datasets, measured by the retrieval success rate (RSR), while reducing the response time by 82% compared to VLM-based methods on tour videos by activating slow reasoning only when fast intuition fails. Furthermore, we integrate FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1 humanoid robot, enabling natural language interaction and real-time navigation.",
    "pdf_url": "https://arxiv.org/pdf/2509.13733v3",
    "github_url": null,
    "published": "2025-09-17T06:36:41+00:00",
    "updated": "2025-11-25T09:17:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.13720v1",
    "title": "EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility",
    "authors": [
      "Zeng",
      "Peng",
      "Ye"
    ],
    "summary": "Zero-shot object navigation (ZSON) in large-scale outdoor environments faces many challenges; we specifically address a coupled one: long-range targets that reduce to tiny projections and intermittent visibility due to partial or complete occlusion. We present a unified, lightweight closed-loop system built on an aligned multi-scale image tile hierarchy. Through hierarchical target-saliency fusion, it summarizes localized semantic contrast into a stable coarse-layer regional saliency that provides the target direction and indicates target visibility. This regional saliency supports visibility-aware heading maintenance through keyframe memory, saliency-weighted fusion of historical headings, and active search during temporary invisibility. The system avoids whole-image rescaling, enables deterministic bottom-up aggregation, supports zero-shot navigation, and runs efficiently on a mobile robot. Across simulation and real-world outdoor trials, the system detects semantic targets beyond 150m, maintains a correct heading through visibility changes with 82.6% probability, and improves overall task success by 17.5% compared with the SOTA methods, demonstrating robust ZSON toward distant and intermittently observable targets.",
    "pdf_url": "https://arxiv.org/pdf/2509.13720v1",
    "github_url": null,
    "published": "2025-09-17T06:15:06+00:00",
    "updated": "2025-09-17T06:15:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.12912v1",
    "title": "Spotting the Unfriendly Robot - Towards better Metrics for Interactions",
    "authors": [
      "Wenzel",
      "Probst"
    ],
    "summary": "Establishing standardized metrics for Social Robot Navigation (SRN) algorithms for assessing the quality and social compliance of robot behavior around humans is essential for SRN research. Currently, commonly used evaluation metrics lack the ability to quantify how cooperative an agent behaves in interaction with humans. Concretely, in a simple frontal approach scenario, no metric specifically captures if both agents cooperate or if one agent stays on collision course and the other agent is forced to evade. To address this limitation, we propose two new metrics, a conflict intensity metric and the responsibility metric. Together, these metrics are capable of evaluating the quality of human-robot interactions by showing how much a given algorithm has contributed to reducing a conflict and which agent actually took responsibility of the resolution. This work aims to contribute to the development of a comprehensive and standardized evaluation methodology for SRN, ultimately enhancing the safety, efficiency, and social acceptance of robots in human-centric environments.",
    "pdf_url": "https://arxiv.org/pdf/2509.12912v1",
    "github_url": null,
    "published": "2025-09-16T10:05:52+00:00",
    "updated": "2025-09-16T10:05:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.12890v1",
    "title": "Responsibility and Engagement - Evaluating Interactions in Social Robot Navigation",
    "authors": [
      "Probst",
      "Wenzel",
      "Dasi"
    ],
    "summary": "In Social Robot Navigation (SRN), the availability of meaningful metrics is crucial for evaluating trajectories from human-robot interactions. In the SRN context, such interactions often relate to resolving conflicts between two or more agents. Correspondingly, the shares to which agents contribute to the resolution of such conflicts are important. This paper builds on recent work, which proposed a Responsibility metric capturing such shares. We extend this framework in two directions: First, we model the conflict buildup phase by introducing a time normalization. Second, we propose the related Engagement metric, which captures how the agents' actions intensify a conflict. In a comprehensive series of simulated scenarios with dyadic, group and crowd interactions, we show that the metrics carry meaningful information about the cooperative resolution of conflicts in interactions. They can be used to assess behavior quality and foresightedness. We extensively discuss applicability, design choices and limitations of the proposed metrics.",
    "pdf_url": "https://arxiv.org/pdf/2509.12890v1",
    "github_url": null,
    "published": "2025-09-16T09:44:08+00:00",
    "updated": "2025-09-16T09:44:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.12618v1",
    "title": "ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation",
    "authors": [
      "Zhang",
      "Zhu",
      "Pan"
    ],
    "summary": "The Vision-and-Language Navigation (VLN) task requires an agent to follow natural language instructions and navigate through complex environments. Existing MLLM-based VLN methods primarily rely on imitation learning (IL) and often use DAgger for post-training to mitigate covariate shift. While effective, these approaches incur substantial data collection and training costs. Reinforcement learning (RL) offers a promising alternative. However, prior VLN RL methods lack dynamic interaction with the environment and depend on expert trajectories for reward shaping, rather than engaging in open-ended active exploration. This restricts the agent's ability to discover diverse and plausible navigation routes. To address these limitations, we propose ActiveVLN, a VLN framework that explicitly enables active exploration through multi-turn RL. In the first stage, a small fraction of expert trajectories is used for IL to bootstrap the agent. In the second stage, the agent iteratively predicts and executes actions, automatically collects diverse trajectories, and optimizes multiple rollouts via the GRPO objective. To further improve RL efficiency, we introduce a dynamic early-stopping strategy to prune long-tail or likely failed trajectories, along with additional engineering optimizations. Experiments show that ActiveVLN achieves the largest performance gains over IL baselines compared to both DAgger-based and prior RL-based post-training methods, while reaching competitive performance with state-of-the-art approaches despite using a smaller model. Code and data will be released soon.",
    "pdf_url": "https://arxiv.org/pdf/2509.12618v1",
    "github_url": null,
    "published": "2025-09-16T03:31:46+00:00",
    "updated": "2025-09-16T03:31:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.12129v2",
    "title": "Embodied Navigation Foundation Model",
    "authors": [
      "Zhang",
      "Li",
      "Qi"
    ],
    "summary": "Navigation is a fundamental capability in embodied AI, representing the intelligence required to perceive and interact within physical environments following language instructions. Despite significant progress in large Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance on general vision-language tasks, their generalization ability in embodied navigation remains largely confined to narrow task settings and embodiment-specific architectures. In this work, we introduce a cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained on eight million navigation samples that encompass quadrupeds, drones, wheeled robots, and vehicles, and spanning diverse tasks such as vision-and-language navigation, object searching, target tracking, and autonomous driving. NavFoM employs a unified architecture that processes multimodal navigation inputs from varying camera configurations and navigation horizons. To accommodate diverse camera setups and temporal horizons, NavFoM incorporates identifier tokens that embed camera view information of embodiments and the temporal context of tasks. Furthermore, to meet the demands of real-world deployment, NavFoM controls all observation tokens using a dynamically adjusted sampling strategy under a limited token length budget. Extensive evaluations on public benchmarks demonstrate that our model achieves state-of-the-art or highly competitive performance across multiple navigation tasks and embodiments without requiring task-specific fine-tuning. Additional real-world experiments further confirm the strong generalization capability and practical applicability of our approach.",
    "pdf_url": "https://arxiv.org/pdf/2509.12129v2",
    "github_url": null,
    "published": "2025-09-15T16:52:43+00:00",
    "updated": "2025-09-16T18:15:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.11197v1",
    "title": "DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "Fang",
      "Wang"
    ],
    "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE), which links language instructions to perception and control in the real world, is a core capability of embodied robots. Recently, large-scale pretrained foundation models have been leveraged as shared priors for perception, reasoning, and action, enabling zero-shot VLN without task-specific training. However, existing zero-shot VLN methods depend on costly perception and passive scene understanding, collapsing control to point-level choices. As a result, they are expensive to deploy, misaligned in action semantics, and short-sighted in planning. To address these issues, we present DreamNav that focuses on the following three aspects: (1) for reducing sensory cost, our EgoView Corrector aligns viewpoints and stabilizes egocentric perception; (2) instead of point-level actions, our Trajectory Predictor favors global trajectory-level planning to better align with instruction semantics; and (3) to enable anticipatory and long-horizon planning, we propose an Imagination Predictor to endow the agent with proactive thinking capability. On VLN-CE and real-world tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the strongest egocentric baseline with extra information by up to 7.49\\% and 18.15\\% in terms of SR and SPL metrics. To our knowledge, this is the first zero-shot VLN method to unify trajectory-level planning and active imagination while using only egocentric inputs.",
    "pdf_url": "https://arxiv.org/pdf/2509.11197v1",
    "github_url": null,
    "published": "2025-09-14T09:54:20+00:00",
    "updated": "2025-09-14T09:54:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.10884v1",
    "title": "Nav-R1: Reasoning and Navigation in Embodied Scenes",
    "authors": [
      "Liu",
      "Huang",
      "Zhang"
    ],
    "summary": "Embodied navigation requires agents to integrate perception, reasoning, and action for robust interaction in complex 3D environments. Existing approaches often suffer from incoherent and unstable reasoning traces that hinder generalization across diverse environments, and difficulty balancing long-horizon semantic reasoning with low-latency control for real-time navigation. To address these challenges, we propose Nav-R1, an embodied foundation model that unifies reasoning in embodied environments. We first construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought (CoT) for embodied tasks, which enables cold-start initialization with structured reasoning. Building on this foundation, we design a GRPO-based reinforcement learning framework with three complementary rewards: format, understanding, and navigation, to improve structural adherence, semantic grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow reasoning paradigm, decoupling deliberate semantic reasoning from low-latency reactive control for efficient yet coherent navigation. Extensive evaluations on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms strong baselines, with over 8% average improvement in reasoning and navigation performance. Real-world deployment on a mobile robot further validates its robustness under limited onboard resources. Code: https://github.com/AIGeeksGroup/Nav-R1. Website: https://aigeeksgroup.github.io/Nav-R1.",
    "pdf_url": "https://arxiv.org/pdf/2509.10884v1",
    "github_url": "https://github.com/AIGeeksGroup/Nav-R1",
    "published": "2025-09-13T16:31:03+00:00",
    "updated": "2025-09-13T16:31:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.10813v2",
    "title": "InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts",
    "authors": [
      "Zhong",
      "Cao",
      "Jin"
    ],
    "summary": "The advancement of Embodied AI heavily relies on large-scale, simulatable 3D scene datasets characterized by scene diversity and realistic layouts. However, existing datasets typically suffer from limitations in data scale or diversity, sanitized layouts lacking small items, and severe object collisions. To address these shortcomings, we introduce \\textbf{InternScenes}, a novel large-scale simulatable indoor scene dataset comprising approximately 40,000 diverse scenes by integrating three disparate scene sources, real-world scans, procedurally generated scenes, and designer-created scenes, including 1.96M 3D objects and covering 15 common scene types and 288 object classes. We particularly preserve massive small items in the scenes, resulting in realistic and complex layouts with an average of 41.5 objects per region. Our comprehensive data processing pipeline ensures simulatability by creating real-to-sim replicas for real-world scans, enhances interactivity by incorporating interactive objects into these scenes, and resolves object collisions by physical simulations. We demonstrate the value of InternScenes with two benchmark applications: scene layout generation and point-goal navigation. Both show the new challenges posed by the complex and realistic layouts. More importantly, InternScenes paves the way for scaling up the model training for both tasks, making the generation and navigation in such complex scenes possible. We commit to open-sourcing the data, models, and benchmarks to benefit the whole community.",
    "pdf_url": "https://arxiv.org/pdf/2509.10813v2",
    "github_url": null,
    "published": "2025-09-13T14:25:17+00:00",
    "updated": "2025-10-14T05:26:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.10454v1",
    "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation",
    "authors": [
      "Yin",
      "Wei",
      "Xu"
    ],
    "summary": "In this paper, we propose a training-free framework for vision-and-language navigation (VLN). Existing zero-shot VLN methods are mainly designed for discrete environments or involve unsupervised training in continuous simulator environments, which makes it challenging to generalize and deploy them in real-world scenarios. To achieve a training-free framework in continuous environments, our framework formulates navigation guidance as graph constraint optimization by decomposing instructions into explicit spatial constraints. The constraint-driven paradigm decodes spatial semantics through constraint solving, enabling zero-shot adaptation to unseen environments. Specifically, we construct a spatial constraint library covering all types of spatial relationship mentioned in VLN instructions. The human instruction is decomposed into a directed acyclic graph, with waypoint nodes, object nodes and edges, which are used as queries to retrieve the library to build the graph constraints. The graph constraint optimization is solved by the constraint solver to determine the positions of waypoints, obtaining the robot's navigation path and final goal. To handle cases of no solution or multiple solutions, we construct a navigation tree and the backtracking mechanism. Extensive experiments on standard benchmarks demonstrate significant improvements in success rate and navigation efficiency compared to state-of-the-art zero-shot VLN methods. We further conduct real-world experiments to show that our framework can effectively generalize to new environments and instruction sets, paving the way for a more robust and autonomous navigation framework.",
    "pdf_url": "https://arxiv.org/pdf/2509.10454v1",
    "github_url": null,
    "published": "2025-09-12T17:59:58+00:00",
    "updated": "2025-09-12T17:59:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.08435v1",
    "title": "PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot Diffusion Planner Flow Matching",
    "authors": [
      "Ye",
      "Gao",
      "Xu"
    ],
    "summary": "Diffusion models offer powerful generative capabilities for robot trajectory planning, yet their practical deployment on robots is hindered by a critical bottleneck: a reliance on imitation learning from expert demonstrations. This paradigm is often impractical for specialized robots where data is scarce and creates an inefficient, theoretically suboptimal training pipeline. To overcome this, we introduce PegasusFlow, a hierarchical rolling-denoising framework that enables direct and parallel sampling of trajectory score gradients from environmental interaction, completely bypassing the need for expert data. Our core innovation is a novel sampling algorithm, Weighted Basis Function Optimization (WBFO), which leverages spline basis representations to achieve superior sample efficiency and faster convergence compared to traditional methods like MPPI. The framework is embedded within a scalable, asynchronous parallel simulation architecture that supports massively parallel rollouts for efficient data collection. Extensive experiments on trajectory optimization and robotic navigation tasks demonstrate that our approach, particularly Action-Value WBFO (AVWBFO) combined with a reinforcement learning warm-start, significantly outperforms baselines. In a challenging barrier-crossing task, our method achieved a 100% success rate and was 18% faster than the next-best method, validating its effectiveness for complex terrain locomotion planning. https://masteryip.github.io/pegasusflow.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2509.08435v1",
    "github_url": null,
    "published": "2025-09-10T09:31:17+00:00",
    "updated": "2025-09-10T09:31:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.07220v1",
    "title": "OmniAcc: Personalized Accessibility Assistant Using Generative AI",
    "authors": [
      "Karki",
      "Han",
      "Mahmud"
    ],
    "summary": "Individuals with ambulatory disabilities often encounter significant barriers when navigating urban environments due to the lack of accessible information and tools. This paper presents OmniAcc, an AI-powered interactive navigation system that utilizes GPT-4, satellite imagery, and OpenStreetMap data to identify, classify, and map wheelchair-accessible features such as ramps and crosswalks in the built environment. OmniAcc offers personalized route planning, real-time hands-free navigation, and instant query responses regarding physical accessibility. By using zero-shot learning and customized prompts, the system ensures precise detection of accessibility features, while supporting validation through structured workflows. This paper introduces OmniAcc and explores its potential to assist urban planners and mobility-aid users, demonstrated through a case study on crosswalk detection. With a crosswalk detection accuracy of 97.5%, OmniAcc highlights the transformative potential of AI in improving navigation and fostering more inclusive urban spaces.",
    "pdf_url": "https://arxiv.org/pdf/2509.07220v1",
    "github_url": null,
    "published": "2025-09-08T21:03:48+00:00",
    "updated": "2025-09-08T21:03:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.06644v4",
    "title": "T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation",
    "authors": [
      "Zhao",
      "Lyu",
      "Li"
    ],
    "summary": "Agricultural robotic agents have been becoming powerful helpers in a wide range of agricultural tasks, however, still heavily rely on manual operation or fixed railways for movement. To address this limitation, the AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling agents to navigate to the target positions following the natural language instructions. AgriVLN effectively understands the simple instructions, but often misunderstands the complex ones. To bridge this gap, we propose the method of Translator for Agricultural Robotic Agents on Vision-and-Language Navigation (T-araVLN), in which the Instruction Translator module translates the original instruction to be more refined and precise. When evaluated on the A2A benchmark, our T-araVLN effectively improves Success Rate from 0.47 to 0.63 and reduces Navigation Error from 2.91m to 2.28m, demonstrating the state-of-the-art performance in the agricultural domain. Code: https://github.com/AlexTraveling/T-araVLN.",
    "pdf_url": "https://arxiv.org/pdf/2509.06644v4",
    "github_url": "https://github.com/AlexTraveling/T-araVLN",
    "published": "2025-09-08T12:59:36+00:00",
    "updated": "2025-09-18T12:15:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.05672v1",
    "title": "Sharing but Not Caring: Similar Outcomes for Shared Control and Switching Control in Telepresence-Robot Navigation",
    "authors": [
      "Kalliokoski",
      "Center",
      "LaValle"
    ],
    "summary": "Telepresence robots enable users to interact with remote environments, but efficient and intuitive navigation remains a challenge. In this work, we developed and evaluated a shared control method, in which the robot navigates autonomously while allowing users to affect the path generation to better suit their needs. We compared this with control switching, where users toggle between direct and automated control. We hypothesized that shared control would maintain efficiency comparable to control switching while potentially reducing user workload. The results of two consecutive user studies (each with final sample of n=20) showed that shared control does not degrade navigation efficiency, but did not show a significant reduction in task load compared to control switching. Further research is needed to explore the underlying factors that influence user preference and performance in these control systems.",
    "pdf_url": "https://arxiv.org/pdf/2509.05672v1",
    "github_url": null,
    "published": "2025-09-06T10:27:05+00:00",
    "updated": "2025-09-06T10:27:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.02761v3",
    "title": "Plan Verification for LLM-Based Embodied Task Completion Agents",
    "authors": [
      "Hariharan",
      "Dongre",
      "Hakkani-Tr"
    ],
    "summary": "Large language model (LLM) based task plans and corresponding human demonstrations for embodied AI may be noisy, with unnecessary actions, redundant navigation, and logical errors that reduce policy quality. We propose an iterative verification framework in which a Judge LLM critiques action sequences and a Planner LLM applies the revisions, yielding progressively cleaner and more spatially coherent trajectories. Unlike rule-based approaches, our method relies on natural language prompting, enabling broad generalization across error types including irrelevant actions, contradictions, and missing steps. On a set of manually annotated actions from the TEACh embodied AI dataset, our framework achieves up to 90% recall and 100% precision across four state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout). The refinement loop converges quickly, with 96.5% of sequences requiring at most three iterations, while improving both temporal efficiency and spatial action organization. Crucially, the method preserves human error-recovery patterns rather than collapsing them, supporting future work on robust corrective behavior. By establishing plan verification as a reliable LLM capability for spatial planning and action refinement, we provide a scalable path to higher-quality training data for imitation learning in embodied AI.",
    "pdf_url": "https://arxiv.org/pdf/2509.02761v3",
    "github_url": null,
    "published": "2025-09-02T19:06:56+00:00",
    "updated": "2025-09-24T03:01:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.01836v1",
    "title": "Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk Assessment",
    "authors": [
      "Alam",
      "Rodrigues-Jr",
      "Spadon"
    ],
    "summary": "Accurate vessel trajectory prediction is essential for enhancing situational awareness and preventing collisions. Still, existing data-driven models are constrained mainly to single-vessel forecasting, overlooking vessel interactions, navigation rules, and explicit collision risk assessment. We present a transformer-based framework for multi-vessel trajectory prediction with integrated collision risk analysis. For a given target vessel, the framework identifies nearby vessels. It jointly predicts their future trajectories through parallel streams encoding kinematic and derived physical features, causal convolutions for temporal locality, spatial transformations for positional encoding, and hybrid positional embeddings that capture both local motion patterns and long-range dependencies. Evaluated on large-scale real-world AIS data using joint multi-vessel metrics, the model demonstrates superior forecasting capabilities beyond traditional single-vessel displacement errors. By simulating interactions among predicted trajectories, the framework further quantifies potential collision risks, offering actionable insights to strengthen maritime safety and decision support.",
    "pdf_url": "https://arxiv.org/pdf/2509.01836v1",
    "github_url": null,
    "published": "2025-09-01T23:38:01+00:00",
    "updated": "2025-09-01T23:38:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.01364v1",
    "title": "TopoNav: Topological Graphs as a Key Enabler for Advanced Object Navigation",
    "authors": [
      "Liu",
      "Zhang",
      "Peng"
    ],
    "summary": "Object Navigation (ObjectNav) has made great progress with large language models (LLMs), but still faces challenges in memory management, especially in long-horizon tasks and dynamic scenes. To address this, we propose TopoNav, a new framework that leverages topological structures as spatial memory. By building and updating a topological graph that captures scene connections, adjacency, and semantic meaning, TopoNav helps agents accumulate spatial knowledge over time, retrieve key information, and reason effectively toward distant goals. Our experiments show that TopoNav achieves state-of-the-art performance on benchmark ObjectNav datasets, with higher success rates and more efficient paths. It particularly excels in diverse and complex environments, as it connects temporary visual inputs with lasting spatial understanding.",
    "pdf_url": "https://arxiv.org/pdf/2509.01364v1",
    "github_url": null,
    "published": "2025-09-01T11:05:38+00:00",
    "updated": "2025-09-01T11:05:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.00210v1",
    "title": "Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment",
    "authors": [
      "Tang",
      "zhang",
      "Liu"
    ],
    "summary": "Achieving human-like reasoning in deep learning models for complex tasks in unknown environments remains a critical challenge in embodied intelligence. While advanced vision-language models (VLMs) excel in static scene understanding, their limitations in spatio-temporal reasoning and adaptation to dynamic, open-set tasks like task-oriented navigation and embodied question answering (EQA) persist due to inadequate modeling of fine-grained spatio-temporal cues and physical world comprehension. To address this, we propose VEME, a novel cross-modal alignment method that enhances generalization in unseen scenes by learning an ego-centric, experience-centered world model. Our framework integrates three key components: (1) a cross-modal alignment framework bridging objects, spatial representations, and visual semantics with spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic, implicit cognitive map activated by world embedding to enable task-relevant geometric-semantic memory recall; and (3) an instruction-based navigation and reasoning framework leveraging embodied priors for long-term planning and efficient exploration. By embedding geometry-aware spatio-temporal episodic experiences, our method significantly improves reasoning and planning in dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate 1%-3% accuracy and exploration efficiency improvement compared to traditional approaches.",
    "pdf_url": "https://arxiv.org/pdf/2509.00210v1",
    "github_url": null,
    "published": "2025-08-29T19:47:25+00:00",
    "updated": "2025-08-29T19:47:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.21455v1",
    "title": "Assessing Human Cooperation for Enhancing Social Robot Navigation",
    "authors": [
      "Arunachalam",
      "Singamaneni",
      "Alami"
    ],
    "summary": "Socially aware robot navigation is a planning paradigm where the robot navigates in human environments and tries to adhere to social constraints while interacting with the humans in the scene. These navigation strategies were further improved using human prediction models, where the robot takes the potential future trajectory of humans while computing its own. Though these strategies significantly improve the robot's behavior, it faces difficulties from time to time when the human behaves in an unexpected manner. This happens as the robot fails to understand human intentions and cooperativeness, and the human does not have a clear idea of what the robot is planning to do. In this paper, we aim to address this gap through effective communication at an appropriate time based on a geometric analysis of the context and human cooperativeness in head-on crossing scenarios. We provide an assessment methodology and propose some evaluation metrics that could distinguish a cooperative human from a non-cooperative one. Further, we also show how geometric reasoning can be used to generate appropriate verbal responses or robot actions.",
    "pdf_url": "https://arxiv.org/pdf/2508.21455v1",
    "github_url": null,
    "published": "2025-08-29T09:38:21+00:00",
    "updated": "2025-08-29T09:38:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.16913v1",
    "title": "Chat-Driven Reconfiguration of Model Predictive Control",
    "authors": [
      "Miyaoka",
      "Inoue",
      "Maestre"
    ],
    "summary": "Traditional control personalization requires users to understand optimization parameters and provide repetitive numerical feedback, creating significant barriers for non-expert users. To deal with this issue, we propose ChatMPC, a model predictive control framework that enables users to personalize control systems and adapt to environmental changes through natural language interaction. The framework operates in two modes: personalization, where users iteratively adjust control behavior to their preferences, and co-development, where users provide real-time environmental information that complements sensor data. We establish convergence guarantees under different user behavior models, demonstrating exponential convergence for consistent feedback and finite-time convergence with logarithmic interaction complexity for tolerance-based users. We validate ChatMPC through experiments in robot navigation with personalized obstacle avoidance and semi-autonomous driving with conversational obstacle reporting. Both experiments achieve real-time performance and demonstrate effective adaptation to user preferences and environmental changes.",
    "pdf_url": "https://arxiv.org/pdf/2508.16913v1",
    "github_url": null,
    "published": "2025-08-23T06:03:01+00:00",
    "updated": "2025-08-23T06:03:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.15354v1",
    "title": "Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey",
    "authors": [
      "Xiong",
      "Huang",
      "Yu"
    ],
    "summary": "Embodied navigation (EN) advances traditional navigation by enabling robots to perform complex egocentric tasks through sensing, social, and motion intelligence. In contrast to classic methodologies that rely on explicit localization and pre-defined maps, EN leverages egocentric perception and human-like interaction strategies. This survey introduces a comprehensive EN formulation structured into five stages: Transition, Observation, Fusion, Reward-policy construction, and Action (TOFRA). The TOFRA framework serves to synthesize the current state of the art, provide a critical review of relevant platforms and evaluation metrics, and identify critical open research challenges. A list of studies is available at https://github.com/Franky-X/Awesome-Embodied-Navigation.",
    "pdf_url": "https://arxiv.org/pdf/2508.15354v1",
    "github_url": "https://github.com/Franky-X/Awesome-Embodied-Navigation",
    "published": "2025-08-21T08:33:51+00:00",
    "updated": "2025-08-21T08:33:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.15232v1",
    "title": "AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation",
    "authors": [
      "Wu",
      "Zhang",
      "Chen"
    ],
    "summary": "Aerial Vision-and-Language Navigation (VLN) is an emerging task that enables Unmanned Aerial Vehicles (UAVs) to navigate outdoor environments using natural language instructions and visual cues. However, due to the extended trajectories and complex maneuverability of UAVs, achieving reliable UAV-VLN performance is challenging and often requires human intervention or overly detailed instructions. To harness the advantages of UAVs' high mobility, which could provide multi-grained perspectives, while maintaining a manageable motion space for learning, we introduce a novel task called Dual-Altitude UAV Collaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinct altitudes: a high-altitude UAV responsible for broad environmental reasoning, and a low-altitude UAV tasked with precise navigation. To support the training and evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising 13,838 collaborative high-low UAV demonstration trajectories, each paired with target-oriented language instructions. This dataset includes both unseen maps and an unseen object validation set to systematically evaluate the model's generalization capabilities across novel environments and unfamiliar targets. To consolidate their complementary strengths, we propose a dual-UAV collaborative VLN framework, AeroDuo, where the high-altitude UAV integrates a multimodal large language model (Pilot-LLM) for target reasoning, while the low-altitude UAV employs a lightweight multi-stage policy for navigation and target grounding. The two UAVs work collaboratively and only exchange minimal coordinate information to ensure efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2508.15232v1",
    "github_url": null,
    "published": "2025-08-21T04:43:35+00:00",
    "updated": "2025-08-21T04:43:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.16654v3",
    "title": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning",
    "authors": [
      "Liu",
      "Zhou",
      "Zhang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural language instructions and navigate complex environments. Current approaches often adopt a \"black-box\" paradigm, where a single Large Language Model (LLM) makes end-to-end decisions. However, it is plagued by critical vulnerabilities, including poor spatial reasoning, weak cross-modal grounding, and memory overload in long-horizon tasks. To systematically address these issues, we propose Memory Spatial Navigation(MSNav), a framework that fuses three modules into a synergistic architecture, which transforms fragile inference into a robust, integrated intelligence. MSNav integrates three modules: Memory Module, a dynamic map memory module that tackles memory overload through selective node pruning, enhancing long-range exploration; Spatial Module, a module for spatial reasoning and object relationship inference that improves endpoint recognition; and Decision Module, a module using LLM-based path planning to execute robust actions. Powering Spatial Module, we also introduce an Instruction-Object-Space (I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp), which outperforms leading commercial LLMs in object list extraction, achieving higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art performance with significant improvements in Success Rate (SR) and Success weighted by Path Length (SPL).",
    "pdf_url": "https://arxiv.org/pdf/2508.16654v3",
    "github_url": null,
    "published": "2025-08-20T05:41:22+00:00",
    "updated": "2025-09-10T11:47:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.13421v1",
    "title": "Virtuous Machines: Towards Artificial General Science",
    "authors": [
      "Wehr",
      "Rideaux",
      "Fox"
    ],
    "summary": "Artificial intelligence systems are transforming scientific discovery by accelerating specific research tasks, from protein structure prediction to materials design, yet remain confined to narrow domains requiring substantial human oversight. The exponential growth of scientific literature and increasing domain specialisation constrain researchers' capacity to synthesise knowledge across disciplines and develop unifying theories, motivating exploration of more general-purpose AI systems for science. Here we show that a domain-agnostic, agentic AI system can independently navigate the scientific workflow - from hypothesis generation through data collection to manuscript preparation. The system autonomously designed and executed three psychological studies on visual working memory, mental rotation, and imagery vividness, executed one new online data collection with 288 participants, developed analysis pipelines through 8-hour+ continuous coding sessions, and produced completed manuscripts. The results demonstrate the capability of AI scientific discovery pipelines to conduct non-trivial research with theoretical reasoning and methodological rigour comparable to experienced researchers, though with limitations in conceptual nuance and theoretical interpretation. This is a step toward embodied AI that can test hypotheses through real-world experiments, accelerating discovery by autonomously exploring regions of scientific space that human cognitive and resource constraints might otherwise leave unexplored. It raises important questions about the nature of scientific understanding and the attribution of scientific credit.",
    "pdf_url": "https://arxiv.org/pdf/2508.13421v1",
    "github_url": null,
    "published": "2025-08-19T00:35:56+00:00",
    "updated": "2025-08-19T00:35:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.11144v1",
    "title": "CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets",
    "authors": [
      "Jain",
      "Rothenhusler",
      "Bansak"
    ],
    "summary": "Machine learning (ML) tasks often utilize large-scale data that is drawn from several distinct sources, such as different locations, treatment arms, or groups. In such settings, practitioners often desire predictions that not only exhibit good overall accuracy, but also remain reliable within each source and preserve the differences that matter across sources. For instance, several asylum and refugee resettlement programs now use ML-based employment predictions to guide where newly arriving families are placed within a host country, which requires generating informative and differentiated predictions for many and often small source locations. However, this task is made challenging by several common characteristics of the data in these settings: the presence of numerous distinct data sources, distributional shifts between them, and substantial variation in sample sizes across sources. This paper introduces Clustered Transfer Residual Learning (CTRL), a meta-learning method that combines the strengths of cross-domain residual learning and adaptive pooling/clustering in order to simultaneously improve overall accuracy and preserve source-level heterogeneity. We provide theoretical results that clarify how our objective navigates the trade-off between data quantity and data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5 large-scale datasets. This includes a dataset from the national asylum program in Switzerland, where the algorithmic geographic assignment of asylum seekers is currently being piloted. CTRL consistently outperforms the benchmarks across several key metrics and when using a range of different base learners.",
    "pdf_url": "https://arxiv.org/pdf/2508.11144v1",
    "github_url": null,
    "published": "2025-08-15T01:27:17+00:00",
    "updated": "2025-08-15T01:27:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.10416v1",
    "title": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model",
    "authors": [
      "Yu",
      "Long",
      "Yang"
    ],
    "summary": "Existing vision-and-language navigation models often deviate from the correct trajectory when executing instructions. However, these models lack effective error correction capability, hindering their recovery from errors. To address this challenge, we propose Self-correction Flywheel, a novel post-training paradigm. Instead of considering the model's error trajectories on the training set as a drawback, our paradigm emphasizes their significance as a valuable data source. We have developed a method to identify deviations in these error trajectories and devised innovative techniques to automatically generate self-correction data for perception and action. These self-correction data serve as fuel to power the model's continued training. The brilliance of our paradigm is revealed when we re-evaluate the model on the training set, uncovering new error trajectories. At this time, the self-correction flywheel begins to spin. Through multiple flywheel iterations, we progressively enhance our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2% and 16.4%. Real robot tests in various indoor and outdoor environments demonstrate \\method's superior capability of error correction, dynamic obstacle avoidance, and long instruction following.",
    "pdf_url": "https://arxiv.org/pdf/2508.10416v1",
    "github_url": null,
    "published": "2025-08-14T07:39:26+00:00",
    "updated": "2025-08-14T07:39:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.09444v1",
    "title": "DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation",
    "authors": [
      "Shi",
      "Deng",
      "Li"
    ],
    "summary": "Vision-Language Navigation in Continuous Environments (VLN-CE) requires agents to follow natural language instructions through free-form 3D spaces. Existing VLN-CE approaches typically use a two-stage waypoint planning framework, where a high-level waypoint predictor generates the navigable waypoints, and then a navigation planner suggests the intermediate goals in the high-level action space. However, this two-stage decomposition framework suffers from: (1) global sub-optimization due to the proxy objective in each stage, and (2) a performance bottleneck caused by the strong reliance on the quality of the first-stage predicted waypoints. To address these limitations, we propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE policy that unifies the traditional two stages, i.e. waypoint generation and planning, into a single diffusion policy. Notably, DifNav employs a conditional diffusion policy to directly model multi-modal action distributions over future actions in continuous navigation space, eliminating the need for a waypoint predictor while enabling the agent to capture multiple possible instruction-following behaviors. To address the issues of compounding error in imitation learning and enhance spatial reasoning in long-horizon navigation tasks, we employ DAgger for online policy training and expert trajectory augmentation, and use the aggregated data to further fine-tune the policy. This approach significantly improves the policy's robustness and its ability to recover from error states. Extensive experiments on benchmark datasets demonstrate that, even without a waypoint predictor, the proposed method substantially outperforms previous state-of-the-art two-stage waypoint-based models in terms of navigation performance. Our code is available at: https://github.com/Tokishx/DifNav.",
    "pdf_url": "https://arxiv.org/pdf/2508.09444v1",
    "github_url": "https://github.com/Tokishx/DifNav",
    "published": "2025-08-13T02:51:43+00:00",
    "updated": "2025-08-13T02:51:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.09262v1",
    "title": "Harnessing Input-Adaptive Inference for Efficient VLN",
    "authors": [
      "Kang",
      "Perincherry",
      "Coalson"
    ],
    "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of history-aware multi-modal transformer models. Given a language instruction, these models process observation and navigation history to predict the most appropriate action for an agent. While they have significantly improved performance, the scale of these models can be a bottleneck in practical settings with limited computational resources. In this work, we propose a novel input-adaptive navigation method to enhance VLN model efficiency. We first show that existing input-adaptive mechanisms fail to reduce computations without substantial performance degradation. To address this, we introduce three adaptive algorithms, each deployed at a different level: (1) To improve spatial efficiency, we selectively process panoramic views at each observation of an agent. (2) To improve intra-model efficiency, we propose importance-based adaptive thresholding for the early-exit methods. (3) To improve temporal efficiency, we implement a caching mechanism that prevents reprocessing of views previously seen by the agent. In evaluations on seven VLN benchmarks, we demonstrate over a 2$\\times$ reduction in computation across three off-the-shelf agents in both standard and continuous environments. Our code is publicly available at https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.",
    "pdf_url": "https://arxiv.org/pdf/2508.09262v1",
    "github_url": "https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation",
    "published": "2025-08-12T18:05:33+00:00",
    "updated": "2025-08-12T18:05:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.08831v1",
    "title": "DiffPhysCam: Differentiable Physics-Based Camera Simulation for Inverse Rendering and Embodied AI",
    "authors": [
      "Chen",
      "Batagoda",
      "Negrut"
    ],
    "summary": "We introduce DiffPhysCam, a differentiable camera simulator designed to support robotics and embodied AI applications by enabling gradient-based optimization in visual perception pipelines. Generating synthetic images that closely mimic those from real cameras is essential for training visual models and enabling end-to-end visuomotor learning. Moreover, differentiable rendering allows inverse reconstruction of real-world scenes as digital twins, facilitating simulation-based robotics training. However, existing virtual cameras offer limited control over intrinsic settings, poorly capture optical artifacts, and lack tunable calibration parameters -- hindering sim-to-real transfer. DiffPhysCam addresses these limitations through a multi-stage pipeline that provides fine-grained control over camera settings, models key optical effects such as defocus blur, and supports calibration with real-world data. It enables both forward rendering for image synthesis and inverse rendering for 3D scene reconstruction, including mesh and material texture optimization. We show that DiffPhysCam enhances robotic perception performance in synthetic image tasks. As an illustrative example, we create a digital twin of a real-world scene using inverse rendering, simulate it in a multi-physics environment, and demonstrate navigation of an autonomous ground vehicle using images generated by DiffPhysCam.",
    "pdf_url": "https://arxiv.org/pdf/2508.08831v1",
    "github_url": null,
    "published": "2025-08-12T10:38:20+00:00",
    "updated": "2025-08-12T10:38:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.07642v2",
    "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents",
    "authors": [
      "Ma",
      "Zhang",
      "Wang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) poses significant challenges for agents to interpret natural language instructions and navigate complex 3D environments. While recent progress has been driven by large-scale pre-training and data augmentation, current methods still struggle to generalize to unseen scenarios, particularly when complex spatial and temporal reasoning is required. In this work, we propose SkillNav, a modular framework that introduces structured, skill-based reasoning into Transformer-based VLN agents. Our method decomposes navigation into a set of interpretable atomic skills (e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each handled by a specialized agent. To support targeted skill training without manual data annotation, we construct a synthetic dataset pipeline that generates diverse, linguistically natural, skill-specific instruction-trajectory pairs. We then introduce a novel training-free Vision-Language Model (VLM)-based router, which dynamically selects the most suitable agent at each time step by aligning sub-goals with visual observations and historical actions. SkillNav obtains competitive results on commonly used benchmarks and establishes state-of-the-art generalization to the GSA-R2R, a benchmark with novel instruction styles and unseen environments.",
    "pdf_url": "https://arxiv.org/pdf/2508.07642v2",
    "github_url": null,
    "published": "2025-08-11T05:50:30+00:00",
    "updated": "2025-10-01T00:48:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.07406v1",
    "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots",
    "authors": [
      "Zhao",
      "Lyu",
      "Li"
    ],
    "summary": "Agricultural robots have emerged as powerful members in agricultural tasks, nevertheless, still heavily rely on manual operation or untransportable railway for movement, resulting in limited mobility and poor adaptability. Vision-and-Language Navigation (VLN) enables robots to navigate to the target destinations following natural language instructions, demonstrating strong performance on several domains. However, none of the existing benchmarks or methods is specifically designed for agricultural scenes. To bridge this gap, we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560 episodes across six diverse agricultural scenes, in which all realistic RGB videos are captured by front-facing camera on a quadruped robot at a height of 0.38 meters, aligning with the practical deployment conditions. Meanwhile, we propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN) baseline based on Vision-Language Model (VLM) prompted with carefully crafted templates, which can understand both given instructions and agricultural environments to generate appropriate low-level actions for robot control. When evaluated on A2A, AgriVLN performs well on short instructions but struggles with long instructions, because it often fails to track which part of the instruction is currently being executed. To address this, we further propose Subtask List (STL) instruction decomposition module and integrate it into AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare AgriVLN with several existing VLN methods, demonstrating the state-of-the-art performance in the agricultural domain.",
    "pdf_url": "https://arxiv.org/pdf/2508.07406v1",
    "github_url": null,
    "published": "2025-08-10T16:07:23+00:00",
    "updated": "2025-08-10T16:07:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.06990v1",
    "title": "Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation",
    "authors": [
      "Hu",
      "Wu",
      "Xu"
    ],
    "summary": "Semantic navigation requires an agent to navigate toward a specified target in an unseen environment. Employing an imaginative navigation strategy that predicts future scenes before taking action, can empower the agent to find target faster. Inspired by this idea, we propose SGImagineNav, a novel imaginative navigation framework that leverages symbolic world modeling to proactively build a global environmental representation. SGImagineNav maintains an evolving hierarchical scene graphs and uses large language models to predict and explore unseen parts of the environment. While existing methods solely relying on past observations, this imaginative scene graph provides richer semantic context, enabling the agent to proactively estimate target locations. Building upon this, SGImagineNav adopts an adaptive navigation strategy that exploits semantic shortcuts when promising and explores unknown areas otherwise to gather additional context. This strategy continuously expands the known environment and accumulates valuable semantic contexts, ultimately guiding the agent toward the target. SGImagineNav is evaluated in both real-world scenarios and simulation benchmarks. SGImagineNav consistently outperforms previous methods, improving success rate to 65.4 and 66.8 on HM3D and HSSD, and demonstrating cross-floor and cross-room navigation in real-world environments, underscoring its effectiveness and generalizability.",
    "pdf_url": "https://arxiv.org/pdf/2508.06990v1",
    "github_url": null,
    "published": "2025-08-09T14:01:08+00:00",
    "updated": "2025-08-09T14:01:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.05855v1",
    "title": "Safety of Embodied Navigation: A Survey",
    "authors": [
      "Wang",
      "Hu",
      "Mu"
    ],
    "summary": "As large language models (LLMs) continue to advance and gain influence, the development of embodied AI has accelerated, drawing significant attention, particularly in navigation scenarios. Embodied navigation requires an agent to perceive, interact with, and adapt to its environment while moving toward a specified target in unfamiliar settings. However, the integration of embodied navigation into critical applications raises substantial safety concerns. Given their deployment in dynamic, real-world environments, ensuring the safety of such systems is critical. This survey provides a comprehensive analysis of safety in embodied navigation from multiple perspectives, encompassing attack strategies, defense mechanisms, and evaluation methodologies. Beyond conducting a comprehensive examination of existing safety challenges, mitigation technologies, and various datasets and metrics that assess effectiveness and robustness, we explore unresolved issues and future research directions in embodied navigation safety. These include potential attack methods, mitigation strategies, more reliable evaluation techniques, and the implementation of verification frameworks. By addressing these critical gaps, this survey aims to provide valuable insights that can guide future research toward the development of safer and more reliable embodied navigation systems. Furthermore, the findings of this study have broader implications for enhancing societal safety and increasing industrial efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2508.05855v1",
    "github_url": null,
    "published": "2025-08-07T21:09:48+00:00",
    "updated": "2025-08-07T21:09:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.05634v1",
    "title": "Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling",
    "authors": [
      "Yao",
      "Zhang",
      "Xia"
    ],
    "summary": "Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent's behavior through constrained reinforcement learning. The system helps regulate the agent's actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on https://gen-safe-nav.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2508.05634v1",
    "github_url": null,
    "published": "2025-08-07T17:59:43+00:00",
    "updated": "2025-08-07T17:59:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.05543v1",
    "title": "CleanUpBench: Embodied Sweeping and Grasping Benchmark",
    "authors": [
      "Li",
      "Chen",
      "Zhao"
    ],
    "summary": "Embodied AI benchmarks have advanced navigation, manipulation, and reasoning, but most target complex humanoid agents or large-scale simulations that are far from real-world deployment. In contrast, mobile cleaning robots with dual mode capabilities, such as sweeping and grasping, are rapidly emerging as realistic and commercially viable platforms. However, no benchmark currently exists that systematically evaluates these agents in structured, multi-target cleaning tasks, revealing a critical gap between academic research and real-world applications. We introduce CleanUpBench, a reproducible and extensible benchmark for evaluating embodied agents in realistic indoor cleaning scenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service robot equipped with a sweeping mechanism and a six-degree-of-freedom robotic arm, enabling interaction with heterogeneous objects. The benchmark includes manually designed environments and one procedurally generated layout to assess generalization, along with a comprehensive evaluation suite covering task completion, spatial efficiency, motion quality, and control performance. To support comparative studies, we provide baseline agents based on heuristic strategies and map-based planning. CleanUpBench bridges the gap between low-level skill evaluation and full-scene testing, offering a scalable testbed for grounded, embodied intelligence in everyday settings.",
    "pdf_url": "https://arxiv.org/pdf/2508.05543v1",
    "github_url": null,
    "published": "2025-08-07T16:20:31+00:00",
    "updated": "2025-08-07T16:20:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.05021v1",
    "title": "MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding",
    "authors": [
      "Zhang",
      "Li",
      "Liu"
    ],
    "summary": "Visual navigation in unknown environments based solely on natural language descriptions is a key capability for intelligent robots. In this work, we propose a navigation framework built upon off-the-shelf Visual Language Models (VLMs), enhanced with two human-inspired mechanisms: perspective-based active grounding, which dynamically adjusts the robot's viewpoint for improved visual inspection, and historical memory backtracking, which enables the system to retain and re-evaluate uncertain observations over time. Unlike existing approaches that passively rely on incidental visual inputs, our method actively optimizes perception and leverages memory to resolve ambiguity, significantly improving vision-language grounding in complex, unseen environments. Our framework operates in a zero-shot manner, achieving strong generalization to diverse and open-ended language descriptions without requiring labeled data or model fine-tuning. Experimental results on Habitat-Matterport 3D (HM3D) show that our method outperforms state-of-the-art approaches in language-driven object navigation. We further demonstrate its practicality through real-world deployment on a quadruped robot, achieving robust and effective navigation performance.",
    "pdf_url": "https://arxiv.org/pdf/2508.05021v1",
    "github_url": null,
    "published": "2025-08-07T04:15:08+00:00",
    "updated": "2025-08-07T04:15:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.04678v1",
    "title": "Open Scene Graphs for Open-World Object-Goal Navigation",
    "authors": [
      "Loo",
      "Wu",
      "Hsu"
    ],
    "summary": "How can we build general-purpose robot systems for open-world semantic navigation, e.g., searching a novel environment for a target object specified in natural language? To tackle this challenge, we introduce OSG Navigator, a modular system composed of foundation models, for open-world Object-Goal Navigation (ObjectNav). Foundation models provide enormous semantic knowledge about the world, but struggle to organise and maintain spatial information effectively at scale. Key to OSG Navigator is the Open Scene Graph representation, which acts as spatial memory for OSG Navigator. It organises spatial information hierarchically using OSG schemas, which are templates, each describing the common structure of a class of environments. OSG schemas can be automatically generated from simple semantic labels of a given environment, e.g., \"home\" or \"supermarket\". They enable OSG Navigator to adapt zero-shot to new environment types. We conducted experiments using both Fetch and Spot robots in simulation and in the real world, showing that OSG Navigator achieves state-of-the-art performance on ObjectNav benchmarks and generalises zero-shot over diverse goals, environments, and robot embodiments.",
    "pdf_url": "https://arxiv.org/pdf/2508.04678v1",
    "github_url": null,
    "published": "2025-08-06T17:43:29+00:00",
    "updated": "2025-08-06T17:43:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.04598v1",
    "title": "$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything",
    "authors": [
      "Zhang",
      "Hao",
      "Tang"
    ],
    "summary": "Embodied navigation is a fundamental capability of embodied intelligence, enabling robots to move and interact within physical environments. However, existing navigation tasks primarily focus on predefined object navigation or instruction following, which significantly differs from human needs in real-world scenarios involving complex, open-ended scenes. To bridge this gap, we introduce a challenging long-horizon navigation task that requires understanding high-level human instructions and performing spatial-aware object navigation in real-world environments. Existing embodied navigation methods struggle with such tasks due to their limitations in comprehending high-level human instructions and localizing objects with an open vocabulary. In this paper, we propose $NavA^3$, a hierarchical framework divided into two stages: global and local policies. In the global policy, we leverage the reasoning capabilities of Reasoning-VLM to parse high-level human instructions and integrate them with global 3D scene views. This allows us to reason and navigate to regions most likely to contain the goal object. In the local policy, we have collected a dataset of 1.0 million samples of spatial-aware object affordances to train the NaviAfford model (PointingVLM), which provides robust open-vocabulary object localization and spatial awareness for precise goal identification and navigation in complex environments. Extensive experiments demonstrate that $NavA^3$ achieves SOTA results in navigation performance and can successfully complete longhorizon navigation tasks across different robot embodiments in real-world settings, paving the way for universal embodied navigation. The dataset and code will be made available. Project website: https://NavigationA3.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2508.04598v1",
    "github_url": null,
    "published": "2025-08-06T16:17:34+00:00",
    "updated": "2025-08-06T16:17:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.03350v1",
    "title": "Investigation of Air Fluidization during Intruder Penetration in Sand",
    "authors": [
      "Wang",
      "Peng",
      "Vergara"
    ],
    "summary": "Self-burrowing robots navigating through granular media benefit from airflow-assisted burrowing, which reduces penetration resistance. However, the mechanisms underlying airflow-granular interactions remain poorly understood. To address this knowledge gap, we employ a coupled computational fluid dynamics and discrete element method (CFD-DEM) approach, supplemented by experimental cone penetration tests (CPT) under varying airflow conditions, to investigate the effects of aeration on penetration resistance. Experimental results reveal a nonlinear relationship between penetration resistance reduction and depth, wherein resistance approaches near-zero values up to a critical depth, beyond which the effectiveness of fluidization diminishes. Simulations demonstrate that higher airflow rates enhance the mobilization of overlying grains, increasing the critical depth. A detailed meso- and micro-scale analysis of particle motion, contact forces, and fluid pressure fields reveals four distinct penetration stages: particle ejection and channel formation, channel sealing, channel refill, and final compaction. These findings contribute to a deeper understanding of granular aeration mechanisms and their implications for geotechnical engineering, excavation technologies, and the development of self-burrowing robotic systems.",
    "pdf_url": "https://arxiv.org/pdf/2508.03350v1",
    "github_url": null,
    "published": "2025-08-05T11:55:20+00:00",
    "updated": "2025-08-05T11:55:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.03246v1",
    "title": "Force-Compliance MPC and Robot-User CBFs for Interactive Navigation and User-Robot Safety in Hexapod Guide Robots",
    "authors": [
      "Fan",
      "Gao",
      "Chen"
    ],
    "summary": "Guiding the visually impaired in complex environments requires real-time two-way interaction and safety assurance. We propose a Force-Compliance Model Predictive Control (FC-MPC) and Robot-User Control Barrier Functions (CBFs) for force-compliant navigation and obstacle avoidance in Hexapod guide robots. FC-MPC enables two-way interaction by estimating user-applied forces and moments using the robot's dynamic model and the recursive least squares (RLS) method, and then adjusting the robot's movements accordingly, while Robot-User CBFs ensure the safety of both the user and the robot by handling static and dynamic obstacles, and employ weighted slack variables to overcome feasibility issues in complex dynamic environments. We also adopt an Eight-Way Connected DBSCAN method for obstacle clustering, reducing computational complexity from O(n2) to approximately O(n), enabling real-time local perception on resource-limited on-board robot computers. Obstacles are modeled using Minimum Bounding Ellipses (MBEs), and their trajectories are predicted through Kalman filtering. Implemented on the HexGuide robot, the system seamlessly integrates force compliance, autonomous navigation, and obstacle avoidance. Experimental results demonstrate the system's ability to adapt to user force commands while guaranteeing user and robot safety simultaneously during navigation in complex environments.",
    "pdf_url": "https://arxiv.org/pdf/2508.03246v1",
    "github_url": null,
    "published": "2025-08-05T09:24:13+00:00",
    "updated": "2025-08-05T09:24:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.03053v1",
    "title": "SkeNa: Learning to Navigate Unseen Environments Based on Abstract Hand-Drawn Maps",
    "authors": [
      "Xu",
      "Xiang",
      "Wei"
    ],
    "summary": "A typical human strategy for giving navigation guidance is to sketch route maps based on the environmental layout. Inspired by this, we introduce Sketch map-based visual Navigation (SkeNa), an embodied navigation task in which an agent must reach a goal in an unseen environment using only a hand-drawn sketch map as guidance. To support research for SkeNa, we present a large-scale dataset named SoR, comprising 54k trajectory and sketch map pairs across 71 indoor scenes. In SoR, we introduce two navigation validation sets with varying levels of abstraction in hand-drawn sketches, categorized based on their preservation of spatial scales in the environment, to facilitate future research. To construct SoR, we develop an automated sketch-generation pipeline that efficiently converts floor plans into hand-drawn representations. To solve SkeNa, we propose SkeNavigator, a navigation framework that aligns visual observations with hand-drawn maps to estimate navigation targets. It employs a Ray-based Map Descriptor (RMD) to enhance sketch map valid feature representation using equidistant sampling points and boundary distances. To improve alignment with visual observations, a Dual-Map Aligned Goal Predictor (DAGP) leverages the correspondence between sketch map features and on-site constructed exploration map features to predict goal position and guide navigation. SkeNavigator outperforms prior floor plan navigation methods by a large margin, improving SPL on the high-abstract validation set by 105% relatively. Our code and dataset will be released.",
    "pdf_url": "https://arxiv.org/pdf/2508.03053v1",
    "github_url": null,
    "published": "2025-08-05T03:56:32+00:00",
    "updated": "2025-08-05T03:56:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.02917v1",
    "title": "Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces",
    "authors": [
      "Ksene",
      "Lison"
    ],
    "summary": "Vision-and-Language Navigation (VLN) refers to the task of enabling autonomous robots to navigate unfamiliar environments by following natural language instructions. While recent Large Vision-Language Models (LVLMs) have shown promise in this task, most current VLM systems rely on models specifically designed and optimized for navigation, leaving the potential of off-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used low-level action spaces with egocentric views and atomic actions (such as \"turn left\" or \"move forward\"), newer models tend to favor panoramic action spaces with discrete navigable viewpoints. This paper investigates (1) whether off-the-shelf LVLMs (fine-tuned without architectural modifications or simulator-based training) can effectively support VLN tasks and (2) whether such models can support both low-level and panoramic action paradigms. To this end, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the Room-to-Room (R2R) dataset and evaluate its empirical performance across both low-level and panoramic action spaces. The best resulting model achieves a 41% success rate on the R2R test set, demonstrating that while off-the-shelf LVLMs can learn to perform Vision-and-Language Navigation, they still lag behind models specifically designed for this task.",
    "pdf_url": "https://arxiv.org/pdf/2508.02917v1",
    "github_url": null,
    "published": "2025-08-04T21:45:21+00:00",
    "updated": "2025-08-04T21:45:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.02549v4",
    "title": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming",
    "authors": [
      "Wang",
      "Wang",
      "Fan"
    ],
    "summary": "Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth inputs to provide rich spatial cues for action planning, but these sensors can be costly or less accessible in real-world deployments. Recent approaches based on Vision-Language Action (VLA) models achieve strong results with monocular input, yet they still lag behind methods using panoramic RGB-D information. We present MonoDream, a lightweight VLA framework that enables monocular agents to learn a Unified Navigation Representation (UNR). This shared feature representation jointly aligns navigation-relevant visual semantics (e.g., global layout, depth, and future cues) and language-grounded action intent, enabling more reliable action prediction. MonoDream further introduces Latent Panoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to predict latent features of panoramic RGB and depth observations at both current and future steps based on only monocular input. Experiments on multiple VLN benchmarks show that MonoDream consistently improves monocular navigation performance and significantly narrows the gap with panoramic-based agents.",
    "pdf_url": "https://arxiv.org/pdf/2508.02549v4",
    "github_url": null,
    "published": "2025-08-04T16:01:30+00:00",
    "updated": "2025-11-27T10:49:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.02046v2",
    "title": "NaviMaster: Learning a Unified Policy for GUI and Embodied Navigation Tasks",
    "authors": [
      "Luo",
      "Yan",
      "Gong"
    ],
    "summary": "Recent advances in Graphical User Interface (GUI) and embodied navigation have driven progress, yet these domains have largely evolved in isolation, with disparate datasets and training paradigms. In this paper, we observe that both tasks can be formulated as Markov Decision Processes (MDP), suggesting a foundational principle for their unification. Hence, we present NaviMaster, the first unified agent capable of unifying GUI navigation and embodied navigation within a single framework. Specifically, NaviMaster (i) proposes a visual-target trajectory collection pipeline that generates trajectories for both GUI and embodied tasks using a single formulation. (ii) employs a unified reinforcement learning framework on the mix data to improve generalization. (iii) designs a novel distance-aware reward to ensure efficient learning from the trajectories. Through extensive experiments on out-of-domain benchmarks, NaviMaster is shown to outperform state-of-the-art agents in GUI navigation, spatial affordance prediction, and embodied navigation. Ablation studies further demonstrate the efficacy of our unified training strategy, data mixing strategy, and reward design.",
    "pdf_url": "https://arxiv.org/pdf/2508.02046v2",
    "github_url": null,
    "published": "2025-08-04T04:28:18+00:00",
    "updated": "2025-10-11T08:21:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.01723v1",
    "title": "OpenMap: Instruction Grounding via Open-Vocabulary Visual-Language Mapping",
    "authors": [
      "Li",
      "Yang",
      "Qi"
    ],
    "summary": "Grounding natural language instructions to visual observations is fundamental for embodied agents operating in open-world environments. Recent advances in visual-language mapping have enabled generalizable semantic representations by leveraging vision-language models (VLMs). However, these methods often fall short in aligning free-form language commands with specific scene instances, due to limitations in both instance-level semantic consistency and instruction interpretation. We present OpenMap, a zero-shot open-vocabulary visual-language map designed for accurate instruction grounding in navigation tasks. To address semantic inconsistencies across views, we introduce a Structural-Semantic Consensus constraint that jointly considers global geometric structure and vision-language similarity to guide robust 3D instance-level aggregation. To improve instruction interpretation, we propose an LLM-assisted Instruction-to-Instance Grounding module that enables fine-grained instance selection by incorporating spatial context and expressive target descriptions. We evaluate OpenMap on ScanNet200 and Matterport3D, covering both semantic mapping and instruction-to-target retrieval tasks. Experimental results show that OpenMap outperforms state-of-the-art baselines in zero-shot settings, demonstrating the effectiveness of our method in bridging free-form language and 3D perception for embodied navigation.",
    "pdf_url": "https://arxiv.org/pdf/2508.01723v1",
    "github_url": null,
    "published": "2025-08-03T11:25:52+00:00",
    "updated": "2025-08-03T11:25:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.00400v1",
    "title": "Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents",
    "authors": [
      "Gajo",
      "Merales",
      "Escarcha"
    ],
    "summary": "We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store simulation for benchmarking embodied agents against human performance in shopping tasks. Addressing a gap in retail-specific sim environments for embodied agent training, Sari Sandbox features over 250 interactive grocery items across three store configurations, controlled via an API. It supports both virtual reality (VR) for human interaction and a vision language model (VLM)-powered embodied agent. We also introduce SariBench, a dataset of annotated human demonstrations across varied task difficulties. Our sandbox enables embodied agents to navigate, inspect, and manipulate retail items, providing baselines against human performance. We conclude with benchmarks, performance analysis, and recommendations for enhancing realism and scalability. The source code can be accessed via https://github.com/upeee/sari-sandbox-env.",
    "pdf_url": "https://arxiv.org/pdf/2508.00400v1",
    "github_url": "https://github.com/upeee/sari-sandbox-env",
    "published": "2025-08-01T08:01:38+00:00",
    "updated": "2025-08-01T08:01:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.00390v1",
    "title": "SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation",
    "authors": [
      "Cai",
      "Dong",
      "Rao"
    ],
    "summary": "Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable agents to accurately localize targets and plan flight paths in complex environments based on natural language instructions, with broad applications in intelligent inspection, disaster rescue, and urban monitoring. Recent progress in Vision-Language Models (VLMs) has provided strong semantic understanding for this task, while reinforcement learning (RL) has emerged as a promising post-training strategy to further improve generalization. However, existing RL methods often suffer from inefficient use of training data, slow convergence, and insufficient consideration of the difficulty variation among training samples, which limits further performance improvement. To address these challenges, we propose \\textbf{Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS)}, a novel training framework that systematically integrates Curriculum Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator (SA-DE) to quantify the complexity of training samples and a Gaussian Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution, enabling a smooth progression from easy to challenging tasks. This design significantly improves training efficiency, accelerates convergence, and enhances overall model performance. Extensive experiments on the CityNav benchmark demonstrate that SA-GCS consistently outperforms strong baselines across all metrics, achieves faster and more stable convergence, and generalizes well across models of different scales, highlighting its robustness and scalability. The implementation of our approach is publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2508.00390v1",
    "github_url": null,
    "published": "2025-08-01T07:35:48+00:00",
    "updated": "2025-08-01T07:35:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.00288v4",
    "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents",
    "authors": [
      "Xiao",
      "Sun",
      "Shao"
    ],
    "summary": "Aerial navigation is a fundamental yet underexplored capability in embodied intelligence, enabling agents to operate in large-scale, unstructured environments where traditional navigation paradigms fall short. However, most existing research follows the Vision-and-Language Navigation (VLN) paradigm, which heavily depends on sequential linguistic instructions, limiting its scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark for large-scale Object Goal Navigation (ObjectNav) by aerial agents in open-world environments, where agents operate based on high-level semantic goals without relying on detailed instructional guidance as in VLN. UAV-ON comprises 14 high-fidelity Unreal Engine environments with diverse semantic regions and complex spatial layouts, covering urban, natural, and mixed-use settings. It defines 1270 annotated target objects, each characterized by an instance-level instruction that encodes category, physical footprint, and visual descriptors, allowing grounded reasoning. These instructions serve as semantic goals, introducing realistic ambiguity and complex reasoning challenges for aerial agents. To evaluate the benchmark, we implement several baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that integrates instruction semantics with egocentric observations for long-horizon, goal-directed exploration. Empirical results show that all baselines struggle in this setting, highlighting the compounded challenges of aerial navigation and semantic goal grounding. UAV-ON aims to advance research on scalable UAV autonomy driven by semantic goal descriptions in complex real-world environments.",
    "pdf_url": "https://arxiv.org/pdf/2508.00288v4",
    "github_url": null,
    "published": "2025-08-01T03:23:06+00:00",
    "updated": "2025-08-22T00:25:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.23042v2",
    "title": "Goal-Based Vision-Language Driving",
    "authors": [
      "Patapati",
      "Srinivasan"
    ],
    "summary": "Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations. We introduce NovaDrive, a single-branch vision-language architecture that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a single branch. A lightweight, two-stage cross-attention block first aligns waypoint tokens with the HD map, then refines attention over fine-grained image and depth patches. Coupled with a novel smoothness loss that discourages abrupt steering and speed changes, this design eliminates the need for recurrent memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language backbone, enabling real-time inference. On the nuScenes / Waymo subset of the MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from 2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention fusion each contribute the most to these gains. Beyond safety, NovaDrive's shorter routes (resulting from the novel smoothness loss) translate to lower fuel or battery usage, pointing toward leaner, more easily updated driving stacks. NovaDrive can be extended to other embodied-AI domains as well.",
    "pdf_url": "https://arxiv.org/pdf/2507.23042v2",
    "github_url": null,
    "published": "2025-07-30T19:12:42+00:00",
    "updated": "2025-10-13T04:53:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.21450v1",
    "title": "Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation",
    "authors": [
      "Chen",
      "Kang",
      "Wang"
    ],
    "summary": "Vision Language Navigation (VLN) typically requires agents to navigate to specified objects or remote regions in unknown scenes by obeying linguistic commands. Such tasks require organizing historical visual observations for linguistic grounding, which is critical for long-sequence navigational decisions. However, current agents suffer from overly detailed scene representation and ambiguous vision-language alignment, which weaken their comprehension of navigation-friendly high-level scene priors and easily lead to behaviors that violate linguistic commands. To tackle these issues, we propose a navigation policy by recursively summarizing along-the-way visual perceptions, which are adaptively aligned with commands to enhance linguistic grounding. In particular, by structurally modeling historical trajectories as compact neural grids, several Recursive Visual Imagination (RVI) techniques are proposed to motivate agents to focus on the regularity of visual transitions and semantic scene layouts, instead of dealing with misleading geometric details. Then, an Adaptive Linguistic Grounding (ALG) technique is proposed to align the learned situational memories with different linguistic components purposefully. Such fine-grained semantic matching facilitates the accurate anticipation of navigation actions and progress. Our navigation policy outperforms the state-of-the-art methods on the challenging VLN-CE and ObjectNav tasks, showing the superiority of our RVI and ALG techniques for VLN.",
    "pdf_url": "https://arxiv.org/pdf/2507.21450v1",
    "github_url": null,
    "published": "2025-07-29T02:40:07+00:00",
    "updated": "2025-07-29T02:40:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.20395v1",
    "title": "MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models",
    "authors": [
      "Einarsson"
    ],
    "summary": "As Large Language Models (LLMs) increasingly power autonomous agents in robotics and embodied AI, understanding their spatial reasoning capabilities becomes crucial for ensuring reliable real-world deployment. Despite advances in language understanding, current research lacks evaluation of how LLMs perform spatial navigation without visual cues, a fundamental requirement for agents operating with limited sensory information. This paper addresses this gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our methodology employs a function-calling interface where models navigate mazes of varying complexity ($5\\times 5$ to $15\\times 15$ grids) using only coordinate feedback and distance-to-wall information, excluding visual input to test fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across identical mazes in both English and Icelandic to assess cross-linguistic transfer of spatial abilities. Our findings reveal striking disparities: while OpenAI's O3 achieves perfect navigation for mazes up to size $30\\times 30$, other models exhibit catastrophic failure beyond $9\\times 9$ mazes, with 100% of failures attributed to excessive looping behavior where models revisit a cell at least 10 times. We document a significant performance degradation in Icelandic, with models solving mazes 3-4 sizes smaller than in English, suggesting spatial reasoning in LLMs emerges from linguistic patterns rather than language-agnostic mechanisms. These results have important implications for global deployment of LLM-powered autonomous systems, showing spatial intelligence remains fundamentally constrained by training data availability and highlighting the need for architectural innovations to achieve reliable navigation across linguistic contexts.",
    "pdf_url": "https://arxiv.org/pdf/2507.20395v1",
    "github_url": null,
    "published": "2025-07-27T19:33:45+00:00",
    "updated": "2025-07-27T19:33:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.18033v1",
    "title": "OpenNav: Open-World Navigation with Multimodal Large Language Models",
    "authors": [
      "Yuan",
      "Wang",
      "Waslander"
    ],
    "summary": "Pre-trained large language models (LLMs) have demonstrated strong common-sense reasoning abilities, making them promising for robotic navigation and planning tasks. However, despite recent progress, bridging the gap between language descriptions and actual robot actions in the open-world, beyond merely invoking limited predefined motion primitives, remains an open challenge. In this work, we aim to enable robots to interpret and decompose complex language instructions, ultimately synthesizing a sequence of trajectory points to complete diverse navigation tasks given open-set instructions and open-set objects. We observe that multi-modal large language models (MLLMs) exhibit strong cross-modal understanding when processing free-form language instructions, demonstrating robust scene comprehension. More importantly, leveraging their code-generation capability, MLLMs can interact with vision-language perception models to generate compositional 2D bird-eye-view value maps, effectively integrating semantic knowledge from MLLMs with spatial information from maps to reinforce the robot's spatial understanding. To further validate our approach, we effectively leverage large-scale autonomous vehicle datasets (AVDs) to validate our proposed zero-shot vision-language navigation framework in outdoor navigation tasks, demonstrating its capability to execute a diverse range of free-form natural language navigation instructions while maintaining robustness against object detection errors and linguistic ambiguities. Furthermore, we validate our system on a Husky robot in both indoor and outdoor scenes, demonstrating its real-world robustness and applicability. Supplementary videos are available at https://trailab.github.io/OpenNav-website/",
    "pdf_url": "https://arxiv.org/pdf/2507.18033v1",
    "github_url": null,
    "published": "2025-07-24T02:05:28+00:00",
    "updated": "2025-07-24T02:05:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.14731v2",
    "title": "X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots",
    "authors": [
      "Wang",
      "Tan",
      "Fung"
    ],
    "summary": "Existing navigation methods are primarily designed for specific robot embodiments, limiting their generalizability across diverse robot platforms. In this paper, we introduce X-Nav, a novel framework for end-to-end cross-embodiment navigation where a single unified policy can be deployed across various embodiments for both wheeled and quadrupedal robots. X-Nav consists of two learning stages: 1) multiple expert policies are trained using deep reinforcement learning with privileged observations on a wide range of randomly generated robot embodiments; and 2) a single general policy is distilled from the expert policies via navigation action chunking with transformer (Nav-ACT). The general policy directly maps visual and proprioceptive observations to low-level control commands, enabling generalization to novel robot embodiments. Simulated experiments demonstrated that X-Nav achieved zero-shot transfer to both unseen embodiments and photorealistic environments. A scalability study showed that the performance of X-Nav improves when trained with an increasing number of randomly generated embodiments. An ablation study confirmed the design choices of X-Nav. Furthermore, real-world experiments were conducted to validate the generalizability of X-Nav in real-world environments.",
    "pdf_url": "https://arxiv.org/pdf/2507.14731v2",
    "github_url": null,
    "published": "2025-07-19T19:40:53+00:00",
    "updated": "2025-11-26T17:05:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.13152v3",
    "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models",
    "authors": [
      "Dong",
      "Zhao",
      "Gao"
    ],
    "summary": "Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.",
    "pdf_url": "https://arxiv.org/pdf/2507.13152v3",
    "github_url": null,
    "published": "2025-07-17T14:13:50+00:00",
    "updated": "2025-08-26T08:52:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.13019v2",
    "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities",
    "authors": [
      "Wang",
      "Xia",
      "Zhao"
    ],
    "summary": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2507.13019v2",
    "github_url": null,
    "published": "2025-07-17T11:46:00+00:00",
    "updated": "2025-09-26T07:50:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.12753v1",
    "title": "osmAG-LLM: Zero-Shot Open-Vocabulary Object Navigation via Semantic Maps and Large Language Models Reasoning",
    "authors": [
      "Xie",
      "Schwertfeger",
      "Blum"
    ],
    "summary": "Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features, achieving a high level of detail and guiding robots to find objects specified by open-vocabulary language queries. While the issue of scalability for such approaches has received some attention, another fundamental problem is that high-detail object mapping quickly becomes outdated, as objects get moved around a lot. In this work, we develop a mapping and navigation system for object-goal navigation that, from the ground up, considers the possibilities that a queried object can have moved, or may not be mapped at all. Instead of striving for high-fidelity mapping detail, we consider that the main purpose of a map is to provide environment grounding and context, which we combine with the semantic priors of LLMs to reason about object locations and deploy an active, online approach to navigate to the objects. Through simulated and real-world experiments we find that our approach tends to have higher retrieval success at shorter path lengths for static objects and by far outperforms prior approaches in cases of dynamic or unmapped object queries. We provide our code and dataset at: https://anonymous.4open.science/r/osmAG-LLM.",
    "pdf_url": "https://arxiv.org/pdf/2507.12753v1",
    "github_url": null,
    "published": "2025-07-17T03:14:37+00:00",
    "updated": "2025-07-17T03:14:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.10894v1",
    "title": "NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization",
    "authors": [
      "He",
      "Wang",
      "Chen"
    ],
    "summary": "Language-guided navigation is a cornerstone of embodied AI, enabling agents to interpret language instructions and navigate complex environments. However, expert-provided instructions are limited in quantity, while synthesized annotations often lack quality, making them insufficient for large-scale research. To address this, we propose NavComposer, a novel framework for automatically generating high-quality navigation instructions. NavComposer explicitly decomposes semantic entities such as actions, scenes, and objects, and recomposes them into natural language instructions. Its modular architecture allows flexible integration of state-of-the-art techniques, while the explicit use of semantic entities enhances both the richness and accuracy of instructions. Moreover, it operates in a data-agnostic manner, supporting adaptation to diverse navigation trajectories without domain-specific training. Complementing NavComposer, we introduce NavInstrCritic, a comprehensive annotation-free evaluation system that assesses navigation instructions on three dimensions: contrastive matching, semantic consistency, and linguistic diversity. NavInstrCritic provides a holistic evaluation of instruction quality, addressing limitations of traditional metrics that rely heavily on expert annotations. By decoupling instruction generation and evaluation from specific navigation agents, our method enables more scalable and generalizable research. Extensive experiments provide direct and practical evidence for the effectiveness of our method.",
    "pdf_url": "https://arxiv.org/pdf/2507.10894v1",
    "github_url": null,
    "published": "2025-07-15T01:20:22+00:00",
    "updated": "2025-07-15T01:20:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.07299v2",
    "title": "MLFM: Multi-Layered Feature Maps for Richer Language Understanding in Zero-Shot Semantic Navigation",
    "authors": [
      "Raychaudhuri",
      "Cancelli",
      "Campari"
    ],
    "summary": "Recent progress in large vision-language models has driven improvements in language-based semantic navigation, where an embodied agent must reach a target object described in natural language. Yet we still lack a clear, language-focused evaluation framework to test how well agents ground the words in their instructions. We address this gap by proposing LangNav, an open-vocabulary multi-object navigation dataset with natural language goal descriptions (e.g. 'go to the red short candle on the table') and corresponding fine-grained linguistic annotations (e.g., attributes: color=red, size=short; relations: support=on). These labels enable systematic evaluation of language understanding. To evaluate on this setting, we extend multi-object navigation task setting to Language-guided Multi-Object Navigation (LaMoN), where the agent must find a sequence of goals specified using language. Furthermore, we propose Multi-Layered Feature Map (MLFM), a novel method that builds a queryable, multi-layered semantic map from pretrained vision-language features and proves effective for reasoning over fine-grained attributes and spatial relations in goal descriptions. Experiments on LangNav show that MLFM outperforms state-of-the-art zero-shot mapping-based navigation baselines.",
    "pdf_url": "https://arxiv.org/pdf/2507.07299v2",
    "github_url": null,
    "published": "2025-07-09T21:46:43+00:00",
    "updated": "2025-10-17T00:58:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.06747v1",
    "title": "LOVON: Legged Open-Vocabulary Object Navigator",
    "authors": [
      "Peng",
      "Cao",
      "Zhang"
    ],
    "summary": "Object navigation in open-world environments remains a formidable and pervasive challenge for robotic systems, particularly when it comes to executing long-horizon tasks that require both open-world object detection and high-level task planning. Traditional methods often struggle to integrate these components effectively, and this limits their capability to deal with complex, long-range navigation missions. In this paper, we propose LOVON, a novel framework that integrates large language models (LLMs) for hierarchical task planning with open-vocabulary visual detection models, tailored for effective long-range object navigation in dynamic, unstructured environments. To tackle real-world challenges including visual jittering, blind zones, and temporary target loss, we design dedicated solutions such as Laplacian Variance Filtering for visual stabilization. We also develop a functional execution logic for the robot that guarantees LOVON's capabilities in autonomous navigation, task adaptation, and robust task completion. Extensive evaluations demonstrate the successful completion of long-sequence tasks involving real-time detection, search, and navigation toward open-vocabulary dynamic targets. Furthermore, real-world experiments across different legged robots (Unitree Go2, B2, and H1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.",
    "pdf_url": "https://arxiv.org/pdf/2507.06747v1",
    "github_url": null,
    "published": "2025-07-09T11:02:46+00:00",
    "updated": "2025-07-09T11:02:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.06719v1",
    "title": "A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding",
    "authors": [
      "Liu",
      "Zheng",
      "Chen"
    ],
    "summary": "Open-vocabulary 3D visual grounding aims to localize target objects based on free-form language queries, which is crucial for embodied AI applications such as autonomous navigation, robotics, and augmented reality. Learning 3D language fields through neural representations enables accurate understanding of 3D scenes from limited viewpoints and facilitates the localization of target objects in complex environments. However, existing language field methods struggle to accurately localize instances using spatial relations in language queries, such as ``the book on the chair.'' This limitation mainly arises from inadequate reasoning about spatial relations in both language queries and 3D scenes. In this work, we propose SpatialReasoner, a novel neural representation-based framework with large language model (LLM)-driven spatial reasoning that constructs a visual properties-enhanced hierarchical feature field for open-vocabulary 3D visual grounding. To enable spatial reasoning in language queries, SpatialReasoner fine-tunes an LLM to capture spatial relations and explicitly infer instructions for the target, anchor, and spatial relation. To enable spatial reasoning in 3D scenes, SpatialReasoner incorporates visual properties (opacity and color) to construct a hierarchical feature field. This field represents language and instance features using distilled CLIP features and masks extracted via the Segment Anything Model (SAM). The field is then queried using the inferred instructions in a hierarchical manner to localize the target 3D instance based on the spatial relation in the language query. Extensive experiments show that our framework can be seamlessly integrated into different neural representations, outperforming baseline models in 3D visual grounding while empowering their spatial reasoning capability.",
    "pdf_url": "https://arxiv.org/pdf/2507.06719v1",
    "github_url": null,
    "published": "2025-07-09T10:20:38+00:00",
    "updated": "2025-07-09T10:20:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.06564v1",
    "title": "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments",
    "authors": [
      "Li",
      "Huai",
      "Li"
    ],
    "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across various sectors, driven by their mobility and adaptability. This paper introduces SkyVLN, a novel framework integrating vision-and-language navigation (VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in complex urban environments. Unlike traditional navigation methods, SkyVLN leverages Large Language Models (LLMs) to interpret natural language instructions and visual observations, enabling UAVs to navigate through dynamic 3D spaces with improved accuracy and robustness. We present a multimodal navigation agent equipped with a fine-grained spatial verbalizer and a history path memory mechanism. These components allow the UAV to disambiguate spatial contexts, handle ambiguous instructions, and backtrack when necessary. The framework also incorporates an NMPC module for dynamic obstacle avoidance, ensuring precise trajectory tracking and collision prevention. To validate our approach, we developed a high-fidelity 3D urban simulation environment using AirSim, featuring realistic imagery and dynamic urban elements. Extensive experiments demonstrate that SkyVLN significantly improves navigation success rates and efficiency, particularly in new and unseen environments.",
    "pdf_url": "https://arxiv.org/pdf/2507.06564v1",
    "github_url": null,
    "published": "2025-07-09T05:38:32+00:00",
    "updated": "2025-07-09T05:38:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.06208v3",
    "title": "Ontological differentiation as a measure of semantic accuracy",
    "authors": [
      "Garcia-Cuadrillero",
      "Revuelta",
      "Capitan"
    ],
    "summary": "Understanding semantic relationships within complex networks derived from lexical resources is fundamental for network science and language modeling. While network embedding methods capture contextual similarity, quantifying semantic distance based directly on explicit definitional structure remains challenging. Accurate measures of semantic similarity allow for navigation on lexical networks based on maximizing semantic similarity in each navigation jump (Semantic Navigation, SN). This work introduces Ontological Differentiation (OD), a formal method for measuring divergence between concepts by analyzing overlap during recursive definition expansion. The methodology is applied to networks extracted from the Simple English Wiktionary, comparing OD scores with other measures of semantic similarity proposed in the literature (cosine similarity based on random-walk network exploration). We find weak correlations between direct pairwise OD scores and cosine similarities across $\\sim$~2 million word pairs, sampled from a pool representing over 50\\% of the entries in the Wiktionary lexicon. This establishes OD as a largely independent, definition-based semantic metric, whose orthogonality to cosine similarity becomes more pronounced when low-semantic-content terms were removed from the dataset. Additionally, we use cumulative OD scores to evaluate paths generated by vector-based SN and structurally optimal Shortest Paths (SP) across networks. We find SN paths consistently exhibit significantly lower cumulative OD scores than shortest paths, suggesting that SN produces trajectories more coherent with the dictionary's definitional structure, as measured by OD. Ontological Differentiation thus provides a novel, definition-grounded tool for analyzing, validating, and potentially constructing navigation processes in lexical networks.",
    "pdf_url": "https://arxiv.org/pdf/2507.06208v3",
    "github_url": null,
    "published": "2025-07-08T17:37:29+00:00",
    "updated": "2025-11-03T20:07:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.05240v1",
    "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling",
    "authors": [
      "Wei",
      "Wan",
      "Yu"
    ],
    "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
    "pdf_url": "https://arxiv.org/pdf/2507.05240v1",
    "github_url": null,
    "published": "2025-07-07T17:49:41+00:00",
    "updated": "2025-07-07T17:49:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.04430v1",
    "title": "\"Hi AirStar, Guide Me to the Badminton Court.\"",
    "authors": [
      "Wang",
      "Chen",
      "Zheng"
    ],
    "summary": "Unmanned Aerial Vehicles, operating in environments with relatively few obstacles, offer high maneuverability and full three-dimensional mobility. This allows them to rapidly approach objects and perform a wide range of tasks often challenging for ground robots, making them ideal for exploration, inspection, aerial imaging, and everyday assistance. In this paper, we introduce AirStar, a UAV-centric embodied platform that turns a UAV into an intelligent aerial assistant: a large language model acts as the cognitive core for environmental understanding, contextual reasoning, and task planning. AirStar accepts natural interaction through voice commands and gestures, removing the need for a remote controller and significantly broadening its user base. It combines geospatial knowledge-driven long-distance navigation with contextual reasoning for fine-grained short-range control, resulting in an efficient and accurate vision-and-language navigation (VLN) capability.Furthermore, the system also offers built-in capabilities such as cross-modal question answering, intelligent filming, and target tracking. With a highly extensible framework, it supports seamless integration of new functionalities, paving the way toward a general-purpose, instruction-driven intelligent UAV agent. The supplementary PPT is available at \\href{https://buaa-colalab.github.io/airstar.github.io}{https://buaa-colalab.github.io/airstar.github.io}.",
    "pdf_url": "https://arxiv.org/pdf/2507.04430v1",
    "github_url": null,
    "published": "2025-07-06T15:29:07+00:00",
    "updated": "2025-07-06T15:29:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.08831v2",
    "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments",
    "authors": [
      "Sun",
      "Xing",
      "Weng"
    ],
    "summary": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent follows instructions and moves freely to reach a destination, is a key research problem in embodied AI. However, most navigation policies are sensitive to viewpoint changes, i.e., variations in camera height and viewing angle that alter the agent's observation. In this paper, we introduce a generalized scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View Invariant Learning), a view-invariant post-training strategy that enhances the robustness of existing navigation policies to changes in camera viewpoint. VIL employs a contrastive learning framework to learn sparse and view-invariant features. Additionally, we introduce a teacher-student framework for the Waypoint Predictor Module, a core component of most VLNCE baselines, where a view-dependent teacher model distills knowledge into a view-invariant student model. We employ an end-to-end training paradigm to jointly optimize these components, thus eliminating the cost for individual module training. Empirical results show that our method outperforms state-of-the-art approaches on V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE setting and find that, despite being trained for varied viewpoints, it often still improves performance. On the more challenging RxR-CE dataset, our method also achieved state-of-the-art performance across all metrics when compared to other map-free methods. This suggests that adding VIL does not diminish the standard viewpoint performance and can serve as a plug-and-play post-training method.",
    "pdf_url": "https://arxiv.org/pdf/2507.08831v2",
    "github_url": null,
    "published": "2025-07-05T18:04:35+00:00",
    "updated": "2025-07-15T01:49:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.04086v1",
    "title": "Are Learning-Based Approaches Ready for Real-World Indoor Navigation? A Case for Imitation Learning",
    "authors": [
      "Selvaraj",
      "Mitrevski",
      "Houben"
    ],
    "summary": "Traditional indoor robot navigation methods provide a reliable solution when adapted to constrained scenarios, but lack flexibility or require manual re-tuning when deployed in more complex settings. In contrast, learning-based approaches learn directly from sensor data and environmental interactions, enabling easier adaptability. While significant work has been presented in the context of learning navigation policies, learning-based methods are rarely compared to traditional navigation methods directly, which is a problem for their ultimate acceptance in general navigation contexts. In this work, we explore the viability of imitation learning (IL) for indoor navigation, using expert (joystick) demonstrations to train various navigation policy networks based on RGB images, LiDAR, and a combination of both, and we compare our IL approach to a traditional potential field-based navigation method. We evaluate the approach on a physical mobile robot platform equipped with a 2D LiDAR and a camera in an indoor university environment. Our multimodal model demonstrates superior navigation capabilities in most scenarios, but faces challenges in dynamic environments, likely due to limited diversity in the demonstrations. Nevertheless, the ability to learn directly from data and generalise across layouts suggests that IL can be a practical navigation approach, and potentially a useful initialisation strategy for subsequent lifelong learning.",
    "pdf_url": "https://arxiv.org/pdf/2507.04086v1",
    "github_url": null,
    "published": "2025-07-05T16:18:48+00:00",
    "updated": "2025-07-05T16:18:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.04047v2",
    "title": "Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation",
    "authors": [
      "Zhu",
      "Wang",
      "Li"
    ],
    "summary": "Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce \\underline{\\textbf{M}}ove \\underline{\\textbf{t}}o \\underline{\\textbf{U}}nderstand (\\textbf{\\model}), a unified framework that integrates active perception with \\underline{\\textbf{3D}} vision-language learning, enabling embodied agents to effectively explore and understand their environment. This is achieved by three key innovations: 1) Online query-based representation learning, enabling direct spatial memory construction from RGB-D frames, eliminating the need for explicit 3D reconstruction. 2) A unified objective for grounding and exploring, which represents unexplored locations as frontier queries and jointly optimizes object grounding and frontier selection. 3) End-to-end trajectory learning that combines \\textbf{V}ision-\\textbf{L}anguage-\\textbf{E}xploration pre-training over a million diverse trajectories collected from both simulated and real-world RGB-D sequences. Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14\\%, 23\\%, 9\\%, and 2\\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. \\model's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.",
    "pdf_url": "https://arxiv.org/pdf/2507.04047v2",
    "github_url": null,
    "published": "2025-07-05T14:15:52+00:00",
    "updated": "2025-07-30T11:32:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.01667v1",
    "title": "What does really matter in image goal navigation?",
    "authors": [
      "Monaci",
      "Weinzaepfel",
      "Wolf"
    ],
    "summary": "Image goal navigation requires two different skills: firstly, core navigation skills, including the detection of free space and obstacles, and taking decisions based on an internal representation; and secondly, computing directional information by comparing visual observations to the goal image. Current state-of-the-art methods either rely on dedicated image-matching, or pre-training of computer vision modules on relative pose estimation. In this paper, we study whether this task can be efficiently solved with end-to-end training of full agents with RL, as has been claimed by recent work. A positive answer would have impact beyond Embodied AI and allow training of relative pose estimation from reward for navigation alone. In a large study we investigate the effect of architectural choices like late fusion, channel stacking, space-to-depth projections and cross-attention, and their role in the emergence of relative pose estimators from navigation training. We show that the success of recent methods is influenced up to a certain extent by simulator settings, leading to shortcuts in simulation. However, we also show that these capabilities can be transferred to more realistic setting, up to some extend. We also find evidence for correlations between navigation performance and probed (emerging) relative pose estimation performance, an important sub skill.",
    "pdf_url": "https://arxiv.org/pdf/2507.01667v1",
    "github_url": null,
    "published": "2025-07-02T12:50:26+00:00",
    "updated": "2025-07-02T12:50:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.01125v1",
    "title": "VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online Semantic Gaussian Splatting",
    "authors": [
      "Nagami",
      "Chen",
      "Yu"
    ],
    "summary": "We present VISTA (Viewpoint-based Image selection with Semantic Task Awareness), an active exploration method for robots to plan informative trajectories that improve 3D map quality in areas most relevant for task completion. Given an open-vocabulary search instruction (e.g., \"find a person\"), VISTA enables a robot to explore its environment to search for the object of interest, while simultaneously building a real-time semantic 3D Gaussian Splatting reconstruction of the scene. The robot navigates its environment by planning receding-horizon trajectories that prioritize semantic similarity to the query and exploration of unseen regions of the environment. To evaluate trajectories, VISTA introduces a novel, efficient viewpoint-semantic coverage metric that quantifies both the geometric view diversity and task relevance in the 3D scene. On static datasets, our coverage metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in computation speed and reconstruction quality. In quadrotor hardware experiments, VISTA achieves 6x higher success rates in challenging maps, compared to baseline methods, while matching baseline performance in less challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying it on a quadrotor drone and a Spot quadruped robot. Open-source code will be released upon acceptance of the paper.",
    "pdf_url": "https://arxiv.org/pdf/2507.01125v1",
    "github_url": null,
    "published": "2025-07-01T18:35:05+00:00",
    "updated": "2025-07-01T18:35:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.23468v2",
    "title": "NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments",
    "authors": [
      "Yao",
      "Gao",
      "Xu"
    ],
    "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to execute sequential navigation actions in complex environments guided by natural language instructions. Current approaches often struggle with generalizing to novel environments and adapting to ongoing changes during navigation. Inspired by human cognition, we present NavMorph, a self-evolving world model framework that enhances environmental understanding and decision-making in VLN-CE tasks. NavMorph employs compact latent representations to model environmental dynamics, equipping agents with foresight for adaptive planning and policy refinement. By integrating a novel Contextual Evolution Memory, NavMorph leverages scene-contextual information to support effective navigation while maintaining online adaptability. Extensive experiments demonstrate that our method achieves notable performance improvements on popular VLN-CE benchmarks. Code is available at https://github.com/Feliciaxyao/NavMorph.",
    "pdf_url": "https://arxiv.org/pdf/2506.23468v2",
    "github_url": "https://github.com/Feliciaxyao/NavMorph",
    "published": "2025-06-30T02:20:00+00:00",
    "updated": "2025-07-22T03:03:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.20376v1",
    "title": "Enhanced Robotic Navigation in Deformable Environments using Learning from Demonstration and Dynamic Modulation",
    "authors": [
      "Chen",
      "Zhao",
      "Campanha"
    ],
    "summary": "This paper presents a novel approach for robot navigation in environments containing deformable obstacles. By integrating Learning from Demonstration (LfD) with Dynamical Systems (DS), we enable adaptive and efficient navigation in complex environments where obstacles consist of both soft and hard regions. We introduce a dynamic modulation matrix within the DS framework, allowing the system to distinguish between traversable soft regions and impassable hard areas in real-time, ensuring safe and flexible trajectory planning. We validate our method through extensive simulations and robot experiments, demonstrating its ability to navigate deformable environments. Additionally, the approach provides control over both trajectory and velocity when interacting with deformable objects, including at intersections, while maintaining adherence to the original DS trajectory and dynamically adapting to obstacles for smooth and reliable navigation.",
    "pdf_url": "https://arxiv.org/pdf/2506.20376v1",
    "github_url": null,
    "published": "2025-06-25T12:40:27+00:00",
    "updated": "2025-06-25T12:40:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.20320v2",
    "title": "Finding the Easy Way Through -- the Probabilistic Gap Planner for Social Robot Navigation",
    "authors": [
      "Probst",
      "Wenzel",
      "Puphal"
    ],
    "summary": "In Social Robot Navigation, autonomous agents need to resolve many sequential interactions with other agents. State-of-the art planners can efficiently resolve the next, imminent interaction cooperatively and do not focus on longer planning horizons. This makes it hard to maneuver scenarios where the agent needs to select a good strategy to find gaps or channels in the crowd. We propose to decompose trajectory planning into two separate steps: Conflict avoidance for finding good, macroscopic trajectories, and cooperative collision avoidance (CCA) for resolving the next interaction optimally. We propose the Probabilistic Gap Planner (PGP) as a conflict avoidance planner. PGP modifies an established probabilistic collision risk model to include a general assumption of cooperativity. PGP biases the short-term CCA planner to head towards gaps in the crowd. In extensive simulations with crowds of varying density, we show that using PGP in addition to state-of-the-art CCA planners improves the agents' performance: On average, agents keep more space to others, create less tension, and cause fewer collisions. This typically comes at the expense of slightly longer paths. PGP runs in real-time on WaPOCHI mobile robot by Honda R&D.",
    "pdf_url": "https://arxiv.org/pdf/2506.20320v2",
    "github_url": null,
    "published": "2025-06-25T11:01:51+00:00",
    "updated": "2025-06-26T06:26:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.19433v2",
    "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System",
    "authors": [
      "He",
      "Dong",
      "Chen"
    ],
    "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.",
    "pdf_url": "https://arxiv.org/pdf/2506.19433v2",
    "github_url": "https://github.com/tsinghua-fib-lab/Mem4Nav",
    "published": "2025-06-24T09:00:43+00:00",
    "updated": "2025-10-10T13:08:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.17960v2",
    "title": "GeNIE: A Generalizable Navigation System for In-the-Wild Environments",
    "authors": [
      "Wang",
      "Liu",
      "Chen"
    ],
    "summary": "Reliable navigation in unstructured, real-world environments remains a significant challenge for embodied agents, especially when operating across diverse terrains, weather conditions, and sensor configurations. In this paper, we introduce GeNIE (Generalizable Navigation System for In-the-Wild Environments), a robust navigation framework designed for global deployment. GeNIE integrates a generalizable traversability prediction model built on SAM2 with a novel path fusion strategy that enhances planning stability in noisy and ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at ICRA 2025, where it was evaluated across six countries spanning three continents. GeNIE took first place and achieved 79% of the maximum possible score, outperforming the second-best team by 17%, and completed the entire competition without a single human intervention. These results set a new benchmark for robust, generalizable outdoor robot navigation. We will release the codebase, pretrained model weights, and newly curated datasets to support future research in real-world navigation.",
    "pdf_url": "https://arxiv.org/pdf/2506.17960v2",
    "github_url": null,
    "published": "2025-06-22T09:36:05+00:00",
    "updated": "2025-10-18T11:13:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.17221v2",
    "title": "VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning",
    "authors": [
      "Qi",
      "Zhang",
      "Yu"
    ],
    "summary": "Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation systems operate on discrete topological graphs, limiting path planning to predefined node connections. We propose VLN-R1, an end-to-end framework that leverages Large Vision-Language Models (LVLM) to directly translate egocentric video streams into continuous navigation actions, adopting GRPO-based training inspired by DeepSeek-R1. To enable effective training, we first construct the VLN-Ego dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling to balance historical and current observations. While large language models can supervise complete textual instructions, they lack fine-grained action-level control. Our framework employs a two-stage training approach: a) Supervised fine-tuning (SFT) to align the model's action sequence text predictions with expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced with a Time-Decayed Reward (TDR) mechanism that strategically weights multi-step future actions. Experimental results show VLN-R1 achieves strong performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied navigation and enhance task-specific reasoning through data-efficient, reward-driven post-training.",
    "pdf_url": "https://arxiv.org/pdf/2506.17221v2",
    "github_url": null,
    "published": "2025-06-20T17:59:59+00:00",
    "updated": "2025-06-25T06:03:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.16623v1",
    "title": "History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation",
    "authors": [
      "Habibpour",
      "Afghah"
    ],
    "summary": "Object Goal Navigation (ObjectNav) challenges robots to find objects in unseen environments, demanding sophisticated reasoning. While Vision-Language Models (VLMs) show potential, current ObjectNav methods often employ them superficially, primarily using vision-language embeddings for object-scene similarity checks rather than leveraging deeper reasoning. This limits contextual understanding and leads to practical issues like repetitive navigation behaviors. This paper introduces a novel zero-shot ObjectNav framework that pioneers the use of dynamic, history-aware prompting to more deeply integrate VLM reasoning into frontier-based exploration. Our core innovation lies in providing the VLM with action history context, enabling it to generate semantic guidance scores for navigation actions while actively avoiding decision loops. We also introduce a VLM-assisted waypoint generation mechanism for refining the final approach to detected objects. Evaluated on the HM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and 24.8% Success weighted by Path Length (SPL). These results are comparable to state-of-the-art zero-shot methods, demonstrating the significant potential of our history-augmented VLM prompting strategy for more robust and context-aware robotic navigation.",
    "pdf_url": "https://arxiv.org/pdf/2506.16623v1",
    "github_url": null,
    "published": "2025-06-19T21:50:16+00:00",
    "updated": "2025-06-19T21:50:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.15377v1",
    "title": "Efficient and Generalizable Environmental Understanding for Visual Navigation",
    "authors": [
      "Wang",
      "Li",
      "Wang"
    ],
    "summary": "Visual Navigation is a core task in Embodied AI, enabling agents to navigate complex environments toward given objectives. Across diverse settings within Navigation tasks, many necessitate the modelling of sequential data accumulated from preceding time steps. While existing methods perform well, they typically process all historical observations simultaneously, overlooking the internal association structure within the data, which may limit the potential for further improvements in task performance. We address this by examining the unique characteristics of Navigation tasks through the lens of causality, introducing a causal framework to highlight the limitations of conventional sequential methods. Leveraging this insight, we propose Causality-Aware Navigation (CAN), which incorporates a Causal Understanding Module to enhance the agent's environmental understanding capability. Empirical evaluations show that our approach consistently outperforms baselines across various tasks and simulation environments. Extensive ablations studies attribute these gains to the Causal Understanding Module, which generalizes effectively in both Reinforcement and Supervised Learning settings without computational overhead.",
    "pdf_url": "https://arxiv.org/pdf/2506.15377v1",
    "github_url": null,
    "published": "2025-06-18T11:47:02+00:00",
    "updated": "2025-06-18T11:47:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.15757v1",
    "title": "Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation",
    "authors": [
      "Wang",
      "Yu",
      "Wu"
    ],
    "summary": "Visual Language Navigation (VLN) is a fundamental task within the field of Embodied AI, focusing on the ability of agents to navigate complex environments based on natural language instructions. Despite the progress made by existing methods, these methods often present some common challenges. First, they rely on pre-trained backbone models for visual perception, which struggle with the dynamic viewpoints in VLN scenarios. Second, the performance is limited when using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results, their computational costs are higher than those without fine-tuning. To address these limitations, we propose Weakly-supervised Partial Contrastive Learning (WPCL), a method that enhances an agent's ability to identify objects from dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM knowledge into the perception process, without requiring VLM fine-tuning. Our method enhances the agent's ability to interpret and respond to environmental cues while ensuring computational efficiency. Experimental results have shown that our method outperforms the baseline methods on multiple benchmarks, which validate the effectiveness, robustness and generalizability of our method.",
    "pdf_url": "https://arxiv.org/pdf/2506.15757v1",
    "github_url": null,
    "published": "2025-06-18T11:43:50+00:00",
    "updated": "2025-06-18T11:43:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.15096v1",
    "title": "DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory",
    "authors": [
      "Ji",
      "Lin",
      "Gao"
    ],
    "summary": "We present DyNaVLM, an end-to-end vision-language navigation framework using Vision-Language Models (VLM). In contrast to prior methods constrained by fixed angular or distance intervals, our system empowers agents to freely select navigation targets via visual-language reasoning. At its core lies a self-refining graph memory that 1) stores object locations as executable topological relations, 2) enables cross-robot memory sharing through distributed graph updates, and 3) enhances VLM's decision-making via retrieval augmentation. Operating without task-specific training or fine-tuning, DyNaVLM demonstrates high performance on GOAT and ObjectNav benchmarks. Real-world tests further validate its robustness and generalization. The system's three innovations: dynamic action space formulation, collaborative graph memory, and training-free deployment, establish a new paradigm for scalable embodied robot, bridging the gap between discrete VLN tasks and continuous real-world navigation.",
    "pdf_url": "https://arxiv.org/pdf/2506.15096v1",
    "github_url": null,
    "published": "2025-06-18T03:06:01+00:00",
    "updated": "2025-06-18T03:06:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.14507v1",
    "title": "Can Pretrained Vision-Language Embeddings Alone Guide Robot Navigation?",
    "authors": [
      "Subedi",
      "Haroon",
      "Ganguly"
    ],
    "summary": "Foundation models have revolutionized robotics by providing rich semantic representations without task-specific training. While many approaches integrate pretrained vision-language models (VLMs) with specialized navigation architectures, the fundamental question remains: can these pretrained embeddings alone successfully guide navigation without additional fine-tuning or specialized modules? We present a minimalist framework that decouples this question by training a behavior cloning policy directly on frozen vision-language embeddings from demonstrations collected by a privileged expert. Our approach achieves a 74% success rate in navigation to language-specified targets, compared to 100% for the state-aware expert, though requiring 3.2 times more steps on average. This performance gap reveals that pretrained embeddings effectively support basic language grounding but struggle with long-horizon planning and spatial reasoning. By providing this empirical baseline, we highlight both the capabilities and limitations of using foundation models as drop-in representations for embodied tasks, offering critical insights for robotics researchers facing practical design tradeoffs between system complexity and performance in resource-constrained scenarios. Our code is available at https://github.com/oadamharoon/text2nav",
    "pdf_url": "https://arxiv.org/pdf/2506.14507v1",
    "github_url": "https://github.com/oadamharoon/text2nav",
    "published": "2025-06-17T13:31:05+00:00",
    "updated": "2025-06-17T13:31:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.13953v1",
    "title": "Socially-aware Object Transportation by a Mobile Manipulator in Static Planar Environments with Obstacles",
    "authors": [
      "Ribeiro",
      "Paes",
      "Macharet"
    ],
    "summary": "Socially-aware robotic navigation is essential in environments where humans and robots coexist, ensuring both safety and comfort. However, most existing approaches have been primarily developed for mobile robots, leaving a significant gap in research that addresses the unique challenges posed by mobile manipulators. In this paper, we tackle the challenge of navigating a robotic mobile manipulator, carrying a non-negligible load, within a static human-populated environment while adhering to social norms. Our goal is to develop a method that enables the robot to simultaneously manipulate an object and navigate between locations in a socially-aware manner. We propose an approach based on the Risk-RRT* framework that enables the coordinated actuation of both the mobile base and manipulator. This approach ensures collision-free navigation while adhering to human social preferences. We compared our approach in a simulated environment to socially-aware mobile-only methods applied to a mobile manipulator. The results highlight the necessity for mobile manipulator-specific techniques, with our method outperforming mobile-only approaches. Our method enabled the robot to navigate, transport an object, avoid collisions, and minimize social discomfort effectively.",
    "pdf_url": "https://arxiv.org/pdf/2506.13953v1",
    "github_url": null,
    "published": "2025-06-16T19:45:30+00:00",
    "updated": "2025-06-16T19:45:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.14831v2",
    "title": "Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review",
    "authors": [
      "Finet",
      "Martins",
      "Hayet"
    ],
    "summary": "With the emergence of powerful data-driven methods in human trajectory prediction (HTP), gaining a finer understanding of multi-agent interactions lies within hand's reach, with important implications in areas such as social robot navigation, autonomous navigation, and crowd modeling. This survey reviews some of the most recent advancements in deep learning-based multi-agent trajectory prediction, focusing on studies published between 2020 and 2025. We categorize the existing methods based on their architectural design, their input representations, and their overall prediction strategies, placing a particular emphasis on models evaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges and future research directions in the field of multi-agent HTP.",
    "pdf_url": "https://arxiv.org/pdf/2506.14831v2",
    "github_url": null,
    "published": "2025-06-13T23:03:43+00:00",
    "updated": "2025-12-16T10:00:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.10875v1",
    "title": "Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material",
    "authors": [
      "Wang",
      "Zhao",
      "Azarm"
    ],
    "summary": "An alternative data-driven modeling approach has been proposed and employed to gain fundamental insights into robot motion interaction with granular terrain at certain length scales. The approach is based on an integration of dimension reduction (Sequentially Truncated Higher-Order Singular Value Decomposition), surrogate modeling (Gaussian Process), and data assimilation techniques (Reduced Order Particle Filter). This approach can be used online and is based on offline data, obtained from the offline collection of high-fidelity simulation data and a set of sparse experimental data. The results have shown that orders of magnitude reduction in computational time can be obtained from the proposed data-driven modeling approach compared with physics-based high-fidelity simulations. With only simulation data as input, the data-driven prediction technique can generate predictions that have comparable accuracy as simulations. With both simulation data and sparse physical experimental measurement as input, the data-driven approach with its embedded data assimilation techniques has the potential in outperforming only high-fidelity simulations for the long-horizon predictions. In addition, it is demonstrated that the data-driven modeling approach can also reproduce the scaling relationship recovered by physics-based simulations for maximum resistive forces, which may indicate its general predictability beyond a case-by-case basis. The results are expected to help robot navigation and exploration in unknown and complex terrains during both online and offline phases.",
    "pdf_url": "https://arxiv.org/pdf/2506.10875v1",
    "github_url": null,
    "published": "2025-06-12T16:43:21+00:00",
    "updated": "2025-06-12T16:43:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.10756v1",
    "title": "Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding",
    "authors": [
      "Zhang",
      "Yu",
      "Xiao"
    ],
    "summary": "Vision-and-language navigation (VLN) is a long-standing challenge in autonomous robotics, aiming to empower agents with the ability to follow human instructions while navigating complex environments. Two key bottlenecks remain in this field: generalization to out-of-distribution environments and reliance on fixed discrete action spaces. To address these challenges, we propose Vision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles (UAVs) to execute language-guided flight. Without the requirement for localization or active ranging sensors, VLFly outputs continuous velocity commands purely from egocentric observations captured by an onboard monocular camera. The VLFly integrates three modules: an instruction encoder based on a large language model (LLM) that reformulates high-level language into structured prompts, a goal retriever powered by a vision-language model (VLM) that matches these prompts to goal images via vision-language similarity, and a waypoint planner that generates executable trajectories for real-time UAV control. VLFly is evaluated across diverse simulation environments without additional fine-tuning and consistently outperforms all baselines. Moreover, real-world VLN tasks in indoor and outdoor environments under direct and indirect instructions demonstrate that VLFly achieves robust open-vocabulary goal understanding and generalized navigation capabilities, even in the presence of abstract language input.",
    "pdf_url": "https://arxiv.org/pdf/2506.10756v1",
    "github_url": null,
    "published": "2025-06-12T14:40:50+00:00",
    "updated": "2025-06-12T14:40:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.10383v1",
    "title": "RICE: Reactive Interaction Controller for Cluttered Canopy Environment",
    "authors": [
      "Parayil",
      "Peynot",
      "Lehnert"
    ],
    "summary": "Robotic navigation in dense, cluttered environments such as agricultural canopies presents significant challenges due to physical and visual occlusion caused by leaves and branches. Traditional vision-based or model-dependent approaches often fail in these settings, where physical interaction without damaging foliage and branches is necessary to reach a target. We present a novel reactive controller that enables safe navigation for a robotic arm in a contact-rich, cluttered, deformable environment using end-effector position and real-time tactile feedback. Our proposed framework's interaction strategy is based on a trade-off between minimizing disturbance by maneuvering around obstacles and pushing through them to move towards the target. We show that over 35 trials in 3 experimental plant setups with an occluded target, the proposed controller successfully reached the target in all trials without breaking any branch and outperformed the state-of-the-art model-free controller in robustness and adaptability. This work lays the foundation for safe, adaptive interaction in cluttered, contact-rich deformable environments, enabling future agricultural tasks such as pruning and harvesting in plant canopies.",
    "pdf_url": "https://arxiv.org/pdf/2506.10383v1",
    "github_url": null,
    "published": "2025-06-12T06:19:22+00:00",
    "updated": "2025-06-12T06:19:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.10172v1",
    "title": "A Navigation Framework Utilizing Vision-Language Models",
    "authors": [
      "Duan",
      "tang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) presents a complex challenge in embodied AI, requiring agents to interpret natural language instructions and navigate through visually rich, unfamiliar environments. Recent advances in large vision-language models (LVLMs), such as CLIP and Flamingo, have significantly improved multimodal understanding but introduced new challenges related to computational cost and real-time deployment. In this project, we propose a modular, plug-and-play navigation framework that decouples vision-language understanding from action planning. By integrating a frozen vision-language model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to achieve flexible, fast, and adaptable navigation without extensive model fine-tuning. Our framework leverages prompt engineering, structured history management, and a two-frame visual input strategy to enhance decision-making continuity across navigation steps. We evaluate our system on the Room-to-Room benchmark within the VLN-CE setting using the Matterport3D dataset and Habitat-Lab simulation environment. Although our initial results reveal challenges in generalizing to unseen environments under strict evaluation settings, our modular approach lays a foundation for scalable and efficient navigation systems, highlighting promising directions for future improvement through enhanced environmental priors and expanded multimodal input integration.",
    "pdf_url": "https://arxiv.org/pdf/2506.10172v1",
    "github_url": null,
    "published": "2025-06-11T20:51:58+00:00",
    "updated": "2025-06-11T20:51:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.09839v1",
    "title": "OctoNav: Towards Generalist Embodied Navigation",
    "authors": [
      "Gao",
      "Jin",
      "Peng"
    ],
    "summary": "Embodied navigation stands as a foundation pillar within the broader pursuit of embodied AI. However, previous navigation research is divided into different tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task objectives and modalities, making datasets and methods are designed individually. In this work, we take steps toward generalist navigation agents, which can follow free-form instructions that include arbitrary compounds of multi-modal and multi-capability. To achieve this, we propose a large-scale benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1. Specifically, OctoNav-Bench features continuous environments and is constructed via a designed annotation pipeline. We thoroughly craft instruction-trajectory pairs, where instructions are diverse in free-form with arbitrary modality and capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1, we build it upon MLLMs and adapt it to a VLA-type model, which can produce low-level actions solely based on 2D visual observations. Moreover, we design a Hybrid Training Paradigm (HTP) that consists of three stages, i.e., Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains specifically designed learning policies and rewards. Importantly, for TBA-SFT and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which show impressive reasoning ability via thinking-before-answer. Thus, we aim to investigate how to achieve thinking-before-action in the embodied navigation field, to improve model's reasoning ability toward generalists. Specifically, we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a cold-start phrase and then leverage Nav-GPRO to improve its thinking ability. Finally, OctoNav-R1 shows superior performance compared with previous methods.",
    "pdf_url": "https://arxiv.org/pdf/2506.09839v1",
    "github_url": null,
    "published": "2025-06-11T15:15:17+00:00",
    "updated": "2025-06-11T15:15:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.09581v1",
    "title": "Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities",
    "authors": [
      "Gonzlez-Santamarta",
      "Rodrguez-Lera",
      "Sobrn-Hidalgo"
    ],
    "summary": "Large Language Models (LLMs) have experienced great advancements in the last year resulting in an increase of these models in several fields to face natural language tasks. The integration of these models in robotics can also help to improve several aspects such as human-robot interaction, navigation, planning and decision-making. Therefore, this paper introduces llama\\_ros, a tool designed to integrate quantized Large Language Models (LLMs) into robotic systems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine, llama\\_ros enables the efficient execution of quantized LLMs as edge artificial intelligence (AI) in robotics systems with resource-constrained environments, addressing the challenges of computational efficiency and memory limitations. By deploying quantized LLMs, llama\\_ros empowers robots to leverage the natural language understanding and generation for enhanced decision-making and interaction which can be paired with prompt engineering, knowledge graphs, ontologies or other tools to improve the capabilities of autonomous robots. Additionally, this paper provides insights into some use cases of using llama\\_ros for planning and explainability in robotics.",
    "pdf_url": "https://arxiv.org/pdf/2506.09581v1",
    "github_url": null,
    "published": "2025-06-11T10:19:49+00:00",
    "updated": "2025-06-11T10:19:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.08566v1",
    "title": "Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations",
    "authors": [
      "Cui",
      "Xie",
      "Zhao"
    ],
    "summary": "Vision-Language Navigation (VLN) enables intelligent agents to navigate environments by integrating visual perception and natural language instructions, yet faces significant challenges due to the scarcity of fine-grained cross-modal alignment annotations. Existing datasets primarily focus on global instruction-trajectory matching, neglecting sub-instruction-level and entity-level alignments critical for accurate navigation action decision-making. To address this limitation, we propose FCA-NIG, a generative framework that automatically constructs navigation instructions with dual-level fine-grained cross-modal annotations. In this framework, an augmented trajectory is first divided into sub-trajectories, which are then processed through GLIP-based landmark detection, crafted instruction construction, OFA-Speaker based R2R-like instruction generation, and CLIP-powered entity selection, generating sub-instruction-trajectory pairs with entity-landmark annotations. Finally, these sub-pairs are aggregated to form a complete instruction-trajectory pair. The framework generates the FCA-R2R dataset, the first large-scale augmentation dataset featuring precise sub-instruction-sub-trajectory and entity-landmark alignments. Extensive experiments demonstrate that training with FCA-R2R significantly improves the performance of multiple state-of-the-art VLN agents, including SF, EnvDrop, RecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances agents' state awareness and decision accuracy, while entity-landmark alignment further boosts navigation performance and generalization. These results highlight the effectiveness of FCA-NIG in generating high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in complex navigation tasks.",
    "pdf_url": "https://arxiv.org/pdf/2506.08566v1",
    "github_url": null,
    "published": "2025-06-10T08:36:51+00:00",
    "updated": "2025-06-10T08:36:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.08002v1",
    "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
    "authors": [
      "Sahoo",
      "Tibrewal",
      "Gkioxari"
    ],
    "summary": "Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/",
    "pdf_url": "https://arxiv.org/pdf/2506.08002v1",
    "github_url": null,
    "published": "2025-06-09T17:59:37+00:00",
    "updated": "2025-06-09T17:59:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.07570v2",
    "title": "OptiScene: LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization",
    "authors": [
      "Yang",
      "Luo",
      "Ding"
    ],
    "summary": "Automatic indoor layout generation has attracted increasing attention due to its potential in interior design, virtual environment construction, and embodied AI. Existing methods fall into two categories: prompt-driven approaches that leverage proprietary LLM services (e.g., GPT APIs) and learning-based methods trained on layout data upon diffusion-based models. Prompt-driven methods often suffer from spatial inconsistency and high computational costs, while learning-based methods are typically constrained by coarse relational graphs and limited datasets, restricting their generalization to diverse room categories. In this paper, we revisit LLM-based indoor layout generation and present 3D-SynthPlace, a large-scale dataset that combines synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline, upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000 scenes, covering four common room types -- bedroom, living room, kitchen, and bathroom -- enriched with diverse objects and high-level spatial annotations. We further introduce OptiScene, a strong open-source LLM optimized for indoor layout generation, fine-tuned based on our 3D-SynthPlace dataset through our two-stage training. For the warum-up stage I, we adopt supervised fine-tuning (SFT), which is taught to first generate high-level spatial descriptions then conditionally predict concrete object placements. For the reinforcing stage II, to better align the generated layouts with human design preferences, we apply multi-turn direct preference optimization (DPO), which significantly improving layout quality and generation success rates. Extensive experiments demonstrate that OptiScene outperforms traditional prompt-driven and learning-based baselines. Moreover, OptiScene shows promising potential in interactive tasks such as scene editing and robot navigation.",
    "pdf_url": "https://arxiv.org/pdf/2506.07570v2",
    "github_url": null,
    "published": "2025-06-09T09:13:06+00:00",
    "updated": "2025-09-19T09:25:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.07164v1",
    "title": "Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs",
    "authors": [
      "Chang",
      "Chen",
      "Li"
    ],
    "summary": "The visual-based SLAM (Simultaneous Localization and Mapping) is a technology widely used in applications such as robotic navigation and virtual reality, which primarily focuses on detecting feature points from visual images to construct an unknown environmental map and simultaneously determines its own location. It usually imposes stringent requirements on hardware power consumption, processing speed and accuracy. Currently, the ORB (Oriented FAST and Rotated BRIEF)-based SLAM systems have exhibited superior performance in terms of processing speed and robustness. However, they still fall short of meeting the demands for real-time processing on mobile platforms. This limitation is primarily due to the time-consuming Oriented FAST calculations accounting for approximately half of the entire SLAM system. This paper presents two methods to accelerate the Oriented FAST feature detection on low-end embedded GPUs. These methods optimize the most time-consuming steps in Oriented FAST feature detection: FAST feature point detection and Harris corner detection, which is achieved by implementing a binary-level encoding strategy to determine candidate points quickly and a separable Harris detection strategy with efficient low-level GPU hardware-specific instructions. Extensive experiments on a Jetson TX2 embedded GPU demonstrate an average speedup of over 7.3 times compared to widely used OpenCV with GPU support. This significant improvement highlights its effectiveness and potential for real-time applications in mobile and resource-constrained environments.",
    "pdf_url": "https://arxiv.org/pdf/2506.07164v1",
    "github_url": null,
    "published": "2025-06-08T14:30:30+00:00",
    "updated": "2025-06-08T14:30:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.06862v1",
    "title": "Multimodal Spatial Language Maps for Robot Navigation and Manipulation",
    "authors": [
      "Huang",
      "Mees",
      "Zeng"
    ],
    "summary": "Grounding language to a navigating agent's observations can leverage pretrained multimodal foundation models to match perceptions to object or event descriptions. However, previous approaches remain disconnected from environment mapping, lack the spatial precision of geometric maps, or neglect additional modality information beyond vision. To address this, we propose multimodal spatial language maps as a spatial map representation that fuses pretrained multimodal features with a 3D reconstruction of the environment. We build these maps autonomously using standard exploration. We present two instances of our maps, which are visual-language maps (VLMaps) and their extension to audio-visual-language maps (AVLMaps) obtained by adding audio information. When combined with large language models (LLMs), VLMaps can (i) translate natural language commands into open-vocabulary spatial goals (e.g., \"in between the sofa and TV\") directly localized in the map, and (ii) be shared across different robot embodiments to generate tailored obstacle maps on demand. Building upon the capabilities above, AVLMaps extend VLMaps by introducing a unified 3D spatial representation integrating audio, visual, and language cues through the fusion of features from pretrained multimodal foundation models. This enables robots to ground multimodal goal queries (e.g., text, images, or audio snippets) to spatial locations for navigation. Additionally, the incorporation of diverse sensory inputs significantly enhances goal disambiguation in ambiguous environments. Experiments in simulation and real-world settings demonstrate that our multimodal spatial language maps enable zero-shot spatial and multimodal goal navigation and improve recall by 50% in ambiguous scenarios. These capabilities extend to mobile robots and tabletop manipulators, supporting navigation and interaction guided by visual, audio, and spatial cues.",
    "pdf_url": "https://arxiv.org/pdf/2506.06862v1",
    "github_url": null,
    "published": "2025-06-07T17:02:13+00:00",
    "updated": "2025-06-07T17:02:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.06804v1",
    "title": "IRS: Instance-Level 3D Scene Graphs via Room Prior Guided LiDAR-Camera Fusion",
    "authors": [
      "Chen",
      "Lin",
      "Li"
    ],
    "summary": "Indoor scene understanding remains a fundamental challenge in robotics, with direct implications for downstream tasks such as navigation and manipulation. Traditional approaches often rely on closed-set recognition or loop closure, limiting their adaptability in open-world environments. With the advent of visual foundation models (VFMs), open-vocabulary recognition and natural language querying have become feasible, unlocking new possibilities for 3D scene graph construction.   In this paper, we propose a robust and efficient framework for instance-level 3D scene graph construction via LiDAR-camera fusion. Leveraging LiDAR's wide field of view (FOV) and long-range sensing capabilities, we rapidly acquire room-level geometric priors. Multi-level VFMs are employed to improve the accuracy and consistency of semantic extraction. During instance fusion, room-based segmentation enables parallel processing, while the integration of geometric and semantic cues significantly enhances fusion accuracy and robustness. Compared to state-of-the-art methods, our approach achieves up to an order-of-magnitude improvement in construction speed while maintaining high semantic precision.   Extensive experiments in both simulated and real-world environments validate the effectiveness of our approach. We further demonstrate its practical value through a language-guided semantic navigation task, highlighting its potential for real-world robotic applications.",
    "pdf_url": "https://arxiv.org/pdf/2506.06804v1",
    "github_url": null,
    "published": "2025-06-07T13:55:34+00:00",
    "updated": "2025-06-07T13:55:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.06630v1",
    "title": "Active Test-time Vision-Language Navigation",
    "authors": [
      "Ko",
      "Kim",
      "Oh"
    ],
    "summary": "Vision-Language Navigation (VLN) policies trained on offline datasets often exhibit degraded task performance when deployed in unfamiliar navigation environments at test time, where agents are typically evaluated without access to external interaction or feedback. Entropy minimization has emerged as a practical solution for reducing prediction uncertainty at test time; however, it can suffer from accumulated errors, as agents may become overconfident in incorrect actions without sufficient contextual grounding. To tackle these challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time active learning framework that enables a practical human-robot interaction via episodic feedback on uncertain navigation outcomes. In particular, ATENA learns to increase certainty in successful episodes and decrease it in failed ones, improving uncertainty calibration. Here, we propose mixture entropy optimization, where entropy is obtained from a combination of the action and pseudo-expert distributions-a hypothetical action distribution assuming the agent's selected action to be optimal-controlling both prediction confidence and action preference. In addition, we propose a self-active learning strategy that enables an agent to evaluate its navigation outcomes based on confident predictions. As a result, the agent stays actively engaged throughout all iterations, leading to well-grounded and adaptive decision-making. Extensive evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate that ATENA successfully overcomes distributional shifts at test time, outperforming the compared baseline methods across various settings.",
    "pdf_url": "https://arxiv.org/pdf/2506.06630v1",
    "github_url": null,
    "published": "2025-06-07T02:24:44+00:00",
    "updated": "2025-06-07T02:24:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.05896v1",
    "title": "Object Navigation with Structure-Semantic Reasoning-Based Multi-level Map and Multimodal Decision-Making LLM",
    "authors": [
      "Yan",
      "He",
      "Li"
    ],
    "summary": "The zero-shot object navigation (ZSON) in unknown open-ended environments coupled with semantically novel target often suffers from the significant decline in performance due to the neglect of high-dimensional implicit scene information and the long-range target searching task. To address this, we proposed an active object navigation framework with Environmental Attributes Map (EAM) and MLLM Hierarchical Reasoning module (MHR) to improve its success rate and efficiency. EAM is constructed by reasoning observed environments with SBERT and predicting unobserved ones with Diffusion, utilizing human space regularities that underlie object-room correlations and area adjacencies. MHR is inspired by EAM to perform frontier exploration decision-making, avoiding the circuitous trajectories in long-range scenarios to improve path efficiency. Experimental results demonstrate that the EAM module achieves 64.5\\% scene mapping accuracy on MP3D dataset, while the navigation task attains SPLs of 28.4\\% and 26.3\\% on HM3D and MP3D benchmarks respectively - representing absolute improvements of 21.4\\% and 46.0\\% over baseline methods.",
    "pdf_url": "https://arxiv.org/pdf/2506.05896v1",
    "github_url": null,
    "published": "2025-06-06T09:08:40+00:00",
    "updated": "2025-06-06T09:08:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.05790v1",
    "title": "Discrete Minds in a Continuous World: Do Language Models Know Time Passes?",
    "authors": [
      "Wang",
      "Bai",
      "Vu"
    ],
    "summary": "While Large Language Models (LLMs) excel at temporal reasoning tasks like event ordering and duration estimation, their ability to perceive the actual passage of time remains unexplored. We investigate whether LLMs perceive the passage of time and adapt their decision-making accordingly through three complementary experiments. First, we introduce the Token-Time Hypothesis, positing that LLMs can map discrete token counts to continuous wall-clock time, and validate this through a dialogue duration judgment task. Second, we demonstrate that LLMs could use this awareness to adapt their response length while maintaining accuracy when users express urgency in question answering tasks. Finally, we develop BombRush, an interactive navigation challenge that examines how LLMs modify behavior under progressive time pressure in dynamic environments. Our findings indicate that LLMs possess certain awareness of time passage, enabling them to bridge discrete linguistic tokens and continuous physical time, though this capability varies with model size and reasoning abilities. This work establishes a theoretical foundation for enhancing temporal awareness in LLMs for time-sensitive applications.",
    "pdf_url": "https://arxiv.org/pdf/2506.05790v1",
    "github_url": null,
    "published": "2025-06-06T06:37:01+00:00",
    "updated": "2025-06-06T06:37:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.05020v3",
    "title": "Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System",
    "authors": [
      "Liu",
      "Ma",
      "Li"
    ],
    "summary": "Heterogeneous multirobot systems show great potential in complex tasks requiring coordinated hybrid cooperation. However, existing methods that rely on static or task-specific models often lack generalizability across diverse tasks and dynamic environments. This highlights the need for generalizable intelligence that can bridge high-level reasoning with low-level execution across heterogeneous agents. To address this, we propose a hierarchical multimodal framework that integrates a prompted large language model (LLM) with a fine-tuned vision-language model (VLM). At the system level, the LLM performs hierarchical task decomposition and constructs a global semantic map, while the VLM provides semantic perception and object localization, where the proposed GridMask significantly enhances the VLM's spatial accuracy for reliable fine-grained manipulation. The aerial robot leverages this global map to generate semantic paths and guide the ground robot's local navigation and manipulation, ensuring robust coordination even in target-absent or ambiguous scenarios. We validate the framework through extensive simulation and real-world experiments on long-horizon object arrangement tasks, demonstrating zero-shot adaptability, robust semantic navigation, and reliable manipulation in dynamic environments. To the best of our knowledge, this work presents the first heterogeneous aerial-ground robotic system that integrates VLM-based perception with LLM-driven reasoning for global high-level task planning and execution.",
    "pdf_url": "https://arxiv.org/pdf/2506.05020v3",
    "github_url": null,
    "published": "2025-06-05T13:27:41+00:00",
    "updated": "2025-10-27T04:26:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.04500v1",
    "title": "\"Don't Do That!\": Guiding Embodied Systems through Large Language Model-based Constraint Generation",
    "authors": [
      "Djuhera",
      "Seffo",
      "Asai"
    ],
    "summary": "Recent advancements in large language models (LLMs) have spurred interest in robotic navigation that incorporates complex spatial, mathematical, and conditional constraints from natural language into the planning problem. Such constraints can be informal yet highly complex, making it challenging to translate into a formal description that can be passed on to a planning algorithm. In this paper, we propose STPR, a constraint generation framework that uses LLMs to translate constraints (expressed as instructions on ``what not to do'') into executable Python functions. STPR leverages the LLM's strong coding capabilities to shift the problem description from language into structured and transparent code, thus circumventing complex reasoning and avoiding potential hallucinations. We show that these LLM-generated functions accurately describe even complex mathematical constraints, and apply them to point cloud representations with traditional search algorithms. Experiments in a simulated Gazebo environment show that STPR ensures full compliance across several constraints and scenarios, while having short runtimes. We also verify that STPR can be used with smaller, code-specific LLMs, making it applicable to a wide range of compact models at low inference cost.",
    "pdf_url": "https://arxiv.org/pdf/2506.04500v1",
    "github_url": null,
    "published": "2025-06-04T22:47:53+00:00",
    "updated": "2025-06-04T22:47:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.03709v1",
    "title": "AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives",
    "authors": [
      "Sikdar",
      "Gandhamal",
      "Sundaram"
    ],
    "summary": "Open-vocabulary semantic segmentation (OVSS) involves assigning labels to each pixel in an image based on textual descriptions, leveraging world models like CLIP. However, they encounter significant challenges in cross-domain generalization, hindering their practical efficacy in real-world applications. Embodied AI systems are transforming autonomous navigation for ground vehicles and drones by enhancing their perception abilities, and in this study, we present AetherVision-Bench, a benchmark for multi-angle segmentation across aerial, and ground perspectives, which facilitates an extensive evaluation of performance across different viewing angles and sensor modalities. We assess state-of-the-art OVSS models on the proposed benchmark and investigate the key factors that impact the performance of zero-shot transfer models. Our work pioneers the creation of a robustness benchmark, offering valuable insights and establishing a foundation for future research.",
    "pdf_url": "https://arxiv.org/pdf/2506.03709v1",
    "github_url": null,
    "published": "2025-06-04T08:41:19+00:00",
    "updated": "2025-06-04T08:41:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.03642v2",
    "title": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data",
    "authors": [
      "Zhang",
      "Liu",
      "Li"
    ],
    "summary": "Visual-spatial understanding, the ability to infer object relationships and layouts from visual input, is fundamental to downstream tasks such as robotic navigation and embodied interaction. However, existing methods face spatial uncertainty and data scarcity, limiting the 3D spatial reasoning capability of pre-trained vision-language models (VLMs). To address these challenges, we present a unified framework for enhancing 3D spatial reasoning in pre-trained VLMs without modifying their architecture. This framework combines SpatialMind, a structured prompting strategy that decomposes complex scenes and questions into interpretable reasoning steps, with ScanForgeQA, a scalable question-answering dataset built from diverse 3D simulation scenes through an automated construction process designed for fine-tuning. Extensive experiments across multiple benchmarks demonstrate the individual and combined effectiveness of our prompting and fine-tuning strategies, and yield insights that may inspire future research on visual-spatial understanding.",
    "pdf_url": "https://arxiv.org/pdf/2506.03642v2",
    "github_url": null,
    "published": "2025-06-04T07:36:33+00:00",
    "updated": "2025-09-19T05:48:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.03516v1",
    "title": "SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models",
    "authors": [
      "Debnath",
      "Stein",
      "Kosecka"
    ],
    "summary": "Object goal navigation is a fundamental task in embodied AI, where an agent is instructed to locate a target object in an unexplored environment. Traditional learning-based methods rely heavily on large-scale annotated data or require extensive interaction with the environment in a reinforcement learning setting, often failing to generalize to novel environments and limiting scalability. To overcome these challenges, we explore a zero-shot setting where the agent operates without task-specific training, enabling more scalable and adaptable solution. Recent advances in Vision Foundation Models (VFMs) offer powerful capabilities for visual understanding and reasoning, making them ideal for agents to comprehend scenes, identify relevant regions, and infer the likely locations of objects. In this work, we present a zero-shot object goal navigation framework that integrates the perceptual strength of VFMs with a model-based planner that is capable of long-horizon decision making through frontier exploration. We evaluate our approach on the HM3D dataset using the Habitat simulator and demonstrate that our method achieves state-of-the-art performance in terms of success weighted by path length for zero-shot object goal navigation.",
    "pdf_url": "https://arxiv.org/pdf/2506.03516v1",
    "github_url": null,
    "published": "2025-06-04T03:04:54+00:00",
    "updated": "2025-06-04T03:04:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.02917v1",
    "title": "Text-guided Generation of Efficient Personalized Inspection Plans",
    "authors": [
      "Sun",
      "Pan",
      "Gao"
    ],
    "summary": "We propose a training-free, Vision-Language Model (VLM)-guided approach for efficiently generating trajectories to facilitate target inspection planning based on text descriptions. Unlike existing Vision-and-Language Navigation (VLN) methods designed for general agents in unknown environments, our approach specifically targets the efficient inspection of known scenes, with widespread applications in fields such as medical, marine, and civil engineering. Leveraging VLMs, our method first extracts points of interest (POIs) from the text description, then identifies a set of waypoints from which POIs are both salient and align with the spatial constraints defined in the prompt. Next, we interact with the VLM to iteratively refine the trajectory, preserving the visibility and prominence of the POIs. Further, we solve a Traveling Salesman Problem (TSP) to find the most efficient visitation order that satisfies the order constraint implied in the text description. Finally, we apply trajectory optimization to generate smooth, executable inspection paths for aerial and underwater vehicles. We have evaluated our method across a series of both handcrafted and real-world scanned environments. The results demonstrate that our approach effectively generates inspection planning trajectories that adhere to user instructions.",
    "pdf_url": "https://arxiv.org/pdf/2506.02917v1",
    "github_url": null,
    "published": "2025-06-03T14:18:37+00:00",
    "updated": "2025-06-03T14:18:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.02354v1",
    "title": "RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models",
    "authors": [
      "Li",
      "Zhang",
      "Qu"
    ],
    "summary": "Object Navigation (ObjectNav) is a fundamental task in embodied artificial intelligence. Although significant progress has been made in semantic map construction and target direction prediction in current research, redundant exploration and exploration failures remain inevitable. A critical but underexplored direction is the timely termination of exploration to overcome these challenges. We observe a diminishing marginal effect between exploration steps and exploration rates and analyze the cost-benefit relationship of exploration. Inspired by this, we propose RATE-Nav, a Region-Aware Termination-Enhanced method. It includes a geometric predictive region segmentation algorithm and region-Based exploration estimation algorithm for exploration rate calculation. By leveraging the visual question answering capabilities of visual language models (VLMs) and exploration rates enables efficient termination.RATE-Nav achieves a success rate of 67.8% and an SPL of 31.3% on the HM3D dataset. And on the more challenging MP3D dataset, RATE-Nav shows approximately 10% improvement over previous zero-shot methods.",
    "pdf_url": "https://arxiv.org/pdf/2506.02354v1",
    "github_url": null,
    "published": "2025-06-03T01:15:00+00:00",
    "updated": "2025-06-03T01:15:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.01551v3",
    "title": "EvolveNav: Empowering LLM-Based Vision-Language Navigation via Self-Improving Embodied Reasoning",
    "authors": [
      "Lin",
      "Nie",
      "Zai"
    ],
    "summary": "Recent studies have revealed the potential of training open-source Large Language Models (LLMs) to unleash LLMs' reasoning ability for enhancing vision-language navigation (VLN) performance, and simultaneously mitigate the domain gap between LLMs' training corpus and the VLN task. However, these approaches predominantly adopt straightforward input-output mapping paradigms, causing the mapping learning difficult and the navigational decisions unexplainable. Chain-of-Thought (CoT) training is a promising way to improve both navigational decision accuracy and interpretability, while the complexity of the navigation task makes the perfect CoT labels unavailable and may lead to overfitting through pure CoT supervised fine-tuning. To address these issues, we propose EvolveNav, a novel sElf-improving embodied reasoning paradigm that realizes adaptable and generalizable navigational reasoning for boosting LLM-based vision-language Navigation. Specifically, EvolveNav involves a two-stage training process: (1) Formalized CoT Supervised Fine-Tuning, where we train the model with curated formalized CoT labels to first activate the model's navigational reasoning capabilities, and simultaneously increase the reasoning speed; (2) Self-Reflective Post-Training, where the model is iteratively trained with its own reasoning outputs as self-enriched CoT labels to enhance the supervision diversity. A self-reflective auxiliary task is also designed to encourage the model to learn correct reasoning patterns by contrasting with wrong ones. Experimental results under both task-specific and cross-task training paradigms demonstrate the consistent superiority of EvolveNav over previous LLM-based VLN approaches on various popular benchmarks, including R2R, REVERIE, CVDN, and SOON. Code is available at https://github.com/expectorlin/EvolveNav.",
    "pdf_url": "https://arxiv.org/pdf/2506.01551v3",
    "github_url": "https://github.com/expectorlin/EvolveNav",
    "published": "2025-06-02T11:28:32+00:00",
    "updated": "2025-10-14T02:26:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.01418v1",
    "title": "SEMNAV: A Semantic Segmentation-Driven Approach to Visual Semantic Navigation",
    "authors": [
      "Flor-Rodrguez",
      "Gutirrez-lvarez",
      "Acevedo-Rodrguez"
    ],
    "summary": "Visual Semantic Navigation (VSN) is a fundamental problem in robotics, where an agent must navigate toward a target object in an unknown environment, mainly using visual information. Most state-of-the-art VSN models are trained in simulation environments, where rendered scenes of the real world are used, at best. These approaches typically rely on raw RGB data from the virtual scenes, which limits their ability to generalize to real-world environments due to domain adaptation issues. To tackle this problem, in this work, we propose SEMNAV, a novel approach that leverages semantic segmentation as the main visual input representation of the environment to enhance the agent's perception and decision-making capabilities. By explicitly incorporating high-level semantic information, our model learns robust navigation policies that improve generalization across unseen environments, both in simulated and real world settings. We also introduce a newly curated dataset, i.e. the SEMNAV dataset, designed for training semantic segmentation-aware navigation models like SEMNAV. Our approach is evaluated extensively in both simulated environments and with real-world robotic platforms. Experimental results demonstrate that SEMNAV outperforms existing state-of-the-art VSN models, achieving higher success rates in the Habitat 2.0 simulation environment, using the HM3D dataset. Furthermore, our real-world experiments highlight the effectiveness of semantic segmentation in mitigating the sim-to-real gap, making our model a promising solution for practical VSN-based robotic applications. We release SEMNAV dataset, code and trained models at https://github.com/gramuah/semnav",
    "pdf_url": "https://arxiv.org/pdf/2506.01418v1",
    "github_url": "https://github.com/gramuah/semnav",
    "published": "2025-06-02T08:19:41+00:00",
    "updated": "2025-06-02T08:19:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.01031v1",
    "title": "NavBench: Probing Multimodal Large Language Models for Embodied Navigation",
    "authors": [
      "Qiao",
      "Hong",
      "Lyu"
    ],
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated strong generalization in vision-language tasks, yet their ability to understand and act within embodied environments remains underexplored. We present NavBench, a benchmark to evaluate the embodied navigation capabilities of MLLMs under zero-shot settings. NavBench consists of two components: (1) navigation comprehension, assessed through three cognitively grounded tasks including global instruction alignment, temporal progress estimation, and local observation-action reasoning, covering 3,200 question-answer pairs; and (2) step-by-step execution in 432 episodes across 72 indoor scenes, stratified by spatial, cognitive, and execution complexity. To support real-world deployment, we introduce a pipeline that converts MLLMs' outputs into robotic actions. We evaluate both proprietary and open-source models, finding that GPT-4o performs well across tasks, while lighter open-source models succeed in simpler cases. Results also show that models with higher comprehension scores tend to achieve better execution performance. Providing map-based context improves decision accuracy, especially in medium-difficulty scenarios. However, most models struggle with temporal understanding, particularly in estimating progress during navigation, which may pose a key challenge.",
    "pdf_url": "https://arxiv.org/pdf/2506.01031v1",
    "github_url": null,
    "published": "2025-06-01T14:21:02+00:00",
    "updated": "2025-06-01T14:21:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.00075v1",
    "title": "Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation",
    "authors": [
      "Pollini",
      "Guterres",
      "Guerra"
    ],
    "summary": "The integration of Large Language Models (LLMs), such as GPT, in industrial robotics enhances operational efficiency and human-robot collaboration. However, the computational complexity and size of these models often provide latency problems in request and response times. This study explores the integration of the ChatGPT natural language model with the Robot Operating System 2 (ROS 2) to mitigate interaction latency and improve robotic system control within a simulated Gazebo environment. We present an architecture that integrates these technologies without requiring a middleware transport platform, detailing how a simulated mobile robot responds to text and voice commands. Experimental results demonstrate that this integration improves execution speed, usability, and accessibility of the human-robot interaction by decreasing the communication latency by 7.01\\% on average. Such improvements facilitate smoother, real-time robot operations, which are crucial for industrial automation and precision tasks.",
    "pdf_url": "https://arxiv.org/pdf/2506.00075v1",
    "github_url": null,
    "published": "2025-05-29T21:16:14+00:00",
    "updated": "2025-05-29T21:16:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.23266v1",
    "title": "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion",
    "authors": [
      "Xie",
      "He",
      "Guo"
    ],
    "summary": "We present Adversarial Object Fusion (AdvOF), a novel attack framework targeting vision-and-language navigation (VLN) agents in service-oriented environments by generating adversarial 3D objects. While foundational models like Large Language Models (LLMs) and Vision Language Models (VLMs) have enhanced service-oriented navigation systems through improved perception and decision-making, their integration introduces vulnerabilities in mission-critical service workflows. Existing adversarial attacks fail to address service computing contexts, where reliability and quality-of-service (QoS) are paramount. We utilize AdvOF to investigate and explore the impact of adversarial environments on the VLM-based perception module of VLN agents. In particular, AdvOF first precisely aggregates and aligns the victim object positions in both 2D and 3D space, defining and rendering adversarial objects. Then, we collaboratively optimize the adversarial object with regularization between the adversarial and victim object across physical properties and VLM perceptions. Through assigning importance weights to varying views, the optimization is processed stably and multi-viewedly by iterative fusions from local updates and justifications. Our extensive evaluations demonstrate AdvOF can effectively degrade agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks. This work advances the understanding of service security in VLM-powered navigation systems, providing computational foundations for robust service composition in physical-world deployments.",
    "pdf_url": "https://arxiv.org/pdf/2505.23266v1",
    "github_url": null,
    "published": "2025-05-29T09:14:50+00:00",
    "updated": "2025-05-29T09:14:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.23153v2",
    "title": "Conceptual Framework Toward Embodied Collective Adaptive Intelligence",
    "authors": [
      "Wang",
      "Liu"
    ],
    "summary": "Collective Adaptive Intelligence (CAI) represent a transformative approach in embodied AI, wherein numerous autonomous agents collaborate, adapt, and self-organize to navigate complex, dynamic environments. By enabling systems to reconfigure themselves in response to unforeseen challenges, CAI facilitate robust performance in real-world scenarios. This article introduces a conceptual framework for designing and analyzing CAI. It delineates key attributes including task generalization, resilience, scalability, and self-assembly, aiming to bridge theoretical foundations with practical methodologies for engineering adaptive, emergent intelligence. By providing a structured foundation for understanding and implementing CAI, this work seeks to guide researchers and practitioners in developing more resilient, scalable, and adaptable AI systems across various domains.",
    "pdf_url": "https://arxiv.org/pdf/2505.23153v2",
    "github_url": null,
    "published": "2025-05-29T06:43:14+00:00",
    "updated": "2025-07-01T03:22:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.20897v2",
    "title": "Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation",
    "authors": [
      "Zhang",
      "Su",
      "Wu"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \\textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.",
    "pdf_url": "https://arxiv.org/pdf/2505.20897v2",
    "github_url": "https://github.com/zhangpingrui/Adaptive-Text-Dreamer",
    "published": "2025-05-27T08:40:20+00:00",
    "updated": "2025-06-22T13:53:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.06487v1",
    "title": "BeliefMapNav: 3D Voxel-Based Belief Map for Zero-Shot Object Navigation",
    "authors": [
      "Zhou",
      "Hu",
      "Zhang"
    ],
    "summary": "Zero-shot object navigation (ZSON) allows robots to find target objects in unfamiliar environments using natural language instructions, without relying on pre-built maps or task-specific training. Recent general-purpose models, such as large language models (LLMs) and vision-language models (VLMs), equip agents with semantic reasoning abilities to estimate target object locations in a zero-shot manner. However, these models often greedily select the next goal without maintaining a global understanding of the environment and are fundamentally limited in the spatial reasoning necessary for effective navigation. To overcome these limitations, we propose a novel 3D voxel-based belief map that estimates the target's prior presence distribution within a voxelized 3D space. This approach enables agents to integrate semantic priors from LLMs and visual embeddings with hierarchical spatial structure, alongside real-time observations, to build a comprehensive 3D global posterior belief of the target's location. Building on this 3D voxel map, we introduce BeliefMapNav, an efficient navigation system with two key advantages: i) grounding LLM semantic reasoning within the 3D hierarchical semantics voxel space for precise target position estimation, and ii) integrating sequential path planning to enable efficient global navigation decisions. Experiments on HM3D, MP3D, and HSSD benchmarks show that BeliefMapNav achieves state-of-the-art (SOTA) Success Rate (SR) and Success weighted by Path Length (SPL), with a notable 46.4% SPL improvement over the previous best SR method, validating its effectiveness and efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2506.06487v1",
    "github_url": null,
    "published": "2025-05-27T07:28:27+00:00",
    "updated": "2025-05-27T07:28:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.18881v1",
    "title": "SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes",
    "authors": [
      "Qiu",
      "You",
      "Gong"
    ],
    "summary": "We present the Semantics-aware Dataset and Benchmark Generation Pipeline for Open-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes pretraining multimodal foundation models to generate infinite unique photo-realistic scene variants that adhere to real-world semantics and daily commonsense for the training and the evaluation of navigation agents, accompanied with a plugin for generating object navigation task episodes compatible to the Habitat simulator. In addition, we offer two pre-generated object navigation task datasets, SD-OVON-3k and SD-OVON-10k, comprising respectively about 3k and 10k episodes of the open-vocabulary object navigation task, derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans of real-world environments and the SD-OVON-Objects dataset with 0.9k manually inspected scanned and artist-created manipulatable object models. Unlike prior datasets limited to static environments, SD-OVON covers dynamic scenes and manipulatable objects, facilitating both real-to-sim and sim-to-real robotic applications. This approach enhances the realism of navigation tasks, the training and the evaluation of open-vocabulary object navigation agents in complex settings. To demonstrate the effectiveness of our pipeline and datasets, we propose two baselines and evaluate them along with state-of-the-art baselines on SD-OVON-3k. The datasets, benchmark and source code are publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2505.18881v1",
    "github_url": null,
    "published": "2025-05-24T21:37:06+00:00",
    "updated": "2025-05-24T21:37:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.16663v1",
    "title": "CoNav: Collaborative Cross-Modal Reasoning for Embodied Navigation",
    "authors": [
      "Hao",
      "Han",
      "Li"
    ],
    "summary": "Embodied navigation demands comprehensive scene understanding and precise spatial reasoning. While image-text models excel at interpreting pixel-level color and lighting cues, 3D-text models capture volumetric structure and spatial relationships. However, unified fusion approaches that jointly fuse 2D images, 3D point clouds, and textual instructions face challenges in limited availability of triple-modality data and difficulty resolving conflicting beliefs among modalities. In this work, we introduce CoNav, a collaborative cross-modal reasoning framework where a pretrained 3D-text model explicitly guides an image-text navigation agent by providing structured spatial-semantic knowledge to resolve ambiguities during navigation. Specifically, we introduce Cross-Modal Belief Alignment, which operationalizes this cross-modal guidance by simply sharing textual hypotheses from the 3D-text model to the navigation agent. Through lightweight fine-tuning on a small 2D-3D-text corpus, the navigation agent learns to integrate visual cues with spatial-semantic knowledge derived from the 3D-text model, enabling effective reasoning in embodied navigation. CoNav achieves significant improvements on four standard embodied navigation benchmarks (R2R, CVDN, REVERIE, SOON) and two spatial reasoning benchmarks (ScanQA, SQA3D). Moreover, under close navigation Success Rate, CoNav often generates shorter paths compared to other methods (as measured by SPL), showcasing the potential and challenges of fusing data from different modalities in embodied navigation. Project Page: https://oceanhao.github.io/CoNav/",
    "pdf_url": "https://arxiv.org/pdf/2505.16663v1",
    "github_url": null,
    "published": "2025-05-22T13:27:54+00:00",
    "updated": "2025-05-22T13:27:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.16498v1",
    "title": "Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models",
    "authors": [
      "Ballardini",
      "Sotelo"
    ],
    "summary": "Achieving full automation in self-driving vehicles remains a challenge, especially in dynamic urban environments where navigation requires real-time adaptability. Existing systems struggle to handle navigation plans when faced with unpredictable changes in road layouts, spontaneous detours, or missing map data, due to their heavy reliance on predefined cartographic information. In this work, we explore the use of Large Language Models to generate Answer Set Programming rules by translating informal navigation instructions into structured, logic-based reasoning. ASP provides non-monotonic reasoning, allowing autonomous vehicles to adapt to evolving scenarios without relying on predefined maps. We present an experimental evaluation in which LLMs generate ASP constraints that encode real-world urban driving logic into a formal knowledge representation. By automating the translation of informal navigation instructions into logical rules, our method improves adaptability and explainability in autonomous navigation. Results show that LLM-driven ASP rule generation supports semantic-based decision-making, offering an explainable framework for dynamic navigation planning that aligns closely with how humans communicate navigational intent.",
    "pdf_url": "https://arxiv.org/pdf/2505.16498v1",
    "github_url": null,
    "published": "2025-05-22T10:32:43+00:00",
    "updated": "2025-05-22T10:32:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.15725v2",
    "title": "UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV Imitation Learning",
    "authors": [
      "Wang",
      "Yang",
      "Liao"
    ],
    "summary": "Unmanned Aerial Vehicles (UAVs) are evolving into language-interactive platforms, enabling more intuitive forms of human-drone interaction. While prior works have primarily focused on high-level planning and long-horizon navigation, we shift attention to language-guided fine-grained trajectory control, where UAVs execute short-range, reactive flight behaviors in response to language instructions. We formalize this problem as the Flying-on-a-Word (Flow) task and introduce UAV imitation learning as an effective approach. In this framework, UAVs learn fine-grained control policies by mimicking expert pilot trajectories paired with atomic language instructions. To support this paradigm, we present UAV-Flow, the first real-world benchmark for language-conditioned, fine-grained UAV control. It includes a task formulation, a large-scale dataset collected in diverse environments, a deployable control framework, and a simulation suite for systematic evaluation. Our design enables UAVs to closely imitate the precise, expert-level flight trajectories of human pilots and supports direct deployment without sim-to-real gap. We conduct extensive experiments on UAV-Flow, benchmarking VLN and VLA paradigms. Results show that VLA models are superior to VLN baselines and highlight the critical role of spatial grounding in the fine-grained Flow setting.",
    "pdf_url": "https://arxiv.org/pdf/2505.15725v2",
    "github_url": null,
    "published": "2025-05-21T16:31:28+00:00",
    "updated": "2025-05-26T11:15:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.14866v1",
    "title": "UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction",
    "authors": [
      "Nilavadi",
      "Rudenko",
      "Linder"
    ],
    "summary": "We introduce a unified approach to forecast the dynamics of human keypoints along with the motion trajectory based on a short sequence of input poses. While many studies address either full-body pose prediction or motion trajectory prediction, only a few attempt to merge them. We propose a motion transformation technique to simultaneously predict full-body pose and trajectory key-points in a global coordinate frame. We utilize an off-the-shelf 3D human pose estimation module, a graph attention network to encode the skeleton structure, and a compact, non-autoregressive transformer suitable for real-time motion prediction for human-robot interaction and human-aware navigation. We introduce a human navigation dataset ``DARKO'' with specific focus on navigational activities that are relevant for human-aware mobile robot navigation. We perform extensive evaluation on Human3.6M, CMU-Mocap, and our DARKO dataset. In comparison to prior work, we show that our approach is compact, real-time, and accurate in predicting human navigation motion across all datasets. Result animations, our dataset, and code will be available at https://nisarganc.github.io/UPTor-page/",
    "pdf_url": "https://arxiv.org/pdf/2505.14866v1",
    "github_url": null,
    "published": "2025-05-20T19:57:25+00:00",
    "updated": "2025-05-20T19:57:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.13729v1",
    "title": "SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation",
    "authors": [
      "Rajvanshi",
      "Sahu",
      "Shan"
    ],
    "summary": "Adaptive collaboration is critical to a team of autonomous robots to perform complicated navigation tasks in large-scale unknown environments. An effective collaboration strategy should be determined and adapted according to each robot's skills and current status to successfully achieve the shared goal. We present SayCoNav, a new approach that leverages large language models (LLMs) for automatically generating this collaboration strategy among a team of robots. Building on the collaboration strategy, each robot uses the LLM to generate its plans and actions in a decentralized way. By sharing information to each other during navigation, each robot also continuously updates its step-by-step plans accordingly. We evaluate SayCoNav on Multi-Object Navigation (MultiON) tasks, that require the team of the robots to utilize their complementary strengths to efficiently search multiple different objects in unknown environments. By validating SayCoNav with varied team compositions and conditions against baseline methods, our experimental results show that SayCoNav can improve search efficiency by at most 44.28% through effective collaboration among heterogeneous robots. It can also dynamically adapt to the changing conditions during task execution.",
    "pdf_url": "https://arxiv.org/pdf/2505.13729v1",
    "github_url": null,
    "published": "2025-05-19T20:58:06+00:00",
    "updated": "2025-05-19T20:58:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.12835v1",
    "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models",
    "authors": [
      "Cai",
      "Dong",
      "Tan"
    ],
    "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital for applications such as disaster response, logistics delivery, and urban inspection. However, existing methods often struggle with insufficient multimodal fusion, weak generalization, and poor interpretability. To address these challenges, we propose FlightGPT, a novel UAV VLN framework built upon Vision-Language Models (VLMs) with powerful multimodal perception capabilities. We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT) using high-quality demonstrations to improve initialization and structured reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by a composite reward that considers goal accuracy, reasoning quality, and format compliance, to enhance generalization and adaptability. Furthermore, FlightGPT introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve decision interpretability. Extensive experiments on the city-scale dataset CityNav demonstrate that FlightGPT achieves state-of-the-art performance across all scenarios, with a 9.22\\% higher success rate than the strongest baseline in unseen environments. Our implementation is publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2505.12835v1",
    "github_url": null,
    "published": "2025-05-19T08:21:20+00:00",
    "updated": "2025-05-19T08:21:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.12443v1",
    "title": "BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation",
    "authors": [
      "Lyu",
      "Li",
      "Qiao"
    ],
    "summary": "Multimodal large language models (MLLMs) have recently gained attention for their generalization and reasoning capabilities in Vision-and-Language Navigation (VLN) tasks, leading to the rise of MLLM-driven navigators. However, MLLMs are vulnerable to jailbreak attacks, where crafted prompts bypass safety mechanisms and trigger undesired outputs. In embodied scenarios, such vulnerabilities pose greater risks: unlike plain text models that generate toxic content, embodied agents may interpret malicious instructions as executable commands, potentially leading to real-world harm. In this paper, we present the first systematic jailbreak attack paradigm targeting MLLM-driven navigator. We propose a three-tiered attack framework and construct malicious queries across four intent categories, concatenated with standard navigation instructions. In the Matterport3D simulator, we evaluate navigation agents powered by five MLLMs and report an average attack success rate over 90%. To test real-world feasibility, we replicate the attack on a physical robot. Our results show that even well-crafted prompts can induce harmful actions and intents in MLLMs, posing risks beyond toxic output and potentially leading to physical harm.",
    "pdf_url": "https://arxiv.org/pdf/2505.12443v1",
    "github_url": null,
    "published": "2025-05-18T14:33:17+00:00",
    "updated": "2025-05-18T14:33:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.12084v1",
    "title": "Bench-NPIN: Benchmarking Non-prehensile Interactive Navigation",
    "authors": [
      "Zhong",
      "Caro",
      "Iskandar"
    ],
    "summary": "Mobile robots are increasingly deployed in unstructured environments where obstacles and objects are movable. Navigation in such environments is known as interactive navigation, where task completion requires not only avoiding obstacles but also strategic interactions with movable objects. Non-prehensile interactive navigation focuses on non-grasping interaction strategies, such as pushing, rather than relying on prehensile manipulation. Despite a growing body of research in this field, most solutions are evaluated using case-specific setups, limiting reproducibility and cross-comparison. In this paper, we present Bench-NPIN, the first comprehensive benchmark for non-prehensile interactive navigation. Bench-NPIN includes multiple components: 1) a comprehensive range of simulated environments for non-prehensile interactive navigation tasks, including navigating a maze with movable obstacles, autonomous ship navigation in icy waters, box delivery, and area clearing, each with varying levels of complexity; 2) a set of evaluation metrics that capture unique aspects of interactive navigation, such as efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-NPIN to evaluate example implementations of established baselines across environments. Bench-NPIN is an open-source Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.",
    "pdf_url": "https://arxiv.org/pdf/2505.12084v1",
    "github_url": "https://github.com/IvanIZ/BenchNPIN",
    "published": "2025-05-17T16:54:18+00:00",
    "updated": "2025-05-17T16:54:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.11886v4",
    "title": "Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation",
    "authors": [
      "Wang",
      "Wang",
      "Li"
    ],
    "summary": "Vision-Language Navigation (VLN) is a critical task for developing embodied agents that can follow natural language instructions to navigate in complex real-world environments. Recent advances in VLN by large pretrained models have significantly improved generalization and instruction grounding compared to traditional approaches. However, the role of reasoning strategies in navigation-an action-centric, long-horizon task-remains underexplored, despite Chain-of-Thought (CoT) reasoning's demonstrated success in static tasks like visual question answering. To address this gap, we conduct the first systematic evaluation of reasoning strategies for VLN, including No-Think (direct action prediction), Pre-Think (reason before action), and Post-Think (reason after action). Surprisingly, our findings reveal the Inference-time Reasoning Collapse issue, where inference-time reasoning degrades navigation accuracy, highlighting the challenges of integrating reasoning into VLN. Based on this insight, we propose Aux-Think, a framework that trains models to internalize structured reasoning patterns through CoT supervision, while inferring action directly without reasoning in online prediction. To support this framework, we release R2R-CoT-320k, the first Chain-of-Thought annotated dataset for VLN. Extensive experiments show that Aux-Think reduces training effort greatly and achieves the best performance under the same data scale.",
    "pdf_url": "https://arxiv.org/pdf/2505.11886v4",
    "github_url": null,
    "published": "2025-05-17T07:34:56+00:00",
    "updated": "2025-10-14T10:16:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.11383v1",
    "title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "Lee",
      "Lee"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is a core task where embodied agents leverage their spatial mobility to navigate in 3D environments toward designated destinations based on natural language instructions. Recently, video-language large models (Video-VLMs) with strong generalization capabilities and rich commonsense knowledge have shown remarkable performance when applied to VLN tasks. However, these models still encounter the following challenges when applied to real-world 3D navigation: 1) Insufficient understanding of 3D geometry and spatial semantics; 2) Limited capacity for large-scale exploration and long-term environmental memory; 3) Poor adaptability to dynamic and changing environments.To address these limitations, we propose Dynam3D, a dynamic layered 3D representation model that leverages language-aligned, generalizable, and hierarchical 3D representations as visual input to train 3D-VLM in navigation action prediction. Given posed RGB-D images, our Dynam3D projects 2D CLIP features into 3D space and constructs multi-level 3D patch-instance-zone representations for 3D geometric and semantic understanding with a dynamic and layer-wise update strategy. Our Dynam3D is capable of online encoding and localization of 3D instances, and dynamically updates them in changing environments to provide large-scale exploration and long-term memory capabilities for navigation. By leveraging large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D sets new state-of-the-art performance on VLN benchmarks including R2R-CE, REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for pre-exploration, lifelong memory, and real-world robot validate the effectiveness of practical deployment.",
    "pdf_url": "https://arxiv.org/pdf/2505.11383v1",
    "github_url": null,
    "published": "2025-05-16T15:46:27+00:00",
    "updated": "2025-05-16T15:46:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.08854v1",
    "title": "Generative AI for Autonomous Driving: Frontiers and Opportunities",
    "authors": [
      "Wang",
      "Xing",
      "Can"
    ],
    "summary": "Generative Artificial Intelligence (GenAI) constitutes a transformative technological wave that reconfigures industries through its unparalleled capabilities for content creation, reasoning, planning, and multimodal understanding. This revolutionary force offers the most promising path yet toward solving one of engineering's grandest challenges: achieving reliable, fully autonomous driving, particularly the pursuit of Level 5 autonomy. This survey delivers a comprehensive and critical synthesis of the emerging role of GenAI across the autonomous driving stack. We begin by distilling the principles and trade-offs of modern generative modeling, encompassing VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). We then map their frontier applications in image, LiDAR, trajectory, occupancy, video generation as well as LLM-guided reasoning and decision making. We categorize practical applications, such as synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI. We identify key obstacles and possibilities such as comprehensive generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects, while proposing research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence. By unifying these threads, the survey provides a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility. An actively maintained repository of cited works is available at https://github.com/taco-group/GenAI4AD.",
    "pdf_url": "https://arxiv.org/pdf/2505.08854v1",
    "github_url": "https://github.com/taco-group/GenAI4AD",
    "published": "2025-05-13T17:59:20+00:00",
    "updated": "2025-05-13T17:59:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.07634v3",
    "title": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents",
    "authors": [
      "Liu",
      "Shi",
      "Nguyen"
    ],
    "summary": "The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2505.07634v3",
    "github_url": null,
    "published": "2025-05-12T15:05:34+00:00",
    "updated": "2025-10-06T10:13:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.07254v1",
    "title": "Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking",
    "authors": [
      "Nagy",
      "Werghi",
      "Hassan"
    ],
    "summary": "This work addresses the critical lack of precision in state estimation in the Kalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of selecting the appropriate motion model. Existing literature commonly relies on constant motion models for estimating the states of objects, neglecting the complex motion dynamics unique to each object. Consequently, trajectory division and imprecise object localization arise, especially under occlusion conditions. The core of these challenges lies in the limitations of the current Kalman filter formulation, which fails to account for the variability of motion dynamics as objects navigate their environments. This work introduces a novel formulation of the Kalman filter that incorporates motion dynamics, allowing the motion model to adaptively adjust according to changes in the object's movement. The proposed Kalman filter substantially improves state estimation, localization, and trajectory prediction compared to the traditional Kalman filter. This is reflected in tracking performance that surpasses recent benchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\\% and 0.81\\% in higher order tracking accuracy (HOTA) and multi-object tracking accuracy (MOTA), respectively. Furthermore, the proposed Kalman filter consistently outperforms the baseline across various detectors. Additionally, it shows an enhanced capability in managing long occlusions compared to the baseline Kalman filter, achieving margins of 1.22\\% in higher order tracking accuracy (HOTA) and 1.55\\% in multi-object tracking accuracy (MOTA) on the KITTI dataset. The formulation's efficiency is evident, with an additional processing time of only approximately 0.078 ms per frame, ensuring its applicability in real-time applications.",
    "pdf_url": "https://arxiv.org/pdf/2505.07254v1",
    "github_url": null,
    "published": "2025-05-12T06:09:32+00:00",
    "updated": "2025-05-12T06:09:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.06729v2",
    "title": "STRIVE: Structured Representation Integrating VLM Reasoning for Efficient Object Navigation",
    "authors": [
      "Zhu",
      "Li",
      "Liu"
    ],
    "summary": "Vision-Language Models (VLMs) have been increasingly integrated into object navigation tasks for their rich prior knowledge and strong reasoning abilities. However, applying VLMs to navigation poses two key challenges: effectively representing complex environment information and determining \\textit{when and how} to query VLMs. Insufficient environment understanding and over-reliance on VLMs (e.g. querying at every step) can lead to unnecessary backtracking and reduced navigation efficiency, especially in continuous environments. To address these challenges, we propose a novel framework that constructs a multi-layer representation of the environment during navigation. This representation consists of viewpoint, object nodes, and room nodes. Viewpoints and object nodes facilitate intra-room exploration and accurate target localization, while room nodes support efficient inter-room planning. Building on this representation, we propose a novel two-stage navigation policy, integrating high-level planning guided by VLM reasoning with low-level VLM-assisted exploration to efficiently locate a goal object. We evaluated our approach on three simulated benchmarks (HM3D, RoboTHOR, and MP3D), and achieved state-of-the-art performance on both the success rate ($\\mathord{\\uparrow}\\, 7.1\\%$) and navigation efficiency ($\\mathord{\\uparrow}\\, 12.5\\%$). We further validate our method on a real robot platform, demonstrating strong robustness across 15 object navigation tasks in 10 different indoor environments. Project page is available at https://zwandering.github.io/STRIVE.github.io/ .",
    "pdf_url": "https://arxiv.org/pdf/2505.06729v2",
    "github_url": null,
    "published": "2025-05-10T18:38:41+00:00",
    "updated": "2025-09-15T21:34:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.06218v1",
    "title": "Let Humanoids Hike! Integrative Skill Development on Complex Trails",
    "authors": [
      "Lin",
      "Yu"
    ],
    "summary": "Hiking on complex trails demands balance, agility, and adaptive decision-making over unpredictable terrain. Current humanoid research remains fragmented and inadequate for hiking: locomotion focuses on motor skills without long-term goals or situational awareness, while semantic navigation overlooks real-world embodiment and local terrain variability. We propose training humanoids to hike on complex trails, driving integrative skill development across visual perception, decision making, and motor execution. We develop a learning framework, LEGO-H, that enables a vision-equipped humanoid robot to hike complex trails autonomously. We introduce two technical innovations: 1) A temporal vision transformer variant - tailored into Hierarchical Reinforcement Learning framework - anticipates future local goals to guide movement, seamlessly integrating locomotion with goal-directed navigation. 2) Latent representations of joint movement patterns, combined with hierarchical metric learning - enhance Privileged Learning scheme - enable smooth policy transfer from privileged training to onboard execution. These components allow LEGO-H to handle diverse physical and environmental challenges without relying on predefined motion patterns. Experiments across varied simulated trails and robot morphologies highlight LEGO-H's versatility and robustness, positioning hiking as a compelling testbed for embodied autonomy and LEGO-H as a baseline for future humanoid development.",
    "pdf_url": "https://arxiv.org/pdf/2505.06218v1",
    "github_url": null,
    "published": "2025-05-09T17:53:02+00:00",
    "updated": "2025-05-09T17:53:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.06131v3",
    "title": "LOG-Nav: Efficient Layout-Aware Object-Goal Navigation with Hierarchical Planning",
    "authors": [
      "Hou",
      "Xiao",
      "Xue"
    ],
    "summary": "We introduce LOG-Nav, an efficient layout-aware object-goal navigation approach designed for complex multi-room indoor environments. By planning hierarchically leveraging a global topologigal map with layout information and local imperative approach with detailed scene representation memory, LOG-Nav achieves both efficient and effective navigation. The process is managed by an LLM-powered agent, ensuring seamless effective planning and navigation, without the need for human interaction, complex rewards, or costly training. Our experimental results on the MP3D benchmark achieves 85\\% object navigation success rate (SR) and 79\\% success rate weighted by path length (SPL) (over 40\\% point improvement in SR and 60\\% improvement in SPL compared to exsisting methods). Furthermore, we validate the robustness of our approach through virtual agent and real-world robotic deployment, showcasing its capability in practical scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2505.06131v3",
    "github_url": null,
    "published": "2025-05-09T15:39:37+00:00",
    "updated": "2025-12-08T11:41:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.07868v2",
    "title": "VISTA: Generative Visual Imagination for Vision-and-Language Navigation",
    "authors": [
      "Huang",
      "Wu",
      "Li"
    ],
    "summary": "Vision-and-Language Navigation (VLN) tasks agents with locating specific objects in unseen environments using natural language instructions and visual cues. Many existing VLN approaches typically follow an 'observe-and-reason' schema, that is, agents observe the environment and decide on the next action to take based on the visual observations of their surroundings. They often face challenges in long-horizon scenarios due to limitations in immediate observation and vision-language modality gaps. To overcome this, we present VISTA, a novel framework that employs an 'imagine-and-align' navigation strategy. Specifically, we leverage the generative prior of pre-trained diffusion models for dynamic visual imagination conditioned on both local observations and high-level language instructions. A Perceptual Alignment Filter module then grounds these goal imaginations against current observations, guiding an interpretable and structured reasoning process for action selection. Experiments show that VISTA sets new state-of-the-art results on Room-to-Room (R2R) and RoboTHOR benchmarks, e.g.,+3.6% increase in Success Rate on R2R. Extensive ablation analysis underscores the value of integrating forward-looking imagination, perceptual alignment, and structured reasoning for robust navigation in long-horizon environments.",
    "pdf_url": "https://arxiv.org/pdf/2505.07868v2",
    "github_url": null,
    "published": "2025-05-09T09:07:10+00:00",
    "updated": "2025-05-17T01:06:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.05622v1",
    "title": "CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory",
    "authors": [
      "Zhang",
      "Gao",
      "Yu"
    ],
    "summary": "Aerial vision-and-language navigation (VLN), requiring drones to interpret natural language instructions and navigate complex urban environments, emerges as a critical embodied AI challenge that bridges human-robot interaction, 3D spatial reasoning, and real-world deployment. Although existing ground VLN agents achieved notable results in indoor and outdoor settings, they struggle in aerial VLN due to the absence of predefined navigation graphs and the exponentially expanding action space in long-horizon exploration. In this work, we propose \\textbf{CityNavAgent}, a large language model (LLM)-empowered agent that significantly reduces the navigation complexity for urban aerial VLN. Specifically, we design a hierarchical semantic planning module (HSPM) that decomposes the long-horizon task into sub-goals with different semantic levels. The agent reaches the target progressively by achieving sub-goals with different capacities of the LLM. Additionally, a global memory module storing historical trajectories into a topological graph is developed to simplify navigation for visited targets. Extensive benchmark experiments show that our method achieves state-of-the-art performance with significant improvement. Further experiments demonstrate the effectiveness of different modules of CityNavAgent for aerial VLN in continuous city environments. The code is available at \\href{https://github.com/VinceOuti/CityNavAgent}{link}.",
    "pdf_url": "https://arxiv.org/pdf/2505.05622v1",
    "github_url": "https://github.com/VinceOuti/CityNavAgent",
    "published": "2025-05-08T20:01:35+00:00",
    "updated": "2025-05-08T20:01:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.05108v2",
    "title": "Multi-agent Embodied AI: Advances and Future Directions",
    "authors": [
      "Feng",
      "Xue",
      "Yuan"
    ],
    "summary": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the application of advanced technologies in the intelligent era, where AI systems are integrated with physical bodies that enable them to perceive, reason, and interact with their environments. Through the use of sensors for input and actuators for action, these systems can learn and adapt based on real-world feedback, allowing them to perform tasks effectively in dynamic and unpredictable environments. As techniques such as deep learning (DL), reinforcement learning (RL), and large language models (LLMs) mature, embodied AI has become a leading field in both academia and industry, with applications spanning robotics, healthcare, transportation, and manufacturing. However, most research has focused on single-agent systems that often assume static, closed environments, whereas real-world embodied AI must navigate far more complex scenarios. In such settings, agents must not only interact with their surroundings but also collaborate with other agents, necessitating sophisticated mechanisms for adaptation, real-time learning, and collaborative problem-solving. Despite increasing interest in multi-agent systems, existing research remains narrow in scope, often relying on simplified models that fail to capture the full complexity of dynamic, open environments for multi-agent embodied AI. Moreover, no comprehensive survey has systematically reviewed the advancements in this area. As embodied AI rapidly evolves, it is crucial to deepen our understanding of multi-agent embodied AI to address the challenges presented by real-world applications. To fill this gap and foster further development in the field, this paper reviews the current state of research, analyzes key contributions, and identifies challenges and future directions, providing insights to guide innovation and progress in this field.",
    "pdf_url": "https://arxiv.org/pdf/2505.05108v2",
    "github_url": null,
    "published": "2025-05-08T10:13:53+00:00",
    "updated": "2025-06-21T18:06:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.03460v1",
    "title": "LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal Delivery Based on Agentic UAVs",
    "authors": [
      "Zhang",
      "Tian",
      "Lin"
    ],
    "summary": "The growing demand for intelligent logistics, particularly fine-grained terminal delivery, underscores the need for autonomous UAV (Unmanned Aerial Vehicle)-based delivery systems. However, most existing last-mile delivery studies rely on ground robots, while current UAV-based Vision-Language Navigation (VLN) tasks primarily focus on coarse-grained, long-range goals, making them unsuitable for precise terminal delivery. To bridge this gap, we propose LogisticsVLN, a scalable aerial delivery system built on multimodal large language models (MLLMs) for autonomous terminal delivery. LogisticsVLN integrates lightweight Large Language Models (LLMs) and Visual-Language Models (VLMs) in a modular pipeline for request understanding, floor localization, object detection, and action-decision making. To support research and evaluation in this new setting, we construct the Vision-Language Delivery (VLD) dataset within the CARLA simulator. Experimental results on the VLD dataset showcase the feasibility of the LogisticsVLN system. In addition, we conduct subtask-level evaluations of each module of our system, offering valuable insights for improving the robustness and real-world deployment of foundation model-based vision-language delivery systems.",
    "pdf_url": "https://arxiv.org/pdf/2505.03460v1",
    "github_url": null,
    "published": "2025-05-06T12:00:49+00:00",
    "updated": "2025-05-06T12:00:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.03174v1",
    "title": "Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets",
    "authors": [
      "Roque",
      "Maquiling",
      "Lopez"
    ],
    "summary": "Instruction-Action (IA) data pairs are valuable for training robotic systems, especially autonomous vehicles (AVs), but having humans manually annotate this data is costly and time-inefficient. This paper explores the potential of using mobile application Global Positioning System (GPS) references and Natural Language Processing (NLP) to automatically generate large volumes of IA commands and responses without having a human generate or retroactively tag the data. In our pilot data collection, by driving to various destinations and collecting voice instructions from GPS applications, we demonstrate a means to collect and categorize the diverse sets of instructions, further accompanied by video data to form complete vision-language-action triads. We provide details on our completely automated data collection prototype system, ADVLAT-Engine. We characterize collected GPS voice instructions into eight different classifications, highlighting the breadth of commands and referentialities available for curation from freely available mobile applications. Through research and exploration into the automation of IA data pairs using GPS references, the potential to increase the speed and volume at which high-quality IA datasets are created, while minimizing cost, can pave the way for robust vision-language-action (VLA) models to serve tasks in vision-language navigation (VLN) and human-interactive autonomous systems.",
    "pdf_url": "https://arxiv.org/pdf/2505.03174v1",
    "github_url": null,
    "published": "2025-05-06T04:38:41+00:00",
    "updated": "2025-05-06T04:38:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.02388v1",
    "title": "MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans",
    "authors": [
      "Yu",
      "Jia",
      "Chen"
    ],
    "summary": "Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale, simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research. Project website: https://meta-scenes.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2505.02388v1",
    "github_url": null,
    "published": "2025-05-05T06:13:25+00:00",
    "updated": "2025-05-05T06:13:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.02293v1",
    "title": "Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety",
    "authors": [
      "Choi",
      "Aloor",
      "Li"
    ],
    "summary": "Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination.   To overcome this challenge, we propose a method that integrates multiple layers of safety by combining MARL with safety filters. First, MARL is used to learn strategies that minimize multiple agent interactions, where multiple indicates more than two. Particularly, we focus on interactions likely to result in conflicting constraints within the engagement distance. Next, for agents that enter the engagement distance, we prioritize pairs requiring the most urgent corrective actions. Finally, a dedicated safety filter provides tactical corrective actions to resolve these conflicts. Crucially, the design decisions for all layers of this framework are grounded in reachability analysis and a control barrier-value function-based filtering mechanism.   We validate our Layered Safe MARL framework in 1) hardware experiments using Crazyflie drones and 2) high-density advanced aerial mobility (AAM) operation scenarios, where agents navigate to designated waypoints while avoiding collisions. The results show that our method significantly reduces conflict while maintaining safety without sacrificing much efficiency (i.e., shorter travel time and distance) compared to baselines that do not incorporate layered safety. The project website is available at https://dinamo-mit.github.io/Layered-Safe-MARL/",
    "pdf_url": "https://arxiv.org/pdf/2505.02293v1",
    "github_url": null,
    "published": "2025-05-04T23:42:52+00:00",
    "updated": "2025-05-04T23:42:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.01931v1",
    "title": "Semantic Intelligence: Integrating GPT-4 with A Planning in Low-Cost Robotics",
    "authors": [
      "Barkley",
      "George",
      "Farimani"
    ],
    "summary": "Classical robot navigation often relies on hardcoded state machines and purely geometric path planners, limiting a robot's ability to interpret high-level semantic instructions. In this paper, we first assess GPT-4's ability to act as a path planner compared to the A* algorithm, then present a hybrid planning framework that integrates GPT-4's semantic reasoning with A* on a low-cost robot platform operating on ROS2 Humble. Our approach eliminates explicit finite state machine (FSM) coding by using prompt-based GPT-4 reasoning to handle task logic while maintaining the accurate paths computed by A*. The GPT-4 module provides semantic understanding of instructions and environmental cues (e.g., recognizing toxic obstacles or crowded areas to avoid, or understanding low-battery situations requiring alternate route selection), and dynamically adjusts the robot's occupancy grid via obstacle buffering to enforce semantic constraints. We demonstrate multi-step reasoning for sequential tasks, such as first navigating to a resource goal and then reaching a final destination safely. Experiments on a Petoi Bittle robot with an overhead camera and Raspberry Pi Zero 2W compare classical A* against GPT-4-assisted planning. Results show that while A* is faster and more accurate for basic route generation and obstacle avoidance, the GPT-4-integrated system achieves high success rates (96-100%) on semantic tasks that are infeasible for pure geometric planners. This work highlights how affordable robots can exhibit intelligent, context-aware behaviors by leveraging large language model reasoning with minimal hardware and no fine-tuning.",
    "pdf_url": "https://arxiv.org/pdf/2505.01931v1",
    "github_url": null,
    "published": "2025-05-03T21:49:14+00:00",
    "updated": "2025-05-03T21:49:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.01458v1",
    "title": "A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI",
    "authors": [
      "Wong",
      "Kang",
      "Bai"
    ],
    "summary": "Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. We also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, we offer a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods-such as world models and geometric equivariance-to help researchers select suitable tools while accounting for hardware constraints.",
    "pdf_url": "https://arxiv.org/pdf/2505.01458v1",
    "github_url": null,
    "published": "2025-05-01T09:22:23+00:00",
    "updated": "2025-05-01T09:22:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.21432v1",
    "title": "UAV-VLN: End-to-End Vision Language guided Navigation for UAVs",
    "authors": [
      "Saxena",
      "Raghuvanshi",
      "Goveas"
    ],
    "summary": "A core challenge in AI-guided autonomy is enabling agents to navigate realistically and effectively in previously unseen environments based on natural language commands. We propose UAV-VLN, a novel end-to-end Vision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs) that seamlessly integrates Large Language Models (LLMs) with visual perception to facilitate human-interactive navigation. Our system interprets free-form natural language instructions, grounds them into visual observations, and plans feasible aerial trajectories in diverse environments.   UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse high-level semantic goals, while a vision model detects and localizes semantically relevant objects in the environment. By fusing these modalities, the UAV can reason about spatial relationships, disambiguate references in human instructions, and plan context-aware behaviors with minimal task-specific supervision. To ensure robust and interpretable decision-making, the framework includes a cross-modal grounding mechanism that aligns linguistic intent with visual context.   We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios, demonstrating its ability to generalize to novel instructions and environments with minimal task-specific training. Our results show significant improvements in instruction-following accuracy and trajectory efficiency, highlighting the potential of LLM-driven vision-language interfaces for safe, intuitive, and generalizable UAV autonomy.",
    "pdf_url": "https://arxiv.org/pdf/2504.21432v1",
    "github_url": null,
    "published": "2025-04-30T08:40:47+00:00",
    "updated": "2025-04-30T08:40:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.00743v1",
    "title": "DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation",
    "authors": [
      "Yu",
      "Yang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is a challenging task where an agent must understand language instructions and navigate unfamiliar environments using visual cues. The agent must accurately locate the target based on visual information from the environment and complete tasks through interaction with the surroundings. Despite significant advancements in this field, two major limitations persist: (1) Many existing methods input complete language instructions directly into multi-layer Transformer networks without fully exploiting the detailed information within the instructions, thereby limiting the agent's language understanding capabilities during task execution; (2) Current approaches often overlook the modeling of object relationships across different modalities, failing to effectively utilize latent clues between objects, which affects the accuracy and robustness of navigation decisions. We propose a Dual Object Perception-Enhancement Network (DOPE) to address these issues to improve navigation performance. First, we design a Text Semantic Extraction (TSE) to extract relatively essential phrases from the text and input them into the Text Object Perception-Augmentation (TOPA) to fully leverage details such as objects and actions within the instructions. Second, we introduce an Image Object Perception-Augmentation (IOPA), which performs additional modeling of object information across different modalities, enabling the model to more effectively utilize latent clues between objects in images and text, enhancing decision-making accuracy. Extensive experiments on the R2R and REVERIE datasets validate the efficacy of the proposed approach.",
    "pdf_url": "https://arxiv.org/pdf/2505.00743v1",
    "github_url": null,
    "published": "2025-04-30T06:47:13+00:00",
    "updated": "2025-04-30T06:47:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.19322v2",
    "title": "Learned Perceptive Forward Dynamics Model for Safe and Platform-aware Robotic Navigation",
    "authors": [
      "Roth",
      "Frey",
      "Cadena"
    ],
    "summary": "Ensuring safe navigation in complex environments requires accurate real-time traversability assessment and understanding of environmental interactions relative to the robot`s capabilities. Traditional methods, which assume simplified dynamics, often require designing and tuning cost functions to safely guide paths or actions toward the goal. This process is tedious, environment-dependent, and not generalizable. To overcome these issues, we propose a novel learned perceptive Forward Dynamics Model (FDM) that predicts the robot`s future state conditioned on the surrounding geometry and history of proprioceptive measurements, proposing a more scalable, safer, and heuristic-free solution. The FDM is trained on multiple years of simulated navigation experience, including high-risk maneuvers, and real-world interactions to incorporate the full system dynamics beyond rigid body simulation. We integrate our perceptive FDM into a zero-shot Model Predictive Path Integral (MPPI) planning framework, leveraging the learned mapping between actions, future states, and failure probability. This allows for optimizing a simplified cost function, eliminating the need for extensive cost-tuning to ensure safety. On the legged robot ANYmal, the proposed perceptive FDM improves the position estimation by on average 41% over competitive baselines, which translates into a 27% higher navigation success rate in rough simulation environments. Moreover, we demonstrate effective sim-to-real transfer and showcase the benefit of training on synthetic and real data. Code and models are made publicly available under https://github.com/leggedrobotics/fdm.",
    "pdf_url": "https://arxiv.org/pdf/2504.19322v2",
    "github_url": "https://github.com/leggedrobotics/fdm",
    "published": "2025-04-27T18:27:28+00:00",
    "updated": "2025-04-29T09:26:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.16516v2",
    "title": "Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation",
    "authors": [
      "Yue",
      "Zhang",
      "Qin"
    ],
    "summary": "Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agent's ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation.",
    "pdf_url": "https://arxiv.org/pdf/2504.16516v2",
    "github_url": null,
    "published": "2025-04-23T08:41:27+00:00",
    "updated": "2025-04-24T19:36:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.14478v3",
    "title": "ApexNav: An Adaptive Exploration Strategy for Zero-Shot Object Navigation with Target-centric Semantic Fusion",
    "authors": [
      "Zhang",
      "Du",
      "Wu"
    ],
    "summary": "Navigating unknown environments to find a target object is a significant challenge. While semantic information is crucial for navigation, relying solely on it for decision-making may not always be efficient, especially in environments with weak semantic cues. Additionally, many methods are susceptible to misdetections, especially in environments with visually similar objects. To address these limitations, we propose ApexNav, a zero-shot object navigation framework that is both more efficient and reliable. For efficiency, ApexNav adaptively utilizes semantic information by analyzing its distribution in the environment, guiding exploration through semantic reasoning when cues are strong, and switching to geometry-based exploration when they are weak. For reliability, we propose a target-centric semantic fusion method that preserves long-term memory of the target and similar objects, enabling robust object identification even under noisy detections. We evaluate ApexNav on the HM3Dv1, HM3Dv2, and MP3D datasets, where it outperforms state-of-the-art methods in both SR and SPL metrics. Comprehensive ablation studies further demonstrate the effectiveness of each module. Furthermore, real-world experiments validate the practicality of ApexNav in physical environments. The code will be released at https://github.com/Robotics-STAR-Lab/ApexNav.",
    "pdf_url": "https://arxiv.org/pdf/2504.14478v3",
    "github_url": "https://github.com/Robotics-STAR-Lab/ApexNav",
    "published": "2025-04-20T04:03:29+00:00",
    "updated": "2025-09-05T15:20:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.09843v2",
    "title": "ST-Booster: An Iterative SpatioTemporal Perception Booster for Vision-and-Language Navigation in Continuous Environments",
    "authors": [
      "Yue",
      "Zhou",
      "Xie"
    ],
    "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to navigate unknown, continuous spaces based on natural language instructions. Compared to discrete settings, VLN-CE poses two core perception challenges. First, the absence of predefined observation points leads to heterogeneous visual memories and weakened global spatial correlations. Second, cumulative reconstruction errors in three-dimensional scenes introduce structural noise, impairing local feature perception. To address these challenges, this paper proposes ST-Booster, an iterative spatiotemporal booster that enhances navigation performance through multi-granularity perception and instruction-aware reasoning. ST-Booster consists of three key modules -- Hierarchical SpatioTemporal Encoding (HSTE), Multi-Granularity Aligned Fusion (MGAF), and ValueGuided Waypoint Generation (VGWG). HSTE encodes long-term global memory using topological graphs and captures shortterm local details via grid maps. MGAF aligns these dualmap representations with instructions through geometry-aware knowledge fusion. The resulting representations are iteratively refined through pretraining tasks. During reasoning, VGWG generates Guided Attention Heatmaps (GAHs) to explicitly model environment-instruction relevance and optimize waypoint selection. Extensive comparative experiments and performance analyses are conducted, demonstrating that ST-Booster outperforms existing state-of-the-art methods, particularly in complex, disturbance-prone environments.",
    "pdf_url": "https://arxiv.org/pdf/2504.09843v2",
    "github_url": null,
    "published": "2025-04-14T03:29:08+00:00",
    "updated": "2025-12-02T07:31:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.09587v3",
    "title": "GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation",
    "authors": [
      "Xu",
      "Hu",
      "Gao"
    ],
    "summary": "Language-goal aerial navigation is a critical challenge in embodied AI, requiring UAVs to localize targets in complex environments such as urban blocks based on textual specification. Existing methods, often adapted from indoor navigation, struggle to scale due to limited field of view, semantic ambiguity among objects, and lack of structured spatial reasoning. In this work, we propose GeoNav, a geospatially aware multimodal agent to enable long-range navigation. GeoNav operates in three phases-landmark navigation, target search, and precise localization-mimicking human coarse-to-fine spatial strategies. To support such reasoning, it dynamically builds two different types of spatial memory. The first is a global but schematic cognitive map, which fuses prior textual geographic knowledge and embodied visual cues into a top-down, annotated form for fast navigation to the landmark region. The second is a local but delicate scene graph representing hierarchical spatial relationships between blocks, landmarks, and objects, which is used for definite target localization. On top of this structured representation, GeoNav employs a spatially aware, multimodal chain-of-thought prompting mechanism to enable multimodal large language models with efficient and interpretable decision-making across stages. On the CityNav urban navigation benchmark, GeoNav surpasses the current state-of-the-art by up to 12.53% in success rate and significantly improves navigation efficiency, even in hard-level tasks. Ablation studies highlight the importance of each module, showcasing how geospatial representations and coarse-to-fine reasoning enhance UAV navigation.",
    "pdf_url": "https://arxiv.org/pdf/2504.09587v3",
    "github_url": null,
    "published": "2025-04-13T14:12:42+00:00",
    "updated": "2025-05-12T00:59:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.08581v1",
    "title": "FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level Interactive Agents",
    "authors": [
      "Tan",
      "Ji",
      "Zhu"
    ],
    "summary": "The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality when it comes to queries upon object components. In this work, we present FMLGS, an approach that supports part-level open-vocabulary query within 3D Gaussian Splatting (3DGS). We propose an efficient pipeline for building and querying consistent object- and part-level semantics based on Segment Anything Model 2 (SAM2). We designed a semantic deviation strategy to solve the problem of language ambiguity among object parts, which interpolates the semantic features of fine-grained targets for enriched information. Once trained, we can query both objects and their describable parts using natural language. Comparisons with other state-of-the-art methods prove that our method can not only better locate specified part-level targets, but also achieve first-place performance concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further integrate FMLGS as a virtual agent that can interactively navigate through 3D scenes, locate targets, and respond to user demands through a chat interface, which demonstrates the potential of our work to be further expanded and applied in the future.",
    "pdf_url": "https://arxiv.org/pdf/2504.08581v1",
    "github_url": null,
    "published": "2025-04-11T14:33:27+00:00",
    "updated": "2025-04-11T14:33:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.08806v1",
    "title": "Endowing Embodied Agents with Spatial Reasoning Capabilities for Vision-and-Language Navigation",
    "authors": [
      "Ling",
      "Qianqian"
    ],
    "summary": "Enhancing the spatial perception capabilities of mobile robots is crucial for achieving embodied Vision-and-Language Navigation (VLN). Although significant progress has been made in simulated environments, directly transferring these capabilities to real-world scenarios often results in severe hallucination phenomena, causing robots to lose effective spatial awareness. To address this issue, we propose BrainNav, a bio-inspired spatial cognitive navigation framework inspired by biological spatial cognition theories and cognitive map theory. BrainNav integrates dual-map (coordinate map and topological map) and dual-orientation (relative orientation and absolute orientation) strategies, enabling real-time navigation through dynamic scene capture and path planning. Its five core modules-Hippocampal Memory Hub, Visual Cortex Perception Engine, Parietal Spatial Constructor, Prefrontal Decision Center, and Cerebellar Motion Execution Unit-mimic biological cognitive functions to reduce spatial hallucinations and enhance adaptability. Validated in a zero-shot real-world lab environment using the Limo Pro robot, BrainNav, compatible with GPT-4, outperforms existing State-of-the-Art (SOTA) Vision-and-Language Navigation in Continuous Environments (VLN-CE) methods without fine-tuning.",
    "pdf_url": "https://arxiv.org/pdf/2504.08806v1",
    "github_url": null,
    "published": "2025-04-09T02:19:22+00:00",
    "updated": "2025-04-09T02:19:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.00848v1",
    "title": "Zero-Shot 4D Lidar Panoptic Segmentation",
    "authors": [
      "Zhang",
      "Oep",
      "Leal-Taix"
    ],
    "summary": "Zero-shot 4D segmentation and recognition of arbitrary objects in Lidar is crucial for embodied navigation, with applications ranging from streaming perception to semantic mapping and localization. However, the primary challenge in advancing research and developing generalized, versatile methods for spatio-temporal scene understanding in Lidar lies in the scarcity of datasets that provide the necessary diversity and scale of annotations.To overcome these challenges, we propose SAL-4D (Segment Anything in Lidar--4D), a method that utilizes multi-modal robotic sensor setups as a bridge to distill recent developments in Video Object Segmentation (VOS) in conjunction with off-the-shelf Vision-Language foundation models to Lidar. We utilize VOS models to pseudo-label tracklets in short video sequences, annotate these tracklets with sequence-level CLIP tokens, and lift them to the 4D Lidar space using calibrated multi-modal sensory setups to distill them to our SAL-4D model. Due to temporal consistent predictions, we outperform prior art in 3D Zero-Shot Lidar Panoptic Segmentation (LPS) over $5$ PQ, and unlock Zero-Shot 4D-LPS.",
    "pdf_url": "https://arxiv.org/pdf/2504.00848v1",
    "github_url": null,
    "published": "2025-04-01T14:36:12+00:00",
    "updated": "2025-04-01T14:36:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.00682v2",
    "title": "Immersive Explainability: Visualizing Robot Navigation Decisions through XAI Semantic Scene Projections in Virtual Reality",
    "authors": [
      "Heuvel",
      "Mller",
      "Wessels"
    ],
    "summary": "End-to-end robot policies achieve high performance through neural networks trained via reinforcement learning (RL). Yet, their black box nature and abstract reasoning pose challenges for human-robot interaction (HRI), because humans may experience difficulty in understanding and predicting the robot's navigation decisions, hindering trust development. We present a virtual reality (VR) interface that visualizes explainable AI (XAI) outputs and the robot's lidar perception to support intuitive interpretation of RL-based navigation behavior. By visually highlighting objects based on their attribution scores, the interface grounds abstract policy explanations in the scene context. This XAI visualization bridges the gap between obscure numerical XAI attribution scores and a human-centric semantic level of explanation. A within-subjects study with 24 participants evaluated the effectiveness of our interface for four visualization conditions combining XAI and lidar. Participants ranked scene objects across navigation scenarios based on their importance to the robot, followed by a questionnaire assessing subjective understanding and predictability. Results show that semantic projection of attributions significantly enhances non-expert users' objective understanding and subjective awareness of robot behavior. In addition, lidar visualization further improves perceived predictability, underscoring the value of integrating XAI and sensor for transparent, trustworthy HRI.",
    "pdf_url": "https://arxiv.org/pdf/2504.00682v2",
    "github_url": null,
    "published": "2025-04-01T11:52:39+00:00",
    "updated": "2025-10-18T00:28:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.00580v2",
    "title": "MRHaD: Mixed Reality-based Hand-Drawn Map Editing Interface for Mobile Robot Navigation",
    "authors": [
      "Taki",
      "Kobayashi",
      "Iglesius"
    ],
    "summary": "Mobile robot navigation systems are increasingly relied upon in dynamic and complex environments, yet they often struggle with map inaccuracies and the resulting inefficient path planning. This paper presents MRHaD, a Mixed Reality-based Hand-drawn Map Editing Interface that enables intuitive, real-time map modifications through natural hand gestures. By integrating the MR head-mounted display with the robotic navigation system, operators can directly create hand-drawn restricted zones (HRZ), thereby bridging the gap between 2D map representations and the real-world environment. Comparative experiments against conventional 2D editing methods demonstrate that MRHaD significantly improves editing efficiency, map accuracy, and overall usability, contributing to safer and more efficient mobile robot operations. The proposed approach provides a robust technical foundation for advancing human-robot collaboration and establishing innovative interaction models that enhance the hybrid future of robotics and human society. For additional material, please check: https://mertcookimg.github.io/mrhad/",
    "pdf_url": "https://arxiv.org/pdf/2504.00580v2",
    "github_url": null,
    "published": "2025-04-01T09:34:02+00:00",
    "updated": "2025-07-28T01:14:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.24065v1",
    "title": "COSMO: Combination of Selective Memorization for Low-cost Vision-and-Language Navigation",
    "authors": [
      "Zhang",
      "Qiao",
      "Wang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) tasks have gained prominence within artificial intelligence research due to their potential application in fields like home assistants. Many contemporary VLN approaches, while based on transformer architectures, have increasingly incorporated additional components such as external knowledge bases or map information to enhance performance. These additions, while boosting performance, also lead to larger models and increased computational costs. In this paper, to achieve both high performance and low computational costs, we propose a novel architecture with the COmbination of Selective MemOrization (COSMO). Specifically, COSMO integrates state-space modules and transformer modules, and incorporates two VLN-customized selective state space modules: the Round Selective Scan (RSS) and the Cross-modal Selective State Space Module (CS3). RSS facilitates comprehensive inter-modal interactions within a single scan, while the CS3 module adapts the selective state space module into a dual-stream architecture, thereby enhancing the acquisition of cross-modal interactions. Experimental validations on three mainstream VLN benchmarks, REVERIE, R2R, and R2R-CE, not only demonstrate competitive navigation performance of our model but also show a significant reduction in computational costs.",
    "pdf_url": "https://arxiv.org/pdf/2503.24065v1",
    "github_url": null,
    "published": "2025-03-31T13:24:10+00:00",
    "updated": "2025-03-31T13:24:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.22942v1",
    "title": "Adaptive Interactive Navigation of Quadruped Robots using Large Language Models",
    "authors": [
      "Zhou",
      "Mu",
      "Song"
    ],
    "summary": "Robotic navigation in complex environments remains a critical research challenge. Traditional navigation methods focus on optimal trajectory generation within free space, struggling in environments lacking viable paths to the goal, such as disaster zones or cluttered warehouses. To address this gap, we propose an adaptive interactive navigation approach that proactively interacts with environments to create feasible paths to reach originally unavailable goals. Specifically, we present a primitive tree for task planning with large language models (LLMs), facilitating effective reasoning to determine interaction objects and sequences. To ensure robust subtask execution, we adopt reinforcement learning to pre-train a comprehensive skill library containing versatile locomotion and interaction behaviors for motion planning. Furthermore, we introduce an adaptive replanning method featuring two LLM-based modules: an advisor serving as a flexible replanning trigger and an arborist for autonomous plan adjustment. Integrated with the tree structure, the replanning mechanism allows for convenient node addition and pruning, enabling rapid plan modification in unknown environments. Comprehensive simulations and experiments have demonstrated our method's effectiveness and adaptivity in diverse scenarios. The supplementary video is available at page: https://youtu.be/W5ttPnSap2g.",
    "pdf_url": "https://arxiv.org/pdf/2503.22942v1",
    "github_url": null,
    "published": "2025-03-29T02:17:52+00:00",
    "updated": "2025-03-29T02:17:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.22201v1",
    "title": "Multi-modal Knowledge Distillation-based Human Trajectory Forecasting",
    "authors": [
      "Jeong",
      "Lee",
      "Park"
    ],
    "summary": "Pedestrian trajectory forecasting is crucial in various applications such as autonomous driving and mobile robot navigation. In such applications, camera-based perception enables the extraction of additional modalities (human pose, text) to enhance prediction accuracy. Indeed, we find that textual descriptions play a crucial role in integrating additional modalities into a unified understanding. However, online extraction of text requires the use of VLM, which may not be feasible for resource-constrained systems. To address this challenge, we propose a multi-modal knowledge distillation framework: a student model with limited modality is distilled from a teacher model trained with full range of modalities. The comprehensive knowledge of a teacher model trained with trajectory, human pose, and text is distilled into a student model using only trajectory or human pose as a sole supplement. In doing so, we separately distill the core locomotion insights from intra-agent multi-modality and inter-agent interaction. Our generalizable framework is validated with two state-of-the-art models across three datasets on both ego-view (JRDB, SIT) and BEV-view (ETH/UCY) setups, utilizing both annotated and VLM-generated text captions. Distilled student models show consistent improvement in all prediction metrics for both full and instantaneous observations, improving up to ~13%. The code is available at https://github.com/Jaewoo97/KDTF.",
    "pdf_url": "https://arxiv.org/pdf/2503.22201v1",
    "github_url": "https://github.com/Jaewoo97/KDTF",
    "published": "2025-03-28T07:32:51+00:00",
    "updated": "2025-03-28T07:32:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.21997v2",
    "title": "Beyond Omakase: Designing Shared Control for Navigation Robots with Blind People",
    "authors": [
      "Kamikubo",
      "Kayukawa",
      "Kaniwa"
    ],
    "summary": "Autonomous navigation robots can increase the independence of blind people but often limit user control, following what is called in Japanese an \"omakase\" approach where decisions are left to the robot. This research investigates ways to enhance user control in social robot navigation, based on two studies conducted with blind participants. The first study, involving structured interviews (N=14), identified crowded spaces as key areas with significant social challenges. The second study (N=13) explored navigation tasks with an autonomous robot in these environments and identified design strategies across different modes of autonomy. Participants preferred an active role, termed the \"boss\" mode, where they managed crowd interactions, while the \"monitor\" mode helped them assess the environment, negotiate movements, and interact with the robot. These findings highlight the importance of shared control and user involvement for blind users, offering valuable insights for designing future social navigation robots.",
    "pdf_url": "https://arxiv.org/pdf/2503.21997v2",
    "github_url": null,
    "published": "2025-03-27T21:32:24+00:00",
    "updated": "2025-03-31T05:46:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.21168v2",
    "title": "TAGA: A Tangent-Based Reactive Approach for Socially Compliant Robot Navigation Around Human Groups",
    "authors": [
      "Roy",
      "Rahman"
    ],
    "summary": "Robot navigation in densely populated environments presents significant challenges, particularly regarding the interplay between individual and group dynamics. Current navigation models predominantly address interactions with individual pedestrians while failing to account for human groups that naturally form in real-world settings. Conversely, the limited models implementing group-aware navigation typically prioritize group dynamics at the expense of individual interactions, both of which are essential for socially appropriate navigation. This research extends an existing simulation framework to incorporate both individual pedestrians and human groups. We present Tangent Action for Group Avoidance (TAGA), a modular reactive mechanism that can be integrated with existing navigation frameworks to enhance their group-awareness capabilities. TAGA dynamically modifies robot trajectories using tangent action-based avoidance strategies while preserving the underlying model's capacity to navigate around individuals. Additionally, we introduce Group Collision Rate (GCR), a novel metric to quantitatively assess how effectively robots maintain group integrity during navigation. Through comprehensive simulation-based benchmarking, we demonstrate that integrating TAGA with state-of-the-art navigation models (ORCA, Social Force, DS-RNN, and AG-RL) reduces group intrusions by 45.7-78.6% while maintaining comparable success rates and navigation efficiency. Future work will focus on real-world implementation and validation of this approach.",
    "pdf_url": "https://arxiv.org/pdf/2503.21168v2",
    "github_url": null,
    "published": "2025-03-27T05:37:16+00:00",
    "updated": "2025-08-22T16:18:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.20425v3",
    "title": "Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation",
    "authors": [
      "Alcedo",
      "Lima",
      "Alami"
    ],
    "summary": "Navigating in environments alongside humans requires agents to reason under uncertainty and account for the beliefs and intentions of those around them. Under a sequential decision-making framework, egocentric navigation can naturally be represented as a Markov Decision Process (MDP). However, social navigation additionally requires reasoning about the hidden beliefs of others, inherently leading to a Partially Observable Markov Decision Process (POMDP), where agents lack direct access to others' mental states. Inspired by Theory of Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based reinforcement learning architecture for social navigation, addressing the challenge of belief tracking in partially observable environments; and (2) a perspective-shift operator for belief estimation, leveraging recent work on Influence-based Abstractions (IBA) in structured multi-agent settings.",
    "pdf_url": "https://arxiv.org/pdf/2503.20425v3",
    "github_url": null,
    "published": "2025-03-26T10:59:08+00:00",
    "updated": "2025-09-02T14:25:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.18525v4",
    "title": "RoboTron-Nav: A Unified Framework for Embodied Navigation Integrating Perception, Planning, and Prediction",
    "authors": [
      "Zhong",
      "Feng",
      "Yan"
    ],
    "summary": "In language-guided visual navigation, agents locate target objects in unseen environments using natural language instructions. For reliable navigation in unfamiliar scenes, agents should possess strong perception, planning, and prediction capabilities. Additionally, when agents revisit previously explored areas during long-term navigation, they may retain irrelevant and redundant historical perceptions, leading to suboptimal results. In this work, we propose RoboTron-Nav, a unified framework that integrates perception, planning, and prediction capabilities through multitask collaborations on navigation and embodied question answering tasks, thereby enhancing navigation performances. Furthermore, RoboTron-Nav employs an adaptive 3D-aware history sampling strategy to effectively and efficiently utilize historical observations. By leveraging large language model, RoboTron-Nav comprehends diverse commands and complex visual scenes, resulting in appropriate navigation actions. RoboTron-Nav achieves an 81.1% success rate in object goal navigation on the $\\mathrm{CHORES}$-$\\mathbb{S}$ benchmark, setting a new state-of-the-art performance. Project page: https://yvfengzhong.github.io/RoboTron-Nav",
    "pdf_url": "https://arxiv.org/pdf/2503.18525v4",
    "github_url": null,
    "published": "2025-03-24T10:29:47+00:00",
    "updated": "2025-08-08T11:19:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.18065v3",
    "title": "Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation",
    "authors": [
      "Wei",
      "Lin",
      "Nie"
    ],
    "summary": "Data scarcity is a long-standing challenge in the Vision-Language Navigation (VLN) field, which extremely hinders the generalization of agents to unseen environments. Previous works primarily rely on additional simulator data or web-collected images/videos to improve the generalization. However, the simulator environments still face limited diversity, and the web-collected data often requires extensive labor to remove the noise. In this paper, we propose a Rewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates the unseen observation-instruction pairs via rewriting human-annotated training data. Benefiting from our rewriting mechanism, new observation-instruction pairs can be obtained in both simulator-free and labor-saving manners to promote generalization. Specifically, we first introduce Object-Enriched Observation Rewriting, where we combine Vision-Language Models (VLMs) and Large Language Models (LLMs) to derive rewritten object-enriched scene descriptions, enabling observation synthesis with diverse objects and spatial layouts via Text-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast Instruction Rewriting, which generates observation-aligned rewritten instructions by requiring LLMs to reason the difference between original and new observations. We further develop a mixing-then-focusing training strategy with a random observation cropping scheme, effectively enhancing data distribution diversity while suppressing augmentation data noise during training. Experiments on both the discrete environments (R2R, REVERIE, and R4R datasets) and continuous environments (R2R-CE dataset) show the superior performance and impressive generalization ability of our method. Code is available at https://github.com/SaDil13/VLN-RAM.",
    "pdf_url": "https://arxiv.org/pdf/2503.18065v3",
    "github_url": "https://github.com/SaDil13/VLN-RAM",
    "published": "2025-03-23T13:18:17+00:00",
    "updated": "2025-11-04T08:39:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.16394v1",
    "title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?",
    "authors": [
      "Perincherry",
      "Krantz",
      "Lee"
    ],
    "summary": "Vision-and-Language Navigation (VLN) agents are tasked with navigating an unseen environment using natural language instructions. In this work, we study if visual representations of sub-goals implied by the instructions can serve as navigational cues and lead to increased navigation performance. To synthesize these visual representations or imaginations, we leverage a text-to-image diffusion model on landmark references contained in segmented instructions. These imaginations are provided to VLN agents as an added modality to act as landmark cues and an auxiliary loss is added to explicitly encourage relating these with their corresponding referring expressions. Our findings reveal an increase in success rate (SR) of around 1 point and up to 0.5 points in success scaled by inverse path length (SPL) across agents. These results suggest that the proposed approach reinforces visual understanding compared to relying on language instructions alone. Code and data for our work can be found at https://www.akhilperincherry.com/VLN-Imagine-website/.",
    "pdf_url": "https://arxiv.org/pdf/2503.16394v1",
    "github_url": null,
    "published": "2025-03-20T17:53:12+00:00",
    "updated": "2025-03-20T17:53:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.17406v1",
    "title": "IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes",
    "authors": [
      "Zhang",
      "Zantout",
      "Kachana"
    ],
    "summary": "With the recent rise of large language models, vision-language models, and other general foundation models, there is growing potential for multimodal, multi-task robotics that can operate in diverse environments given natural language input. One such application is indoor navigation using natural language instructions. However, despite recent progress, this problem remains challenging due to the 3D spatial reasoning and semantic understanding required. Additionally, the language used may be imperfect or misaligned with the scene, further complicating the task. To address this challenge, we curate a benchmark dataset, IRef-VLA, for Interactive Referential Vision and Language-guided Action in 3D Scenes with imperfect references. IRef-VLA is the largest real-world dataset for the referential grounding task, consisting of over 11.5K scanned 3D rooms from existing datasets, 7.6M heuristically generated semantic relations, and 4.7M referential statements. Our dataset also contains semantic object and room annotations, scene graphs, navigable free space annotations, and is augmented with statements where the language has imperfections or ambiguities. We verify the generalizability of our dataset by evaluating with state-of-the-art models to obtain a performance baseline and also develop a graph-search baseline to demonstrate the performance bound and generation of alternatives using scene-graph knowledge. With this benchmark, we aim to provide a resource for 3D scene understanding that aids the development of robust, interactive navigation systems. The dataset and all source code is publicly released at https://github.com/HaochenZ11/IRef-VLA.",
    "pdf_url": "https://arxiv.org/pdf/2503.17406v1",
    "github_url": "https://github.com/HaochenZ11/IRef-VLA",
    "published": "2025-03-20T16:16:10+00:00",
    "updated": "2025-03-20T16:16:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.14931v1",
    "title": "Advancing a taxonomy for proxemics in robot social navigation",
    "authors": [
      "Nahum",
      "Edan",
      "Oron-Gilad"
    ],
    "summary": "Deploying robots in human environments requires effective social robot navigation. This article focuses on proxemics, proposing a new taxonomy and suggesting future directions through an analysis of state-of-the-art studies and the identification of research gaps. The various factors that affect the dynamic properties of proxemics patterns in human-robot interaction are thoroughly explored. To establish a coherent proxemics framework, we identified and organized the key parameters and attributes that shape proxemics behavior. Building on this framework, we introduce a novel approach to define proxemics in robot navigation, emphasizing the significant attributes that influence its structure and size. This leads to the development of a new taxonomy that serves as a foundation for guiding future research and development. Our findings underscore the complexity of defining personal distance, revealing it as a complex, multi-dimensional challenge. Furthermore, we highlight the flexible and dynamic nature of personal zone boundaries, which should be adaptable to different contexts and circumstances. Additionally, we propose a new layer for implementing proxemics in the navigation of social robots.",
    "pdf_url": "https://arxiv.org/pdf/2503.14931v1",
    "github_url": null,
    "published": "2025-03-19T06:33:03+00:00",
    "updated": "2025-03-19T06:33:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.14328v2",
    "title": "Risk-Sensitive Model Predictive Control for Interaction-Aware Planning -- A Sequential Convexification Algorithm",
    "authors": [
      "Wang",
      "Schuurmans",
      "Patrinos"
    ],
    "summary": "This paper considers risk-sensitive model predictive control for stochastic systems with a decision-dependent distribution. This class of systems is commonly found in human-robot interaction scenarios. We derive computationally tractable convex upper bounds to both the objective function, and to frequently used penalty terms for collision avoidance, allowing us to efficiently solve the generally nonconvex optimal control problem as a sequence of convex problems. Simulations of a robot navigating a corridor demonstrate the effectiveness and the computational advantage of the proposed approach.",
    "pdf_url": "https://arxiv.org/pdf/2503.14328v2",
    "github_url": null,
    "published": "2025-03-18T15:01:37+00:00",
    "updated": "2025-05-30T08:20:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.14229v3",
    "title": "HA-VLN 2.0: An Open Benchmark and Leaderboard for Human-Aware Navigation in Discrete and Continuous Environments with Dynamic Multi-Human Interactions",
    "authors": [
      "Dong",
      "Wu",
      "He"
    ],
    "summary": "Vision-and-Language Navigation (VLN) has been studied mainly in either discrete or continuous settings, with little attention to dynamic, crowded environments. We present HA-VLN 2.0, a unified benchmark introducing explicit social-awareness constraints. Our contributions are: (i) a standardized task and metrics capturing both goal accuracy and personal-space adherence; (ii) HAPS 2.0 dataset and simulators modeling multi-human interactions, outdoor contexts, and finer language-motion alignment; (iii) benchmarks on 16,844 socially grounded instructions, revealing sharp performance drops of leading agents under human dynamics and partial observability; and (iv) real-world robot experiments validating sim-to-real transfer, with an open leaderboard enabling transparent comparison. Results show that explicit social modeling improves navigation robustness and reduces collisions, underscoring the necessity of human-centric approaches. By releasing datasets, simulators, baselines, and protocols, HA-VLN 2.0 provides a strong foundation for safe, socially responsible navigation research.",
    "pdf_url": "https://arxiv.org/pdf/2503.14229v3",
    "github_url": null,
    "published": "2025-03-18T13:05:55+00:00",
    "updated": "2025-10-09T18:17:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.13966v1",
    "title": "FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation Tasks",
    "authors": [
      "Zhang",
      "Qiao",
      "Wang"
    ],
    "summary": "The aspiration of the Vision-and-Language Navigation (VLN) task has long been to develop an embodied agent with robust adaptability, capable of seamlessly transferring its navigation capabilities across various tasks. Despite remarkable advancements in recent years, most methods necessitate dataset-specific training, thereby lacking the capability to generalize across diverse datasets encompassing distinct types of instructions. Large language models (LLMs) have demonstrated exceptional reasoning and generalization abilities, exhibiting immense potential in robot action planning. In this paper, we propose FlexVLN, an innovative hierarchical approach to VLN that integrates the fundamental navigation ability of a supervised-learning-based Instruction Follower with the robust generalization ability of the LLM Planner, enabling effective generalization across diverse VLN datasets. Moreover, a verification mechanism and a multi-model integration mechanism are proposed to mitigate potential hallucinations by the LLM Planner and enhance execution accuracy of the Instruction Follower. We take REVERIE, SOON, and CVDN-target as out-of-domain datasets for assessing generalization ability. The generalization performance of FlexVLN surpasses that of all the previous methods to a large extent.",
    "pdf_url": "https://arxiv.org/pdf/2503.13966v1",
    "github_url": null,
    "published": "2025-03-18T06:58:41+00:00",
    "updated": "2025-03-18T06:58:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.13098v1",
    "title": "LIVEPOINT: Fully Decentralized, Safe, Deadlock-Free Multi-Robot Control in Cluttered Environments with High-Dimensional Inputs",
    "authors": [
      "Chen",
      "Chandra"
    ],
    "summary": "Fully decentralized, safe, and deadlock-free multi-robot navigation in dynamic, cluttered environments is a critical challenge in robotics. Current methods require exact state measurements in order to enforce safety and liveness e.g. via control barrier functions (CBFs), which is challenging to achieve directly from onboard sensors like lidars and cameras. This work introduces LIVEPOINT, a decentralized control framework that synthesizes universal CBFs over point clouds to enable safe, deadlock-free real-time multi-robot navigation in dynamic, cluttered environments. Further, LIVEPOINT ensures minimally invasive deadlock avoidance behavior by dynamically adjusting agents' speeds based on a novel symmetric interaction metric. We validate our approach in simulation experiments across highly constrained multi-robot scenarios like doorways and intersections. Results demonstrate that LIVEPOINT achieves zero collisions or deadlocks and a 100% success rate in challenging settings compared to optimization-based baselines such as MPC and ORCA and neural methods such as MPNet, which fail in such environments. Despite prioritizing safety and liveness, LIVEPOINT is 35% smoother than baselines in the doorway environment, and maintains agility in constrained environments while still being safe and deadlock-free.",
    "pdf_url": "https://arxiv.org/pdf/2503.13098v1",
    "github_url": null,
    "published": "2025-03-17T12:07:25+00:00",
    "updated": "2025-03-17T12:07:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.11091v1",
    "title": "Aerial Vision-and-Language Navigation with Grid-based View Selection and Map Construction",
    "authors": [
      "Zhao",
      "Li",
      "Pan"
    ],
    "summary": "Aerial Vision-and-Language Navigation (Aerial VLN) aims to obtain an unmanned aerial vehicle agent to navigate aerial 3D environments following human instruction. Compared to ground-based VLN, aerial VLN requires the agent to decide the next action in both horizontal and vertical directions based on the first-person view observations. Previous methods struggle to perform well due to the longer navigation path, more complicated 3D scenes, and the neglect of the interplay between vertical and horizontal actions. In this paper, we propose a novel grid-based view selection framework that formulates aerial VLN action prediction as a grid-based view selection task, incorporating vertical action prediction in a manner that accounts for the coupling with horizontal actions, thereby enabling effective altitude adjustments. We further introduce a grid-based bird's eye view map for aerial space to fuse the visual information in the navigation history, provide contextual scene information, and mitigate the impact of obstacles. Finally, a cross-modal transformer is adopted to explicitly align the long navigation history with the instruction. We demonstrate the superiority of our method in extensive experiments.",
    "pdf_url": "https://arxiv.org/pdf/2503.11091v1",
    "github_url": null,
    "published": "2025-03-14T05:20:43+00:00",
    "updated": "2025-03-14T05:20:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.11081v1",
    "title": "MoMa-Kitchen: A 100K+ Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation",
    "authors": [
      "Zhang",
      "Gao",
      "Wu"
    ],
    "summary": "In mobile manipulation, navigation and manipulation are often treated as separate problems, resulting in a significant gap between merely approaching an object and engaging with it effectively. Many navigation approaches primarily define success by proximity to the target, often overlooking the necessity for optimal positioning that facilitates subsequent manipulation. To address this, we introduce MoMa-Kitchen, a benchmark dataset comprising over 100k samples that provide training data for models to learn optimal final navigation positions for seamless transition to manipulation. Our dataset includes affordance-grounded floor labels collected from diverse kitchen environments, in which robotic mobile manipulators of different models attempt to grasp target objects amidst clutter. Using a fully automated pipeline, we simulate diverse real-world scenarios and generate affordance labels for optimal manipulation positions. Visual data are collected from RGB-D inputs captured by a first-person view camera mounted on the robotic arm, ensuring consistency in viewpoint during data collection. We also develop a lightweight baseline model, NavAff, for navigation affordance grounding that demonstrates promising performance on the MoMa-Kitchen benchmark. Our approach enables models to learn affordance-based final positioning that accommodates different arm types and platform heights, thereby paving the way for more robust and generalizable integration of navigation and manipulation in embodied AI. Project page: \\href{https://momakitchen.github.io/}{https://momakitchen.github.io/}.",
    "pdf_url": "https://arxiv.org/pdf/2503.11081v1",
    "github_url": null,
    "published": "2025-03-14T04:47:38+00:00",
    "updated": "2025-03-14T04:47:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.11006v1",
    "title": "Observation-Graph Interaction and Key-Detail Guidance for Vision and Language Navigation",
    "authors": [
      "Xie",
      "Ou",
      "Ma"
    ],
    "summary": "Vision and Language Navigation (VLN) requires an agent to navigate through environments following natural language instructions. However, existing methods often struggle with effectively integrating visual observations and instruction details during navigation, leading to suboptimal path planning and limited success rates. In this paper, we propose OIKG (Observation-graph Interaction and Key-detail Guidance), a novel framework that addresses these limitations through two key components: (1) an observation-graph interaction module that decouples angular and visual information while strengthening edge representations in the navigation space, and (2) a key-detail guidance module that dynamically extracts and utilizes fine-grained location and object information from instructions. By enabling more precise cross-modal alignment and dynamic instruction interpretation, our approach significantly improves the agent's ability to follow complex navigation instructions. Extensive experiments on the R2R and RxR datasets demonstrate that OIKG achieves state-of-the-art performance across multiple evaluation metrics, validating the effectiveness of our method in enhancing navigation precision through better observation-instruction alignment.",
    "pdf_url": "https://arxiv.org/pdf/2503.11006v1",
    "github_url": null,
    "published": "2025-03-14T02:05:16+00:00",
    "updated": "2025-03-14T02:05:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.10070v1",
    "title": "AhaRobot: A Low-Cost Open-Source Bimanual Mobile Manipulator for Embodied AI",
    "authors": [
      "Cui",
      "Yuan",
      "Zheng"
    ],
    "summary": "Navigation and manipulation in open-world environments remain unsolved challenges in the Embodied AI. The high cost of commercial mobile manipulation robots significantly limits research in real-world scenes. To address this issue, we propose AhaRobot, a low-cost and fully open-source dual-arm mobile manipulation robot system with a hardware cost of only $1,000 (excluding optional computational resources), which is less than 1/15 of the cost of popular mobile robots. The AhaRobot system consists of three components: (1) a novel low-cost hardware architecture primarily composed of off-the-shelf components, (2) an optimized control solution to enhance operational precision integrating dual-motor backlash control and static friction compensation, and (3) a simple remote teleoperation method RoboPilot. We use handles to control the dual arms and pedals for whole-body movement. The teleoperation process is low-burden and easy to operate, much like piloting. RoboPilot is designed for remote data collection in embodied scenarios. Experimental results demonstrate that RoboPilot significantly enhances data collection efficiency in complex manipulation tasks, achieving a 30% increase compared to methods using 3D mouse and leader-follower systems. It also excels at completing extremely long-horizon tasks in one go. Furthermore, AhaRobot can be used to learn end-to-end policies and autonomously perform complex manipulation tasks, such as pen insertion and cleaning up the floor. We aim to build an affordable yet powerful platform to promote the development of embodied tasks on real devices, advancing more robust and reliable embodied AI. All hardware and software systems are available at https://aha-robot.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2503.10070v1",
    "github_url": null,
    "published": "2025-03-13T05:34:43+00:00",
    "updated": "2025-03-13T05:34:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.10069v2",
    "title": "SmartWay: Enhanced Waypoint Prediction and Backtracking for Zero-Shot Vision-and-Language Navigation",
    "authors": [
      "Shi",
      "Li",
      "Lyu"
    ],
    "summary": "Vision-and-Language Navigation (VLN) in continuous environments requires agents to interpret natural language instructions while navigating unconstrained 3D spaces. Existing VLN-CE frameworks rely on a two-stage approach: a waypoint predictor to generate waypoints and a navigator to execute movements. However, current waypoint predictors struggle with spatial awareness, while navigators lack historical reasoning and backtracking capabilities, limiting adaptability. We propose a zero-shot VLN-CE framework integrating an enhanced waypoint predictor with a Multi-modal Large Language Model (MLLM)-based navigator. Our predictor employs a stronger vision encoder, masked cross-attention fusion, and an occupancy-aware loss for better waypoint quality. The navigator incorporates history-aware reasoning and adaptive path planning with backtracking, improving robustness. Experiments on R2R-CE and MP3D benchmarks show our method achieves state-of-the-art (SOTA) performance in zero-shot settings, demonstrating competitive results compared to fully supervised methods. Real-world validation on Turtlebot 4 further highlights its adaptability.",
    "pdf_url": "https://arxiv.org/pdf/2503.10069v2",
    "github_url": null,
    "published": "2025-03-13T05:32:57+00:00",
    "updated": "2025-06-17T05:47:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.09938v1",
    "title": "PanoGen++: Domain-Adapted Text-Guided Panoramic Environment Generation for Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "Zhou",
      "Xie"
    ],
    "summary": "Vision-and-language navigation (VLN) tasks require agents to navigate three-dimensional environments guided by natural language instructions, offering substantial potential for diverse applications. However, the scarcity of training data impedes progress in this field. This paper introduces PanoGen++, a novel framework that addresses this limitation by generating varied and pertinent panoramic environments for VLN tasks. PanoGen++ incorporates pre-trained diffusion models with domain-specific fine-tuning, employing parameter-efficient techniques such as low-rank adaptation to minimize computational costs. We investigate two settings for environment generation: masked image inpainting and recursive image outpainting. The former maximizes novel environment creation by inpainting masked regions based on textual descriptions, while the latter facilitates agents' learning of spatial relationships within panoramas. Empirical evaluations on room-to-room (R2R), room-for-room (R4R), and cooperative vision-and-dialog navigation (CVDN) datasets reveal significant performance enhancements: a 2.44% increase in success rate on the R2R test leaderboard, a 0.63% improvement on the R4R validation unseen set, and a 0.75-meter enhancement in goal progress on the CVDN validation unseen set. PanoGen++ augments the diversity and relevance of training environments, resulting in improved generalization and efficacy in VLN tasks.",
    "pdf_url": "https://arxiv.org/pdf/2503.09938v1",
    "github_url": null,
    "published": "2025-03-13T01:16:58+00:00",
    "updated": "2025-03-13T01:16:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.09758v1",
    "title": "Multi-Agent LLM Actor-Critic Framework for Social Robot Navigation",
    "authors": [
      "Wang",
      "Obi",
      "Min"
    ],
    "summary": "Recent advances in robotics and large language models (LLMs) have sparked growing interest in human-robot collaboration and embodied intelligence. To enable the broader deployment of robots in human-populated environments, socially-aware robot navigation (SAN) has become a key research area. While deep reinforcement learning approaches that integrate human-robot interaction (HRI) with path planning have demonstrated strong benchmark performance, they often struggle to adapt to new scenarios and environments. LLMs offer a promising avenue for zero-shot navigation through commonsense inference. However, most existing LLM-based frameworks rely on centralized decision-making, lack robust verification mechanisms, and face inconsistencies in translating macro-actions into precise low-level control signals. To address these challenges, we propose SAMALM, a decentralized multi-agent LLM actor-critic framework for multi-robot social navigation. In this framework, a set of parallel LLM actors, each reflecting distinct robot personalities or configurations, directly generate control signals. These actions undergo a two-tier verification process via a global critic that evaluates group-level behaviors and individual critics that assess each robot's context. An entropy-based score fusion mechanism further enhances self-verification and re-query, improving both robustness and coordination. Experimental results confirm that SAMALM effectively balances local autonomy with global oversight, yielding socially compliant behaviors and strong adaptability across diverse multi-robot scenarios. More details and videos about this work are available at: https://sites.google.com/view/SAMALM.",
    "pdf_url": "https://arxiv.org/pdf/2503.09758v1",
    "github_url": null,
    "published": "2025-03-12T18:59:53+00:00",
    "updated": "2025-03-12T18:59:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.08858v3",
    "title": "SICNav-Diffusion: Safe and Interactive Crowd Navigation with Diffusion Trajectory Predictions",
    "authors": [
      "Samavi",
      "Lem",
      "Sato"
    ],
    "summary": "To navigate crowds without collisions, robots must interact with humans by forecasting their future motion and reacting accordingly. While learning-based prediction models have shown success in generating likely human trajectory predictions, integrating these stochastic models into a robot controller presents several challenges. The controller needs to account for interactive coupling between planned robot motion and human predictions while ensuring both predictions and robot actions are safe (i.e. collision-free). To address these challenges, we present a receding horizon crowd navigation method for single-robot multi-human environments. We first propose a diffusion model to generate joint trajectory predictions for all humans in the scene. We then incorporate these multi-modal predictions into a SICNav Bilevel MPC problem that simultaneously solves for a robot plan (upper-level) and acts as a safety filter to refine the predictions for non-collision (lower-level). Combining planning and prediction refinement into one bilevel problem ensures that the robot plan and human predictions are coupled. We validate the open-loop trajectory prediction performance of our diffusion model on the commonly used ETH/UCY benchmark and evaluate the closed-loop performance of our robot navigation method in simulation and extensive real-robot experiments demonstrating safe, efficient, and reactive robot motion.",
    "pdf_url": "https://arxiv.org/pdf/2503.08858v3",
    "github_url": null,
    "published": "2025-03-11T19:54:50+00:00",
    "updated": "2025-06-30T16:22:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.08367v2",
    "title": "Embodied Crowd Counting",
    "authors": [
      "Long",
      "Wang",
      "Wan"
    ],
    "summary": "Occlusion is one of the fundamental challenges in crowd counting. In the community, various data-driven approaches have been developed to address this issue, yet their effectiveness is limited. This is mainly because most existing crowd counting datasets on which the methods are trained are based on passive cameras, restricting their ability to fully sense the environment. Recently, embodied navigation methods have shown significant potential in precise object detection in interactive scenes. These methods incorporate active camera settings, holding promise in addressing the fundamental issues in crowd counting. However, most existing methods are designed for indoor navigation, showing unknown performance in analyzing complex object distribution in large scale scenes, such as crowds. Besides, most existing embodied navigation datasets are indoor scenes with limited scale and object quantity, preventing them from being introduced into dense crowd analysis. Based on this, a novel task, Embodied Crowd Counting (ECC), is proposed. We first build up an interactive simulator, Embodied Crowd Counting Dataset (ECCD), which enables large scale scenes and large object quantity. A prior probability distribution that approximates realistic crowd distribution is introduced to generate crowds. Then, a zero-shot navigation method (ZECC) is proposed. This method contains a MLLM driven coarse-to-fine navigation mechanism, enabling active Z-axis exploration, and a normal-line-based crowd distribution analysis method for fine counting. Experimental results against baselines show that the proposed method achieves the best trade-off between counting accuracy and navigation cost.",
    "pdf_url": "https://arxiv.org/pdf/2503.08367v2",
    "github_url": null,
    "published": "2025-03-11T12:23:34+00:00",
    "updated": "2025-11-25T08:42:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.08306v4",
    "title": "Reasoning in visual navigation of end-to-end trained agents: a dynamical systems approach",
    "authors": [
      "Janny",
      "Poirier",
      "Antsfeld"
    ],
    "summary": "Progress in Embodied AI has made it possible for end-to-end-trained agents to navigate in photo-realistic environments with high-level reasoning and zero-shot or language-conditioned behavior, but benchmarks are still dominated by simulation. In this work, we focus on the fine-grained behavior of fast-moving real robots and present a large-scale experimental study involving \\numepisodes{} navigation episodes in a real environment with a physical robot, where we analyze the type of reasoning emerging from end-to-end training. In particular, we study the presence of realistic dynamics which the agent learned for open-loop forecasting, and their interplay with sensing. We analyze the way the agent uses latent memory to hold elements of the scene structure and information gathered during exploration. We probe the planning capabilities of the agent, and find in its memory evidence for somewhat precise plans over a limited horizon. Furthermore, we show in a post-hoc analysis that the value function learned by the agent relates to long-term planning. Put together, our experiments paint a new picture on how using tools from computer vision and sequential decision making have led to new capabilities in robotics and control. An interactive tool is available at europe.naverlabs.com/research/publications/reasoning-in-visual-navigation-of-end-to-end-trained-agents.",
    "pdf_url": "https://arxiv.org/pdf/2503.08306v4",
    "github_url": null,
    "published": "2025-03-11T11:16:47+00:00",
    "updated": "2025-04-15T08:24:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.08174v1",
    "title": "Investigating the Effectiveness of a Socratic Chain-of-Thoughts Reasoning Method for Task Planning in Robotics, A Case Study",
    "authors": [
      "Bot",
      "Xu"
    ],
    "summary": "Large language models (LLMs) have demonstrated unprecedented capability in reasoning with natural language. Coupled with this development is the emergence of embodied AI in robotics. Despite showing promise for verbal and written reasoning tasks, it remains unknown whether LLMs are capable of navigating complex spatial tasks with physical actions in the real world. To this end, it is of interest to investigate applying LLMs to robotics in zero-shot learning scenarios, and in the absence of fine-tuning - a feat which could significantly improve human-robot interaction, alleviate compute cost, and eliminate low-level programming tasks associated with robot tasks.   To explore this question, we apply GPT-4(Omni) with a simulated Tiago robot in Webots engine for an object search task. We evaluate the effectiveness of three reasoning strategies based on Chain-of-Thought (CoT) sub-task list generation with the Socratic method (SocraCoT) (in order of increasing rigor): (1) Non-CoT/Non-SocraCoT, (2) CoT only, and (3) SocraCoT. Performance was measured in terms of the proportion of tasks successfully completed and execution time (N = 20). Our preliminary results show that when combined with chain-of-thought reasoning, the Socratic method can be used for code generation for robotic tasks that require spatial awareness. In extension of this finding, we propose EVINCE-LoC; a modified EVINCE method that could further enhance performance in highly complex and or dynamic testing scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2503.08174v1",
    "github_url": null,
    "published": "2025-03-11T08:36:37+00:00",
    "updated": "2025-03-11T08:36:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.07902v2",
    "title": "LTLCodeGen: Code Generation of Syntactically Correct Temporal Logic for Robot Task Planning",
    "authors": [
      "Rabiei",
      "R.",
      "Dai"
    ],
    "summary": "This paper focuses on planning robot navigation tasks from natural language specifications. We develop a modular approach, where a large language model (LLM) translates the natural language instructions into a linear temporal logic (LTL) formula with propositions defined by object classes in a semantic occupancy map. The LTL formula and the semantic occupancy map are provided to a motion planning algorithm to generate a collision-free robot path that satisfies the natural language instructions. Our main contribution is LTLCodeGen, a method to translate natural language to syntactically correct LTL using code generation. We demonstrate the complete task planning method in real-world experiments involving human speech to provide navigation instructions to a mobile robot. We also thoroughly evaluate our approach in simulated and real-world experiments in comparison to end-to-end LLM task planning and state-of-the-art LLM-to-LTL translation methods.",
    "pdf_url": "https://arxiv.org/pdf/2503.07902v2",
    "github_url": null,
    "published": "2025-03-10T22:43:13+00:00",
    "updated": "2025-08-06T06:20:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.07323v2",
    "title": "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning",
    "authors": [
      "Zhao",
      "Wu",
      "Wang"
    ],
    "summary": "This paper advances motion agents empowered by large language models (LLMs) toward autonomous navigation in dynamic and cluttered environments, significantly surpassing first and recent seminal but limited studies on LLM's spatial reasoning, where movements are restricted in four directions in simple, static environments in the presence of only single agents much less multiple agents. Specifically, we investigate LLMs as spatial reasoners to overcome these limitations by uniformly encoding environments (e.g., real indoor floorplans), agents which can be dynamic obstacles and their paths as discrete tokens akin to language tokens. Our training-free framework supports multi-agent coordination, closed-loop replanning, and dynamic obstacle avoidance without retraining or fine-tuning. We show that LLMs can generalize across agents, tasks, and environments using only text-based interactions, opening new possibilities for semantically grounded, interactive navigation in both simulation and embodied systems.",
    "pdf_url": "https://arxiv.org/pdf/2503.07323v2",
    "github_url": null,
    "published": "2025-03-10T13:39:09+00:00",
    "updated": "2025-06-05T12:17:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.06937v2",
    "title": "Handle Object Navigation as Weighted Traveling Repairman Problem",
    "authors": [
      "Liu",
      "Xu",
      "Yuan"
    ],
    "summary": "Zero-Shot Object Navigation (ZSON) requires agents to navigate to objects specified via open-ended natural language without predefined categories or prior environmental knowledge. While recent methods leverage foundation models or multi-modal maps, they often rely on 2D representations and greedy strategies or require additional training or modules with high computation load, limiting performance in complex environments and real applications. We propose WTRP-Searcher, a novel framework that formulates ZSON as a Weighted Traveling Repairman Problem (WTRP), minimizing the weighted waiting time of viewpoints. Using a Vision-Language Model (VLM), we score viewpoints based on object-description similarity, projected onto a 2D map with depth information. An open-vocabulary detector identifies targets, dynamically updating goals, while a 3D embedding feature map enhances spatial awareness and environmental recall. WTRP-Searcher outperforms existing methods, offering efficient global planning and improved performance in complex ZSON tasks. Code and design will be open-sourced upon acceptance.",
    "pdf_url": "https://arxiv.org/pdf/2503.06937v2",
    "github_url": null,
    "published": "2025-03-10T05:32:45+00:00",
    "updated": "2025-09-18T15:28:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.03208v1",
    "title": "Embodied Escaping: End-to-End Reinforcement Learning for Robot Navigation in Narrow Environment",
    "authors": [
      "Zheng",
      "Zhang",
      "Jiang"
    ],
    "summary": "Autonomous navigation is a fundamental task for robot vacuum cleaners in indoor environments. Since their core function is to clean entire areas, robots inevitably encounter dead zones in cluttered and narrow scenarios. Existing planning methods often fail to escape due to complex environmental constraints, high-dimensional search spaces, and high difficulty maneuvers. To address these challenges, this paper proposes an embodied escaping model that leverages reinforcement learning-based policy with an efficient action mask for dead zone escaping. To alleviate the issue of the sparse reward in training, we introduce a hybrid training policy that improves learning efficiency. In handling redundant and ineffective action options, we design a novel action representation to reshape the discrete action space with a uniform turning radius. Furthermore, we develop an action mask strategy to select valid action quickly, balancing precision and efficiency. In real-world experiments, our robot is equipped with a Lidar, IMU, and two-wheel encoders. Extensive quantitative and qualitative experiments across varying difficulty levels demonstrate that our robot can consistently escape from challenging dead zones. Moreover, our approach significantly outperforms compared path planning and reinforcement learning methods in terms of success rate and collision avoidance.",
    "pdf_url": "https://arxiv.org/pdf/2503.03208v1",
    "github_url": null,
    "published": "2025-03-05T05:53:08+00:00",
    "updated": "2025-03-05T05:53:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.02247v5",
    "title": "WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation",
    "authors": [
      "Nie",
      "Guo",
      "Duan"
    ],
    "summary": "Object Goal Navigation-requiring an agent to locate a specific object in an unseen environment-remains a core challenge in embodied AI. Although recent progress in Vision-Language Model (VLM)-based agents has demonstrated promising perception and decision-making abilities through prompting, none has yet established a fully modular world model design that reduces risky and costly interactions with the environment by predicting the future state of the world. We introduce WMNav, a novel World Model-based Navigation framework powered by Vision-Language Models (VLMs). It predicts possible outcomes of decisions and builds memories to provide feedback to the policy module. To retain the predicted state of the environment, WMNav proposes the online maintained Curiosity Value Map as part of the world model memory to provide dynamic configuration for navigation policy. By decomposing according to a human-like thinking process, WMNav effectively alleviates the impact of model hallucination by making decisions based on the feedback difference between the world model plan and observation. To further boost efficiency, we implement a two-stage action proposer strategy: broad exploration followed by precise localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses existing zero-shot benchmarks in both success rate and exploration efficiency (absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.",
    "pdf_url": "https://arxiv.org/pdf/2503.02247v5",
    "github_url": null,
    "published": "2025-03-04T03:51:36+00:00",
    "updated": "2025-07-19T03:44:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.02223v1",
    "title": "DQO-MAP: Dual Quadrics Multi-Object mapping with Gaussian Splatting",
    "authors": [
      "Li",
      "Ye",
      "Hao"
    ],
    "summary": "Accurate object perception is essential for robotic applications such as object navigation. In this paper, we propose DQO-MAP, a novel object-SLAM system that seamlessly integrates object pose estimation and reconstruction. We employ 3D Gaussian Splatting for high-fidelity object reconstruction and leverage quadrics for precise object pose estimation. Both of them management is handled on the CPU, while optimization is performed on the GPU, significantly improving system efficiency. By associating objects with unique IDs, our system enables rapid object extraction from the scene. Extensive experimental results on object reconstruction and pose estimation demonstrate that DQO-MAP achieves outstanding performance in terms of precision, reconstruction quality, and computational efficiency. The code and dataset are available at: https://github.com/LiHaoy-ux/DQO-MAP.",
    "pdf_url": "https://arxiv.org/pdf/2503.02223v1",
    "github_url": "https://github.com/LiHaoy-ux/DQO-MAP",
    "published": "2025-03-04T02:55:07+00:00",
    "updated": "2025-03-04T02:55:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.01474v2",
    "title": "Interactive Navigation for Legged Manipulators with Learned Arm-Pushing Controller",
    "authors": [
      "Bi",
      "Chen",
      "Zheng"
    ],
    "summary": "Interactive navigation is crucial in scenarios where proactively interacting with objects can yield shorter paths, thus significantly improving traversal efficiency. Existing methods primarily focus on using the robot body to relocate large obstacles (which could be comparable to the size of a robot). However, they prove ineffective in narrow or constrained spaces where the robot's dimensions restrict its manipulation capabilities. This paper introduces a novel interactive navigation framework for legged manipulators, featuring an active arm-pushing mechanism that enables the robot to reposition movable obstacles in space-constrained environments. To this end, we develop a reinforcement learning-based arm-pushing controller with a two-stage reward strategy for large-object manipulation. Specifically, this strategy first directs the manipulator to a designated pushing zone to achieve a kinematically feasible contact configuration. Then, the end effector is guided to maintain its position at appropriate contact points for stable object displacement while preventing toppling. The simulations validate the robustness of the arm-pushing controller, showing that the two-stage reward strategy improves policy convergence and long-term performance. Real-world experiments further demonstrate the effectiveness of the proposed navigation framework, which achieves shorter paths and reduced traversal time. The open-source project can be found at https://github.com/Zhihaibi/Interactive-Navigation-for-legged-manipulator.git.",
    "pdf_url": "https://arxiv.org/pdf/2503.01474v2",
    "github_url": "https://github.com/Zhihaibi/Interactive-Navigation-for-legged-manipulator",
    "published": "2025-03-03T12:29:48+00:00",
    "updated": "2025-07-21T14:00:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.00727v1",
    "title": "From Understanding the World to Intervening in It: A Unified Multi-Scale Framework for Embodied Cognition",
    "authors": [
      "Wang"
    ],
    "summary": "In this paper, we propose AUKAI, an Adaptive Unified Knowledge-Action Intelligence for embodied cognition that seamlessly integrates perception, memory, and decision-making via multi-scale error feedback. Interpreting AUKAI as an embedded world model, our approach simultaneously predicts state transitions and evaluates intervention utility. The framework is underpinned by rigorous theoretical analysis drawn from convergence theory, optimal control, and Bayesian inference, which collectively establish conditions for convergence, stability, and near-optimal performance. Furthermore, we present a hybrid implementation that combines the strengths of neural networks with symbolic reasoning modules, thereby enhancing interpretability and robustness. Finally, we demonstrate the potential of AUKAI through a detailed application in robotic navigation and obstacle avoidance, and we outline comprehensive experimental plans to validate its effectiveness in both simulated and real-world environments.",
    "pdf_url": "https://arxiv.org/pdf/2503.00727v1",
    "github_url": null,
    "published": "2025-03-02T04:43:08+00:00",
    "updated": "2025-03-02T04:43:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.00397v3",
    "title": "Floorplan-SLAM: A Real-Time, High-Accuracy, and Long-Term Multi-Session Point-Plane SLAM for Efficient Floorplan Reconstruction",
    "authors": [
      "Wang",
      "Lv",
      "Wei"
    ],
    "summary": "Floorplan reconstruction provides structural priors essential for reliable indoor robot navigation and high-level scene understanding. However, existing approaches either require time-consuming offline processing with a complete map, or rely on expensive sensors and substantial computational resources. To address the problems, we propose Floorplan-SLAM, which incorporates floorplan reconstruction tightly into a multi-session SLAM system by seamlessly interacting with plane extraction, pose estimation, and back-end optimization, achieving real-time, high-accuracy, and long-term floorplan reconstruction using only a stereo camera. Specifically, we present a robust plane extraction algorithm that operates in a compact plane parameter space and leverages spatially complementary features to accurately detect planar structures, even in weakly textured scenes. Furthermore, we propose a floorplan reconstruction module tightly coupled with the SLAM system, which uses continuously optimized plane landmarks and poses to formulate and solve a novel optimization problem, thereby enabling real-time incremental floorplan reconstruction. Note that by leveraging the map merging capability of multi-session SLAM, our method supports long-term floorplan reconstruction across multiple sessions without redundant data collection. Experiments on the VECtor and the self-collected datasets indicate that Floorplan-SLAM significantly outperforms state-of-the-art methods in terms of plane extraction robustness, pose estimation accuracy, and floorplan reconstruction fidelity and speed, achieving real-time performance at 25-45 FPS without GPU acceleration, which reduces the floorplan reconstruction time for a 1000 square meters scene from over 10 hours to just 9.44 minutes.",
    "pdf_url": "https://arxiv.org/pdf/2503.00397v3",
    "github_url": null,
    "published": "2025-03-01T08:18:11+00:00",
    "updated": "2025-03-05T08:09:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.19024v1",
    "title": "Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments",
    "authors": [
      "Li",
      "Zhou",
      "Hong"
    ],
    "summary": "Vision-and-Language Navigation (VLN) empowers agents to associate time-sequenced visual observations with corresponding instructions to make sequential decisions. However, generalization remains a persistent challenge, particularly when dealing with visually diverse scenes or transitioning from simulated environments to real-world deployment. In this paper, we address the mismatch between human-centric instructions and quadruped robots with a low-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav) approach to mitigate this issue. This work represents the first attempt to highlight the generalization gap in VLN across varying heights of visual observation in realistic robot deployments. Our approach leverages weighted historical observations as enriched spatiotemporal contexts for instruction following, effectively managing feature collisions within cells by assigning appropriate weights to identical features across different viewpoints. This enables low-height robots to overcome challenges such as visual obstructions and perceptual mismatches. Additionally, we transfer the connectivity graph from the HM3D and Gibson datasets as an extra resource to enhance spatial priors and a more comprehensive representation of real-world scenarios, leading to improved performance and generalizability of the waypoint predictor in real-world environments. Extensive experiments demonstrate that our Ground-level Viewpoint Navigation (GVnav) approach significantly improves performance in both simulated environments and real-world deployments with quadruped robots.",
    "pdf_url": "https://arxiv.org/pdf/2502.19024v1",
    "github_url": null,
    "published": "2025-02-26T10:30:40+00:00",
    "updated": "2025-02-26T10:30:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.18041v6",
    "title": "OpenFly: A Comprehensive Platform for Aerial Vision-Language Navigation",
    "authors": [
      "Gao",
      "Li",
      "You"
    ],
    "summary": "Vision-Language Navigation (VLN) aims to guide agents by leveraging language instructions and visual cues, playing a pivotal role in embodied AI. Indoor VLN has been extensively studied, whereas outdoor aerial VLN remains underexplored. The potential reason is that outdoor aerial view encompasses vast areas, making data collection more challenging, which results in a lack of benchmarks. To address this problem, we propose OpenFly, a platform comprising various rendering engines, a versatile toolchain, and a large-scale benchmark for aerial VLN. Firstly, we integrate diverse rendering engines and advanced techniques for environment simulation, including Unreal Engine, GTA V, Google Earth, and 3D Gaussian Splatting (3D GS). Particularly, 3D GS supports real-to-sim rendering, further enhancing the realism of our environments. Secondly, we develop a highly automated toolchain for aerial VLN data collection, streamlining point cloud acquisition, scene semantic segmentation, flight trajectory creation, and instruction generation. Thirdly, based on the toolchain, we construct a large-scale aerial VLN dataset with 100k trajectories, covering diverse heights and lengths across 18 scenes. Moreover, we propose OpenFly-Agent, a keyframe-aware VLN model emphasizing key observations during flight. For benchmarking, extensive experiments and analyses are conducted, evaluating several recent VLN methods and showcasing the superiority of our OpenFly platform and agent. The toolchain, dataset, and codes will be open-sourced.",
    "pdf_url": "https://arxiv.org/pdf/2502.18041v6",
    "github_url": null,
    "published": "2025-02-25T09:57:18+00:00",
    "updated": "2025-07-31T07:55:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.17581v1",
    "title": "Intention Recognition in Real-Time Interactive Navigation Maps",
    "authors": [
      "Zhao",
      "Arefin",
      "Meneguzzi"
    ],
    "summary": "In this demonstration, we develop IntentRec4Maps, a system to recognise users' intentions in interactive maps for real-world navigation. IntentRec4Maps uses the Google Maps Platform as the real-world interactive map, and a very effective approach for recognising users' intentions in real-time. We showcase the recognition process of IntentRec4Maps using two different Path-Planners and a Large Language Model (LLM).   GitHub: https://github.com/PeijieZ/IntentRec4Maps",
    "pdf_url": "https://arxiv.org/pdf/2502.17581v1",
    "github_url": "https://github.com/PeijieZ/IntentRec4Maps",
    "published": "2025-02-24T19:04:18+00:00",
    "updated": "2025-02-24T19:04:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.16515v1",
    "title": "Path Planning using Instruction-Guided Probabilistic Roadmaps",
    "authors": [
      "Bao",
      "Yonetani"
    ],
    "summary": "This work presents a novel data-driven path planning algorithm named Instruction-Guided Probabilistic Roadmap (IG-PRM). Despite the recent development and widespread use of mobile robot navigation, the safe and effective travels of mobile robots still require significant engineering effort to take into account the constraints of robots and their tasks. With IG-PRM, we aim to address this problem by allowing robot operators to specify such constraints through natural language instructions, such as ``aim for wider paths'' or ``mind small gaps''. The key idea is to convert such instructions into embedding vectors using large-language models (LLMs) and use the vectors as a condition to predict instruction-guided cost maps from occupancy maps. By constructing a roadmap based on the predicted costs, we can find instruction-guided paths via the standard shortest path search. Experimental results demonstrate the effectiveness of our approach on both synthetic and real-world indoor navigation environments.",
    "pdf_url": "https://arxiv.org/pdf/2502.16515v1",
    "github_url": null,
    "published": "2025-02-23T09:26:20+00:00",
    "updated": "2025-02-23T09:26:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.14254v2",
    "title": "Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation",
    "authors": [
      "Zhang",
      "Liu",
      "Zhang"
    ],
    "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have made them powerful tools in embodied navigation, enabling agents to leverage commonsense and spatial reasoning for efficient exploration in unfamiliar environments. Existing LLM-based approaches convert global memory, such as semantic or topological maps, into language descriptions to guide navigation. While this improves efficiency and reduces redundant exploration, the loss of geometric information in language-based representations hinders spatial reasoning, especially in intricate environments. To address this, VLM-based approaches directly process ego-centric visual inputs to select optimal directions for exploration. However, relying solely on a first-person perspective makes navigation a partially observed decision-making problem, leading to suboptimal decisions in complex environments. In this paper, we present a novel vision-language model (VLM)-based navigation framework that addresses these challenges by adaptively retrieving task-relevant cues from a global memory module and integrating them with the agent's egocentric observations. By dynamically aligning global contextual information with local perception, our approach enhances spatial reasoning and decision-making in long-horizon tasks. Experimental results demonstrate that the proposed method surpasses previous state-of-the-art approaches in object navigation tasks, providing a more effective and scalable solution for embodied navigation.",
    "pdf_url": "https://arxiv.org/pdf/2502.14254v2",
    "github_url": null,
    "published": "2025-02-20T04:41:40+00:00",
    "updated": "2025-06-10T21:36:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.13637v1",
    "title": "Exploring Mutual Cross-Modal Attention for Context-Aware Human Affordance Generation",
    "authors": [
      "Roy",
      "Bhattacharya",
      "Ghosh"
    ],
    "summary": "Human affordance learning investigates contextually relevant novel pose prediction such that the estimated pose represents a valid human action within the scene. While the task is fundamental to machine perception and automated interactive navigation agents, the exponentially large number of probable pose and action variations make the problem challenging and non-trivial. However, the existing datasets and methods for human affordance prediction in 2D scenes are significantly limited in the literature. In this paper, we propose a novel cross-attention mechanism to encode the scene context for affordance prediction by mutually attending spatial feature maps from two different modalities. The proposed method is disentangled among individual subtasks to efficiently reduce the problem complexity. First, we sample a probable location for a person within the scene using a variational autoencoder (VAE) conditioned on the global scene context encoding. Next, we predict a potential pose template from a set of existing human pose candidates using a classifier on the local context encoding around the predicted location. In the subsequent steps, we use two VAEs to sample the scale and deformation parameters for the predicted pose template by conditioning on the local context and template class. Our experiments show significant improvements over the previous baseline of human affordance injection into complex 2D scenes.",
    "pdf_url": "https://arxiv.org/pdf/2502.13637v1",
    "github_url": null,
    "published": "2025-02-19T11:24:45+00:00",
    "updated": "2025-02-19T11:24:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.13451v4",
    "title": "MapNav: A Novel Memory Representation via Annotated Semantic Maps for Vision-and-Language Navigation",
    "authors": [
      "Zhang",
      "Hao",
      "Xu"
    ],
    "summary": "Vision-and-language navigation (VLN) is a key task in Embodied AI, requiring agents to navigate diverse and unseen environments while following natural language instructions. Traditional approaches rely heavily on historical observations as spatio-temporal contexts for decision making, leading to significant storage and computational overhead. In this paper, we introduce MapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map (ASM) to replace historical frames. Specifically, our approach constructs a top-down semantic map at the start of each episode and update it at each timestep, allowing for precise object mapping and structured navigation information. Then, we enhance this map with explicit textual labels for key regions, transforming abstract semantics into clear navigation cues and generate our ASM. MapNav agent using the constructed ASM as input, and use the powerful end-to-end capabilities of VLM to empower VLN. Extensive experiments demonstrate that MapNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, validating the effectiveness of our method. Moreover, we will release our ASM generation source code and dataset to ensure reproducibility, contributing valuable resources to the field. We believe that our proposed MapNav can be used as a new memory representation method in VLN, paving the way for future research in this field.",
    "pdf_url": "https://arxiv.org/pdf/2502.13451v4",
    "github_url": null,
    "published": "2025-02-19T05:52:34+00:00",
    "updated": "2025-07-10T02:53:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.11142v3",
    "title": "NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM",
    "authors": [
      "Wang",
      "Zhu",
      "Lee"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users' communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models.",
    "pdf_url": "https://arxiv.org/pdf/2502.11142v3",
    "github_url": null,
    "published": "2025-02-16T14:17:36+00:00",
    "updated": "2025-03-07T06:06:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.09238v1",
    "title": "OpenBench: A New Benchmark and Baseline for Semantic Navigation in Smart Logistics",
    "authors": [
      "Wang",
      "Huo",
      "Xu"
    ],
    "summary": "The increasing demand for efficient last-mile delivery in smart logistics underscores the role of autonomous robots in enhancing operational efficiency and reducing costs. Traditional navigation methods, which depend on high-precision maps, are resource-intensive, while learning-based approaches often struggle with generalization in real-world scenarios. To address these challenges, this work proposes the Openstreetmap-enhanced oPen-air sEmantic Navigation (OPEN) system that combines foundation models with classic algorithms for scalable outdoor navigation. The system uses off-the-shelf OpenStreetMap (OSM) for flexible map representation, thereby eliminating the need for extensive pre-mapping efforts. It also employs Large Language Models (LLMs) to comprehend delivery instructions and Vision-Language Models (VLMs) for global localization, map updates, and house number recognition. To compensate the limitations of existing benchmarks that are inadequate for assessing last-mile delivery, this work introduces a new benchmark specifically designed for outdoor navigation in residential areas, reflecting the real-world challenges faced by autonomous delivery systems. Extensive experiments in simulated and real-world environments demonstrate the proposed system's efficacy in enhancing navigation efficiency and reliability. To facilitate further research, our code and benchmark are publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2502.09238v1",
    "github_url": null,
    "published": "2025-02-13T11:55:33+00:00",
    "updated": "2025-02-13T11:55:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.08791v2",
    "title": "VL-Explore: Zero-shot Vision-Language Exploration and Target Discovery by Mobile Robots",
    "authors": [
      "Zhang",
      "Abdullah",
      "Koppal"
    ],
    "summary": "Vision-language navigation (VLN) has emerged as a promising paradigm, enabling mobile robots to perform zero-shot inference and execute tasks without specific pre-programming. However, current systems often separate map exploration and path planning, with exploration relying on inefficient algorithms due to limited (partially observed) environmental information. In this paper, we present a novel navigation pipeline named \"VL-Explore\" for simultaneous exploration and target discovery in unknown environments, leveraging the capabilities of a vision-language model named CLIP. Our approach requires only monocular vision and operates without any prior map or knowledge about the target. For comprehensive evaluations, we designed a functional prototype of a UGV (unmanned ground vehicle) system named \"Open Rover\", a customized platform for general-purpose VLN tasks. We integrated and deployed the VL-Explore pipeline on Open Rover to evaluate its throughput, obstacle avoidance capability, and trajectory performance across various real-world scenarios. Experimental results demonstrate that VL-Explore consistently outperforms traditional map-traversal algorithms and achieves performance comparable to path-planning methods that depend on prior map and target knowledge. Notably, VL-Explore offers real-time active navigation without requiring pre-captured candidate images or pre-built node graphs, addressing key limitations of existing VLN pipelines.",
    "pdf_url": "https://arxiv.org/pdf/2502.08791v2",
    "github_url": null,
    "published": "2025-02-12T21:07:10+00:00",
    "updated": "2025-07-22T21:17:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.07306v2",
    "title": "TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation",
    "authors": [
      "Rajabi",
      "Kosecka"
    ],
    "summary": "In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate $k$ path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM. Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps on the complex R2R-Habitat instruction dataset and quantify in detail the effect of visual grounding on navigation performance.",
    "pdf_url": "https://arxiv.org/pdf/2502.07306v2",
    "github_url": null,
    "published": "2025-02-11T07:09:37+00:00",
    "updated": "2025-06-09T21:25:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.02664v2",
    "title": "Differentiable Composite Neural Signed Distance Fields for Robot Navigation in Dynamic Indoor Environments",
    "authors": [
      "Bukhari",
      "Lawson",
      "Qureshi"
    ],
    "summary": "Neural Signed Distance Fields (SDFs) provide a differentiable environment representation to readily obtain collision checks and well-defined gradients for robot navigation tasks. However, updating neural SDFs as the scene evolves entails re-training, which is tedious, time consuming, and inefficient, making it unsuitable for robot navigation with limited field-of-view in dynamic environments. Towards this objective, we propose a compositional framework of neural SDFs to solve robot navigation in indoor environments using only an onboard RGB-D sensor. Our framework embodies a dual mode procedure for trajectory optimization, with different modes using complementary methods of modeling collision costs and collision avoidance gradients. The primary stage queries the robot body's SDF, swept along the route to goal, at the obstacle point cloud, enabling swift local optimization of trajectories. The secondary stage infers the visible scene's SDF by aligning and composing the SDF representations of its constituents, providing better informed costs and gradients for trajectory optimization. The dual mode procedure combines the best of both stages, achieving a success rate of 98%, 14.4% higher than baseline with comparable amortized plan time on iGibson 2.0. We also demonstrate its effectiveness in adapting to real-world indoor scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2502.02664v2",
    "github_url": null,
    "published": "2025-02-04T19:07:29+00:00",
    "updated": "2025-03-06T18:31:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.01857v2",
    "title": "IG-MCTS: Human-in-the-Loop Cooperative Navigation under Incomplete Information",
    "authors": [
      "Chen",
      "Zhao",
      "Chinchali"
    ],
    "summary": "Human-robot cooperative navigation is challenging under incomplete information. We introduce CoNav-Maze, a simulated environment where a robot navigates with local perception while a human operator provides guidance based on an inaccurate map. The robot can share its onboard camera views to help the operator refine their understanding of the environment. To enable efficient cooperation, we propose Information Gain Monte Carlo Tree Search (IG-MCTS), an online planning algorithm that jointly optimizes autonomous movement and informative communication. IG-MCTS leverages a learned Neural Human Perception Model (NHPM) -- trained on a crowdsourced mapping dataset -- to predict how the human's internal map evolves as new observations are shared. User studies show that IG-MCTS significantly reduces communication demands and yields eye-tracking metrics indicative of lower cognitive load, while maintaining task performance comparable to teleoperation and instruction-following baselines. Finally, we illustrate generalization beyond discrete mazes through a continuous-space waterway navigation setting, in which NHPM benefits from deeper encoder-decoder architectures and IG-MCTS leverages a dynamically constructed Voronoi-partitioned traversability graph.",
    "pdf_url": "https://arxiv.org/pdf/2502.01857v2",
    "github_url": null,
    "published": "2025-02-03T22:08:04+00:00",
    "updated": "2025-10-09T18:20:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.01536v3",
    "title": "VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion",
    "authors": [
      "Zhu",
      "Mou",
      "Li"
    ],
    "summary": "Recent success in legged robot locomotion is attributed to the integration of reinforcement learning and physical simulators. However, these policies often encounter challenges when deployed in real-world environments due to sim-to-real gaps, as simulators typically fail to replicate visual realism and complex real-world geometry. Moreover, the lack of realistic visual rendering limits the ability of these policies to support high-level tasks requiring RGB-based perception like ego-centric navigation. This paper presents a Real-to-Sim-to-Real framework that generates photorealistic and physically interactive \"digital twin\" simulation environments for visual navigation and locomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based scene reconstruction from multi-view images and integrates these environments into simulations that support ego-centric visual perception and mesh-based physical interactions. To demonstrate its effectiveness, we train a reinforcement learning policy within the simulator to perform a visual goal-tracking task. Extensive experiments show that our framework achieves RGB-only sim-to-real policy transfer. Additionally, our framework facilitates the rapid adaptation of robot policies with effective exploration capability in complex new environments, highlighting its potential for applications in households and factories.",
    "pdf_url": "https://arxiv.org/pdf/2502.01536v3",
    "github_url": null,
    "published": "2025-02-03T17:15:05+00:00",
    "updated": "2025-06-03T05:45:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.00543v1",
    "title": "VertiFormer: A Data-Efficient Multi-Task Transformer for Off-Road Robot Mobility",
    "authors": [
      "Nazeri",
      "Pokhrel",
      "Card"
    ],
    "summary": "Sophisticated learning architectures, e.g., Transformers, present a unique opportunity for robots to understand complex vehicle-terrain kinodynamic interactions for off-road mobility. While internet-scale data are available for Natural Language Processing (NLP) and Computer Vision (CV) tasks to train Transformers, real-world mobility data are difficult to acquire with physical robots navigating off-road terrain. Furthermore, training techniques specifically designed to process text and image data in NLP and CV may not apply to robot mobility. In this paper, we propose VertiFormer, a novel data-efficient multi-task Transformer model trained with only one hour of data to address such challenges of applying Transformer architectures for robot mobility on extremely rugged, vertically challenging, off-road terrain. Specifically, VertiFormer employs a new learnable masked modeling and next token prediction paradigm to predict the next pose, action, and terrain patch to enable a variety of off-road mobility tasks simultaneously, e.g., forward and inverse kinodynamics modeling. The non-autoregressive design mitigates computational bottlenecks and error propagation associated with autoregressive models. VertiFormer's unified modality representation also enhances learning of diverse temporal mappings and state representations, which, combined with multiple objective functions, further improves model generalization. Our experiments offer insights into effectively utilizing Transformers for off-road robot mobility with limited data and demonstrate our efficiently trained Transformer can facilitate multiple off-road mobility tasks onboard a physical mobile robot.",
    "pdf_url": "https://arxiv.org/pdf/2502.00543v1",
    "github_url": null,
    "published": "2025-02-01T20:21:00+00:00",
    "updated": "2025-02-01T20:21:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.00114v2",
    "title": "Mobile Robot Navigation Using Hand-Drawn Maps: A Vision Language Model Approach",
    "authors": [
      "Tan",
      "Fung",
      "Wang"
    ],
    "summary": "Hand-drawn maps can be used to convey navigation instructions between humans and robots in a natural and efficient manner. However, these maps can often contain inaccuracies such as scale distortions and missing landmarks which present challenges for mobile robot navigation. This paper introduces a novel Hand-drawn Map Navigation (HAM-Nav) architecture that leverages pre-trained vision language models (VLMs) for robot navigation across diverse environments, hand-drawing styles, and robot embodiments, even in the presence of map inaccuracies. HAM-Nav integrates a unique Selective Visual Association Prompting approach for topological map-based position estimation and navigation planning as well as a Predictive Navigation Plan Parser to infer missing landmarks. Extensive experiments were conducted in photorealistic simulated environments, using both wheeled and legged robots, demonstrating the effectiveness of HAM-Nav in terms of navigation success rates and Success weighted by Path Length. Furthermore, a user study in real-world environments highlighted the practical utility of hand-drawn maps for robot navigation as well as successful navigation outcomes compared against a non-hand-drawn map approach.",
    "pdf_url": "https://arxiv.org/pdf/2502.00114v2",
    "github_url": null,
    "published": "2025-01-31T19:03:33+00:00",
    "updated": "2025-04-28T18:14:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.17403v1",
    "title": "General Scene Adaptation for Vision-and-Language Navigation",
    "authors": [
      "Hong",
      "Qiao",
      "Wang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on one-time execution of individual instructions across multiple environments, aiming to develop agents capable of functioning in any environment in a zero-shot manner. However, real-world navigation robots often operate in persistent environments with relatively consistent physical layouts, visual observations, and language styles from instructors. Such a gap in the task setting presents an opportunity to improve VLN agents by incorporating continuous adaptation to specific environments. To better reflect these real-world conditions, we introduce GSA-VLN, a novel task requiring agents to execute navigation instructions within a specific scene and simultaneously adapt to it for improved performance over time. To evaluate the proposed task, one has to address two challenges in existing VLN datasets: the lack of OOD data, and the limited number and style diversity of instructions for each scene. Therefore, we propose a new dataset, GSA-R2R, which significantly expands the diversity and quantity of environments and instructions for the R2R dataset to evaluate agent adaptability in both ID and OOD contexts. Furthermore, we design a three-stage instruction orchestration pipeline that leverages LLMs to refine speaker-generated instructions and apply role-playing techniques to rephrase instructions into different speaking styles. This is motivated by the observation that each individual user often has consistent signatures or preferences in their instructions. We conducted extensive experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various methods. Based on our findings, we propose a novel method, GR-DUET, which incorporates memory-based navigation graphs with an environment-specific training strategy, achieving state-of-the-art results on all GSA-R2R splits.",
    "pdf_url": "https://arxiv.org/pdf/2501.17403v1",
    "github_url": null,
    "published": "2025-01-29T03:57:56+00:00",
    "updated": "2025-01-29T03:57:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.11858v2",
    "title": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents",
    "authors": [
      "Cheng",
      "Tu",
      "Li"
    ],
    "summary": "Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.",
    "pdf_url": "https://arxiv.org/pdf/2501.11858v2",
    "github_url": "https://github.com/thunlp/EmbodiedEval",
    "published": "2025-01-21T03:22:10+00:00",
    "updated": "2025-04-11T04:26:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.10074v3",
    "title": "SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-of-Thought for Embodied Task Planning",
    "authors": [
      "Liu",
      "Chi",
      "Wu"
    ],
    "summary": "Spatial reasoning is an essential problem in embodied AI research. Efforts to enhance spatial reasoning abilities through supplementary spatial data and fine-tuning have proven limited and ineffective when addressing complex embodied tasks, largely due to their dependence on language-based outputs. While some approaches have introduced a point-based action space to mitigate this issue, they fall short in managing more intricate tasks within complex environments. This deficiency arises from their failure to fully exploit the inherent thinking and reasoning capabilities that are fundamental strengths of Vision-Language Models (VLMs). To address these limitations, we propose a novel approach named SpatialCoT, specifically designed to bolster the spatial reasoning capabilities of VLMs. Our approach comprises two stages: spatial coordinate bi-directional alignment, which aligns vision-language inputs with spatial coordinates, and chain-of-thought spatial grounding, which harnesses the reasoning capabilities of language models for advanced spatial reasoning. We evaluate SpatialCoT on challenging navigation and manipulation tasks, both in simulation and real-world settings. Experimental results demonstrate that our method significantly outperforms previous state-of-the-art approaches in both tasks.",
    "pdf_url": "https://arxiv.org/pdf/2501.10074v3",
    "github_url": null,
    "published": "2025-01-17T09:46:27+00:00",
    "updated": "2025-01-23T02:31:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.05750v3",
    "title": "Semantic Mapping in Indoor Embodied AI -- A Survey on Advances, Challenges, and Future Directions",
    "authors": [
      "Raychaudhuri",
      "Chang"
    ],
    "summary": "Intelligent embodied agents (e.g. robots) need to perform complex semantic tasks in unfamiliar environments. Among many skills that the agents need to possess, building and maintaining a semantic map of the environment is most crucial in long-horizon tasks. A semantic map captures information about the environment in a structured way, allowing the agent to reference it for advanced reasoning throughout the task. While existing surveys in embodied AI focus on general advancements or specific tasks like navigation and manipulation, this paper provides a comprehensive review of semantic map-building approaches in embodied AI, specifically for indoor navigation. We categorize these approaches based on their structural representation (spatial grids, topological graphs, dense point-clouds or hybrid maps) and the type of information they encode (implicit features or explicit environmental data). We also explore the strengths and limitations of the map building techniques, highlight current challenges, and propose future research directions. We identify that the field is moving towards developing open-vocabulary, queryable, task-agnostic map representations, while high memory demands and computational inefficiency still remaining to be open challenges. This survey aims to guide current and future researchers in advancing semantic mapping techniques for embodied AI systems.",
    "pdf_url": "https://arxiv.org/pdf/2501.05750v3",
    "github_url": null,
    "published": "2025-01-10T06:58:14+00:00",
    "updated": "2025-08-10T18:26:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.04947v1",
    "title": "Seeing with Partial Certainty: Conformal Prediction for Robotic Scene Recognition in Built Environments",
    "authors": [
      "Xu",
      "Kamat",
      "Menassa"
    ],
    "summary": "In assistive robotics serving people with disabilities (PWD), accurate place recognition in built environments is crucial to ensure that robots navigate and interact safely within diverse indoor spaces. Language interfaces, particularly those powered by Large Language Models (LLM) and Vision Language Models (VLM), hold significant promise in this context, as they can interpret visual scenes and correlate them with semantic information. However, such interfaces are also known for their hallucinated predictions. In addition, language instructions provided by humans can also be ambiguous and lack precise details about specific locations, objects, or actions, exacerbating the hallucination issue. In this work, we introduce Seeing with Partial Certainty (SwPC) - a framework designed to measure and align uncertainty in VLM-based place recognition, enabling the model to recognize when it lacks confidence and seek assistance when necessary. This framework is built on the theory of conformal prediction to provide statistical guarantees on place recognition while minimizing requests for human help in complex indoor environment settings. Through experiments on the widely used richly-annotated scene dataset Matterport3D, we show that SwPC significantly increases the success rate and decreases the amount of human intervention required relative to the prior art. SwPC can be utilized with any VLMs directly without requiring model fine-tuning, offering a promising, lightweight approach to uncertainty modeling that complements and scales alongside the expanding capabilities of foundational models.",
    "pdf_url": "https://arxiv.org/pdf/2501.04947v1",
    "github_url": null,
    "published": "2025-01-09T03:50:00+00:00",
    "updated": "2025-01-09T03:50:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.04515v1",
    "title": "SplineFormer: An Explainable Transformer-Based Approach for Autonomous Endovascular Navigation",
    "authors": [
      "Jianu",
      "Doust",
      "Li"
    ],
    "summary": "Endovascular navigation is a crucial aspect of minimally invasive procedures, where precise control of curvilinear instruments like guidewires is critical for successful interventions. A key challenge in this task is accurately predicting the evolving shape of the guidewire as it navigates through the vasculature, which presents complex deformations due to interactions with the vessel walls. Traditional segmentation methods often fail to provide accurate real-time shape predictions, limiting their effectiveness in highly dynamic environments. To address this, we propose SplineFormer, a new transformer-based architecture, designed specifically to predict the continuous, smooth shape of the guidewire in an explainable way. By leveraging the transformer's ability, our network effectively captures the intricate bending and twisting of the guidewire, representing it as a spline for greater accuracy and smoothness. We integrate our SplineFormer into an end-to-end robot navigation system by leveraging the condensed information. The experimental results demonstrate that our SplineFormer is able to perform endovascular navigation autonomously and achieves a 50% success rate when cannulating the brachiocephalic artery on the real robot.",
    "pdf_url": "https://arxiv.org/pdf/2501.04515v1",
    "github_url": null,
    "published": "2025-01-08T14:05:24+00:00",
    "updated": "2025-01-08T14:05:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.04279v1",
    "title": "OpenIN: Open-Vocabulary Instance-Oriented Navigation in Dynamic Domestic Environments",
    "authors": [
      "Tang",
      "Wang",
      "Deng"
    ],
    "summary": "In daily domestic settings, frequently used objects like cups often have unfixed positions and multiple instances within the same category, and their carriers frequently change as well. As a result, it becomes challenging for a robot to efficiently navigate to a specific instance. To tackle this challenge, the robot must capture and update scene changes and plans continuously. However, current object navigation approaches primarily focus on the semantic level and lack the ability to dynamically update scene representation. In contrast, this paper captures the relationships between frequently used objects and their static carriers. It constructs an open-vocabulary Carrier-Relationship Scene Graph (CRSG) and updates the carrying status during robot navigation to reflect the dynamic changes of the scene. Based on the CRSG, we further propose an instance navigation strategy that models the navigation process as a Markov Decision Process. At each step, decisions are informed by the Large Language Model's commonsense knowledge and visual-language feature similarity. We designed a series of long-sequence navigation tasks for frequently used everyday items in the Habitat simulator. The results demonstrate that by updating the CRSG, the robot can efficiently navigate to moved targets. Additionally, we deployed our algorithm on a real robot and validated its practical effectiveness. The project page can be found here: https://OpenIN-nav.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2501.04279v1",
    "github_url": null,
    "published": "2025-01-08T05:01:59+00:00",
    "updated": "2025-01-08T05:01:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.05478v2",
    "title": "Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models",
    "authors": [
      "Mansour",
      "Aly",
      "Tharwat"
    ],
    "summary": "Large Language Models (LLMs) such as GPT-4, trained on huge amount of datasets spanning multiple domains, exhibit significant reasoning, understanding, and planning capabilities across various tasks. This study presents the first-ever work in Arabic language integration within the Vision-and-Language Navigation (VLN) domain in robotics, an area that has been notably underexplored in existing research. We perform a comprehensive evaluation of state-of-the-art multi-lingual Small Language Models (SLMs), including GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the Arabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure LLM-based instruction-following navigation agent, to assess the impact of language on navigation reasoning through zero-shot sequential action prediction using the R2R dataset. Through comprehensive experiments, we demonstrate that our framework is capable of high-level planning for navigation tasks when provided with instructions in both English and Arabic. However, certain models struggled with reasoning and planning in the Arabic language due to inherent limitations in their capabilities, sub-optimal performance, and parsing issues. These findings highlight the importance of enhancing planning and reasoning capabilities in language models for effective navigation, emphasizing this as a key area for further development while also unlocking the potential of Arabic-language models for impactful real-world applications.",
    "pdf_url": "https://arxiv.org/pdf/2501.05478v2",
    "github_url": null,
    "published": "2025-01-07T16:01:25+00:00",
    "updated": "2025-06-17T16:28:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.09024v1",
    "title": "Social-LLaVA: Enhancing Robot Navigation through Human-Language Reasoning in Social Spaces",
    "authors": [
      "Payandeh",
      "Song",
      "Nazeri"
    ],
    "summary": "Most existing social robot navigation techniques either leverage hand-crafted rules or human demonstrations to connect robot perception to socially compliant actions. However, there remains a significant gap in effectively translating perception into socially compliant actions, much like how human reasoning naturally occurs in dynamic environments. Considering the recent success of Vision-Language Models (VLMs), we propose using language to bridge the gap in human-like reasoning between perception and socially aware robot actions. We create a vision-language dataset, Social robot Navigation via Explainable Interactions (SNEI), featuring 40K human-annotated Visual Question Answers (VQAs) based on 2K human-robot social interactions in unstructured, crowded public spaces, spanning perception, prediction, chain-of-thought reasoning, action, and explanation. We fine-tune a VLM, Social-LLaVA, using SNEI to demonstrate the practical application of our dataset. Social-LLaVA outperforms state-of-the-art models like GPT-4V and Gemini, based on the average of fifteen different human-judge scores across 50 VQA. Deployed onboard a mobile robot, Social-LLaVA enables human-like reasoning, marking a promising step toward socially compliant robot navigation in dynamic public spaces through language reasoning.",
    "pdf_url": "https://arxiv.org/pdf/2501.09024v1",
    "github_url": null,
    "published": "2024-12-30T23:59:30+00:00",
    "updated": "2024-12-30T23:59:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.20977v2",
    "title": "UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI",
    "authors": [
      "Zhong",
      "Wu",
      "Wang"
    ],
    "summary": "We introduce UnrealZoo, a collection of over 100 photo-realistic 3D virtual worlds built on Unreal Engine, designed to reflect the complexity and variability of open-world environments. We also provide a rich variety of playable entities, including humans, animals, robots, and vehicles for embodied AI research. We extend UnrealCV with optimized APIs and tools for data collection, environment augmentation, distributed training, and benchmarking. These improvements achieve significant improvements in the efficiency of rendering and communication, enabling advanced applications such as multi-agent interactions. Our experimental evaluation across visual navigation and tracking tasks reveals two key insights: 1) environmental diversity provides substantial benefits for developing generalizable reinforcement learning (RL) agents, and 2) current embodied agents face persistent challenges in open-world scenarios, including navigation in unstructured terrain, adaptation to unseen morphologies, and managing latency in the close-loop control systems for interacting in highly dynamic objects. UnrealZoo thus serves as both a comprehensive testing ground and a pathway toward developing more capable embodied AI systems for real-world deployment.",
    "pdf_url": "https://arxiv.org/pdf/2412.20977v2",
    "github_url": null,
    "published": "2024-12-30T14:31:01+00:00",
    "updated": "2025-08-12T11:56:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.19159v1",
    "title": "Mobile Robots through Task-Based Human Instructions using Incremental Curriculum Learning",
    "authors": [
      "Muttaqien",
      "Yorozu",
      "Ohya"
    ],
    "summary": "This paper explores the integration of incremental curriculum learning (ICL) with deep reinforcement learning (DRL) techniques to facilitate mobile robot navigation through task-based human instruction. By adopting a curriculum that mirrors the progressive complexity encountered in human learning, our approach systematically enhances robots' ability to interpret and execute complex instructions over time. We explore the principles of DRL and its synergy with ICL, demonstrating how this combination not only improves training efficiency but also equips mobile robots with the generalization capability required for navigating through dynamic indoor environments. Empirical results indicate that robots trained with our ICL-enhanced DRL framework outperform those trained without curriculum learning, highlighting the benefits of structured learning progressions in robotic training.",
    "pdf_url": "https://arxiv.org/pdf/2412.19159v1",
    "github_url": null,
    "published": "2024-12-26T10:38:40+00:00",
    "updated": "2024-12-26T10:38:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.18292v5",
    "title": "Enhancing Multi-Robot Semantic Navigation Through Multimodal Chain-of-Thought Score Collaboration",
    "authors": [
      "Shen",
      "Luo",
      "Chen"
    ],
    "summary": "Understanding how humans cooperatively utilize semantic knowledge to explore unfamiliar environments and decide on navigation directions is critical for house service multi-robot systems. Previous methods primarily focused on single-robot centralized planning strategies, which severely limited exploration efficiency. Recent research has considered decentralized planning strategies for multiple robots, assigning separate planning models to each robot, but these approaches often overlook communication costs. In this work, we propose Multimodal Chain-of-Thought Co-Navigation (MCoCoNav), a modular approach that utilizes multimodal Chain-of-Thought to plan collaborative semantic navigation for multiple robots. MCoCoNav combines visual perception with Vision Language Models (VLMs) to evaluate exploration value through probabilistic scoring, thus reducing time costs and achieving stable outputs. Additionally, a global semantic map is used as a communication bridge, minimizing communication overhead while integrating observational results. Guided by scores that reflect exploration trends, robots utilize this map to assess whether to explore new frontier points or revisit history nodes. Experiments on HM3D_v0.2 and MP3D demonstrate the effectiveness of our approach. Our code is available at https://github.com/FrankZxShen/MCoCoNav.git.",
    "pdf_url": "https://arxiv.org/pdf/2412.18292v5",
    "github_url": "https://github.com/FrankZxShen/MCoCoNav",
    "published": "2024-12-24T09:00:31+00:00",
    "updated": "2025-08-26T05:01:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.17282v1",
    "title": "LMD-PGN: Cross-Modal Knowledge Distillation from First-Person-View Images to Third-Person-View BEV Maps for Universal Point Goal Navigation",
    "authors": [
      "Uemura",
      "Tanaka",
      "Tsukahara"
    ],
    "summary": "Point goal navigation (PGN) is a mapless navigation approach that trains robots to visually navigate to goal points without relying on pre-built maps. Despite significant progress in handling complex environments using deep reinforcement learning, current PGN methods are designed for single-robot systems, limiting their generalizability to multi-robot scenarios with diverse platforms. This paper addresses this limitation by proposing a knowledge transfer framework for PGN, allowing a teacher robot to transfer its learned navigation model to student robots, including those with unknown or black-box platforms. We introduce a novel knowledge distillation (KD) framework that transfers first-person-view (FPV) representations (view images, turning/forward actions) to universally applicable third-person-view (TPV) representations (local maps, subgoals). The state is redefined as reconstructed local maps using SLAM, while actions are mapped to subgoals on a predefined grid. To enhance training efficiency, we propose a sampling-efficient KD approach that aligns training episodes via a noise-robust local map descriptor (LMD). Although validated on 2D wheeled robots, this method can be extended to 3D action spaces, such as drones. Experiments conducted in Habitat-Sim demonstrate the feasibility of the proposed framework, requiring minimal implementation effort. This study highlights the potential for scalable and cross-platform PGN solutions, expanding the applicability of embodied AI systems in multi-robot scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2412.17282v1",
    "github_url": null,
    "published": "2024-12-23T05:05:44+00:00",
    "updated": "2024-12-23T05:05:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.15664v1",
    "title": "SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control",
    "authors": [
      "Zhang",
      "Starke",
      "Guzov"
    ],
    "summary": "Synthesizing natural human motion that adapts to complex environments while allowing creative control remains a fundamental challenge in motion synthesis. Existing models often fall short, either by assuming flat terrain or lacking the ability to control motion semantics through text. To address these limitations, we introduce SCENIC, a diffusion model designed to generate human motion that adapts to dynamic terrains within virtual scenes while enabling semantic control through natural language. The key technical challenge lies in simultaneously reasoning about complex scene geometry while maintaining text control. This requires understanding both high-level navigation goals and fine-grained environmental constraints. The model must ensure physical plausibility and precise navigation across varied terrain, while also preserving user-specified text control, such as ``carefully stepping over obstacles\" or ``walking upstairs like a zombie.\" Our solution introduces a hierarchical scene reasoning approach. At its core is a novel scene-dependent, goal-centric canonicalization that handles high-level goal constraint, and is complemented by an ego-centric distance field that captures local geometric details. This dual representation enables our model to generate physically plausible motion across diverse 3D scenes. By implementing frame-wise text alignment, our system achieves seamless transitions between different motion styles while maintaining scene constraints. Experiments demonstrate our novel diffusion model generates arbitrarily long human motions that both adapt to complex scenes with varying terrain surfaces and respond to textual prompts. Additionally, we show SCENIC can generalize to four real-scene datasets. Our code, dataset, and models will be released at \\url{https://virtualhumans.mpi-inf.mpg.de/scenic/}.",
    "pdf_url": "https://arxiv.org/pdf/2412.15664v1",
    "github_url": null,
    "published": "2024-12-20T08:25:15+00:00",
    "updated": "2024-12-20T08:25:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.14401v2",
    "title": "The One RING: a Robotic Indoor Navigation Generalist",
    "authors": [
      "Eftekhar",
      "Hendrix",
      "Weihs"
    ],
    "summary": "Modern robots vary significantly in shape, size, and sensor configurations used to perceive and interact with their environments. However, most navigation policies are embodiment-specific--a policy trained on one robot typically fails to generalize to another, even with minor changes in body size or camera viewpoint. As custom hardware becomes increasingly common, there is a growing need for a single policy that generalizes across embodiments, eliminating the need to retrain for each specific robot. In this paper, we introduce RING (Robotic Indoor Navigation Generalist), an embodiment-agnostic policy that turns any mobile robot into an effective indoor semantic navigator. Trained entirely in simulation, RING leverages large-scale randomization over robot embodiments to enable robust generalization to many real-world platforms. To support this, we augment the AI2-THOR simulator to instantiate robots with controllable configurations, varying in body size, rotation pivot point, and camera parameters. On the visual object-goal navigation task, RING achieves strong cross-embodiment (XE) generalization--72.1% average success rate across five simulated embodiments (a 16.7% absolute improvement on the Chores-S benchmark) and 78.9% across four real-world platforms, including Stretch RE-1, LoCoBot, and Unitree Go1--matching or even surpassing embodiment-specific policies. We further deploy RING on the RB-Y1 wheeled humanoid in a real-world kitchen environment, showcasing its out-of-the-box potential for mobile manipulation platforms. (Project website: https://one-ring-policy.allen.ai)",
    "pdf_url": "https://arxiv.org/pdf/2412.14401v2",
    "github_url": null,
    "published": "2024-12-18T23:15:41+00:00",
    "updated": "2025-05-23T21:41:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.13026v2",
    "title": "NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for Vision and Language Navigation",
    "authors": [
      "Wanchoo",
      "Zuo",
      "Gonzalez"
    ],
    "summary": "We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN) corpus built on top of two popular datasets (R2R and RxR). The paper introduces four core, cognitively motivated and linguistically grounded, navigation concepts and an algorithm for generating large-scale silver annotations of naturally occurring linguistic realizations of these concepts in navigation instructions. We pair the annotated instructions with video clips of an agent acting on these instructions. NAVCON contains 236, 316 concept annotations for approximately 30, 0000 instructions and 2.7 million aligned images (from approximately 19, 000 instructions) showing what the agent sees when executing an instruction. To our knowledge, this is the first comprehensive resource of navigation concepts. We evaluated the quality of the silver annotations by conducting human evaluation studies on NAVCON samples. As further validation of the quality and usefulness of the resource, we trained a model for detecting navigation concepts and their linguistic realizations in unseen instructions. Additionally, we show that few-shot learning with GPT-4o performs well on this task using large-scale silver annotations of NAVCON.",
    "pdf_url": "https://arxiv.org/pdf/2412.13026v2",
    "github_url": null,
    "published": "2024-12-17T15:48:25+00:00",
    "updated": "2024-12-18T03:05:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.11523v2",
    "title": "ON as ALC: Active Loop Closing Object Goal Navigation",
    "authors": [
      "Iwata",
      "Tanaka",
      "Miyazaki"
    ],
    "summary": "In simultaneous localization and mapping, active loop closing (ALC) is an active vision problem that aims to visually guide a robot to maximize the chances of revisiting previously visited points, thereby resetting the drift errors accumulated in the incrementally built map during travel. However, current mainstream navigation strategies that leverage such incomplete maps as workspace prior knowledge often fail in modern long-term autonomy long-distance travel scenarios where map accumulation errors become significant. To address these limitations of map-based navigation, this paper is the first to explore mapless navigation in the embodied AI field, in particular, to utilize object-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques that efficiently explore target objects without using such a prior map. Specifically, in this work, we start from an off-the-shelf mapless ON planner, extend it to utilize a prior map, and further show that the performance in long-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss\" and ``ON loss\". This study highlights a simple and effective approach, called ALC-ON (ALCON), to accelerate the progress of challenging long-distance ALC technology by leveraging the growing frontier-guided, data-driven, and LLM-guided ON technologies.",
    "pdf_url": "https://arxiv.org/pdf/2412.11523v2",
    "github_url": null,
    "published": "2024-12-16T07:59:23+00:00",
    "updated": "2025-05-14T15:19:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.10137v4",
    "title": "Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous Environments",
    "authors": [
      "Chen",
      "An",
      "Huang"
    ],
    "summary": "We address the task of Vision-Language Navigation in Continuous Environments (VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly challenging due to the absence of expert demonstrations for training and minimal environment structural prior to guide navigation. To confront these challenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes zero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion process. CA-Nav continuously translates sub-instructions into navigation plans using two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and the Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria for decomposed sub-instructions as constraints and tracks navigation progress by switching sub-instructions in a constraint-aware manner. CVM, guided by CSM's constraints, generates a value map on the fly and refines it using superpixel clustering to improve navigation stability. CA-Nav achieves the state-of-the-art performance on two VLN-CE benchmarks, surpassing the previous best method by 12 percent and 13 percent in Success Rate on the validation unseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates its effectiveness in real-world robot deployments across various indoor scenes and instructions.",
    "pdf_url": "https://arxiv.org/pdf/2412.10137v4",
    "github_url": null,
    "published": "2024-12-13T13:38:41+00:00",
    "updated": "2025-04-15T02:20:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.09878v1",
    "title": "SonicBoom: Contact Localization Using Array of Microphones",
    "authors": [
      "Lee",
      "Yoo",
      "Oh"
    ],
    "summary": "In cluttered environments where visual sensors encounter heavy occlusion, such as in agricultural settings, tactile signals can provide crucial spatial information for the robot to locate rigid objects and maneuver around them. We introduce SonicBoom, a holistic hardware and learning pipeline that enables contact localization through an array of contact microphones. While conventional sound source localization methods effectively triangulate sources in air, localization through solid media with irregular geometry and structure presents challenges that are difficult to model analytically. We address this challenge through a feature engineering and learning based approach, autonomously collecting 18,000 robot interaction sound pairs to learn a mapping between acoustic signals and collision locations on the robot end effector link. By leveraging relative features between microphones, SonicBoom achieves localization errors of 0.42cm for in distribution interactions and maintains robust performance of 2.22cm error even with novel objects and contact conditions. We demonstrate the system's practical utility through haptic mapping of occluded branches in mock canopy settings, showing that acoustic based sensing can enable reliable robot navigation in visually challenging environments.",
    "pdf_url": "https://arxiv.org/pdf/2412.09878v1",
    "github_url": null,
    "published": "2024-12-13T05:50:13+00:00",
    "updated": "2024-12-13T05:50:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.09624v4",
    "title": "GenEx: Generating an Explorable World",
    "authors": [
      "Lu",
      "Shu",
      "Xiao"
    ],
    "summary": "Understanding, navigating, and exploring the 3D physical real world has long been a central challenge in the development of artificial intelligence. In this work, we take a step toward this goal by introducing GenEx, a system capable of planning complex embodied world exploration, guided by its generative imagination that forms priors (expectations) about the surrounding environments. GenEx generates an entire 3D-consistent imaginative environment from as little as a single RGB image, bringing it to life through panoramic video streams. Leveraging scalable 3D world data curated from Unreal Engine, our generative model is rounded in the physical world. It captures a continuous 360-degree environment with little effort, offering a boundless landscape for AI agents to explore and interact with. GenEx achieves high-quality world generation, robust loop consistency over long trajectories, and demonstrates strong 3D capabilities such as consistency and active 3D mapping. Powered by generative imagination of the world, GPT-assisted agents are equipped to perform complex embodied tasks, including both goal-agnostic exploration and goal-driven navigation. These agents utilize predictive expectation regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices. In summary, we demonstrate that GenEx provides a transformative platform for advancing embodied AI in imaginative spaces and brings potential for extending these capabilities to real-world exploration.",
    "pdf_url": "https://arxiv.org/pdf/2412.09624v4",
    "github_url": null,
    "published": "2024-12-12T18:59:57+00:00",
    "updated": "2025-01-20T16:51:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.09082v3",
    "title": "Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method",
    "authors": [
      "Song",
      "Chen",
      "Liu"
    ],
    "summary": "Existing Vision-Language Navigation (VLN) methods primarily focus on single-stage navigation, limiting their effectiveness in multi-stage and long-horizon tasks within complex and dynamic environments. To address these limitations, we propose a novel VLN task, named Long-Horizon Vision-Language Navigation (LH-VLN), which emphasizes long-term planning and decision consistency across consecutive subtasks. Furthermore, to support LH-VLN, we develop an automated data generation platform NavGen, which constructs datasets with complex task structures and improves data utility through a bidirectional, multi-granularity generation approach. To accurately evaluate complex tasks, we construct the Long-Horizon Planning and Reasoning in VLN (LHPR-VLN) benchmark consisting of 3,260 tasks with an average of 150 task steps, serving as the first dataset specifically designed for the long-horizon vision-language navigation task. Furthermore, we propose Independent Success Rate (ISR), Conditional Success Rate (CSR), and CSR weight by Ground Truth (CGT) metrics, to provide fine-grained assessments of task completion. To improve model adaptability in complex tasks, we propose a novel Multi-Granularity Dynamic Memory (MGDM) module that integrates short-term memory blurring with long-term memory retrieval to enable flexible navigation in dynamic environments. Our platform, benchmark and method supply LH-VLN with a robust data generation pipeline, comprehensive model evaluation dataset, reasonable metrics, and a novel VLN model, establishing a foundational framework for advancing LH-VLN.",
    "pdf_url": "https://arxiv.org/pdf/2412.09082v3",
    "github_url": null,
    "published": "2024-12-12T09:08:13+00:00",
    "updated": "2025-03-19T13:31:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.08591v2",
    "title": "RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation",
    "authors": [
      "Han",
      "Ma",
      "Zhumakhanova"
    ],
    "summary": "Vision-and-Language Navigation (VLN) suffers from the limited diversity and scale of training data, primarily constrained by the manual curation of existing simulators. To address this, we introduce RoomTour3D, a video-instruction dataset derived from web-based room tour videos that capture real-world indoor spaces and human walking demonstrations. Unlike existing VLN datasets, RoomTour3D leverages the scale and diversity of online videos to generate open-ended human walking trajectories and open-world navigable instructions. To compensate for the lack of navigation data in online videos, we perform 3D reconstruction and obtain 3D trajectories of walking paths augmented with additional information on the room types, object locations and 3D shape of surrounding scenes. Our dataset includes $\\sim$100K open-ended description-enriched trajectories with $\\sim$200K instructions, and 17K action-enriched trajectories from 1847 room tour environments. We demonstrate experimentally that RoomTour3D enables significant improvements across multiple VLN tasks including CVDN, SOON, R2R, and REVERIE. Moreover, RoomTour3D facilitates the development of trainable zero-shot VLN agents, showcasing the potential and challenges of advancing towards open-world navigation.",
    "pdf_url": "https://arxiv.org/pdf/2412.08591v2",
    "github_url": null,
    "published": "2024-12-11T18:10:21+00:00",
    "updated": "2025-03-19T10:05:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.08467v2",
    "title": "Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel",
    "authors": [
      "Wang",
      "Li",
      "Hong"
    ],
    "summary": "Creating high-quality data for training robust language-instructed agents is a long-lasting challenge in embodied AI. In this paper, we introduce a Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale navigational instruction-trajectory pairs by iteratively refining the data pool through the collaboration between two models, the instruction generator and the navigator, without any human-in-the-loop annotation. Specifically, SRDF starts with using a base generator to create an initial data pool for training a base navigator, followed by applying the trained navigator to filter the data pool. This leads to higher-fidelity data to train a better generator, which can, in turn, produce higher-quality data for training the next-round navigator. Such a flywheel establishes a data self-refining process, yielding a continuously improved and highly effective dataset for large-scale language-guided navigation learning. Our experiments demonstrate that after several flywheel rounds, the navigator elevates the performance boundary from 70% to 78% SPL on the classic R2R test set, surpassing human performance (76%) for the first time. Meanwhile, this process results in a superior generator, evidenced by a SPICE increase from 23.5 to 26.2, better than all previous VLN instruction generation methods. Finally, we demonstrate the scalability of our method through increasing environment and instruction diversity, and the generalization ability of our pre-trained navigator across various downstream navigation tasks, surpassing state-of-the-art methods by a large margin in all cases.",
    "pdf_url": "https://arxiv.org/pdf/2412.08467v2",
    "github_url": null,
    "published": "2024-12-11T15:32:24+00:00",
    "updated": "2025-02-28T08:06:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.10439v3",
    "title": "CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs",
    "authors": [
      "Cao",
      "Zhang",
      "Yu"
    ],
    "summary": "Object goal navigation (ObjectNav) is a fundamental task in embodied AI, requiring an agent to locate a target object in previously unseen environments. This task is particularly challenging because it requires both perceptual and cognitive processes, including object recognition and decision-making. While substantial advancements in perception have been driven by the rapid development of visual foundation models, progress on the cognitive aspect remains constrained, primarily limited to either implicit learning through simulator rollouts or explicit reliance on predefined heuristic rules. Inspired by neuroscientific findings demonstrating that humans maintain and dynamically update fine-grained cognitive states during object search tasks in novel environments, we propose CogNav, a framework designed to mimic this cognitive process using large language models. Specifically, we model the cognitive process using a finite state machine comprising fine-grained cognitive states, ranging from exploration to identification. Transitions between states are determined by a large language model based on a dynamically constructed heterogeneous cognitive map, which contains spatial and semantic information about the scene being explored. Extensive evaluations on the HM3D, MP3D, and RoboTHOR benchmarks demonstrate that our cognitive process modeling significantly improves the success rate of ObjectNav at least by relative 14% over the state-of-the-arts.",
    "pdf_url": "https://arxiv.org/pdf/2412.10439v3",
    "github_url": null,
    "published": "2024-12-11T09:50:35+00:00",
    "updated": "2025-08-28T04:36:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.06465v5",
    "title": "Agent Journey Beyond RGB: Hierarchical Semantic-Spatial Representation Enrichment for Vision-and-Language Navigation",
    "authors": [
      "Zhang",
      "Xu",
      "Li"
    ],
    "summary": "Navigating unseen environments from natural language instructions remains challenging for egocentric agents in Vision-and-Language Navigation (VLN). Humans naturally ground concrete semantic knowledge within spatial layouts during indoor navigation. Although prior work has introduced diverse environment representations to improve reasoning, auxiliary modalities are often naively concatenated with RGB features, which underutilizes each modality's distinct contribution. We propose a hierarchical Semantic Understanding and Spatial Awareness (SUSA) architecture to enable agents to perceive and ground environments at multiple scales. Specifically, the Textual Semantic Understanding (TSU) module supports local action prediction by generating view-level descriptions, capturing fine-grained semantics and narrowing the modality gap between instructions and environments. Complementarily, the Depth Enhanced Spatial Perception (DSP) module incrementally builds a trajectory-level depth exploration map, providing a coarse-grained representation of global spatial layout. Extensive experiments show that the hierarchical representation enrichment of SUSA significantly improves navigation performance over the baseline on discrete VLN benchmarks (REVERIE, R2R, and SOON) and generalizes better to the continuous R2R-CE benchmark.",
    "pdf_url": "https://arxiv.org/pdf/2412.06465v5",
    "github_url": null,
    "published": "2024-12-09T13:10:28+00:00",
    "updated": "2025-11-13T08:51:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.06413v2",
    "title": "World-Consistent Data Generation for Vision-and-Language Navigation",
    "authors": [
      "Zhong",
      "Zhang",
      "Zhang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is a challenging task that requires an agent to navigate through photorealistic environments following natural-language instructions. One main obstacle existing in VLN is data scarcity, leading to poor generalization performance over unseen environments. Though data argumentation is a promising way for scaling up the dataset, how to generate VLN data both diverse and world-consistent remains problematic. To cope with this issue, we propose the world-consistent data generation (WCGEN), an efficacious data-augmentation framework satisfying both diversity and world-consistency, aimed at enhancing the generalization of agents to novel environments. Roughly, our framework consists of two stages, the trajectory stage which leverages a point-cloud based technique to ensure spatial coherency among viewpoints, and the viewpoint stage which adopts a novel angle synthesis method to guarantee spatial and wraparound consistency within the entire observation. By accurately predicting viewpoint changes with 3D knowledge, our approach maintains the world-consistency during the generation procedure. Experiments on a wide range of datasets verify the effectiveness of our method, demonstrating that our data augmentation strategy enables agents to achieve new state-of-the-art results on all navigation tasks, and is capable of enhancing the VLN agents' generalization ability to unseen environments.",
    "pdf_url": "https://arxiv.org/pdf/2412.06413v2",
    "github_url": null,
    "published": "2024-12-09T11:40:54+00:00",
    "updated": "2025-06-25T10:03:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.06224v2",
    "title": "Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks",
    "authors": [
      "Zhang",
      "Wang",
      "Wang"
    ],
    "summary": "A practical navigation agent must be capable of handling a wide range of interaction demands, such as following instructions, searching objects, answering questions, tracking people, and more. Existing models for embodied navigation fall short of serving as practical generalists in the real world, as they are often constrained by specific task configurations or pre-defined maps with discretized waypoints. In this work, we present Uni-NaVid, the first video-based vision-language-action (VLA) model designed to unify diverse embodied navigation tasks and enable seamless navigation for mixed long-horizon tasks in unseen real-world environments. Uni-NaVid achieves this by harmonizing the input and output data configurations for all commonly used embodied navigation tasks and thereby integrating all tasks in one model. For training Uni-NaVid, we collect 3.6 million navigation data samples in total from four essential navigation sub-tasks and foster synergy in learning across them. Extensive experiments on comprehensive navigation benchmarks clearly demonstrate the advantages of unification modeling in Uni-NaVid and show it achieves state-of-the-art performance. Additionally, real-world experiments confirm the model's effectiveness and efficiency, shedding light on its strong generalizability.",
    "pdf_url": "https://arxiv.org/pdf/2412.06224v2",
    "github_url": null,
    "published": "2024-12-09T05:55:55+00:00",
    "updated": "2025-02-06T10:14:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.13211v3",
    "title": "ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home Rearrangement Tasks",
    "authors": [
      "Shukla",
      "Tao",
      "Su"
    ],
    "summary": "High-quality benchmarks are the foundation for embodied AI research, enabling significant advancements in long-horizon navigation, manipulation and rearrangement tasks. However, as frontier tasks in robotics get more advanced, they require faster simulation speed, more intricate test environments, and larger demonstration datasets. To this end, we present MS-HAB, a holistic benchmark for low-level manipulation and in-home object rearrangement. First, we provide a GPU-accelerated implementation of the Home Assistant Benchmark (HAB). We support realistic low-level control and achieve over 3x the speed of prior magical grasp implementations at a fraction of the GPU memory usage. Second, we train extensive reinforcement learning (RL) and imitation learning (IL) baselines for future work to compare against. Finally, we develop a rule-based trajectory filtering system to sample specific demonstrations from our RL policies which match predefined criteria for robot behavior and safety. Combining demonstration filtering with our fast environments enables efficient, controlled data generation at scale.",
    "pdf_url": "https://arxiv.org/pdf/2412.13211v3",
    "github_url": null,
    "published": "2024-12-09T01:29:24+00:00",
    "updated": "2025-02-28T10:10:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.10402v1",
    "title": "TANGO: Training-free Embodied AI Agents for Open-world Tasks",
    "authors": [
      "Ziliotto",
      "Campari",
      "Serafini"
    ],
    "summary": "Large Language Models (LLMs) have demonstrated excellent capabilities in composing various modules together to create programs that can perform complex reasoning tasks on images. In this paper, we propose TANGO, an approach that extends the program composition via LLMs already observed for images, aiming to integrate those capabilities into embodied agents capable of observing and acting in the world. Specifically, by employing a simple PointGoal Navigation model combined with a memory-based exploration policy as a foundational primitive for guiding an agent through the world, we show how a single model can address diverse tasks without additional training. We task an LLM with composing the provided primitives to solve a specific task, using only a few in-context examples in the prompt. We evaluate our approach on three key Embodied AI tasks: Open-Set ObjectGoal Navigation, Multi-Modal Lifelong Navigation, and Open Embodied Question Answering, achieving state-of-the-art results without any specific fine-tuning in challenging zero-shot scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2412.10402v1",
    "github_url": null,
    "published": "2024-12-05T21:52:20+00:00",
    "updated": "2024-12-05T21:52:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.04453v2",
    "title": "NaVILA: Legged Robot Vision-Language-Action Model for Navigation",
    "authors": [
      "Cheng",
      "Ji",
      "Yang"
    ],
    "summary": "This paper proposes to solve the problem of Vision-and-Language Navigation with legged robots, which not only provides a flexible way for humans to command but also allows the robot to navigate through more challenging and cluttered scenes. However, it is non-trivial to translate human language instructions all the way to low-level leg joint actions. We propose NaVILA, a 2-level framework that unifies a Vision-Language-Action model (VLA) with locomotion skills. Instead of directly predicting low-level actions from VLA, NaVILA first generates mid-level actions with spatial information in the form of language, (e.g., \"moving forward 75cm\"), which serves as an input for a visual locomotion RL policy for execution. NaVILA substantially improves previous approaches on existing benchmarks. The same advantages are demonstrated in our newly developed benchmarks with IsaacLab, featuring more realistic scenes, low-level controls, and real-world robot experiments. We show more results at https://navila-bot.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2412.04453v2",
    "github_url": null,
    "published": "2024-12-05T18:58:17+00:00",
    "updated": "2025-02-17T18:27:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.02795v1",
    "title": "Hijacking Vision-and-Language Navigation Agents with Adversarial Environmental Attacks",
    "authors": [
      "Yang",
      "Shi",
      "Slyman"
    ],
    "summary": "Assistive embodied agents that can be instructed in natural language to perform tasks in open-world environments have the potential to significantly impact labor tasks like manufacturing or in-home care -- benefiting the lives of those who come to depend on them. In this work, we consider how this benefit might be hijacked by local modifications in the appearance of the agent's operating environment. Specifically, we take the popular Vision-and-Language Navigation (VLN) task as a representative setting and develop a whitebox adversarial attack that optimizes a 3D attack object's appearance to induce desired behaviors in pretrained VLN agents that observe it in the environment. We demonstrate that the proposed attack can cause VLN agents to ignore their instructions and execute alternative actions after encountering the attack object -- even for instructions and agent paths not considered when optimizing the attack. For these novel settings, we find our attacks can induce early-termination behaviors or divert an agent along an attacker-defined multi-step trajectory. Under both conditions, environmental attacks significantly reduce agent capabilities to successfully follow user instructions.",
    "pdf_url": "https://arxiv.org/pdf/2412.02795v1",
    "github_url": null,
    "published": "2024-12-03T19:54:32+00:00",
    "updated": "2024-12-03T19:54:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.01292v2",
    "title": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences",
    "authors": [
      "Zhi",
      "Chen",
      "Li"
    ],
    "summary": "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.",
    "pdf_url": "https://arxiv.org/pdf/2412.01292v2",
    "github_url": null,
    "published": "2024-12-02T09:07:57+00:00",
    "updated": "2025-02-02T11:49:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.01250v3",
    "title": "Collaborative Instance Object Navigation: Leveraging Uncertainty-Awareness to Minimize Human-Agent Dialogues",
    "authors": [
      "Taioli",
      "Zorzi",
      "Franchi"
    ],
    "summary": "Language-driven instance object navigation assumes that human users initiate the task by providing a detailed description of the target instance to the embodied agent. While this description is crucial for distinguishing the target from visually similar instances in a scene, providing it prior to navigation can be demanding for human. To bridge this gap, we introduce Collaborative Instance object Navigation (CoIN), a new task setting where the agent actively resolve uncertainties about the target instance during navigation in natural, template-free, open-ended dialogues with human. We propose a novel training-free method, Agent-user Interaction with UncerTainty Awareness (AIUTA), which operates independently from the navigation policy, and focuses on the human-agent interaction reasoning with Vision-Language Models (VLMs) and Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue within the agent to obtain a complete and accurate observation description with a novel uncertainty estimation technique. Then, an Interaction Trigger module determines whether to ask a question to the human, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, with a curated dataset designed for challenging multi-instance scenarios. CoIN-Bench supports both online evaluation with humans and reproducible experiments with simulated user-agent interactions. On CoIN-Bench, we show that AIUTA serves as a competitive baseline, while existing language-driven instance navigation methods struggle in complex multi-instance scenes. Code and benchmark will be available upon acceptance at https://intelligolabs.github.io/CoIN/",
    "pdf_url": "https://arxiv.org/pdf/2412.01250v3",
    "github_url": null,
    "published": "2024-12-02T08:16:38+00:00",
    "updated": "2025-03-18T16:09:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.01857v2",
    "title": "Planning from Imagination: Episodic Simulation and Episodic Memory for Vision-and-Language Navigation",
    "authors": [
      "Pan",
      "Xu",
      "Liu"
    ],
    "summary": "Humans navigate unfamiliar environments using episodic simulation and episodic memory, which facilitate a deeper understanding of the complex relationships between environments and objects. Developing an imaginative memory system inspired by human mechanisms can enhance the navigation performance of embodied agents in unseen environments. However, existing Vision-and-Language Navigation (VLN) agents lack a memory mechanism of this kind. To address this, we propose a novel architecture that equips agents with a reality-imagination hybrid memory system. This system enables agents to maintain and expand their memory through both imaginative mechanisms and navigation actions. Additionally, we design tailored pre-training tasks to develop the agent's imaginative capabilities. Our agent can imagine high-fidelity RGB images for future scenes, achieving state-of-the-art result in Success rate weighted by Path Length (SPL).",
    "pdf_url": "https://arxiv.org/pdf/2412.01857v2",
    "github_url": null,
    "published": "2024-11-30T16:49:14+00:00",
    "updated": "2024-12-25T08:59:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.18539v1",
    "title": "AdaVLN: Towards Visual Language Navigation in Continuous Indoor Environments with Moving Humans",
    "authors": [
      "Loh",
      "Bednarz",
      "Xia"
    ],
    "summary": "Visual Language Navigation is a task that challenges robots to navigate in realistic environments based on natural language instructions. While previous research has largely focused on static settings, real-world navigation must often contend with dynamic human obstacles. Hence, we propose an extension to the task, termed Adaptive Visual Language Navigation (AdaVLN), which seeks to narrow this gap. AdaVLN requires robots to navigate complex 3D indoor environments populated with dynamically moving human obstacles, adding a layer of complexity to navigation tasks that mimic the real-world. To support exploration of this task, we also present AdaVLN simulator and AdaR2R datasets. The AdaVLN simulator enables easy inclusion of fully animated human models directly into common datasets like Matterport3D. We also introduce a \"freeze-time\" mechanism for both the navigation task and simulator, which pauses world state updates during agent inference, enabling fair comparisons and experimental reproducibility across different hardware. We evaluate several baseline models on this task, analyze the unique challenges introduced by AdaVLN, and demonstrate its potential to bridge the sim-to-real gap in VLN research.",
    "pdf_url": "https://arxiv.org/pdf/2411.18539v1",
    "github_url": null,
    "published": "2024-11-27T17:36:08+00:00",
    "updated": "2024-11-27T17:36:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.17030v1",
    "title": "g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks",
    "authors": [
      "Wang",
      "Lee"
    ],
    "summary": "We introduce Generalizable 3D-Language Feature Fields (g3D-LF), a 3D representation model pre-trained on large-scale 3D-language dataset for embodied tasks. Our g3D-LF processes posed RGB-D images from agents to encode feature fields for: 1) Novel view representation predictions from any position in the 3D scene; 2) Generations of BEV maps centered on the agent; 3) Querying targets using multi-granularity language within the above-mentioned representations. Our representation can be generalized to unseen environments, enabling real-time construction and dynamic updates. By volume rendering latent features along sampled rays and integrating semantic and spatial relationships through multiscale encoders, our g3D-LF produces representations at different scales and perspectives, aligned with multi-granularity language, via multi-level contrastive learning. Furthermore, we prepare a large-scale 3D-language dataset to align the representations of the feature fields with language. Extensive experiments on Vision-and-Language Navigation under both Panorama and Monocular settings, Zero-shot Object Navigation, and Situated Question Answering tasks highlight the significant advantages and effectiveness of our g3D-LF for embodied tasks.",
    "pdf_url": "https://arxiv.org/pdf/2411.17030v1",
    "github_url": null,
    "published": "2024-11-26T01:54:52+00:00",
    "updated": "2024-11-26T01:54:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.16425v2",
    "title": "TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation",
    "authors": [
      "Zhong",
      "Gao",
      "Ding"
    ],
    "summary": "The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find a previously unseen object by navigating in unfamiliar environments. Such a goal-oriented exploration heavily relies on the ability to perceive, understand, and reason based on the spatial information of the environment. However, current LLM-based approaches convert visual observations to language descriptions and reason in the linguistic space, leading to the loss of spatial information. In this paper, we introduce TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information. To fully unlock the MLLM's spatial reasoning potential in top-view perspective, we propose the Adaptive Visual Prompt Generation (AVPG) method to adaptively construct semantically-rich top-view map. It enables the agent to directly utilize spatial information contained in the top-view map to conduct thorough reasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to predict and to utilize target locations, facilitating global and human-like exploration. Experiments on MP3D and HM3D datasets demonstrate the superiority of our TopV-Nav.",
    "pdf_url": "https://arxiv.org/pdf/2411.16425v2",
    "github_url": null,
    "published": "2024-11-25T14:27:55+00:00",
    "updated": "2025-03-26T07:26:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.16053v2",
    "title": "UnitedVLN: Generalizable Gaussian Splatting for Continuous Vision-Language Navigation",
    "authors": [
      "Dai",
      "Zhao",
      "Chen"
    ],
    "summary": "Vision-and-Language Navigation (VLN), where an agent follows instructions to reach a target destination, has recently seen significant advancements. In contrast to navigation in discrete environments with predefined trajectories, VLN in Continuous Environments (VLN-CE) presents greater challenges, as the agent is free to navigate any unobstructed location and is more vulnerable to visual occlusions or blind spots. Recent approaches have attempted to address this by imagining future environments, either through predicted future visual images or semantic features, rather than relying solely on current observations. However, these RGB-based and feature-based methods lack intuitive appearance-level information or high-level semantic complexity crucial for effective navigation. To overcome these limitations, we introduce a novel, generalizable 3DGS-based pre-training paradigm, called UnitedVLN, which enables agents to better explore future environments by unitedly rendering high-fidelity 360 visual images and semantic features. UnitedVLN employs two key schemes: search-then-query sampling and separate-then-united rendering, which facilitate efficient exploitation of neural primitives, helping to integrate both appearance and semantic information for more robust navigation. Extensive experiments demonstrate that UnitedVLN outperforms state-of-the-art methods on existing VLN-CE benchmarks.",
    "pdf_url": "https://arxiv.org/pdf/2411.16053v2",
    "github_url": null,
    "published": "2024-11-25T02:44:59+00:00",
    "updated": "2025-03-16T10:43:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.14811v2",
    "title": "Fine-Grained Alignment in Vision-and-Language Navigation through Bayesian Optimization",
    "authors": [
      "Song",
      "Gianni",
      "Yang"
    ],
    "summary": "This paper addresses the challenge of fine-grained alignment in Vision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D environments based on natural language instructions. Current approaches use contrastive learning to align language with visual trajectory sequences. Nevertheless, they encounter difficulties with fine-grained vision negatives. To enhance cross-modal embeddings, we introduce a novel Bayesian Optimization-based adversarial optimization framework for creating fine-grained contrastive vision samples. To validate the proposed methodology, we conduct a series of experiments to assess the effectiveness of the enriched embeddings on fine-grained vision negatives. We conduct experiments on two common VLN benchmarks R2R and REVERIE, experiments on the them demonstrate that these embeddings benefit navigation, and can lead to a promising performance enhancement. Our source code and trained models are available at: https://anonymous.4open.science/r/FGVLN.",
    "pdf_url": "https://arxiv.org/pdf/2411.14811v2",
    "github_url": null,
    "published": "2024-11-22T09:12:02+00:00",
    "updated": "2024-11-30T08:47:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.13262v1",
    "title": "FASTNav: Fine-tuned Adaptive Small-language-models Trained for Multi-point Robot Navigation",
    "authors": [
      "Chen",
      "Han",
      "Li"
    ],
    "summary": "With the rapid development of large language models (LLM), robots are starting to enjoy the benefits of new interaction methods that large language models bring. Because edge computing fulfills the needs for rapid response, privacy, and network autonomy, we believe it facilitates the extensive deployment of large models for robot navigation across various industries. To enable local deployment of language models on edge devices, we adopt some model boosting methods. In this paper, we propose FASTNav - a method for boosting lightweight LLMs, also known as small language models (SLMs), for robot navigation. The proposed method contains three modules: fine-tuning, teacher-student iteration, and language-based multi-point robot navigation. We train and evaluate models with FASTNav in both simulation and real robots, proving that we can deploy them with low cost, high accuracy and low response time. Compared to other model compression methods, FASTNav shows potential in the local deployment of language models and tends to be a promising solution for language-guided robot navigation on edge devices.",
    "pdf_url": "https://arxiv.org/pdf/2411.13262v1",
    "github_url": null,
    "published": "2024-11-20T12:28:13+00:00",
    "updated": "2024-11-20T12:28:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.12150v2",
    "title": "HEIGHT: Heterogeneous Interaction Graph Transformer for Robot Navigation in Crowded and Constrained Environments",
    "authors": [
      "Liu",
      "Xia",
      "Pouria"
    ],
    "summary": "We study the problem of robot navigation in dense and interactive crowds with environmental constraints such as corridors and furniture. Previous methods fail to consider all types of interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-based representation of crowded and constrained scenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different components in the environment and propose a heterogeneous spatio-temporal (st) graph to model distinct interactions among humans, robots, and obstacles. Based on the heterogeneous st-graph, we propose HEIGHT, a novel navigation policy network architecture with different components to capture heterogeneous interactions among entities through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robot to avoid collisions adaptively. Through extensive simulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success and efficiency in challenging navigation scenarios. Furthermore, we demonstrate that our pipeline achieves better zero-shot generalization capability than previous works when the densities of humans and obstacles change. More videos are available at https://sites.google.com/view/crowdnav-height/home.",
    "pdf_url": "https://arxiv.org/pdf/2411.12150v2",
    "github_url": null,
    "published": "2024-11-19T00:56:35+00:00",
    "updated": "2025-05-01T20:03:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.11609v1",
    "title": "VLN-Game: Vision-Language Equilibrium Search for Zero-Shot Semantic Navigation",
    "authors": [
      "Yu",
      "Liu",
      "Han"
    ],
    "summary": "Following human instructions to explore and search for a specified target in an unfamiliar environment is a crucial skill for mobile service robots. Most of the previous works on object goal navigation have typically focused on a single input modality as the target, which may lead to limited consideration of language descriptions containing detailed attributes and spatial relationships. To address this limitation, we propose VLN-Game, a novel zero-shot framework for visual target navigation that can process object names and descriptive language targets effectively. To be more precise, our approach constructs a 3D object-centric spatial map by integrating pre-trained visual-language features with a 3D reconstruction of the physical environment. Then, the framework identifies the most promising areas to explore in search of potential target candidates. A game-theoretic vision language model is employed to determine which target best matches the given language description. Experiments conducted on the Habitat-Matterport 3D (HM3D) dataset demonstrate that the proposed framework achieves state-of-the-art performance in both object goal navigation and language-based navigation tasks. Moreover, we show that VLN-Game can be easily deployed on real-world robots. The success of VLN-Game highlights the promising potential of using game-theoretic methods with compact vision-language models to advance decision-making capabilities in robotic systems. The supplementary video and code can be accessed via the following link: https://sites.google.com/view/vln-game.",
    "pdf_url": "https://arxiv.org/pdf/2411.11609v1",
    "github_url": null,
    "published": "2024-11-18T14:30:46+00:00",
    "updated": "2024-11-18T14:30:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.11394v1",
    "title": "InstruGen: Automatic Instruction Generation for Vision-and-Language Navigation Via Large Multimodal Models",
    "authors": [
      "Yan",
      "Xu",
      "Zhang"
    ],
    "summary": "Recent research on Vision-and-Language Navigation (VLN) indicates that agents suffer from poor generalization in unseen environments due to the lack of realistic training environments and high-quality path-instruction pairs. Most existing methods for constructing realistic navigation scenes have high costs, and the extension of instructions mainly relies on predefined templates or rules, lacking adaptability. To alleviate the issue, we propose InstruGen, a VLN path-instruction pairs generation paradigm. Specifically, we use YouTube house tour videos as realistic navigation scenes and leverage the powerful visual understanding and generation abilities of large multimodal models (LMMs) to automatically generate diverse and high-quality VLN path-instruction pairs. Our method generates navigation instructions with different granularities and achieves fine-grained alignment between instructions and visual observations, which was difficult to achieve with previous methods. Additionally, we design a multi-stage verification mechanism to reduce hallucinations and inconsistency of LMMs. Experimental results demonstrate that agents trained with path-instruction pairs generated by InstruGen achieves state-of-the-art performance on the R2R and RxR benchmarks, particularly in unseen environments. Code is available at https://github.com/yanyu0526/InstruGen.",
    "pdf_url": "https://arxiv.org/pdf/2411.11394v1",
    "github_url": "https://github.com/yanyu0526/InstruGen",
    "published": "2024-11-18T09:11:48+00:00",
    "updated": "2024-11-18T09:11:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.08579v1",
    "title": "NavAgent: Multi-scale Urban Street View Fusion For UAV Embodied Vision-and-Language Navigation",
    "authors": [
      "Liu",
      "Yao",
      "Yue"
    ],
    "summary": "Vision-and-Language Navigation (VLN), as a widely discussed research direction in embodied intelligence, aims to enable embodied agents to navigate in complicated visual environments through natural language commands. Most existing VLN methods focus on indoor ground robot scenarios. However, when applied to UAV VLN in outdoor urban scenes, it faces two significant challenges. First, urban scenes contain numerous objects, which makes it challenging to match fine-grained landmarks in images with complex textual descriptions of these landmarks. Second, overall environmental information encompasses multiple modal dimensions, and the diversity of representations significantly increases the complexity of the encoding process. To address these challenges, we propose NavAgent, the first urban UAV embodied navigation model driven by a large Vision-Language Model. NavAgent undertakes navigation tasks by synthesizing multi-scale environmental information, including topological maps (global), panoramas (medium), and fine-grained landmarks (local). Specifically, we utilize GLIP to build a visual recognizer for landmark capable of identifying and linguisticizing fine-grained landmarks. Subsequently, we develop dynamically growing scene topology map that integrate environmental information and employ Graph Convolutional Networks to encode global environmental data. In addition, to train the visual recognizer for landmark, we develop NavAgent-Landmark2K, the first fine-grained landmark dataset for real urban street scenes. In experiments conducted on the Touchdown and Map2seq datasets, NavAgent outperforms strong baseline models. The code and dataset will be released to the community to facilitate the exploration and development of outdoor VLN.",
    "pdf_url": "https://arxiv.org/pdf/2411.08579v1",
    "github_url": null,
    "published": "2024-11-13T12:51:49+00:00",
    "updated": "2024-11-13T12:51:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.07848v3",
    "title": "Zero-shot Object-Centric Instruction Following: Integrating Foundation Models with Traditional Navigation",
    "authors": [
      "Raychaudhuri",
      "Ta",
      "Ashton"
    ],
    "summary": "Large scale scenes such as multifloor homes can be robustly and efficiently mapped with a 3D graph of landmarks estimated jointly with robot poses in a factor graph, a technique commonly used in commercial robots such as drones and robot vacuums. In this work, we propose Language-Inferred Factor Graph for Instruction Following (LIFGIF), a zero-shot method to ground natural language instructions in such a map. LIFGIF also includes a policy for following natural language navigation instructions in a novel environment while the map is constructed, enabling robust navigation performance in the physical world. To evaluate LIFGIF, we present a new dataset, Object-Centric VLN (OC-VLN), in order to evaluate grounding of object-centric natural language navigation instructions. We compare to two state-of-the-art zero-shot baselines from related tasks, Object Goal Navigation and Vision Language Navigation, to demonstrate that LIFGIF outperforms them across all our evaluation metrics on OCVLN. Finally, we successfully demonstrate the effectiveness of LIFGIF for performing zero-shot object-centric instruction following in the real world on a Boston Dynamics Spot robot.",
    "pdf_url": "https://arxiv.org/pdf/2411.07848v3",
    "github_url": null,
    "published": "2024-11-12T15:01:40+00:00",
    "updated": "2025-05-07T22:19:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.06736v5",
    "title": "MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory",
    "authors": [
      "Park",
      "Cho",
      "Ahn"
    ],
    "summary": "Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers frequently become performance bottlenecks due to repeated failures. In this paper, we argue that the primary cause of failure in many low-level controllers is the absence of an episodic memory system. To address this, we introduce MrSteve (Memory Recall Steve), a novel low-level controller equipped with Place Event Memory (PEM), a form of episodic memory that captures what, where, and when information from episodes. This directly addresses the main limitation of the popular low-level controller, Steve-1. Unlike previous models that rely on short-term memory, PEM organizes spatial and event-based data, enabling efficient recall and navigation in long-horizon tasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing agents to alternate between exploration and task-solving based on recalled events. Our approach significantly improves task-solving and exploration efficiency compared to existing methods. We will release our code and demos on the project page: https://sites.google.com/view/mr-steve.",
    "pdf_url": "https://arxiv.org/pdf/2411.06736v5",
    "github_url": null,
    "published": "2024-11-11T06:04:53+00:00",
    "updated": "2025-04-11T01:35:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.04796v1",
    "title": "MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation",
    "authors": [
      "Paul",
      "Roychoudhury",
      "Bhowmick"
    ],
    "summary": "Visual odometry (VO) is essential for enabling accurate point-goal navigation of embodied agents in indoor environments where GPS and compass sensors are unreliable and inaccurate. However, traditional VO methods face challenges in wide-baseline scenarios, where fast robot motions and low frames per second (FPS) during inference hinder their performance, leading to drift and catastrophic failures in point-goal navigation. Recent deep-learned VO methods show robust performance but suffer from sample inefficiency during training; hence, they require huge datasets and compute resources. So, we propose a robust and sample-efficient VO pipeline based on motion priors available while an agent is navigating an environment. It consists of a training-free action-prior based geometric VO module that estimates a coarse relative pose which is further consumed as a motion prior by a deep-learned VO model, which finally produces a fine relative pose to be used by the navigation policy. This strategy helps our pipeline achieve up to 2x sample efficiency during training and demonstrates superior accuracy and robustness in point-goal navigation tasks compared to state-of-the-art VO method(s). Realistic indoor environments of the Gibson dataset is used in the AI-Habitat simulator to evaluate the proposed approach using navigation metrics (like success/SPL) and pose metrics (like RPE/ATE). We hope this method further opens a direction of work where motion priors from various sources can be utilized to improve VO estimates and achieve better results in embodied navigation tasks.",
    "pdf_url": "https://arxiv.org/pdf/2411.04796v1",
    "github_url": null,
    "published": "2024-11-07T15:36:49+00:00",
    "updated": "2024-11-07T15:36:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.04408v1",
    "title": "Repairing Neural Networks for Safety in Robotic Systems using Predictive Models",
    "authors": [
      "Majd",
      "Clark",
      "Fainekos"
    ],
    "summary": "This paper introduces a new method for safety-aware robot learning, focusing on repairing policies using predictive models. Our method combines behavioral cloning with neural network repair in a two-step supervised learning framework. It first learns a policy from expert demonstrations and then applies repair subject to predictive models to enforce safety constraints. The predictive models can encompass various aspects relevant to robot learning applications, such as proprioceptive states and collision likelihood. Our experimental results demonstrate that the learned policy successfully adheres to a predefined set of safety constraints on two applications: mobile robot navigation, and real-world lower-leg prostheses. Additionally, we have shown that our method effectively reduces repeated interaction with the robot, leading to substantial time savings during the learning process.",
    "pdf_url": "https://arxiv.org/pdf/2411.04408v1",
    "github_url": null,
    "published": "2024-11-07T03:57:37+00:00",
    "updated": "2024-11-07T03:57:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.03873v2",
    "title": "Biomechanics-Aware Trajectory Optimization for Online Navigation during Robotic Physiotherapy",
    "authors": [
      "Belli",
      "Melis",
      "Prendergast"
    ],
    "summary": "Robotic devices provide a great opportunity to assist in delivering physical therapy and rehabilitation movements, yet current robot-assisted methods struggle to incorporate biomechanical metrics essential for safe and effective therapy. We introduce BATON, a Biomechanics-Aware Trajectory Optimization approach to online robotic Navigation of human musculoskeletal loads for rotator cuff rehabilitation. BATON embeds a high-fidelity OpenSim model of the human shoulder into an optimal control framework, generating strain-minimizing trajectories for real-time control of therapeutic movements. \\addedText{Its core strength lies in the ability to adapt biomechanics-informed trajectories online to unpredictable volitional human actions or reflexive reactions during physical human-robot interaction based on robot-sensed motion and forces. BATON's adaptability is enabled by a real-time, model-based estimator that infers changes in muscle activity via a rapid redundancy solver driven by robot pose and force/torque sensor data. We validated BATON through physical human-robot interaction experiments, assessing response speed, motion smoothness, and interaction forces.",
    "pdf_url": "https://arxiv.org/pdf/2411.03873v2",
    "github_url": null,
    "published": "2024-11-06T12:40:59+00:00",
    "updated": "2025-07-11T12:27:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.03669v1",
    "title": "Imagined Potential Games: A Framework for Simulating, Learning and Evaluating Interactive Behaviors",
    "authors": [
      "Sun",
      "Wang",
      "Hung"
    ],
    "summary": "Interacting with human agents in complex scenarios presents a significant challenge for robotic navigation, particularly in environments that necessitate both collision avoidance and collaborative interaction, such as indoor spaces. Unlike static or predictably moving obstacles, human behavior is inherently complex and unpredictable, stemming from dynamic interactions with other agents. Existing simulation tools frequently fail to adequately model such reactive and collaborative behaviors, impeding the development and evaluation of robust social navigation strategies. This paper introduces a novel framework utilizing distributed potential games to simulate human-like interactions in highly interactive scenarios. Within this framework, each agent imagines a virtual cooperative game with others based on its estimation. We demonstrate this formulation can facilitate the generation of diverse and realistic interaction patterns in a configurable manner across various scenarios. Additionally, we have developed a gym-like environment leveraging our interactive agent model to facilitate the learning and evaluation of interactive navigation algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2411.03669v1",
    "github_url": null,
    "published": "2024-11-06T05:08:49+00:00",
    "updated": "2024-11-06T05:08:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.05831v1",
    "title": "To Ask or Not to Ask? Detecting Absence of Information in Vision and Language Navigation",
    "authors": [
      "Abraham",
      "Garg",
      "Dayoub"
    ],
    "summary": "Recent research in Vision Language Navigation (VLN) has overlooked the development of agents' inquisitive abilities, which allow them to ask clarifying questions when instructions are incomplete. This paper addresses how agents can recognize \"when\" they lack sufficient information, without focusing on \"what\" is missing, particularly in VLN tasks with vague instructions. Equipping agents with this ability enhances efficiency by reducing potential digressions and seeking timely assistance. The challenge in identifying such uncertain points is balancing between being overly cautious (high recall) and overly confident (high precision). We propose an attention-based instruction-vagueness estimation module that learns associations between instructions and the agent's trajectory. By leveraging instruction-to-path alignment information during training, the module's vagueness estimation performance improves by around 52% in terms of precision-recall balance. In our ablative experiments, we also demonstrate the effectiveness of incorporating this additional instruction-to-path attention network alongside the cross-modal attention networks within the navigator module. Our results show that the attention scores from the instruction-to-path attention network serve as better indicators for estimating vagueness.",
    "pdf_url": "https://arxiv.org/pdf/2411.05831v1",
    "github_url": null,
    "published": "2024-11-06T04:21:15+00:00",
    "updated": "2024-11-06T04:21:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.03575v1",
    "title": "Semantic Navigation for AI-assisted Ideation",
    "authors": [
      "Sandholm",
      "Dong",
      "Mukherjee"
    ],
    "summary": "We present a novel AI-based ideation assistant and evaluate it in a user study with a group of innovators. The key contribution of our work is twofold: we propose a method of idea exploration in a constrained domain by means of LLM-supported semantic navigation of problem and solution spaces, and employ novel automated data input filtering to improve generations. We found that semantic exploration is preferred to the traditional prompt-output interactions, measured both in explicit survey rankings, and in terms of innovation assistant engagement, where 2.1x more generations were performed using semantic exploration. We also show that filtering input data with metrics such as relevancy, coherence and human alignment leads to improved generations in the same metrics as well as enhanced quality of experience among innovators.",
    "pdf_url": "https://arxiv.org/pdf/2411.03575v1",
    "github_url": null,
    "published": "2024-11-06T00:32:21+00:00",
    "updated": "2024-11-06T00:32:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.01814v1",
    "title": "Enhancing Social Robot Navigation with Integrated Motion Prediction and Trajectory Planning in Dynamic Human Environments",
    "authors": [
      "Canh",
      "HoangVan",
      "Chong"
    ],
    "summary": "Navigating safely in dynamic human environments is crucial for mobile service robots, and social navigation is a key aspect of this process. In this paper, we proposed an integrative approach that combines motion prediction and trajectory planning to enable safe and socially-aware robot navigation. The main idea of the proposed method is to leverage the advantages of Socially Acceptable trajectory prediction and Timed Elastic Band (TEB) by incorporating human interactive information including position, orientation, and motion into the objective function of the TEB algorithms. In addition, we designed social constraints to ensure the safety of robot navigation. The proposed system is evaluated through physical simulation using both quantitative and qualitative metrics, demonstrating its superior performance in avoiding human and dynamic obstacles, thereby ensuring safe navigation. The implementations are open source at: \\url{https://github.com/thanhnguyencanh/SGan-TEB.git}",
    "pdf_url": "https://arxiv.org/pdf/2411.01814v1",
    "github_url": "https://github.com/thanhnguyencanh/SGan-TEB",
    "published": "2024-11-04T05:34:30+00:00",
    "updated": "2024-11-04T05:34:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.22087v1",
    "title": "4D-based Robot Navigation Using Relativistic Image Processing",
    "authors": [
      "Mller",
      "Kranzlmller"
    ],
    "summary": "Machine perception is an important prerequisite for safe interaction and locomotion in dynamic environments. This requires not only the timely perception of surrounding geometries and distances but also the ability to react to changing situations through predefined, learned but also reusable skill endings of a robot so that physical damage or bodily harm can be avoided. In this context, 4D perception offers the possibility of predicting one's own position and changes in the environment over time. In this paper, we present a 4D-based approach to robot navigation using relativistic image processing. Relativistic image processing handles the temporal-related sensor information in a tensor model within a constructive 4D space. 4D-based navigation expands the causal understanding and the resulting interaction radius of a robot through the use of visual and sensory 4D information.",
    "pdf_url": "https://arxiv.org/pdf/2410.22087v1",
    "github_url": null,
    "published": "2024-10-29T14:42:19+00:00",
    "updated": "2024-10-29T14:42:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.21842v2",
    "title": "Diffusion as Reasoning: Enhancing Object Navigation via Diffusion Model Conditioned on LLM-based Object-Room Knowledge",
    "authors": [
      "Ji",
      "Yun",
      "Liu"
    ],
    "summary": "The Object Navigation (ObjectNav) task aims to guide an agent to locate target objects in unseen environments using partial observations. Prior approaches have employed location prediction paradigms to achieve long-term goal reasoning, yet these methods often struggle to effectively integrate contextual relation reasoning. Alternatively, map completion-based paradigms predict long-term goals by generating semantic maps of unexplored areas. However, existing methods in this category fail to fully leverage known environmental information, resulting in suboptimal map quality that requires further improvement. In this work, we propose a novel approach to enhancing the ObjectNav task, by training a diffusion model to learn the statistical distribution patterns of objects in semantic maps, and using the map of the explored regions during navigation as the condition to generate the map of the unknown regions, thereby realizing the long-term goal reasoning of the target object, i.e., diffusion as reasoning (DAR). Meanwhile, we propose the Room Guidance method, which leverages commonsense knowledge derived from large language models (LLMs) to guide the diffusion model in generating room-aware object distributions. Based on the generated map in the unknown region, the agent sets the predicted location of the target as the goal and moves towards it. Experiments on Gibson and MP3D show the effectiveness of our method.",
    "pdf_url": "https://arxiv.org/pdf/2410.21842v2",
    "github_url": null,
    "published": "2024-10-29T08:10:06+00:00",
    "updated": "2025-06-06T02:18:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.20545v1",
    "title": "ChartA11y: Designing Accessible Touch Experiences of Visualizations with Blind Smartphone Users",
    "authors": [
      "Zhang",
      "Thompson",
      "Shah"
    ],
    "summary": "We introduce ChartA11y, an app developed to enable accessible 2-D visualizations on smartphones for blind users through a participatory and iterative design process involving 13 sessions with two blind partners. We also present a design journey for making accessible touch experiences that go beyond simple auditory feedback, incorporating multimodal interactions and multisensory data representations. Together, ChartA11y aimed at providing direct chart accessing and comprehensive chart understanding by applying a two-mode setting: a semantic navigation framework mode and a direct touch mapping mode. By re-designing traditional touch-to-audio interactions, ChartA11y also extends to accessible scatter plots, addressing the under-explored challenges posed by their non-linear data distribution. Our main contributions encompass the detailed participatory design process and the resulting system, ChartA11y, offering a novel approach for blind users to access visualizations on their smartphones.",
    "pdf_url": "https://arxiv.org/pdf/2410.20545v1",
    "github_url": null,
    "published": "2024-10-27T18:24:05+00:00",
    "updated": "2024-10-27T18:24:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.05022v1",
    "title": "Towards Probabilistic Planning of Explanations for Robot Navigation",
    "authors": [
      "Halilovic",
      "Krivic"
    ],
    "summary": "In robotics, ensuring that autonomous systems are comprehensible and accountable to users is essential for effective human-robot interaction. This paper introduces a novel approach that integrates user-centered design principles directly into the core of robot path planning processes. We propose a probabilistic framework for automated planning of explanations for robot navigation, where the preferences of different users regarding explanations are probabilistically modeled to tailor the stochasticity of the real-world human-robot interaction and the communication of decisions of the robot and its actions towards humans. This approach aims to enhance the transparency of robot path planning and adapt to diverse user explanation needs by anticipating the types of explanations that will satisfy individual users.",
    "pdf_url": "https://arxiv.org/pdf/2411.05022v1",
    "github_url": null,
    "published": "2024-10-26T09:52:14+00:00",
    "updated": "2024-10-26T09:52:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.19072v2",
    "title": "Analyzing Human Perceptions of a MEDEVAC Robot in a Simulated Evacuation Scenario",
    "authors": [
      "Jordan",
      "Pandey",
      "Doshi"
    ],
    "summary": "The use of autonomous systems in medical evacuation (MEDEVAC) scenarios is promising, but existing implementations overlook key insights from human-robot interaction (HRI) research. Studies on human-machine teams demonstrate that human perceptions of a machine teammate are critical in governing the machine's performance. Here, we present a mixed factorial design to assess human perceptions of a MEDEVAC robot in a simulated evacuation scenario. Participants were assigned to the role of casualty (CAS) or bystander (BYS) and subjected to three within-subjects conditions based on the MEDEVAC robot's operating mode: autonomous-slow (AS), autonomous-fast (AF), and teleoperation (TO). During each trial, a MEDEVAC robot navigated an 11-meter path, acquiring a casualty and transporting them to an ambulance exchange point while avoiding an idle bystander. Following each trial, subjects completed a questionnaire measuring their emotional states, perceived safety, and social compatibility with the robot. Results indicate a consistent main effect of operating mode on reported emotional states and perceived safety. Pairwise analyses suggest that the employment of the AF operating mode negatively impacted perceptions along these dimensions. There were no persistent differences between casualty and bystander responses.",
    "pdf_url": "https://arxiv.org/pdf/2410.19072v2",
    "github_url": null,
    "published": "2024-10-24T18:28:06+00:00",
    "updated": "2024-10-29T15:44:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.18570v1",
    "title": "Zero-shot Object Navigation with Vision-Language Models Reasoning",
    "authors": [
      "Wen",
      "Huang",
      "Huang"
    ],
    "summary": "Object navigation is crucial for robots, but traditional methods require substantial training data and cannot be generalized to unknown environments. Zero-shot object navigation (ZSON) aims to address this challenge, allowing robots to interact with unknown objects without specific training data. Language-driven zero-shot object navigation (L-ZSON) is an extension of ZSON that incorporates natural language instructions to guide robot navigation and interaction with objects. In this paper, we propose a novel Vision Language model with a Tree-of-thought Network (VLTNet) for L-ZSON. VLTNet comprises four main modules: vision language model understanding, semantic mapping, tree-of-thought reasoning and exploration, and goal identification. Among these modules, Tree-of-Thought (ToT) reasoning and exploration module serves as a core component, innovatively using the ToT reasoning framework for navigation frontier selection during robot exploration. Compared to conventional frontier selection without reasoning, navigation using ToT reasoning involves multi-path reasoning processes and backtracking when necessary, enabling globally informed decision-making with higher accuracy. Experimental results on PASTURE and RoboTHOR benchmarks demonstrate the outstanding performance of our model in LZSON, particularly in scenarios involving complex natural language as target instructions.",
    "pdf_url": "https://arxiv.org/pdf/2410.18570v1",
    "github_url": null,
    "published": "2024-10-24T09:24:07+00:00",
    "updated": "2024-10-24T09:24:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.15044v2",
    "title": "Adanonymizer: Interactively Navigating and Balancing the Duality of Privacy and Output Performance in Human-LLM Interaction",
    "authors": [
      "Zhang",
      "Yi",
      "Xing"
    ],
    "summary": "Current Large Language Models (LLMs) cannot support users to precisely balance privacy protection and output performance during individual consultations. We introduce Adanonymizer, an anonymization plug-in that allows users to control this balance by navigating a trade-off curve. A survey (N=221) revealed a privacy paradox, where users frequently disclosed sensitive information despite acknowledging privacy risks. The study further demonstrated that privacy risks were not significantly correlated with model output performance, highlighting the potential to navigate this trade-off. Adanonymizer normalizes privacy and utility ratings by type and automates the pseudonymization of sensitive terms based on user preferences, significantly reducing user effort. Its 2D color palette interface visualizes the privacy-utility trade-off, allowing users to adjust the balance by manipulating a point. An evaluation (N=36) compared Adanonymizer with ablation methods and differential privacy techniques, where Adanonymizer significantly reduced modification time, achieved better perceived model performance and overall user preference.",
    "pdf_url": "https://arxiv.org/pdf/2410.15044v2",
    "github_url": null,
    "published": "2024-10-19T09:04:01+00:00",
    "updated": "2025-01-27T12:47:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.14851v1",
    "title": "IntelliMove: Enhancing Robotic Planning with Semantic Mapping",
    "authors": [
      "Ngom",
      "Zhang",
      "Zhang"
    ],
    "summary": "Semantic navigation enables robots to understand their environments beyond basic geometry, allowing them to reason about objects, their functions, and their interrelationships. In semantic robotic navigation, creating accurate and semantically enriched maps is fundamental. Planning based on semantic maps not only enhances the robot's planning efficiency and computational speed but also makes the planning more meaningful, supporting a broader range of semantic tasks. In this paper, we introduce two core modules of IntelliMove: IntelliMap, a generic hierarchical semantic topometric map framework developed through an analysis of current technologies strengths and weaknesses, and Semantic Planning, which utilizes the semantic maps from IntelliMap. We showcase use cases that highlight IntelliMove's adaptability and effectiveness. Through experiments in simulated environments, we further demonstrate IntelliMove's capability in semantic navigation.",
    "pdf_url": "https://arxiv.org/pdf/2410.14851v1",
    "github_url": null,
    "published": "2024-10-18T20:32:43+00:00",
    "updated": "2024-10-18T20:32:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.14250v1",
    "title": "Vision-Language Navigation with Energy-Based Policy",
    "authors": [
      "Liu",
      "Wang",
      "Yang"
    ],
    "summary": "Vision-language navigation (VLN) requires an agent to execute actions following human instructions. Existing VLN models are optimized through expert demonstrations by supervised behavioural cloning or incorporating manual reward engineering. While straightforward, these efforts overlook the accumulation of errors in the Markov decision process, and struggle to match the distribution of the expert policy. Going beyond this, we propose an Energy-based Navigation Policy (ENP) to model the joint state-action distribution using an energy-based model. At each step, low energy values correspond to the state-action pairs that the expert is most likely to perform, and vice versa. Theoretically, the optimization objective is equivalent to minimizing the forward divergence between the occupancy measure of the expert and ours. Consequently, ENP learns to globally align with the expert policy by maximizing the likelihood of the actions and modeling the dynamics of the navigation states in a collaborative manner. With a variety of VLN architectures, ENP achieves promising performances on R2R, REVERIE, RxR, and R2R-CE, unleashing the power of existing VLN models.",
    "pdf_url": "https://arxiv.org/pdf/2410.14250v1",
    "github_url": null,
    "published": "2024-10-18T08:01:36+00:00",
    "updated": "2024-10-18T08:01:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.13825v2",
    "title": "AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents",
    "authors": [
      "Yang",
      "Liu",
      "Chaudhary"
    ],
    "summary": "Autonomy via agents using large language models (LLMs) for personalized, standardized tasks boosts human efficiency. Automating web tasks (like booking hotels within a budget) is increasingly sought after. Fulfilling practical needs, the web agent also serves as an important proof-of-concept example for various agent grounding scenarios, with its success promising advancements in many future applications. Prior research often handcrafts web agent strategies (e.g., prompting templates, multi-agent systems, search methods, etc.) and the corresponding in-context examples, which may not generalize well across all real-world scenarios. On the other hand, there has been limited study on the misalignment between a web agent's observation/action representation and the pre-training data of the LLM it's based on. This discrepancy is especially notable when LLMs are primarily trained for language completion rather than tasks involving embodied navigation actions and symbolic web elements. Our study enhances an LLM-based web agent by simply refining its observation and action space to better align with the LLM's capabilities. This approach enables our base agent to significantly outperform previous methods on a wide variety of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose web interaction tasks, our agent AgentOccam surpasses the previous state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute points respectively, and boosts the success rate by 26.6 points (+161%) over similar plain web agents with its observation and action space alignment. We achieve this without using in-context examples, new agent roles, online feedback or search strategies. AgentOccam's simple design highlights LLMs' impressive zero-shot performance on web tasks, and underlines the critical role of carefully tuning observation and action spaces for LLM-based agents.",
    "pdf_url": "https://arxiv.org/pdf/2410.13825v2",
    "github_url": null,
    "published": "2024-10-17T17:50:38+00:00",
    "updated": "2025-05-24T03:55:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.13418v1",
    "title": "Interactive Navigation with Adaptive Non-prehensile Mobile Manipulation",
    "authors": [
      "Dai",
      "Liu",
      "Sreenath"
    ],
    "summary": "This paper introduces a framework for interactive navigation through adaptive non-prehensile mobile manipulation. A key challenge in this process is handling objects with unknown dynamics, which are difficult to infer from visual observation. To address this, we propose an adaptive dynamics model for common movable indoor objects via learned SE(2) dynamics representations. This model is integrated into Model Predictive Path Integral (MPPI) control to guide the robot's interactions. Additionally, the learned dynamics help inform decision-making when navigating around objects that cannot be manipulated.Our approach is validated in both simulation and real-world scenarios, demonstrating its ability to accurately represent object dynamics and effectively manipulate various objects. We further highlight its success in the Navigation Among Movable Objects (NAMO) task by deploying the proposed framework on a dynamically balancing mobile robot, Shmoobot. Project website: https://cmushmoobot.github.io/AdaptivePushing/.",
    "pdf_url": "https://arxiv.org/pdf/2410.13418v1",
    "github_url": null,
    "published": "2024-10-17T10:40:31+00:00",
    "updated": "2024-10-17T10:40:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.12407v2",
    "title": "Beyond Coarse-Grained Matching in Video-Text Retrieval",
    "authors": [
      "Chen",
      "Doughty",
      "Li"
    ],
    "summary": "Video-text retrieval has seen significant advancements, yet the ability of models to discern subtle differences in captions still requires verification. In this paper, we introduce a new approach for fine-grained evaluation. Our approach can be applied to existing datasets by automatically generating hard negative test captions with subtle single-word variations across nouns, verbs, adjectives, adverbs, and prepositions. We perform comprehensive experiments using four state-of-the-art models across two standard benchmarks (MSR-VTT and VATEX) and two specially curated datasets enriched with detailed descriptions (VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our analyses show that the current evaluation benchmarks fall short in detecting a model's ability to perceive subtle single-word differences, 2) our fine-grained evaluation highlights the difficulty models face in distinguishing such subtle variations. To enhance fine-grained understanding, we propose a new baseline that can be easily combined with current methods. Experiments on our fine-grained evaluations demonstrate that this approach enhances a model's ability to understand fine-grained differences.",
    "pdf_url": "https://arxiv.org/pdf/2410.12407v2",
    "github_url": null,
    "published": "2024-10-16T09:42:29+00:00",
    "updated": "2024-10-17T15:59:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.11402v1",
    "title": "M2Diffuser: Diffusion-based Trajectory Optimization for Mobile Manipulation in 3D Scenes",
    "authors": [
      "Yan",
      "Zhang",
      "Han"
    ],
    "summary": "Recent advances in diffusion models have opened new avenues for research into embodied AI agents and robotics. Despite significant achievements in complex robotic locomotion and skills, mobile manipulation-a capability that requires the coordination of navigation and manipulation-remains a challenge for generative AI techniques. This is primarily due to the high-dimensional action space, extended motion trajectories, and interactions with the surrounding environment. In this paper, we introduce M2Diffuser, a diffusion-based, scene-conditioned generative model that directly generates coordinated and efficient whole-body motion trajectories for mobile manipulation based on robot-centric 3D scans. M2Diffuser first learns trajectory-level distributions from mobile manipulation trajectories provided by an expert planner. Crucially, it incorporates an optimization module that can flexibly accommodate physical constraints and task objectives, modeled as cost and energy functions, during the inference process. This enables the reduction of physical violations and execution errors at each denoising step in a fully differentiable manner. Through benchmarking on three types of mobile manipulation tasks across over 20 scenes, we demonstrate that M2Diffuser outperforms state-of-the-art neural planners and successfully transfers the generated trajectories to a real-world robot. Our evaluations underscore the potential of generative AI to enhance the generalization of traditional planning and learning-based robotic methods, while also highlighting the critical role of enforcing physical constraints for safe and robust execution.",
    "pdf_url": "https://arxiv.org/pdf/2410.11402v1",
    "github_url": null,
    "published": "2024-10-15T08:49:35+00:00",
    "updated": "2024-10-15T08:49:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.09874v1",
    "title": "ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination",
    "authors": [
      "Zhao",
      "Cai",
      "Tang"
    ],
    "summary": "Visual navigation is an essential skill for home-assistance robots, providing the object-searching ability to accomplish long-horizon daily tasks. Many recent approaches use Large Language Models (LLMs) for commonsense inference to improve exploration efficiency. However, the planning process of LLMs is limited within texts and it is difficult to represent the spatial occupancy and geometry layout only by texts. Both are important for making rational navigation decisions. In this work, we seek to unleash the spatial perception and planning ability of Vision-Language Models (VLMs), and explore whether the VLM, with only on-board camera captured RGB/RGB-D stream inputs, can efficiently finish the visual navigation tasks in a mapless manner. We achieve this by developing the imagination-powered navigation framework ImagineNav, which imagines the future observation images at valuable robot views and translates the complex navigation planning process into a rather simple best-view image selection problem for VLM. To generate appropriate candidate robot views for imagination, we introduce the Where2Imagine module, which is distilled to align with human navigation habits. Finally, to reach the VLM preferred views, an off-the-shelf point-goal navigation policy is utilized. Empirical experiments on the challenging open-vocabulary object navigation benchmarks demonstrates the superiority of our proposed system.",
    "pdf_url": "https://arxiv.org/pdf/2410.09874v1",
    "github_url": null,
    "published": "2024-10-13T15:31:31+00:00",
    "updated": "2024-10-13T15:31:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.08649v1",
    "title": "E-Motion: Future Motion Simulation via Event Sequence Diffusion",
    "authors": [
      "Wu",
      "Zhu",
      "Hou"
    ],
    "summary": "Forecasting a typical object's future motion is a critical task for interpreting and interacting with dynamic environments in computer vision. Event-based sensors, which could capture changes in the scene with exceptional temporal granularity, may potentially offer a unique opportunity to predict future motion with a level of detail and precision previously unachievable. Inspired by that, we propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion simulation framework. Specifically, we initially employ pre-trained stable video diffusion models to adapt the event sequence dataset. This process facilitates the transfer of extensive knowledge from RGB videos to an event-centric domain. Moreover, we introduce an alignment mechanism that utilizes reinforcement learning techniques to enhance the reverse generation trajectory of the diffusion model, ensuring improved performance and accuracy. Through extensive testing and validation, we demonstrate the effectiveness of our method in various complex scenarios, showcasing its potential to revolutionize motion flow prediction in computer vision applications such as autonomous vehicle guidance, robotic navigation, and interactive media. Our findings suggest a promising direction for future research in enhancing the interpretative power and predictive accuracy of computer vision systems.",
    "pdf_url": "https://arxiv.org/pdf/2410.08649v1",
    "github_url": null,
    "published": "2024-10-11T09:19:23+00:00",
    "updated": "2024-10-11T09:19:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.08500v3",
    "title": "Exploring Spatial Representation to Enhance LLM Reasoning in Aerial Vision-Language Navigation",
    "authors": [
      "Gao",
      "Wang",
      "Han"
    ],
    "summary": "Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned Aerial Vehicles (UAVs) to navigate in outdoor environments through natural language instructions and visual cues. However, it remains challenging due to the complex spatial relationships in aerial scenes.In this paper, we propose a training-free, zero-shot framework for aerial VLN tasks, where the large language model (LLM) is leveraged as the agent for action prediction. Specifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to enhance the spatial reasoning capabilities of LLMs. This is achieved by extracting and projecting instruction-related semantic masks onto a top-down map, which presents spatial and topological information about surrounding landmarks and grows during the navigation process. At each step, a local map centered at the UAV is extracted from the growing top-down map, and transformed into a ma trix representation with distance metrics, serving as the text prompt to LLM for action prediction in response to the given instruction. Experiments conducted in real and simulation environments have proved the effectiveness and robustness of our method, achieving absolute success rate improvements of 26.8% and 5.8% over current state-of-the-art methods on simple and complex navigation tasks, respectively. The dataset and code will be released soon.",
    "pdf_url": "https://arxiv.org/pdf/2410.08500v3",
    "github_url": null,
    "published": "2024-10-11T03:54:48+00:00",
    "updated": "2025-08-11T03:42:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.08189v1",
    "title": "SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation",
    "authors": [
      "Yin",
      "Xu",
      "Wu"
    ],
    "summary": "In this paper, we propose a new framework for zero-shot object navigation. Existing zero-shot object navigation methods prompt LLM with the text of spatially closed objects, which lacks enough scene context for in-depth reasoning. To better preserve the information of environment and fully exploit the reasoning ability of LLM, we propose to represent the observed scene with 3D scene graph. The scene graph encodes the relationships between objects, groups and rooms with a LLM-friendly structure, for which we design a hierarchical chain-of-thought prompt to help LLM reason the goal location according to scene context by traversing the nodes and edges. Moreover, benefit from the scene graph representation, we further design a re-perception mechanism to empower the object navigation framework with the ability to correct perception error. We conduct extensive experiments on MP3D, HM3D and RoboTHOR environments, where SG-Nav surpasses previous state-of-the-art zero-shot methods by more than 10% SR on all benchmarks, while the decision process is explainable. To the best of our knowledge, SG-Nav is the first zero-shot method that achieves even higher performance than supervised object navigation methods on the challenging MP3D benchmark.",
    "pdf_url": "https://arxiv.org/pdf/2410.08189v1",
    "github_url": null,
    "published": "2024-10-10T17:57:19+00:00",
    "updated": "2024-10-10T17:57:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.07087v2",
    "title": "Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology",
    "authors": [
      "Wang",
      "Yang",
      "Wang"
    ],
    "summary": "Developing agents capable of navigating to a target location based on language instructions and visual information, known as vision-language navigation (VLN), has attracted widespread interest. Most research has focused on ground-based agents, while UAV-based VLN remains relatively underexplored. Recent efforts in UAV vision-language navigation predominantly adopt ground-based VLN settings, relying on predefined discrete action spaces and neglecting the inherent disparities in agent movement dynamics and the complexity of navigation tasks between ground and aerial environments. To address these disparities and challenges, we propose solutions from three perspectives: platform, benchmark, and methodology. To enable realistic UAV trajectory simulation in VLN tasks, we propose the OpenUAV platform, which features diverse environments, realistic flight control, and extensive algorithmic support. We further construct a target-oriented VLN dataset consisting of approximately 12k trajectories on this platform, serving as the first dataset specifically designed for realistic UAV VLN tasks. To tackle the challenges posed by complex aerial environments, we propose an assistant-guided UAV object search benchmark called UAV-Need-Help, which provides varying levels of guidance information to help UAVs better accomplish realistic VLN tasks. We also propose a UAV navigation LLM that, given multi-view images, task descriptions, and assistant instructions, leverages the multimodal understanding capabilities of the MLLM to jointly process visual and textual information, and performs hierarchical trajectory generation. The evaluation results of our method significantly outperform the baseline models, while there remains a considerable gap between our results and those achieved by human operators, underscoring the challenge presented by the UAV-Need-Help task.",
    "pdf_url": "https://arxiv.org/pdf/2410.07087v2",
    "github_url": null,
    "published": "2024-10-09T17:29:01+00:00",
    "updated": "2024-10-10T05:02:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.06613v2",
    "title": "ES-Gaussian: Gaussian Splatting Mapping via Error Space-Based Gaussian Completion",
    "authors": [
      "Chen",
      "Zeng",
      "Li"
    ],
    "summary": "Accurate and affordable indoor 3D reconstruction is critical for effective robot navigation and interaction. Traditional LiDAR-based mapping provides high precision but is costly, heavy, and power-intensive, with limited ability for novel view rendering. Vision-based mapping, while cost-effective and capable of capturing visual data, often struggles with high-quality 3D reconstruction due to sparse point clouds. We propose ES-Gaussian, an end-to-end system using a low-altitude camera and single-line LiDAR for high-quality 3D indoor reconstruction. Our system features Visual Error Construction (VEC) to enhance sparse point clouds by identifying and correcting areas with insufficient geometric detail from 2D error maps. Additionally, we introduce a novel 3DGS initialization method guided by single-line LiDAR, overcoming the limitations of traditional multi-view setups and enabling effective reconstruction in resource-constrained environments. Extensive experimental results on our new Dreame-SR dataset and a publicly available dataset demonstrate that ES-Gaussian outperforms existing methods, particularly in challenging scenarios. The project page is available at https://chenlu-china.github.io/ES-Gaussian/.",
    "pdf_url": "https://arxiv.org/pdf/2410.06613v2",
    "github_url": null,
    "published": "2024-10-09T07:09:29+00:00",
    "updated": "2024-10-30T10:21:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.17267v1",
    "title": "Zero-Shot Vision-and-Language Navigation with Collision Mitigation in Continuous Environment",
    "authors": [
      "Jeong",
      "Kang",
      "Kim"
    ],
    "summary": "We propose the zero-shot Vision-and-Language Navigation with Collision Mitigation (VLN-CM), which takes these considerations. VLN-CM is composed of four modules and predicts the direction and distance of the next movement at each step. We utilize large foundation models for each modules. To select the direction, we use the Attention Spot Predictor (ASP), View Selector (VS), and Progress Monitor (PM). The ASP employs a Large Language Model (e.g. ChatGPT) to split navigation instructions into attention spots, which are objects or scenes at the location to move to (e.g. a yellow door). The VS selects from panorama images provided at 30-degree intervals the one that includes the attention spot, using CLIP similarity. We then choose the angle of the selected image as the direction to move in. The PM uses a rule-based approach to decide which attention spot to focus on next, among multiple spots derived from the instructions. If the similarity between the current attention spot and the visual observations decreases consecutively at each step, the PM determines that the agent has passed the current spot and moves on to the next one. For selecting the distance to move, we employed the Open Map Predictor (OMP). The OMP uses panorama depth information to predict an occupancy mask. We then selected a collision-free distance in the predicted direction based on the occupancy mask. We evaluated our method using the validation data of VLN-CE. Our approach showed better performance than several baseline methods, and the OPM was effective in mitigating collisions for the agent.",
    "pdf_url": "https://arxiv.org/pdf/2410.17267v1",
    "github_url": null,
    "published": "2024-10-07T11:59:01+00:00",
    "updated": "2024-10-07T11:59:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.04302v1",
    "title": "PANav: Toward Privacy-Aware Robot Navigation via Vision-Language Models",
    "authors": [
      "Yu",
      "Kasaei",
      "Cao"
    ],
    "summary": "Navigating robots discreetly in human work environments while considering the possible privacy implications of robotic tasks presents significant challenges. Such scenarios are increasingly common, for instance, when robots transport sensitive objects that demand high levels of privacy in spaces crowded with human activities. While extensive research has been conducted on robotic path planning and social awareness, current robotic systems still lack the functionality of privacy-aware navigation in public environments. To address this, we propose a new framework for mobile robot navigation that leverages vision-language models to incorporate privacy awareness into adaptive path planning. Specifically, all potential paths from the starting point to the destination are generated using the A* algorithm. Concurrently, the vision-language model is used to infer the optimal path for privacy-awareness, given the environmental layout and the navigational instruction. This approach aims to minimize the robot's exposure to human activities and preserve the privacy of the robot and its surroundings. Experimental results on the S3DIS dataset demonstrate that our framework significantly enhances mobile robots' privacy awareness of navigation in human-shared public environments. Furthermore, we demonstrate the practical applicability of our framework by successfully navigating a robotic platform through real-world office environments. The supplementary video and code can be accessed via the following link: https://sites.google.com/view/privacy-aware-nav.",
    "pdf_url": "https://arxiv.org/pdf/2410.04302v1",
    "github_url": null,
    "published": "2024-10-05T22:54:31+00:00",
    "updated": "2024-10-05T22:54:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.03603v1",
    "title": "LeLaN: Learning A Language-Conditioned Navigation Policy from In-the-Wild Videos",
    "authors": [
      "Hirose",
      "Glossop",
      "Sridhar"
    ],
    "summary": "The world is filled with a wide variety of objects. For robots to be useful, they need the ability to find arbitrary objects described by people. In this paper, we present LeLaN(Learning Language-conditioned Navigation policy), a novel approach that consumes unlabeled, action-free egocentric data to learn scalable, language-conditioned object navigation. Our framework, LeLaN leverages the semantic knowledge of large vision-language models, as well as robotic foundation models, to label in-the-wild data from a variety of indoor and outdoor environments. We label over 130 hours of data collected in real-world indoor and outdoor environments, including robot observations, YouTube video tours, and human walking data. Extensive experiments with over 1000 real-world trials show that our approach enables training a policy from unlabeled action-free videos that outperforms state-of-the-art robot navigation methods, while being capable of inference at 4 times their speed on edge compute. We open-source our models, datasets and provide supplementary videos on our project page (https://learning-language-navigation.github.io/).",
    "pdf_url": "https://arxiv.org/pdf/2410.03603v1",
    "github_url": null,
    "published": "2024-10-04T17:03:14+00:00",
    "updated": "2024-10-04T17:03:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.03488v1",
    "title": "MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-object Demand-driven Navigation",
    "authors": [
      "Wang",
      "Liu",
      "Cai"
    ],
    "summary": "The process of satisfying daily demands is a fundamental aspect of humans' daily lives. With the advancement of embodied AI, robots are increasingly capable of satisfying human demands. Demand-driven navigation (DDN) is a task in which an agent must locate an object to satisfy a specified demand instruction, such as ``I am thirsty.'' The previous study typically assumes that each demand instruction requires only one object to be fulfilled and does not consider individual preferences. However, the realistic human demand may involve multiple objects. In this paper, we introduce the Multi-object Demand-driven Navigation (MO-DDN) benchmark, which addresses these nuanced aspects, including multi-object search and personal preferences, thus making the MO-DDN task more reflective of real-life scenarios compared to DDN. Building upon previous work, we employ the concept of ``attribute'' to tackle this new task. However, instead of solely relying on attribute features in an end-to-end manner like DDN, we propose a modular method that involves constructing a coarse-to-fine attribute-based exploration agent (C2FAgent). Our experimental results illustrate that this coarse-to-fine exploration strategy capitalizes on the advantages of attributes at various decision-making levels, resulting in superior performance compared to baseline methods. Code and video can be found at https://sites.google.com/view/moddn.",
    "pdf_url": "https://arxiv.org/pdf/2410.03488v1",
    "github_url": null,
    "published": "2024-10-04T14:59:20+00:00",
    "updated": "2024-10-04T14:59:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.02751v1",
    "title": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for Embodied AI",
    "authors": [
      "Elawady",
      "Chhablani",
      "Ramrakhya"
    ],
    "summary": "Intelligent embodied agents need to quickly adapt to new scenarios by integrating long histories of experience into decision-making. For instance, a robot in an unfamiliar house initially wouldn't know the locations of objects needed for tasks and might perform inefficiently. However, as it gathers more experience, it should learn the layout of its environment and remember where objects are, allowing it to complete new tasks more efficiently. To enable such rapid adaptation to new tasks, we present ReLIC, a new approach for in-context reinforcement learning (RL) for embodied agents. With ReLIC, agents are capable of adapting to new environments using 64,000 steps of in-context experience with full attention while being trained through self-generated experience via RL. We achieve this by proposing a novel policy update scheme for on-policy RL called \"partial updates'' as well as a Sink-KV mechanism that enables effective utilization of a long observation history for embodied agents. Our method outperforms a variety of meta-RL baselines in adapting to unseen houses in an embodied multi-object navigation task. In addition, we find that ReLIC is capable of few-shot imitation learning despite never being trained with expert demonstrations. We also provide a comprehensive analysis of ReLIC, highlighting that the combination of large-scale RL training, the proposed partial updates scheme, and the Sink-KV are essential for effective in-context learning. The code for ReLIC and all our experiments is at https://github.com/aielawady/relic",
    "pdf_url": "https://arxiv.org/pdf/2410.02751v1",
    "github_url": "https://github.com/aielawady/relic",
    "published": "2024-10-03T17:58:11+00:00",
    "updated": "2024-10-03T17:58:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.02730v3",
    "title": "DivScene: Towards Open-Vocabulary Object Navigation with Large Vision Language Models in Diverse Scenes",
    "authors": [
      "Wang",
      "Zhang",
      "Fang"
    ],
    "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in tasks like visual question answering and document understanding. However, their potential to comprehend embodied environments and navigate within them remains underexplored. In this work, we first study the challenge of open-vocabulary object navigation by introducing DivScene, a large-scale dataset with 4,614 houses across 81 scene types and 5,707 kinds of target objects. Our dataset provides a much greater diversity of target objects and scene types than existing datasets, enabling a comprehensive task evaluation. We evaluated various methods with LVLMs and LLMs on our dataset and found that current models still fall short of open-vocab object navigation ability. Then, we fine-tuned LVLMs to predict the next action with CoT explanations. We observe that LVLM's navigation ability can be improved substantially with only BFS-generated shortest paths without any human supervision, surpassing GPT-4o by over 20% in success rates.",
    "pdf_url": "https://arxiv.org/pdf/2410.02730v3",
    "github_url": null,
    "published": "2024-10-03T17:49:28+00:00",
    "updated": "2025-09-01T03:33:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.02389v1",
    "title": "Diffusion Meets Options: Hierarchical Generative Skill Composition for Temporally-Extended Tasks",
    "authors": [
      "Feng",
      "Luan",
      "Ma"
    ],
    "summary": "Safe and successful deployment of robots requires not only the ability to generate complex plans but also the capacity to frequently replan and correct execution errors. This paper addresses the challenge of long-horizon trajectory planning under temporally extended objectives in a receding horizon manner. To this end, we propose DOPPLER, a data-driven hierarchical framework that generates and updates plans based on instruction specified by linear temporal logic (LTL). Our method decomposes temporal tasks into chain of options with hierarchical reinforcement learning from offline non-expert datasets. It leverages diffusion models to generate options with low-level actions. We devise a determinantal-guided posterior sampling technique during batch generation, which improves the speed and diversity of diffusion generated options, leading to more efficient querying. Experiments on robot navigation and manipulation tasks demonstrate that DOPPLER can generate sequences of trajectories that progressively satisfy the specified formulae for obstacle avoidance and sequential visitation. Demonstration videos are available online at: https://philiptheother.github.io/doppler/.",
    "pdf_url": "https://arxiv.org/pdf/2410.02389v1",
    "github_url": null,
    "published": "2024-10-03T11:10:37+00:00",
    "updated": "2024-10-03T11:10:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.01273v3",
    "title": "CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction",
    "authors": [
      "Choi",
      "Cho",
      "Kim"
    ],
    "summary": "Real-life robot navigation involves more than just reaching a destination; it requires optimizing movements while addressing scenario-specific goals. An intuitive way for humans to express these goals is through abstract cues like verbal commands or rough sketches. Such human guidance may lack details or be noisy. Nonetheless, we expect robots to navigate as intended. For robots to interpret and execute these abstract instructions in line with human expectations, they must share a common understanding of basic navigation concepts with humans. To this end, we introduce CANVAS, a novel framework that combines visual and linguistic instructions for commonsense-aware navigation. Its success is driven by imitation learning, enabling the robot to learn from human navigation behavior. We present COMMAND, a comprehensive dataset with human-annotated navigation results, spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems in simulated environments. Our experiments show that CANVAS outperforms the strong rule-based system ROS NavStack across all environments, demonstrating superior performance with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also closely aligns with human demonstrations and commonsense constraints, even in unseen environments. Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a total success rate of 69%, highlighting the potential of learning from human demonstrations in simulated environments for real-world applications.",
    "pdf_url": "https://arxiv.org/pdf/2410.01273v3",
    "github_url": null,
    "published": "2024-10-02T06:34:45+00:00",
    "updated": "2025-08-08T04:20:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.00649v1",
    "title": "LASMP: Language Aided Subset Sampling Based Motion Planner",
    "authors": [
      "Bhattacharjee",
      "Sinha",
      "Ekenna"
    ],
    "summary": "This paper presents the Language Aided Subset Sampling Based Motion Planner (LASMP), a system that helps mobile robots plan their movements by using natural language instructions. LASMP uses a modified version of the Rapidly Exploring Random Tree (RRT) method, which is guided by user-provided commands processed through a language model (RoBERTa). The system improves efficiency by focusing on specific areas of the robot's workspace based on these instructions, making it faster and less resource-intensive. Compared to traditional RRT methods, LASMP reduces the number of nodes needed by 55% and cuts random sample queries by 80%, while still generating safe, collision-free paths. Tested in both simulated and real-world environments, LASMP has shown better performance in handling complex indoor scenarios. The results highlight the potential of combining language processing with motion planning to make robot navigation more efficient.",
    "pdf_url": "https://arxiv.org/pdf/2410.00649v1",
    "github_url": null,
    "published": "2024-10-01T13:03:15+00:00",
    "updated": "2024-10-01T13:03:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.20445v1",
    "title": "Robot Navigation Using Physically Grounded Vision-Language Models in Outdoor Environments",
    "authors": [
      "Elnoor",
      "Weerakoon",
      "Seneviratne"
    ],
    "summary": "We present a novel autonomous robot navigation algorithm for outdoor environments that is capable of handling diverse terrain traversability conditions. Our approach, VLM-GroNav, uses vision-language models (VLMs) and integrates them with physical grounding that is used to assess intrinsic terrain properties such as deformability and slipperiness. We use proprioceptive-based sensing, which provides direct measurements of these physical properties, and enhances the overall semantic understanding of the terrains. Our formulation uses in-context learning to ground the VLM's semantic understanding with proprioceptive data to allow dynamic updates of traversability estimates based on the robot's real-time physical interactions with the environment. We use the updated traversability estimations to inform both the local and global planners for real-time trajectory replanning. We validate our method on a legged robot (Ghost Vision 60) and a wheeled robot (Clearpath Husky), in diverse real-world outdoor environments with different deformable and slippery terrains. In practice, we observe significant improvements over state-of-the-art methods by up to 50% increase in navigation success rate.",
    "pdf_url": "https://arxiv.org/pdf/2409.20445v1",
    "github_url": null,
    "published": "2024-09-30T16:03:44+00:00",
    "updated": "2024-09-30T16:03:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.19459v1",
    "title": "Language-guided Robust Navigation for Mobile Robots in Dynamically-changing Environments",
    "authors": [
      "Simons",
      "Liu",
      "Marcus"
    ],
    "summary": "In this paper, we develop an embodied AI system for human-in-the-loop navigation with a wheeled mobile robot. We propose a direct yet effective method of monitoring the robot's current plan to detect changes in the environment that impact the intended trajectory of the robot significantly and then query a human for feedback. We also develop a means to parse human feedback expressed in natural language into local navigation waypoints and integrate it into a global planning system, by leveraging a map of semantic features and an aligned obstacle map. Extensive testing in simulation and physical hardware experiments with a resource-constrained wheeled robot tasked to navigate in a real-world environment validate the efficacy and robustness of our method. This work can support applications like precision agriculture and construction, where persistent monitoring of the environment provides a human with information about the environment state.",
    "pdf_url": "https://arxiv.org/pdf/2409.19459v1",
    "github_url": null,
    "published": "2024-09-28T21:30:23+00:00",
    "updated": "2024-09-28T21:30:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.18800v1",
    "title": "MiniVLN: Efficient Vision-and-Language Navigation by Progressive Knowledge Distillation",
    "authors": [
      "Zhu",
      "Qiao",
      "Zhang"
    ],
    "summary": "In recent years, Embodied Artificial Intelligence (Embodied AI) has advanced rapidly, yet the increasing size of models conflicts with the limited computational capabilities of Embodied AI platforms. To address this challenge, we aim to achieve both high model performance and practical deployability. Specifically, we focus on Vision-and-Language Navigation (VLN), a core task in Embodied AI. This paper introduces a two-stage knowledge distillation framework, producing a student model, MiniVLN, and showcasing the significant potential of distillation techniques in developing lightweight models. The proposed method aims to capture fine-grained knowledge during the pretraining phase and navigation-specific knowledge during the fine-tuning phase. Our findings indicate that the two-stage distillation approach is more effective in narrowing the performance gap between the teacher model and the student model compared to single-stage distillation. On the public R2R and REVERIE benchmarks, MiniVLN achieves performance on par with the teacher model while having only about 12% of the teacher model's parameter count.",
    "pdf_url": "https://arxiv.org/pdf/2409.18800v1",
    "github_url": null,
    "published": "2024-09-27T14:54:54+00:00",
    "updated": "2024-09-27T14:54:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.18794v2",
    "title": "Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in Continuous Environment with Open-Source LLMs",
    "authors": [
      "Qiao",
      "Lyu",
      "Wang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) tasks require an agent to follow textual instructions to navigate through 3D environments. Traditional approaches use supervised learning methods, relying heavily on domain-specific datasets to train VLN models. Recent methods try to utilize closed-source large language models (LLMs) like GPT-4 to solve VLN tasks in zero-shot manners, but face challenges related to expensive token costs and potential data breaches in real-world applications. In this work, we introduce Open-Nav, a novel study that explores open-source LLMs for zero-shot VLN in the continuous environment. Open-Nav employs a spatial-temporal chain-of-thought (CoT) reasoning approach to break down tasks into instruction comprehension, progress estimation, and decision-making. It enhances scene perceptions with fine-grained object and spatial knowledge to improve LLM's reasoning in navigation. Our extensive experiments in both simulated and real-world environments demonstrate that Open-Nav achieves competitive performance compared to using closed-source LLMs.",
    "pdf_url": "https://arxiv.org/pdf/2409.18794v2",
    "github_url": null,
    "published": "2024-09-27T14:47:18+00:00",
    "updated": "2025-02-11T00:55:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.18743v1",
    "title": "OpenObject-NAV: Open-Vocabulary Object-Oriented Navigation Based on Dynamic Carrier-Relationship Scene Graph",
    "authors": [
      "Tang",
      "Wang",
      "Deng"
    ],
    "summary": "In everyday life, frequently used objects like cups often have unfixed positions and multiple instances within the same category, and their carriers frequently change as well. As a result, it becomes challenging for a robot to efficiently navigate to a specific instance. To tackle this challenge, the robot must capture and update scene changes and plans continuously. However, current object navigation approaches primarily focus on semantic-level and lack the ability to dynamically update scene representation. This paper captures the relationships between frequently used objects and their static carriers. It constructs an open-vocabulary Carrier-Relationship Scene Graph (CRSG) and updates the carrying status during robot navigation to reflect the dynamic changes of the scene. Based on the CRSG, we further propose an instance navigation strategy that models the navigation process as a Markov Decision Process. At each step, decisions are informed by Large Language Model's commonsense knowledge and visual-language feature similarity. We designed a series of long-sequence navigation tasks for frequently used everyday items in the Habitat simulator. The results demonstrate that by updating the CRSG, the robot can efficiently navigate to moved targets. Additionally, we deployed our algorithm on a real robot and validated its practical effectiveness.",
    "pdf_url": "https://arxiv.org/pdf/2409.18743v1",
    "github_url": null,
    "published": "2024-09-27T13:33:52+00:00",
    "updated": "2024-09-27T13:33:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.09054v1",
    "title": "Circuits and Systems for Embodied AI: Exploring uJ Multi-Modal Perception for Nano-UAVs on the Kraken Shield",
    "authors": [
      "Potocnik",
      "Mauro",
      "Lamberti"
    ],
    "summary": "Embodied artificial intelligence (AI) requires pushing complex multi-modal models to the extreme edge for time-constrained tasks such as autonomous navigation of robots and vehicles. On small form-factor devices, e.g., nano-sized unmanned aerial vehicles (UAVs), such challenges are exacerbated by stringent constraints on energy efficiency and weight. In this paper, we explore embodied multi-modal AI-based perception for Nano-UAVs with the Kraken shield, a 7g multi-sensor (frame-based and event-based imagers) board based on Kraken, a 22 nm SoC featuring multiple acceleration engines for multi-modal event and frame-based inference based on spiking (SNN) and ternary (TNN) neural networks, respectively. Kraken can execute SNN real-time inference for depth estimation at 1.02k inf/s, 18 J/inf, TNN real-time inference for object classification at 10k inf/s, 6 J/inf, and real-time inference for obstacle avoidance at 221 frame/s, 750 J/inf.",
    "pdf_url": "https://arxiv.org/pdf/2410.09054v1",
    "github_url": null,
    "published": "2024-09-26T12:59:03+00:00",
    "updated": "2024-09-26T12:59:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.17313v1",
    "title": "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation",
    "authors": [
      "Wang",
      "Wu",
      "Cao"
    ],
    "summary": "This study presents a novel evaluation framework for the Vision-Language Navigation (VLN) task. It aims to diagnose current models for various instruction categories at a finer-grained level. The framework is structured around the context-free grammar (CFG) of the task. The CFG serves as the basis for the problem decomposition and the core premise of the instruction categories design. We propose a semi-automatic method for CFG construction with the help of Large-Language Models (LLMs). Then, we induct and generate data spanning five principal instruction categories (i.e. direction change, landmark recognition, region recognition, vertical movement, and numerical comprehension). Our analysis of different models reveals notable performance discrepancies and recurrent issues. The stagnation of numerical comprehension, heavy selective biases over directional concepts, and other interesting findings contribute to the development of future language-guided navigation systems.",
    "pdf_url": "https://arxiv.org/pdf/2409.17313v1",
    "github_url": null,
    "published": "2024-09-25T19:49:39+00:00",
    "updated": "2024-09-25T19:49:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.16484v2",
    "title": "BehAV: Behavioral Rule Guided Autonomy Using VLMs for Robot Navigation in Outdoor Scenes",
    "authors": [
      "Weerakoon",
      "Elnoor",
      "Seneviratne"
    ],
    "summary": "We present BehAV, a novel approach for autonomous robot navigation in outdoor scenes guided by human instructions and leveraging Vision Language Models (VLMs). Our method interprets human commands using a Large Language Model (LLM) and categorizes the instructions into navigation and behavioral guidelines. Navigation guidelines consist of directional commands (e.g., \"move forward until\") and associated landmarks (e.g., \"the building with blue windows\"), while behavioral guidelines encompass regulatory actions (e.g., \"stay on\") and their corresponding objects (e.g., \"pavements\"). We use VLMs for their zero-shot scene understanding capabilities to estimate landmark locations from RGB images for robot navigation. Further, we introduce a novel scene representation that utilizes VLMs to ground behavioral rules into a behavioral cost map. This cost map encodes the presence of behavioral objects within the scene and assigns costs based on their regulatory actions. The behavioral cost map is integrated with a LiDAR-based occupancy map for navigation. To navigate outdoor scenes while adhering to the instructed behaviors, we present an unconstrained Model Predictive Control (MPC)-based planner that prioritizes both reaching landmarks and following behavioral guidelines. We evaluate the performance of BehAV on a quadruped robot across diverse real-world scenarios, demonstrating a 22.49% improvement in alignment with human-teleoperated actions, as measured by Frechet distance, and achieving a 40% higher navigation success rate compared to state-of-the-art methods.",
    "pdf_url": "https://arxiv.org/pdf/2409.16484v2",
    "github_url": null,
    "published": "2024-09-24T22:15:24+00:00",
    "updated": "2024-10-02T19:50:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.15922v5",
    "title": "The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards",
    "authors": [
      "Huang",
      "Liu",
      "Lipovetzky"
    ],
    "summary": "While Vision-Language Models (VLMs) are increasingly used to generate reward signals for training embodied agents to follow instructions, our research reveals that agents guided by VLM rewards often underperform compared to those employing only intrinsic (exploration-driven) rewards, contradicting expectations set by recent work. We hypothesize that false positive rewards -- instances where unintended trajectories are incorrectly rewarded -- are more detrimental than false negatives. Our analysis confirms this hypothesis, revealing that the widely used cosine similarity metric is prone to false positive reward estimates. To address this, we introduce BiMI ({Bi}nary {M}utual {I}nformation), a novel reward function designed to mitigate noise. BiMI significantly enhances learning efficiency across diverse and challenging embodied navigation environments. Our findings offer a nuanced understanding of how different types of reward noise impact agent learning and highlight the importance of addressing multimodal reward signal noise when training embodied agents",
    "pdf_url": "https://arxiv.org/pdf/2409.15922v5",
    "github_url": null,
    "published": "2024-09-24T09:45:20+00:00",
    "updated": "2025-11-08T04:37:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.14899v1",
    "title": "CON: Continual Object Navigation via Data-Free Inter-Agent Knowledge Transfer in Unseen and Unfamiliar Places",
    "authors": [
      "Terashima",
      "Iwata",
      "Tanaka"
    ],
    "summary": "This work explores the potential of brief inter-agent knowledge transfer (KT) to enhance the robotic object goal navigation (ON) in unseen and unfamiliar environments. Drawing on the analogy of human travelers acquiring local knowledge, we propose a framework in which a traveler robot (student) communicates with local robots (teachers) to obtain ON knowledge through minimal interactions. We frame this process as a data-free continual learning (CL) challenge, aiming to transfer knowledge from a black-box model (teacher) to a new model (student). In contrast to approaches like zero-shot ON using large language models (LLMs), which utilize inherently communication-friendly natural language for knowledge representation, the other two major ON approaches -- frontier-driven methods using object feature maps and learning-based ON using neural state-action maps -- present complex challenges where data-free KT remains largely uncharted. To address this gap, we propose a lightweight, plug-and-play KT module targeting non-cooperative black-box teachers in open-world settings. Using the universal assumption that every teacher robot has vision and mobility capabilities, we define state-action history as the primary knowledge base. Our formulation leads to the development of a query-based occupancy map that dynamically represents target object locations, serving as an effective and communication-friendly knowledge representation. We validate the effectiveness of our method through experiments conducted in the Habitat environment.",
    "pdf_url": "https://arxiv.org/pdf/2409.14899v1",
    "github_url": null,
    "published": "2024-09-23T10:50:11+00:00",
    "updated": "2024-09-23T10:50:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.14296v1",
    "title": "HM3D-OVON: A Dataset and Benchmark for Open-Vocabulary Object Goal Navigation",
    "authors": [
      "Yokoyama",
      "Ramrakhya",
      "Das"
    ],
    "summary": "We present the Habitat-Matterport 3D Open Vocabulary Object Goal Navigation dataset (HM3D-OVON), a large-scale benchmark that broadens the scope and semantic range of prior Object Goal Navigation (ObjectNav) benchmarks. Leveraging the HM3DSem dataset, HM3D-OVON incorporates over 15k annotated instances of household objects across 379 distinct categories, derived from photo-realistic 3D scans of real-world environments. In contrast to earlier ObjectNav datasets, which limit goal objects to a predefined set of 6-20 categories, HM3D-OVON facilitates the training and evaluation of models with an open-set of goals defined through free-form language at test-time. Through this open-vocabulary formulation, HM3D-OVON encourages progress towards learning visuo-semantic navigation behaviors that are capable of searching for any object specified by text in an open-vocabulary manner. Additionally, we systematically evaluate and compare several different types of approaches on HM3D-OVON. We find that HM3D-OVON can be used to train an open-vocabulary ObjectNav agent that achieves both higher performance and is more robust to localization and actuation noise than the state-of-the-art ObjectNav approach. We hope that our benchmark and baseline results will drive interest in developing embodied agents that can navigate real-world spaces to find household objects specified through free-form language, taking a step towards more flexible and human-like semantic visual navigation. Code and videos available at: naoki.io/ovon.",
    "pdf_url": "https://arxiv.org/pdf/2409.14296v1",
    "github_url": null,
    "published": "2024-09-22T02:12:29+00:00",
    "updated": "2024-09-22T02:12:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.13838v1",
    "title": "Key-Scan-Based Mobile Robot Navigation: Integrated Mapping, Planning, and Control using Graphs of Scan Regions",
    "authors": [
      "Latha",
      "Arslan"
    ],
    "summary": "Safe autonomous navigation in a priori unknown environments is an essential skill for mobile robots to reliably and adaptively perform diverse tasks (e.g., delivery, inspection, and interaction) in unstructured cluttered environments. Hybrid metric-topological maps, constructed as a pose graph of local submaps, offer a computationally efficient world representation for adaptive mapping, planning, and control at the regional level. In this paper, we consider a pose graph of locally sensed star-convex scan regions as a metric-topological map, with star convexity enabling simple yet effective local navigation strategies. We design a new family of safe local scan navigation policies and present a perception-driven feedback motion planning method through the sequential composition of local scan navigation policies, enabling provably correct and safe robot navigation over the union of local scan regions. We introduce a new concept of bridging and frontier scans for automated key scan selection and exploration for integrated mapping and navigation in unknown environments. We demonstrate the effectiveness of our key-scan-based navigation and mapping framework using a mobile robot equipped with a 360$^{\\circ}$ laser range scanner in 2D cluttered environments through numerical ROS-Gazebo simulations and real hardware~experiments.",
    "pdf_url": "https://arxiv.org/pdf/2409.13838v1",
    "github_url": null,
    "published": "2024-09-20T18:25:09+00:00",
    "updated": "2024-09-20T18:25:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.13682v1",
    "title": "ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation",
    "authors": [
      "Anwar",
      "Welsh",
      "Biswas"
    ],
    "summary": "Navigating and understanding complex environments over extended periods of time is a significant challenge for robots. People interacting with the robot may want to ask questions like where something happened, when it occurred, or how long ago it took place, which would require the robot to reason over a long history of their deployment. To address this problem, we introduce a Retrieval-augmented Memory for Embodied Robots, or ReMEmbR, a system designed for long-horizon video question answering for robot navigation. To evaluate ReMEmbR, we introduce the NaVQA dataset where we annotate spatial, temporal, and descriptive questions to long-horizon robot navigation videos. ReMEmbR employs a structured approach involving a memory building and a querying phase, leveraging temporal information, spatial information, and images to efficiently handle continuously growing robot histories. Our experiments demonstrate that ReMEmbR outperforms LLM and VLM baselines, allowing ReMEmbR to achieve effective long-horizon reasoning with low latency. Additionally, we deploy ReMEmbR on a robot and show that our approach can handle diverse queries. The dataset, code, videos, and other material can be found at the following link: https://nvidia-ai-iot.github.io/remembr",
    "pdf_url": "https://arxiv.org/pdf/2409.13682v1",
    "github_url": null,
    "published": "2024-09-20T17:50:07+00:00",
    "updated": "2024-09-20T17:50:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.13393v1",
    "title": "Hey Robot! Personalizing Robot Navigation through Model Predictive Control with a Large Language Model",
    "authors": [
      "Martinez-Baselga",
      "Groot",
      "Knoedler"
    ],
    "summary": "Robot navigation methods allow mobile robots to operate in applications such as warehouses or hospitals. While the environment in which the robot operates imposes requirements on its navigation behavior, most existing methods do not allow the end-user to configure the robot's behavior and priorities, possibly leading to undesirable behavior (e.g., fast driving in a hospital). We propose a novel approach to adapt robot motion behavior based on natural language instructions provided by the end-user. Our zero-shot method uses an existing Visual Language Model to interpret a user text query or an image of the environment. This information is used to generate the cost function and reconfigure the parameters of a Model Predictive Controller, translating the user's instruction to the robot's motion behavior. This allows our method to safely and effectively navigate in dynamic and challenging environments. We extensively evaluate our method's individual components and demonstrate the effectiveness of our method on a ground robot in simulation and real-world experiments, and across a variety of environments and user specifications.",
    "pdf_url": "https://arxiv.org/pdf/2409.13393v1",
    "github_url": null,
    "published": "2024-09-20T10:48:53+00:00",
    "updated": "2024-09-20T10:48:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.12420v2",
    "title": "Spatially-invariant opinion dynamics on the circle",
    "authors": [
      "Amorim",
      "Bizyaeva",
      "Franci"
    ],
    "summary": "We propose and analyze a nonlinear opinion dynamics model for an agent making decisions about a continuous distribution of options in the presence of input. Inspired by perceptual decision-making, we develop new theory for opinion formation in response to inputs about options distributed on the circle. Options on the circle can represent, e.g., the possible directions of perceived objects and resulting heading directions in planar robotic navigation problems. Interactions among options are encoded through a spatially invariant kernel, which we design to ensure that only a small (finite) subset of options can be favored over the continuum. We leverage the spatial invariance of the model linearization to design flexible, distributed opinion-forming behaviors using spatiotemporal frequency domain and bifurcation analysis. We illustrate our model's versatility with an application to robotic navigation in crowded spaces.",
    "pdf_url": "https://arxiv.org/pdf/2409.12420v2",
    "github_url": null,
    "published": "2024-09-19T02:40:49+00:00",
    "updated": "2024-11-27T17:19:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.11764v2",
    "title": "One Map to Find Them All: Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation",
    "authors": [
      "Busch",
      "Homberger",
      "Ortega-Peimbert"
    ],
    "summary": "The capability to efficiently search for objects in complex environments is fundamental for many real-world robot applications. Recent advances in open-vocabulary vision models have resulted in semantically-informed object navigation methods that allow a robot to search for an arbitrary object without prior training. However, these zero-shot methods have so far treated the environment as unknown for each consecutive query. In this paper we introduce a new benchmark for zero-shot multi-object navigation, allowing the robot to leverage information gathered from previous searches to more efficiently find new objects. To address this problem we build a reusable open-vocabulary feature map tailored for real-time object search. We further propose a probabilistic-semantic map update that mitigates common sources of errors in semantic feature extraction and leverage this semantic uncertainty for informed multi-object exploration. We evaluate our method on a set of object navigation tasks in both simulation as well as with a real robot, running in real-time on a Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art approaches both on single and multi-object navigation tasks. Additional videos, code and the multi-object navigation benchmark will be available on https://finnbsch.github.io/OneMap.",
    "pdf_url": "https://arxiv.org/pdf/2409.11764v2",
    "github_url": null,
    "published": "2024-09-18T07:44:08+00:00",
    "updated": "2025-03-03T18:50:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.10906v1",
    "title": "Multi-Floor Zero-Shot Object Navigation Policy",
    "authors": [
      "Zhang",
      "Wang",
      "Xiao"
    ],
    "summary": "Object navigation in multi-floor environments presents a formidable challenge in robotics, requiring sophisticated spatial reasoning and adaptive exploration strategies. Traditional approaches have primarily focused on single-floor scenarios, overlooking the complexities introduced by multi-floor structures. To address these challenges, we first propose a Multi-floor Navigation Policy (MFNP) and implement it in Zero-Shot object navigation tasks. Our framework comprises three key components: (i) Multi-floor Navigation Policy, which enables an agent to explore across multiple floors; (ii) Multi-modal Large Language Models (MLLMs) for reasoning in the navigation process; and (iii) Inter-Floor Navigation, ensuring efficient floor transitions. We evaluate MFNP on the Habitat-Matterport 3D (HM3D) and Matterport 3D (MP3D) datasets, both include multi-floor scenes. Our experiment results demonstrate that MFNP significantly outperforms all the existing methods in Zero-Shot object navigation, achieving higher success rates and improved exploration efficiency. Ablation studies further highlight the effectiveness of each component in addressing the unique challenges of multi-floor navigation. Meanwhile, we conducted real-world experiments to evaluate the feasibility of our policy. Upon deployment of MFNP, the Unitree quadruped robot demonstrated successful multi-floor navigation and found the target object in a completely unseen environment. By introducing MFNP, we offer a new paradigm for tackling complex, multi-floor environments in object navigation tasks, opening avenues for future research in visual-based navigation in realistic, multi-floor settings.",
    "pdf_url": "https://arxiv.org/pdf/2409.10906v1",
    "github_url": null,
    "published": "2024-09-17T05:53:04+00:00",
    "updated": "2024-09-17T05:53:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.10283v4",
    "title": "ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone Navigation via Scene-Aware Control Barrier Functions",
    "authors": [
      "Sanyal",
      "Roy"
    ],
    "summary": "In the rapidly evolving field of vision-language navigation (VLN), ensuring safety for physical agents remains an open challenge. For a human-in-the-loop language-operated drone to navigate safely, it must understand natural language commands, perceive the environment, and simultaneously avoid hazards in real time. Control Barrier Functions (CBFs) are formal methods that enforce safe operating conditions. Model Predictive Control (MPC) is an optimization framework that plans a sequence of future actions over a prediction horizon, ensuring smooth trajectory tracking while obeying constraints. In this work, we consider a VLN-operated drone platform and enhance its safety by formulating a novel scene-aware CBF that leverages ego-centric observations from a camera which has both Red-Green-Blue as well as Depth (RGB-D) channels. A CBF-less baseline system uses a Vision-Language Encoder with cross-modal attention to convert commands into an ordered sequence of landmarks. An object detection model identifies and verifies these landmarks in the captured images to generate a planned path. To further enhance safety, an Adaptive Safety Margin Algorithm (ASMA) is proposed. ASMA tracks moving objects and performs scene-aware CBF evaluation on-the-fly, which serves as an additional constraint within the MPC framework. By continuously identifying potentially risky observations, the system performs prediction in real time about unsafe conditions and proactively adjusts its control actions to maintain safe navigation throughout the trajectory. Deployed on a Parrot Bebop2 quadrotor in the Gazebo environment using the Robot Operating System (ROS), ASMA achieves 64%-67% increase in success rates with only a slight increase (1.4%-5.8%) in trajectory lengths compared to the baseline CBF-less VLN.",
    "pdf_url": "https://arxiv.org/pdf/2409.10283v4",
    "github_url": null,
    "published": "2024-09-16T13:44:50+00:00",
    "updated": "2025-07-19T18:48:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.10071v5",
    "title": "Towards Physically Realizable Adversarial Attacks in Embodied Vision Navigation",
    "authors": [
      "Chen",
      "Tu",
      "Qi"
    ],
    "summary": "The significant advancements in embodied vision navigation have raised concerns about its susceptibility to adversarial attacks exploiting deep neural networks. Investigating the adversarial robustness of embodied vision navigation is crucial, especially given the threat of 3D physical attacks that could pose risks to human safety. However, existing attack methods for embodied vision navigation often lack physical feasibility due to challenges in transferring digital perturbations into the physical world. Moreover, current physical attacks for object detection struggle to achieve both multi-view effectiveness and visual naturalness in navigation scenarios. To address this, we propose a practical attack method for embodied navigation by attaching adversarial patches to objects, where both opacity and textures are learnable. Specifically, to ensure effectiveness across varying viewpoints, we employ a multi-view optimization strategy based on object-aware sampling, which optimizes the patch's texture based on feedback from the vision-based perception model used in navigation. To make the patch inconspicuous to human observers, we introduce a two-stage opacity optimization mechanism, in which opacity is fine-tuned after texture optimization. Experimental results demonstrate that our adversarial patches decrease the navigation success rate by an average of 22.39%, outperforming previous methods in practicality, effectiveness, and naturalness. Code is available at: https://github.com/chen37058/Physical-Attacks-in-Embodied-Nav",
    "pdf_url": "https://arxiv.org/pdf/2409.10071v5",
    "github_url": "https://github.com/chen37058/Physical-Attacks-in-Embodied-Nav",
    "published": "2024-09-16T08:21:22+00:00",
    "updated": "2025-08-15T02:26:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.10027v4",
    "title": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation with Language Models",
    "authors": [
      "Kim",
      "Kim",
      "Oh"
    ],
    "summary": "Large language models (LLMs) have shown significant potential in guiding embodied agents to execute language instructions across a range of tasks, including robotic manipulation and navigation. However, existing methods are primarily designed for static environments and do not leverage the agent's own experiences to refine its initial plans. Given that real-world environments are inherently stochastic, initial plans based solely on LLMs' general knowledge may fail to achieve their objectives, unlike in static scenarios. To address this limitation, this study introduces the Experience-and-Emotion Map (E2Map), which integrates not only LLM knowledge but also the agent's real-world experiences, drawing inspiration from human emotional responses. The proposed methodology enables one-shot behavior adjustments by updating the E2Map based on the agent's experiences. Our evaluation in stochastic navigation environments, including both simulations and real-world scenarios, demonstrates that the proposed method significantly enhances performance in stochastic environments compared to existing LLM-based approaches. Code and supplementary materials are available at https://e2map.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2409.10027v4",
    "github_url": null,
    "published": "2024-09-16T06:35:18+00:00",
    "updated": "2025-02-03T01:26:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.05593v1",
    "title": "StratXplore: Strategic Novelty-seeking and Instruction-aligned Exploration for Vision and Language Navigation",
    "authors": [
      "Gopinathan",
      "Abu-Khalaf",
      "Suter"
    ],
    "summary": "Embodied navigation requires robots to understand and interact with the environment based on given tasks. Vision-Language Navigation (VLN) is an embodied navigation task, where a robot navigates within a previously seen and unseen environment, based on linguistic instruction and visual inputs. VLN agents need access to both local and global action spaces; former for immediate decision making and the latter for recovering from navigational mistakes. Prior VLN agents rely only on instruction-viewpoint alignment for local and global decision making and back-track to a previously visited viewpoint, if the instruction and its current viewpoint mismatches. These methods are prone to mistakes, due to the complexity of the instruction and partial observability of the environment. We posit that, back-tracking is sub-optimal and agent that is aware of its mistakes can recover efficiently. For optimal recovery, exploration should be extended to unexplored viewpoints (or frontiers). The optimal frontier is a recently observed but unexplored viewpoint that aligns with the instruction and is novel. We introduce a memory-based and mistake-aware path planning strategy for VLN agents, called \\textit{StratXplore}, that presents global and local action planning to select the optimal frontier for path correction. The proposed method collects all past actions and viewpoint features during navigation and then selects the optimal frontier suitable for recovery. Experimental results show this simple yet effective strategy improves the success rate on two VLN datasets with different task complexities.",
    "pdf_url": "https://arxiv.org/pdf/2409.05593v1",
    "github_url": null,
    "published": "2024-09-09T13:23:24+00:00",
    "updated": "2024-09-09T13:23:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.05583v1",
    "title": "Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation",
    "authors": [
      "Gopinathan",
      "Masek",
      "Abu-Khalaf"
    ],
    "summary": "Embodied AI aims to develop robots that can \\textit{understand} and execute human language instructions, as well as communicate in natural languages. On this front, we study the task of generating highly detailed navigational instructions for the embodied robots to follow. Although recent studies have demonstrated significant leaps in the generation of step-by-step instructions from sequences of images, the generated instructions lack variety in terms of their referral to objects and landmarks. Existing speaker models learn strategies to evade the evaluation metrics and obtain higher scores even for low-quality sentences. In this work, we propose SAS (Spatially-Aware Speaker), an instruction generator or \\textit{Speaker} model that utilises both structural and semantic knowledge of the environment to produce richer instructions. For training, we employ a reward learning method in an adversarial setting to avoid systematic bias introduced by language evaluation metrics. Empirically, our method outperforms existing instruction generation models, evaluated using standard metrics. Our code is available at \\url{https://github.com/gmuraleekrishna/SAS}.",
    "pdf_url": "https://arxiv.org/pdf/2409.05583v1",
    "github_url": "https://github.com/gmuraleekrishna/SAS",
    "published": "2024-09-09T13:12:11+00:00",
    "updated": "2024-09-09T13:12:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.05552v2",
    "title": "Seeing is Believing? Enhancing Vision-Language Navigation using Visual Perturbations",
    "authors": [
      "Zhang",
      "Li",
      "Xu"
    ],
    "summary": "Autonomous navigation guided by natural language instructions in embodied environments remains a challenge for vision-language navigation (VLN) agents. Although recent advancements in learning diverse and fine-grained visual environmental representations have shown promise, the fragile performance improvements may not conclusively attribute to enhanced visual grounding,a limitation also observed in related vision-language tasks. In this work, we preliminarily investigate whether advanced VLN models genuinely comprehend the visual content of their environments by introducing varying levels of visual perturbations. These perturbations include ground-truth depth images, perturbed views and random noise. Surprisingly, we experimentally find that simple branch expansion, even with noisy visual inputs, paradoxically improves the navigational efficacy. Inspired by these insights, we further present a versatile Multi-Branch Architecture (MBA) designed to delve into the impact of both the branch quantity and visual quality. The proposed MBA extends a base agent into a multi-branch variant, where each branch processes a different visual input. This approach is embarrassingly simple yet agnostic to topology-based VLN agents. Extensive experiments on three VLN benchmarks (R2R, REVERIE, SOON) demonstrate that our method with optimal visual permutations matches or even surpasses state-of-the-art results. The source code is available at here.",
    "pdf_url": "https://arxiv.org/pdf/2409.05552v2",
    "github_url": null,
    "published": "2024-09-09T12:17:38+00:00",
    "updated": "2025-04-07T12:15:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.04965v2",
    "title": "Socially-Aware Robot Navigation Enhanced by Bidirectional Natural Language Conversations Using Large Language Models",
    "authors": [
      "Wen",
      "Liu",
      "Bethala"
    ],
    "summary": "Robot navigation is crucial across various domains, yet traditional methods focus on efficiency and obstacle avoidance, often overlooking human behavior in shared spaces. With the rise of service robots, socially aware navigation has gained prominence. However, existing approaches primarily predict pedestrian movements or issue alerts, lacking true human-robot interaction. We introduce Hybrid Soft Actor-Critic with Large Language Model (HSAC-LLM), a novel framework for socially aware navigation. By integrating deep reinforcement learning with large language models, HSAC-LLM enables bidirectional natural language interactions, predicting both continuous and discrete navigation actions. When potential collisions arise, the robot proactively communicates with pedestrians to determine avoidance strategies. Experiments in 2D simulation, Gazebo, and real-world environments demonstrate that HSAC-LLM outperforms state-of-the-art DRL methods in interaction, navigation, and obstacle avoidance. This paradigm advances effective human-robot interactions in dynamic settings. Videos are available at https://hsacllm.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2409.04965v2",
    "github_url": null,
    "published": "2024-09-08T04:04:21+00:00",
    "updated": "2025-03-23T19:45:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.04837v2",
    "title": "Context-Aware Replanning with Pre-explored Semantic Map for Object Navigation",
    "authors": [
      "Ko",
      "Su",
      "Chen"
    ],
    "summary": "Pre-explored Semantic Maps, constructed through prior exploration using visual language models (VLMs), have proven effective as foundational elements for training-free robotic applications. However, existing approaches assume the map's accuracy and do not provide effective mechanisms for revising decisions based on incorrect maps. To address this, we introduce Context-Aware Replanning (CARe), which estimates map uncertainty through confidence scores and multi-view consistency, enabling the agent to revise erroneous decisions stemming from inaccurate maps without requiring additional labels. We demonstrate the effectiveness of our proposed method by integrating it with two modern mapping backbones, VLMaps and OpenMask3D, and observe significant performance improvements in object navigation tasks. More details can be found on the project page: https://care-maps.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2409.04837v2",
    "github_url": null,
    "published": "2024-09-07T14:25:08+00:00",
    "updated": "2024-11-02T15:03:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.02669v2",
    "title": "Causality-Aware Transformer Networks for Robotic Navigation",
    "authors": [
      "Wang",
      "Liu",
      "Cao"
    ],
    "summary": "Current research in Visual Navigation reveals opportunities for improvement. First, the direct adoption of RNNs and Transformers often overlooks the specific differences between Embodied AI and traditional sequential data modelling, potentially limiting its performance in Embodied AI tasks. Second, the reliance on task-specific configurations, such as pre-trained modules and dataset-specific logic, compromises the generalizability of these methods. We address these constraints by initially exploring the unique differences between Navigation tasks and other sequential data tasks through the lens of Causality, presenting a causal framework to elucidate the inadequacies of conventional sequential methods for Navigation. By leveraging this causal perspective, we propose Causality-Aware Transformer (CAT) Networks for Navigation, featuring a Causal Understanding Module to enhance the models's Environmental Understanding capability. Meanwhile, our method is devoid of task-specific inductive biases and can be trained in an End-to-End manner, which enhances the method's generalizability across various contexts. Empirical evaluations demonstrate that our methodology consistently surpasses benchmark performances across a spectrum of settings, tasks and simulation environments. Extensive ablation studies reveal that the performance gains can be attributed to the Causal Understanding Module, which demonstrates effectiveness and efficiency in both Reinforcement Learning and Supervised Learning settings.",
    "pdf_url": "https://arxiv.org/pdf/2409.02669v2",
    "github_url": null,
    "published": "2024-09-04T12:53:26+00:00",
    "updated": "2024-10-05T11:34:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.02561v2",
    "title": "Vision-Language Navigation with Continual Learning",
    "authors": [
      "Li",
      "Lv",
      "Tu"
    ],
    "summary": "Vision-language navigation (VLN) is a critical domain within embedded intelligence, requiring agents to navigate 3D environments based on natural language instructions. Traditional VLN research has focused on improving environmental understanding and decision accuracy. However, these approaches often exhibit a significant performance gap when agents are deployed in novel environments, mainly due to the limited diversity of training data. Expanding datasets to cover a broader range of environments is impractical and costly. We propose the Vision-Language Navigation with Continual Learning (VLNCL) paradigm to address this challenge. In this paradigm, agents incrementally learn new environments while retaining previously acquired knowledge. VLNCL enables agents to maintain an environmental memory and extract relevant knowledge, allowing rapid adaptation to new environments while preserving existing information. We introduce a novel dual-loop scenario replay method (Dual-SR) inspired by brain memory replay mechanisms integrated with VLN agents. This method facilitates consolidating past experiences and enhances generalization across new tasks. By utilizing a multi-scenario memory buffer, the agent efficiently organizes and replays task memories, thereby bolstering its ability to adapt quickly to new environments and mitigating catastrophic forgetting. Our work pioneers continual learning in VLN agents, introducing a novel experimental setup and evaluation metrics. We demonstrate the effectiveness of our approach through extensive evaluations and establish a benchmark for the VLNCL paradigm. Comparative experiments with existing continual learning and VLN methods show significant improvements, achieving state-of-the-art performance in continual learning ability and highlighting the potential of our approach in enabling rapid adaptation while preserving prior knowledge.",
    "pdf_url": "https://arxiv.org/pdf/2409.02561v2",
    "github_url": null,
    "published": "2024-09-04T09:28:48+00:00",
    "updated": "2024-09-23T03:17:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.02522v2",
    "title": "Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments",
    "authors": [
      "Li",
      "Lu",
      "Mu"
    ],
    "summary": "Vision Language Navigation in Continuous Environments (VLN-CE) represents a frontier in embodied AI, demanding agents to navigate freely in unbounded 3D spaces solely guided by natural language instructions. This task introduces distinct challenges in multimodal comprehension, spatial reasoning, and decision-making. To address these challenges, we introduce Cog-GA, a generative agent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA employs a dual-pronged strategy to emulate human-like cognitive processes. Firstly, it constructs a cognitive map, integrating temporal, spatial, and semantic elements, thereby facilitating the development of spatial memory within LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints, strategically optimizing the exploration trajectory to maximize navigational efficiency. Each waypoint is accompanied by a dual-channel scene description, categorizing environmental cues into 'what' and 'where' streams as the brain. This segregation enhances the agent's attentional focus, enabling it to discern pertinent spatial information for navigation. A reflective mechanism complements these strategies by capturing feedback from prior navigation experiences, facilitating continual learning and adaptive replanning. Extensive evaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art performance and ability to simulate human-like navigation behaviors. This research significantly contributes to the development of strategic and interpretable VLN-CE agents.",
    "pdf_url": "https://arxiv.org/pdf/2409.02522v2",
    "github_url": null,
    "published": "2024-09-04T08:30:03+00:00",
    "updated": "2024-09-23T03:18:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.02389v2",
    "title": "Multi-modal Situated Reasoning in 3D Scenes",
    "authors": [
      "Linghu",
      "Huang",
      "Niu"
    ],
    "summary": "Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (e.g., text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models' situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.",
    "pdf_url": "https://arxiv.org/pdf/2409.02389v2",
    "github_url": null,
    "published": "2024-09-04T02:37:38+00:00",
    "updated": "2024-11-18T02:32:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.01630v1",
    "title": "SafeEmbodAI: a Safety Framework for Mobile Robots in Embodied AI Systems",
    "authors": [
      "Zhang",
      "Kong",
      "Braunl"
    ],
    "summary": "Embodied AI systems, including AI-powered robots that autonomously interact with the physical world, stand to be significantly advanced by Large Language Models (LLMs), which enable robots to better understand complex language commands and perform advanced tasks with enhanced comprehension and adaptability, highlighting their potential to improve embodied AI capabilities. However, this advancement also introduces safety challenges, particularly in robotic navigation tasks. Improper safety management can lead to failures in complex environments and make the system vulnerable to malicious command injections, resulting in unsafe behaviours such as detours or collisions. To address these issues, we propose \\textit{SafeEmbodAI}, a safety framework for integrating mobile robots into embodied AI systems. \\textit{SafeEmbodAI} incorporates secure prompting, state management, and safety validation mechanisms to secure and assist LLMs in reasoning through multi-modal data and validating responses. We designed a metric to evaluate mission-oriented exploration, and evaluations in simulated environments demonstrate that our framework effectively mitigates threats from malicious commands and improves performance in various environment settings, ensuring the safety of embodied AI systems. Notably, In complex environments with mixed obstacles, our method demonstrates a significant performance increase of 267\\% compared to the baseline in attack scenarios, highlighting its robustness in challenging conditions.",
    "pdf_url": "https://arxiv.org/pdf/2409.01630v1",
    "github_url": null,
    "published": "2024-09-03T05:56:50+00:00",
    "updated": "2024-09-03T05:56:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.01581v1",
    "title": "GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color Point Clouds via 3D Gaussian Splatting",
    "authors": [
      "Guo",
      "Xie",
      "Xie"
    ],
    "summary": "Dense colored point clouds enhance visual perception and are of significant value in various robotic applications. However, existing learning-based point cloud upsampling methods are constrained by computational resources and batch processing strategies, which often require subdividing point clouds into smaller patches, leading to distortions that degrade perceptual quality. To address this challenge, we propose a novel 2D-3D hybrid colored point cloud upsampling framework (GaussianPU) based on 3D Gaussian Splatting (3DGS) for robotic perception. This approach leverages 3DGS to bridge 3D point clouds with their 2D rendered images in robot vision systems. A dual scale rendered image restoration network transforms sparse point cloud renderings into dense representations, which are then input into 3DGS along with precise robot camera poses and interpolated sparse point clouds to reconstruct dense 3D point clouds. We have made a series of enhancements to the vanilla 3DGS, enabling precise control over the number of points and significantly boosting the quality of the upsampled point cloud for robotic scene understanding. Our framework supports processing entire point clouds on a single consumer-grade GPU, such as the NVIDIA GeForce RTX 3090, eliminating the need for segmentation and thus producing high-quality, dense colored point clouds with millions of points for robot navigation and manipulation tasks. Extensive experimental results on generating million-level point cloud data validate the effectiveness of our method, substantially improving the quality of colored point clouds and demonstrating significant potential for applications involving large-scale point clouds in autonomous robotics and human-robot interaction scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2409.01581v1",
    "github_url": null,
    "published": "2024-09-03T03:35:04+00:00",
    "updated": "2024-09-03T03:35:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.14183v2",
    "title": "Robot Navigation with Entity-Based Collision Avoidance using Deep Reinforcement Learning",
    "authors": [
      "Kolomeytsev",
      "Golembiovsky"
    ],
    "summary": "Efficient navigation in dynamic environments is crucial for autonomous robots interacting with moving agents and static obstacles. We present a novel deep reinforcement learning approach that improves robot navigation and interaction with different types of agents and obstacles based on specific safety requirements. Our approach uses information about the entity types, improving collision avoidance and ensuring safer navigation. We introduce a new reward function that penalizes the robot for being close to or colliding with different entities such as adults, bicyclists, children, and static obstacles, while also encouraging the robot's progress toward the goal. We propose an optimized algorithm that significantly accelerates the training, validation, and testing phases, enabling efficient learning in complex environments. Comprehensive experiments demonstrate that our approach consistently outperforms state-of-the-art navigation and collision avoidance methods.",
    "pdf_url": "https://arxiv.org/pdf/2408.14183v2",
    "github_url": null,
    "published": "2024-08-26T11:16:03+00:00",
    "updated": "2025-09-28T16:25:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.11051v2",
    "title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments",
    "authors": [
      "Xu",
      "Pan",
      "Liu"
    ],
    "summary": "Large Language Models (LLMs) have demonstrated potential in Vision-and-Language Navigation (VLN) tasks, yet current applications face challenges. While LLMs excel in general conversation scenarios, they struggle with specialized navigation tasks, yielding suboptimal performance compared to specialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied Agent), a novel Multimodal LLM-based agent and architecture designed for urban VLN tasks that efficiently handles multiple observations. Our approach implements a three-phase tuning technique for effective adaptation to navigation tasks, including single perception tuning for street view description, multiple perception tuning for route summarization, and end-to-end training on VLN datasets. The augmented datasets are synthesized automatically. Experimental results demonstrate FLAME's superiority over existing methods, surpassing state-of-the-art methods by a 7.3% increase in task completion on Touchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs) in complex navigation tasks, representing an advancement towards applications of MLLMs in the field of embodied intelligence.",
    "pdf_url": "https://arxiv.org/pdf/2408.11051v2",
    "github_url": null,
    "published": "2024-08-20T17:57:46+00:00",
    "updated": "2025-01-21T04:06:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.10899v1",
    "title": "All Robots in One: A New Standard and Unified Dataset for Versatile, General-Purpose Embodied Agents",
    "authors": [
      "Wang",
      "Zheng",
      "Nie"
    ],
    "summary": "Embodied AI is transforming how AI systems interact with the physical world, yet existing datasets are inadequate for developing versatile, general-purpose agents. These limitations include a lack of standardized formats, insufficient data diversity, and inadequate data volume. To address these issues, we introduce ARIO (All Robots In One), a new data standard that enhances existing datasets by offering a unified data format, comprehensive sensory modalities, and a combination of real-world and simulated data. ARIO aims to improve the training of embodied AI agents, increasing their robustness and adaptability across various tasks and environments. Building upon the proposed new standard, we present a large-scale unified ARIO dataset, comprising approximately 3 million episodes collected from 258 series and 321,064 tasks. The ARIO standard and dataset represent a significant step towards bridging the gaps of existing data resources. By providing a cohesive framework for data collection and representation, ARIO paves the way for the development of more powerful and versatile embodied AI agents, capable of navigating and interacting with the physical world in increasingly complex and diverse ways. The project is available on https://imaei.github.io/project_pages/ario/",
    "pdf_url": "https://arxiv.org/pdf/2408.10899v1",
    "github_url": null,
    "published": "2024-08-20T14:40:20+00:00",
    "updated": "2024-08-20T14:40:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.10578v1",
    "title": "Where to Fetch: Extracting Visual Scene Representation from Large Pre-Trained Models for Robotic Goal Navigation",
    "authors": [
      "Li",
      "Li",
      "Zhao"
    ],
    "summary": "To complete a complex task where a robot navigates to a goal object and fetches it, the robot needs to have a good understanding of the instructions and the surrounding environment. Large pre-trained models have shown capabilities to interpret tasks defined via language descriptions. However, previous methods attempting to integrate large pre-trained models with daily tasks are not competent in many robotic goal navigation tasks due to poor understanding of the environment. In this work, we present a visual scene representation built with large-scale visual language models to form a feature representation of the environment capable of handling natural language queries. Combined with large language models, this method can parse language instructions into action sequences for a robot to follow, and accomplish goal navigation with querying the scene representation. Experiments demonstrate that our method enables the robot to follow a wide range of instructions and complete complex goal navigation tasks.",
    "pdf_url": "https://arxiv.org/pdf/2408.10578v1",
    "github_url": null,
    "published": "2024-08-20T06:36:40+00:00",
    "updated": "2024-08-20T06:36:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.10388v1",
    "title": "Narrowing the Gap between Vision and Action in Navigation",
    "authors": [
      "Zhang",
      "Kordjamshidi"
    ],
    "summary": "The existing methods for Vision and Language Navigation in the Continuous Environment (VLN-CE) commonly incorporate a waypoint predictor to discretize the environment. This simplifies the navigation actions into a view selection task and improves navigation performance significantly compared to direct training using low-level actions. However, the VLN-CE agents are still far from the real robots since there are gaps between their visual perception and executed actions. First, VLN-CE agents that discretize the visual environment are primarily trained with high-level view selection, which causes them to ignore crucial spatial reasoning within the low-level action movements. Second, in these models, the existing waypoint predictors neglect object semantics and their attributes related to passibility, which can be informative in indicating the feasibility of actions. To address these two issues, we introduce a low-level action decoder jointly trained with high-level action prediction, enabling the current VLN agent to learn and ground the selected visual view to the low-level controls. Moreover, we enhance the current waypoint predictor by utilizing visual representations containing rich semantic information and explicitly masking obstacles based on humans' prior knowledge about the feasibility of actions. Empirically, our agent can improve navigation performance metrics compared to the strong baselines on both high-level and low-level actions.",
    "pdf_url": "https://arxiv.org/pdf/2408.10388v1",
    "github_url": null,
    "published": "2024-08-19T20:09:56+00:00",
    "updated": "2024-08-19T20:09:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.00015v1",
    "title": "Navigating the sociotechnical labyrinth: Dynamic certification for responsible embodied AI",
    "authors": [
      "Bakirtzis",
      "Tubella",
      "Theodorou"
    ],
    "summary": "Sociotechnical requirements shape the governance of artificially intelligent (AI) systems. In an era where embodied AI technologies are rapidly reshaping various facets of contemporary society, their inherent dynamic adaptability presents a unique blend of opportunities and challenges. Traditional regulatory mechanisms, often designed for static -- or slower-paced -- technologies, find themselves at a crossroads when faced with the fluid and evolving nature of AI systems. Moreover, typical problems in AI, for example, the frequent opacity and unpredictability of the behaviour of the systems, add additional sociotechnical challenges.   To address these interconnected issues, we introduce the concept of dynamic certification, an adaptive regulatory framework specifically crafted to keep pace with the continuous evolution of AI systems. The complexity of these challenges requires common progress in multiple domains: technical, socio-governmental, and regulatory. Our proposed transdisciplinary approach is designed to ensure the safe, ethical, and practical deployment of AI systems, aligning them bidirectionally with the real-world contexts in which they operate. By doing so, we aim to bridge the gap between rapid technological advancement and effective regulatory oversight, ensuring that AI systems not only achieve their intended goals but also adhere to ethical standards and societal values.",
    "pdf_url": "https://arxiv.org/pdf/2409.00015v1",
    "github_url": null,
    "published": "2024-08-16T08:35:26+00:00",
    "updated": "2024-08-16T08:35:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.08301v1",
    "title": "VLPG-Nav: Object Navigation Using Visual Language Pose Graph and Object Localization Probability Maps",
    "authors": [
      "Arul",
      "Kumar",
      "Sugirtharaj"
    ],
    "summary": "We present VLPG-Nav, a visual language navigation method for guiding robots to specified objects within household scenes. Unlike existing methods primarily focused on navigating the robot toward objects, our approach considers the additional challenge of centering the object within the robot's camera view. Our method builds a visual language pose graph (VLPG) that functions as a spatial map of VL embeddings. Given an open vocabulary object query, we plan a viewpoint for object navigation using the VLPG. Despite navigating to the viewpoint, real-world challenges like object occlusion, displacement, and the robot's localization error can prevent visibility. We build an object localization probability map that leverages the robot's current observations and prior VLPG. When the object isn't visible, the probability map is updated and an alternate viewpoint is computed. In addition, we propose an object-centering formulation that locally adjusts the robot's pose to center the object in the camera view. We evaluate the effectiveness of our approach through simulations and real-world experiments, evaluating its ability to successfully view and center the object within the camera field of view. VLPG-Nav demonstrates improved performance in locating the object, navigating around occlusions, and centering the object within the robot's camera view, outperforming the selected baselines in the evaluation metrics.",
    "pdf_url": "https://arxiv.org/pdf/2408.08301v1",
    "github_url": null,
    "published": "2024-08-15T17:54:55+00:00",
    "updated": "2024-08-15T17:54:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.05877v2",
    "title": "Towards pedestrian head tracking: A benchmark dataset and a multi-source data fusion network",
    "authors": [
      "Sun",
      "Wang",
      "Liu"
    ],
    "summary": "Pedestrian detection and tracking in crowded video sequences have many applications, including autonomous driving, robot navigation and pedestrian flow analysis. However, detecting and tracking pedestrians in high-density crowds face many challenges, including intra-class occlusions, complex motions, and diverse poses. Although artificial intelligence (AI) models have achieved great progress in head detection, head tracking datasets and methods are extremely lacking. Existing head datasets have limited coverage of complex pedestrian flows and scenes (e.g., pedestrian interactions, occlusions, and object interference). It is of great importance to develop new head tracking datasets and methods. To address these challenges, we present a Chinese Large-scale Cross-scene Pedestrian Head Tracking dataset (Cchead) and a Multi-source Data Fusion Network (MDFN). The dataset has features that are of considerable interest, including 10 diverse scenes of 50,528 frames with about 2,366,249 heads and 2,358 tracks. Our dataset contains diverse pedestrian moving speeds, directions, and complex crowd pedestrian flows with collision avoidance behaviors. Existing state-of-the-art (SOTA) algorithms are tested and compared on the Cchead dataset. MDFN is the first end-to-end convolutional neural network (CNN)-based head detection and tracking network that jointly trains Red, Green, Blue (RGB) frames, pixel-level motion information, depth maps, and density maps in videos. Ablation experiments confirm the significance of multi-source data fusion. Compared with SOTA pedestrian detection and tracking methods, MDFN achieves superior performance across three datasets: Cchead, Restaurant and Crowd of Heads Dataset (CroHD). To promote further development, we share our source code and trained models for global researchers: https://github.com/kailaisun/Cchead.",
    "pdf_url": "https://arxiv.org/pdf/2408.05877v2",
    "github_url": "https://github.com/kailaisun/Cchead",
    "published": "2024-08-12T00:21:36+00:00",
    "updated": "2025-08-20T00:10:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.05090v1",
    "title": "Loc4Plan: Locating Before Planning for Outdoor Vision and Language Navigation",
    "authors": [
      "Tian",
      "Meng",
      "Zheng"
    ],
    "summary": "Vision and Language Navigation (VLN) is a challenging task that requires agents to understand instructions and navigate to the destination in a visual environment.One of the key challenges in outdoor VLN is keeping track of which part of the instruction was completed. To alleviate this problem, previous works mainly focus on grounding the natural language to the visual input, but neglecting the crucial role of the agent's spatial position information in the grounding process. In this work, we first explore the substantial effect of spatial position locating on the grounding of outdoor VLN, drawing inspiration from human navigation. In real-world navigation scenarios, before planning a path to the destination, humans typically need to figure out their current location. This observation underscores the pivotal role of spatial localization in the navigation process. In this work, we introduce a novel framework, Locating be for Planning (Loc4Plan), designed to incorporate spatial perception for action planning in outdoor VLN tasks. The main idea behind Loc4Plan is to perform the spatial localization before planning a decision action based on corresponding guidance, which comprises a block-aware spatial locating (BAL) module and a spatial-aware action planning (SAP) module. Specifically, to help the agent perceive its spatial location in the environment, we propose to learn a position predictor that measures how far the agent is from the next intersection for reflecting its position, which is achieved by the BAL module. After the locating process, we propose the SAP module to incorporate spatial information to ground the corresponding guidance and enhance the precision of action planning. Extensive experiments on the Touchdown and map2seq datasets show that the proposed Loc4Plan outperforms the SOTA methods.",
    "pdf_url": "https://arxiv.org/pdf/2408.05090v1",
    "github_url": null,
    "published": "2024-08-09T14:31:09+00:00",
    "updated": "2024-08-09T14:31:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.04423v1",
    "title": "UNMuTe: Unifying Navigation and Multimodal Dialogue-like Text Generation",
    "authors": [
      "Rawal",
      "Bigazzi",
      "Baraldi"
    ],
    "summary": "Smart autonomous agents are becoming increasingly important in various real-life applications, including robotics and autonomous vehicles. One crucial skill that these agents must possess is the ability to interact with their surrounding entities, such as other agents or humans. In this work, we aim at building an intelligent agent that can efficiently navigate in an environment while being able to interact with an oracle (or human) in natural language and ask for directions when it is unsure about its navigation performance. The interaction is started by the agent that produces a question, which is then answered by the oracle on the basis of the shortest trajectory to the goal. The process can be performed multiple times during navigation, thus enabling the agent to hold a dialogue with the oracle. To this end, we propose a novel computational model, named UNMuTe, that consists of two main components: a dialogue model and a navigator. Specifically, the dialogue model is based on a GPT-2 decoder that handles multimodal data consisting of both text and images. First, the dialogue model is trained to generate question-answer pairs: the question is generated using the current image, while the answer is produced leveraging future images on the path toward the goal. Subsequently, a VLN model is trained to follow the dialogue predicting navigation actions or triggering the dialogue model if it needs help. In our experimental analysis, we show that UNMuTe achieves state-of-the-art performance on the main navigation tasks implying dialogue, i.e. Cooperative Vision and Dialogue Navigation (CVDN) and Navigation from Dialogue History (NDH), proving that our approach is effective in generating useful questions and answers to guide navigation.",
    "pdf_url": "https://arxiv.org/pdf/2408.04423v1",
    "github_url": null,
    "published": "2024-08-08T12:47:52+00:00",
    "updated": "2024-08-08T12:47:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.03807v1",
    "title": "Navigating the Human Maze: Real-Time Robot Pathfinding with Generative Imitation Learning",
    "authors": [
      "Moder",
      "Adhisaputra",
      "Pauli"
    ],
    "summary": "This paper addresses navigation in crowded environments by integrating goal-conditioned generative models with Sampling-based Model Predictive Control (SMPC). We introduce goal-conditioned autoregressive models to generate crowd behaviors, capturing intricate interactions among individuals. The model processes potential robot trajectory samples and predicts the reactions of surrounding individuals, enabling proactive robotic navigation in complex scenarios. Extensive experiments show that this algorithm enables real-time navigation, significantly reducing collision rates and path lengths, and outperforming selected baseline methods. The practical effectiveness of this algorithm is validated on an actual robotic platform, demonstrating its capability in dynamic settings.",
    "pdf_url": "https://arxiv.org/pdf/2408.03807v1",
    "github_url": null,
    "published": "2024-08-07T14:32:41+00:00",
    "updated": "2024-08-07T14:32:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.03515v2",
    "title": "A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems",
    "authors": [
      "Zhang",
      "Kong",
      "Dewitt"
    ],
    "summary": "The integration of Large Language Models (LLMs) like GPT-4o into robotic systems represents a significant advancement in embodied artificial intelligence. These models can process multi-modal prompts, enabling them to generate more context-aware responses. However, this integration is not without challenges. One of the primary concerns is the potential security risks associated with using LLMs in robotic navigation tasks. These tasks require precise and reliable responses to ensure safe and effective operation. Multi-modal prompts, while enhancing the robot's understanding, also introduce complexities that can be exploited maliciously. For instance, adversarial inputs designed to mislead the model can lead to incorrect or dangerous navigational decisions. This study investigates the impact of prompt injections on mobile robot performance in LLM-integrated systems and explores secure prompt strategies to mitigate these risks. Our findings demonstrate a substantial overall improvement of approximately 30.8% in both attack detection and system performance with the implementation of robust defence mechanisms, highlighting their critical role in enhancing security and reliability in mission-oriented tasks.",
    "pdf_url": "https://arxiv.org/pdf/2408.03515v2",
    "github_url": null,
    "published": "2024-08-07T02:48:22+00:00",
    "updated": "2024-09-09T01:55:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.02535v1",
    "title": "Towards Coarse-grained Visual Language Navigation Task Planning Enhanced by Event Knowledge Graph",
    "authors": [
      "Kaichen",
      "Yaoxian",
      "Haiquan"
    ],
    "summary": "Visual language navigation (VLN) is one of the important research in embodied AI. It aims to enable an agent to understand the surrounding environment and complete navigation tasks. VLN instructions could be categorized into coarse-grained and fine-grained commands. Fine-grained command describes a whole task with subtasks step-by-step. In contrast, coarse-grained command gives an abstract task description, which more suites human habits. Most existing work focuses on the former kind of instruction in VLN tasks, ignoring the latter abstract instructions belonging to daily life scenarios. To overcome the above challenge in abstract instruction, we attempt to consider coarse-grained instruction in VLN by event knowledge enhancement. Specifically, we first propose a prompt-based framework to extract an event knowledge graph (named VLN-EventKG) for VLN integrally over multiple mainstream benchmark datasets. Through small and large language model collaboration, we realize knowledge-enhanced navigation planning (named EventNav) for VLN tasks with coarse-grained instruction input. Additionally, we design a novel dynamic history backtracking module to correct potential error action planning in real time. Experimental results in various public benchmarks show our knowledge-enhanced method has superiority in coarse-grained-instruction VLN using our proposed VLN-EventKG with over $5\\%$ improvement in success rate. Our project is available at https://sites.google.com/view/vln-eventkg",
    "pdf_url": "https://arxiv.org/pdf/2408.02535v1",
    "github_url": null,
    "published": "2024-08-05T15:08:26+00:00",
    "updated": "2024-08-05T15:08:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.01867v1",
    "title": "TrustNavGPT: Modeling Uncertainty to Improve Trustworthiness of Audio-Guided LLM-Based Robot Navigation",
    "authors": [
      "Sun",
      "Zhang",
      "Tang"
    ],
    "summary": "While LLMs are proficient at processing text in human conversations, they often encounter difficulties with the nuances of verbal instructions and, thus, remain prone to hallucinate trust in human command. In this work, we present TrustNavGPT, an LLM based audio guided navigation agent that uses affective cues in spoken communication elements such as tone and inflection that convey meaning beyond words, allowing it to assess the trustworthiness of human commands and make effective, safe decisions. Our approach provides a lightweight yet effective approach that extends existing LLMs to model audio vocal features embedded in the voice command and model uncertainty for safe robotic navigation.",
    "pdf_url": "https://arxiv.org/pdf/2408.01867v1",
    "github_url": null,
    "published": "2024-08-03T21:32:43+00:00",
    "updated": "2024-08-03T21:32:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.00343v2",
    "title": "IN-Sight: Interactive Navigation through Sight",
    "authors": [
      "Schoch",
      "Yang",
      "Ma"
    ],
    "summary": "Current visual navigation systems often treat the environment as static, lacking the ability to adaptively interact with obstacles. This limitation leads to navigation failure when encountering unavoidable obstructions. In response, we introduce IN-Sight, a novel approach to self-supervised path planning, enabling more effective navigation strategies through interaction with obstacles. Utilizing RGB-D observations, IN-Sight calculates traversability scores and incorporates them into a semantic map, facilitating long-range path planning in complex, maze-like environments. To precisely navigate around obstacles, IN-Sight employs a local planner, trained imperatively on a differentiable costmap using representation learning techniques. The entire framework undergoes end-to-end training within the state-of-the-art photorealistic Intel SPEAR Simulator. We validate the effectiveness of IN-Sight through extensive benchmarking in a variety of simulated scenarios and ablation studies. Moreover, we demonstrate the system's real-world applicability with zero-shot sim-to-real transfer, deploying our planner on the legged robot platform ANYmal, showcasing its practical potential for interactive navigation in real environments.",
    "pdf_url": "https://arxiv.org/pdf/2408.00343v2",
    "github_url": null,
    "published": "2024-08-01T07:27:54+00:00",
    "updated": "2024-08-12T10:19:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.21452v1",
    "title": "Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments",
    "authors": [
      "Hong",
      "Wang",
      "Huang"
    ],
    "summary": "Real-world navigation often involves dealing with unexpected obstructions such as closed doors, moved objects, and unpredictable entities. However, mainstream Vision-and-Language Navigation (VLN) tasks typically assume instructions perfectly align with the fixed and predefined navigation graphs without any obstructions. This assumption overlooks potential discrepancies in actual navigation graphs and given instructions, which can cause major failures for both indoor and outdoor agents. To address this issue, we integrate diverse obstructions into the R2R dataset by modifying both the navigation graphs and visual observations, introducing an innovative dataset and task, R2R with UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers of path obstructions to generate instruction-reality mismatches for VLN research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods inevitably encounter significant challenges when facing such mismatches, indicating that they rigidly follow instructions rather than navigate adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN), which includes a curriculum training strategy and virtual graph construction to help agents effectively adapt to obstructed environments. Empirical results show that ObVLN not only maintains robust performance in unobstructed scenarios but also achieves a substantial performance advantage with unexpected obstructions.",
    "pdf_url": "https://arxiv.org/pdf/2407.21452v1",
    "github_url": null,
    "published": "2024-07-31T08:55:57+00:00",
    "updated": "2024-07-31T08:55:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.20164v1",
    "title": "Language-Conditioned Offline RL for Multi-Robot Navigation",
    "authors": [
      "Morad",
      "Shankar",
      "Blumenkamp"
    ],
    "summary": "We present a method for developing navigation policies for multi-robot teams that interpret and follow natural language instructions. We condition these policies on embeddings from pretrained Large Language Models (LLMs), and train them via offline reinforcement learning with as little as 20 minutes of randomly-collected data. Experiments on a team of five real robots show that these policies generalize well to unseen commands, indicating an understanding of the LLM latent space. Our method requires no simulators or environment models, and produces low-latency control policies that can be deployed directly to real robots without finetuning. We provide videos of our experiments at https://sites.google.com/view/llm-marl.",
    "pdf_url": "https://arxiv.org/pdf/2407.20164v1",
    "github_url": null,
    "published": "2024-07-29T16:49:30+00:00",
    "updated": "2024-07-29T16:49:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.14758v1",
    "title": "DISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control",
    "authors": [
      "Xu",
      "Luo",
      "Yang"
    ],
    "summary": "Building a general-purpose intelligent home-assistant agent skilled in diverse tasks by human commands is a long-term blueprint of embodied AI research, which poses requirements on task planning, environment modeling, and object interaction. In this work, we study primitive mobile manipulations for embodied agents, i.e. how to navigate and interact based on an instructed verb-noun pair. We propose DISCO, which features non-trivial advancements in contextualized scene modeling and efficient controls. In particular, DISCO incorporates differentiable scene representations of rich semantics in object and affordance, which is dynamically learned on the fly and facilitates navigation planning. Besides, we propose dual-level coarse-to-fine action controls leveraging both global and local cues to accomplish mobile manipulation tasks efficiently. DISCO easily integrates into embodied tasks such as embodied instruction following. To validate our approach, we take the ALFRED benchmark of large-scale long-horizon vision-language navigation and interaction tasks as a test bed. In extensive experiments, we make comprehensive evaluations and demonstrate that DISCO outperforms the art by a sizable +8.6% success rate margin in unseen scenes, even without step-by-step instructions. Our code is publicly released at https://github.com/AllenXuuu/DISCO.",
    "pdf_url": "https://arxiv.org/pdf/2407.14758v1",
    "github_url": "https://github.com/AllenXuuu/DISCO",
    "published": "2024-07-20T05:39:28+00:00",
    "updated": "2024-07-20T05:39:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.12366v2",
    "title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models",
    "authors": [
      "Zhou",
      "Hong",
      "Wang"
    ],
    "summary": "Capitalizing on the remarkable advancements in Large Language Models (LLMs), there is a burgeoning initiative to harness LLMs for instruction following robotic navigation. Such a trend underscores the potential of LLMs to generalize navigational reasoning and diverse language understanding. However, a significant discrepancy in agent performance is observed when integrating LLMs in the Vision-and-Language navigation (VLN) tasks compared to previous downstream specialist models. Furthermore, the inherent capacity of language to interpret and facilitate communication in agent interactions is often underutilized in these integrations. In this work, we strive to bridge the divide between VLN-specialized models and LLM-based navigation paradigms, while maintaining the interpretative prowess of LLMs in generating linguistic navigational reasoning. By aligning visual content in a frozen LLM, we encompass visual observation comprehension for LLMs and exploit a way to incorporate LLMs and navigation policy networks for effective action predictions and navigational reasoning. We demonstrate the data efficiency of the proposed methods and eliminate the gap between LM-based agents and state-of-the-art VLN specialists.",
    "pdf_url": "https://arxiv.org/pdf/2407.12366v2",
    "github_url": null,
    "published": "2024-07-17T07:44:26+00:00",
    "updated": "2024-09-20T01:32:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.11487v1",
    "title": "PRET: Planning with Directed Fidelity Trajectory for Vision and Language Navigation",
    "authors": [
      "Lu",
      "Meng",
      "Zheng"
    ],
    "summary": "Vision and language navigation is a task that requires an agent to navigate according to a natural language instruction. Recent methods predict sub-goals on constructed topology map at each step to enable long-term action planning. However, they suffer from high computational cost when attempting to support such high-level predictions with GCN-like models. In this work, we propose an alternative method that facilitates navigation planning by considering the alignment between instructions and directed fidelity trajectories, which refers to a path from the initial node to the candidate locations on a directed graph without detours. This planning strategy leads to an efficient model while achieving strong performance. Specifically, we introduce a directed graph to illustrate the explored area of the environment, emphasizing directionality. Then, we firstly define the trajectory representation as a sequence of directed edge features, which are extracted from the panorama based on the corresponding orientation. Ultimately, we assess and compare the alignment between instruction and different trajectories during navigation to determine the next navigation target. Our method outperforms previous SOTA method BEVBert on RxR dataset and is comparable on R2R dataset while largely reducing the computational cost. Code is available: https://github.com/iSEE-Laboratory/VLN-PRET.",
    "pdf_url": "https://arxiv.org/pdf/2407.11487v1",
    "github_url": "https://github.com/iSEE-Laboratory/VLN-PRET",
    "published": "2024-07-16T08:22:18+00:00",
    "updated": "2024-07-16T08:22:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.10943v1",
    "title": "GRUtopia: Dream General Robots in a City at Scale",
    "authors": [
      "Wang",
      "Chen",
      "Huang"
    ],
    "summary": "Recent works have been exploring the scaling laws in the field of Embodied AI. Given the prohibitive costs of collecting real-world data, we believe the Simulation-to-Real (Sim2Real) paradigm is a crucial step for scaling the learning of embodied models. This paper introduces project GRUtopia, the first simulated interactive 3D society designed for various robots. It features several advancements: (a) The scene dataset, GRScenes, includes 100k interactive, finely annotated scenes, which can be freely combined into city-scale environments. In contrast to previous works mainly focusing on home, GRScenes covers 89 diverse scene categories, bridging the gap of service-oriented environments where general robots would be initially deployed. (b) GRResidents, a Large Language Model (LLM) driven Non-Player Character (NPC) system that is responsible for social interaction, task generation, and task assignment, thus simulating social scenarios for embodied AI applications. (c) The benchmark, GRBench, supports various robots but focuses on legged robots as primary agents and poses moderately challenging tasks involving Object Loco-Navigation, Social Loco-Navigation, and Loco-Manipulation. We hope that this work can alleviate the scarcity of high-quality data in this field and provide a more comprehensive assessment of Embodied AI research. The project is available at https://github.com/OpenRobotLab/GRUtopia.",
    "pdf_url": "https://arxiv.org/pdf/2407.10943v1",
    "github_url": "https://github.com/OpenRobotLab/GRUtopia",
    "published": "2024-07-15T17:40:46+00:00",
    "updated": "2024-07-15T17:40:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.10547v2",
    "title": "Learning Social Cost Functions for Human-Aware Path Planning",
    "authors": [
      "Eirale",
      "Leonetti",
      "Chiaberge"
    ],
    "summary": "Achieving social acceptance is one of the main goals of Social Robotic Navigation. Despite this topic has received increasing interest in recent years, most of the research has focused on driving the robotic agent along obstacle-free trajectories, planning around estimates of future human motion to respect personal distances and optimize navigation. However, social interactions in everyday life are also dictated by norms that do not strictly depend on movement, such as when standing at the end of a queue rather than cutting it. In this paper, we propose a novel method to recognize common social scenarios and modify a traditional planner's cost function to adapt to them. This solution enables the robot to carry out different social navigation behaviors that would not arise otherwise, maintaining the robustness of traditional navigation. Our approach allows the robot to learn different social norms with a single learned model, rather than having different modules for each task. As a proof of concept, we consider the tasks of queuing and respect interaction spaces of groups of people talking to one another, but the method can be extended to other human activities that do not involve motion.",
    "pdf_url": "https://arxiv.org/pdf/2407.10547v2",
    "github_url": null,
    "published": "2024-07-15T08:57:02+00:00",
    "updated": "2024-10-18T12:25:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.09016v1",
    "title": "OVExp: Open Vocabulary Exploration for Object-Oriented Navigation",
    "authors": [
      "Wei",
      "Wang",
      "Chen"
    ],
    "summary": "Object-oriented embodied navigation aims to locate specific objects, defined by category or depicted in images. Existing methods often struggle to generalize to open vocabulary goals without extensive training data. While recent advances in Vision-Language Models (VLMs) offer a promising solution by extending object recognition beyond predefined categories, efficient goal-oriented exploration becomes more challenging in an open vocabulary setting. We introduce OVExp, a learning-based framework that integrates VLMs for Open-Vocabulary Exploration. OVExp constructs scene representations by encoding observations with VLMs and projecting them onto top-down maps for goal-conditioned exploration. Goals are encoded in the same VLM feature space, and a lightweight transformer-based decoder predicts target locations while maintaining versatile representation abilities. To address the impracticality of fusing dense pixel embeddings with full 3D scene reconstruction for training, we propose constructing maps using low-cost semantic categories and transforming them into CLIP's embedding space via the text encoder. The simple but effective design of OVExp significantly reduces computational costs and demonstrates strong generalization abilities to various navigation settings. Experiments on established benchmarks show OVExp outperforms previous zero-shot methods, can generalize to diverse scenes, and handle different goal modalities.",
    "pdf_url": "https://arxiv.org/pdf/2407.09016v1",
    "github_url": null,
    "published": "2024-07-12T06:07:49+00:00",
    "updated": "2024-07-12T06:07:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.08725v2",
    "title": "MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility",
    "authors": [
      "Wu",
      "He",
      "He"
    ],
    "summary": "Public urban spaces like streetscapes and plazas serve residents and accommodate social life in all its vibrant variations. Recent advances in Robotics and Embodied AI make public urban spaces no longer exclusive to humans. Food delivery bots and electric wheelchairs have started sharing sidewalks with pedestrians, while robot dogs and humanoids have recently emerged in the street. Micromobility enabled by AI for short-distance travel in public urban spaces plays a crucial component in the future transportation system. Ensuring the generalizability and safety of AI models maneuvering mobile machines is essential. In this work, we present MetaUrban, a compositional simulation platform for the AI-driven urban micromobility research. MetaUrban can construct an infinite number of interactive urban scenes from compositional elements, covering a vast array of ground plans, object placements, pedestrians, vulnerable road users, and other mobile agents' appearances and dynamics. We design point navigation and social navigation tasks as the pilot study using MetaUrban for urban micromobility research and establish various baselines of Reinforcement Learning and Imitation Learning. We conduct extensive evaluation across mobile machines, demonstrating that heterogeneous mechanical structures significantly influence the learning and execution of AI policies. We perform a thorough ablation study, showing that the compositional nature of the simulated environments can substantially improve the generalizability and safety of the trained mobile agents. MetaUrban will be made publicly available to provide research opportunities and foster safe and trustworthy embodied AI and micromobility in cities. The code and dataset will be publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2407.08725v2",
    "github_url": null,
    "published": "2024-07-11T17:56:49+00:00",
    "updated": "2024-10-11T09:41:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.07392v1",
    "title": "Malicious Path Manipulations via Exploitation of Representation Vulnerabilities of Vision-Language Navigation Systems",
    "authors": [
      "Islam",
      "Salman",
      "Shams"
    ],
    "summary": "Building on the unprecedented capabilities of large language models for command understanding and zero-shot recognition of multi-modal vision-language transformers, visual language navigation (VLN) has emerged as an effective way to address multiple fundamental challenges toward a natural language interface to robot navigation. However, such vision-language models are inherently vulnerable due to the lack of semantic meaning of the underlying embedding space. Using a recently developed gradient based optimization procedure, we demonstrate that images can be modified imperceptibly to match the representation of totally different images and unrelated texts for a vision-language model. Building on this, we develop algorithms that can adversarially modify a minimal number of images so that the robot will follow a route of choice for commands that require a number of landmarks. We demonstrate that experimentally using a recently proposed VLN system; for a given navigation command, a robot can be made to follow drastically different routes. We also develop an efficient algorithm to detect such malicious modifications reliably based on the fact that the adversarially modified images have much higher sensitivity to added Gaussian noise than the original images.",
    "pdf_url": "https://arxiv.org/pdf/2407.07392v1",
    "github_url": null,
    "published": "2024-07-10T06:32:58+00:00",
    "updated": "2024-07-10T06:32:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.07035v2",
    "title": "Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models",
    "authors": [
      "Zhang",
      "Ma",
      "Li"
    ],
    "summary": "Vision-and-Language Navigation (VLN) has gained increasing attention over recent years and many approaches have emerged to advance their development. The remarkable achievements of foundation models have shaped the challenges and proposed methods for VLN research. In this survey, we provide a top-down review that adopts a principled framework for embodied planning and reasoning, and emphasizes the current methods and future opportunities leveraging foundation models to address VLN challenges. We hope our in-depth discussions could provide valuable resources and insights: on one hand, to milestone the progress and explore opportunities and potential roles for foundation models in this field, and on the other, to organize different challenges and solutions in VLN to foundation model researchers.",
    "pdf_url": "https://arxiv.org/pdf/2407.07035v2",
    "github_url": null,
    "published": "2024-07-09T16:53:36+00:00",
    "updated": "2024-12-29T23:16:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.06886v8",
    "title": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
    "authors": [
      "Liu",
      "Chen",
      "Bai"
    ],
    "summary": "Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications (e.g., intelligent mechatronics systems, smart manufacturing) that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for embodied agents. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss potential future directions. We hope this survey will serve as a foundational reference for the research community. The associated project can be found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.",
    "pdf_url": "https://arxiv.org/pdf/2407.06886v8",
    "github_url": "https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List",
    "published": "2024-07-09T14:14:47+00:00",
    "updated": "2025-08-25T02:20:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.06545v1",
    "title": "Visual-Geometry GP-based Navigable Space for Autonomous Navigation",
    "authors": [
      "Ali",
      "Pushp",
      "Chen"
    ],
    "summary": "Autonomous navigation in unknown environments is challenging and demands the consideration of both geometric and semantic information in order to parse the navigability of the environment. In this work, we propose a novel space modeling framework, Visual-Geometry Sparse Gaussian Process (VG-SGP), that simultaneously considers semantics and geometry of the scene. Our proposed approach can overcome the limitation of visual planners that fail to recognize geometry associated with the semantic and the geometric planners that completely overlook the semantic information which is very critical in real-world navigation. The proposed method leverages dual Sparse Gaussian Processes in an integrated manner; the first is trained to forecast geometrically navigable spaces while the second predicts the semantically navigable areas. This integrated model is able to pinpoint the overlapping (geometric and semantic) navigable space. The simulation and real-world experiments demonstrate that the ability of the proposed VG-SGP model, coupled with our innovative navigation strategy, outperforms models solely reliant on visual or geometric navigation algorithms, highlighting a superior adaptive behavior.",
    "pdf_url": "https://arxiv.org/pdf/2407.06545v1",
    "github_url": null,
    "published": "2024-07-09T04:51:50+00:00",
    "updated": "2024-07-09T04:51:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.05890v2",
    "title": "Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation",
    "authors": [
      "Chen",
      "Lin",
      "Liu"
    ],
    "summary": "LLM-based agents have demonstrated impressive zero-shot performance in vision-language navigation (VLN) task. However, existing LLM-based methods often focus only on solving high-level task planning by selecting nodes in predefined navigation graphs for movements, overlooking low-level control in navigation scenarios. To bridge this gap, we propose AO-Planner, a novel Affordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates various foundation models to achieve affordances-oriented low-level motion planning and high-level decision-making, both performed in a zero-shot setting. Specifically, we employ a Visual Affordances Prompting (VAP) approach, where the visible ground is segmented by SAM to provide navigational affordances, based on which the LLM selects potential candidate waypoints and plans low-level paths towards selected waypoints. We further propose a high-level PathAgent which marks planned paths into the image input and reasons the most probable path by comprehending all environmental information. Finally, we convert the selected path into 3D coordinates using camera intrinsic parameters and depth information, avoiding challenging 3D predictions for LLMs. Experiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner achieves state-of-the-art zero-shot performance (8.8% improvement on SPL). Our method can also serve as a data annotator to obtain pseudo-labels, distilling its waypoint prediction ability into a learning-based predictor. This new predictor does not require any waypoint data from the simulator and achieves 47% SR competing with supervised methods. We establish an effective connection between LLM and 3D world, presenting novel prospects for employing foundation models in low-level motion control.",
    "pdf_url": "https://arxiv.org/pdf/2407.05890v2",
    "github_url": null,
    "published": "2024-07-08T12:52:46+00:00",
    "updated": "2024-08-20T14:51:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.04158v2",
    "title": "ELCC: the Emergent Language Corpus Collection",
    "authors": [
      "Boldt",
      "Mortensen"
    ],
    "summary": "We introduce the Emergent Language Corpus Collection (ELCC): a collection of corpora generated from open source implementations of emergent communication systems across the literature. These systems include a variety of signalling game environments as well as more complex environments like a social deduction game and embodied navigation. Each corpus is annotated with metadata describing the characteristics of the source system as well as a suite of analyses of the corpus (e.g., size, entropy, average message length, performance as transfer learning data). Currently, research studying emergent languages requires directly running different systems which takes time away from actual analyses of such languages, makes studies which compare diverse emergent languages rare, and presents a barrier to entry for researchers without a background in deep learning. The availability of a substantial collection of well-documented emergent language corpora, then, will enable research which can analyze a wider variety of emergent languages, which more effectively uncovers general principles in emergent communication rather than artifacts of particular environments. We provide some quantitative and qualitative analyses with ELCC to demonstrate potential use cases of the resource in this vein.",
    "pdf_url": "https://arxiv.org/pdf/2407.04158v2",
    "github_url": null,
    "published": "2024-07-04T21:23:18+00:00",
    "updated": "2024-12-04T15:23:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.02473v1",
    "title": "Open Scene Graphs for Open World Object-Goal Navigation",
    "authors": [
      "Loo",
      "Wu",
      "Hsu"
    ],
    "summary": "How can we build robots for open-world semantic navigation tasks, like searching for target objects in novel scenes? While foundation models have the rich knowledge and generalisation needed for these tasks, a suitable scene representation is needed to connect them into a complete robot system. We address this with Open Scene Graphs (OSGs), a topo-semantic representation that retains and organises open-set scene information for these models, and has a structure that can be configured for different environment types. We integrate foundation models and OSGs into the OpenSearch system for Open World Object-Goal Navigation, which is capable of searching for open-set objects specified in natural language, while generalising zero-shot across diverse environments and embodiments. Our OSGs enhance reasoning with Large Language Models (LLM), enabling robust object-goal navigation outperforming existing LLM approaches. Through simulation and real-world experiments, we validate OpenSearch's generalisation across varied environments, robots and novel instructions.",
    "pdf_url": "https://arxiv.org/pdf/2407.02473v1",
    "github_url": null,
    "published": "2024-07-02T17:52:12+00:00",
    "updated": "2024-07-02T17:52:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.02539v3",
    "title": "Research on Autonomous Robots Navigation based on Reinforcement Learning",
    "authors": [
      "Wang",
      "Yan",
      "Wang"
    ],
    "summary": "Reinforcement learning continuously optimizes decision-making based on real-time feedback reward signals through continuous interaction with the environment, demonstrating strong adaptive and self-learning capabilities. In recent years, it has become one of the key methods to achieve autonomous navigation of robots. In this work, an autonomous robot navigation method based on reinforcement learning is introduced. We use the Deep Q Network (DQN) and Proximal Policy Optimization (PPO) models to optimize the path planning and decision-making process through the continuous interaction between the robot and the environment, and the reward signals with real-time feedback. By combining the Q-value function with the deep neural network, deep Q network can handle high-dimensional state space, so as to realize path planning in complex environments. Proximal policy optimization is a strategy gradient-based method, which enables robots to explore and utilize environmental information more efficiently by optimizing policy functions. These methods not only improve the robot's navigation ability in the unknown environment, but also enhance its adaptive and self-learning capabilities. Through multiple training and simulation experiments, we have verified the effectiveness and robustness of these models in various complex scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2407.02539v3",
    "github_url": null,
    "published": "2024-07-02T00:44:06+00:00",
    "updated": "2024-08-14T04:49:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.00632v1",
    "title": "CAMON: Cooperative Agents for Multi-Object Navigation with LLM-based Conversations",
    "authors": [
      "Wu",
      "Mu",
      "Zhou"
    ],
    "summary": "Visual navigation tasks are critical for household service robots. As these tasks become increasingly complex, effective communication and collaboration among multiple robots become imperative to ensure successful completion. In recent years, large language models (LLMs) have exhibited remarkable comprehension and planning abilities in the context of embodied agents. However, their application in household scenarios, specifically in the use of multiple agents collaborating to complete complex navigation tasks through communication, remains unexplored. Therefore, this paper proposes a framework for decentralized multi-agent navigation, leveraging LLM-enabled communication and collaboration. By designing the communication-triggered dynamic leadership organization structure, we achieve faster team consensus with fewer communication instances, leading to better navigation effectiveness and collaborative exploration efficiency. With the proposed novel communication scheme, our framework promises to be conflict-free and robust in multi-object navigation tasks, even when there is a surge in team size.",
    "pdf_url": "https://arxiv.org/pdf/2407.00632v1",
    "github_url": null,
    "published": "2024-06-30T09:14:33+00:00",
    "updated": "2024-06-30T09:14:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.20083v1",
    "title": "PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators",
    "authors": [
      "Zeng",
      "Zhang",
      "Ehsani"
    ],
    "summary": "We present PoliFormer (Policy Transformer), an RGB-only indoor navigation agent trained end-to-end with reinforcement learning at scale that generalizes to the real-world without adaptation despite being trained purely in simulation. PoliFormer uses a foundational vision transformer encoder with a causal transformer decoder enabling long-term memory and reasoning. It is trained for hundreds of millions of interactions across diverse environments, leveraging parallelized, multi-machine rollouts for efficient training with high throughput. PoliFormer is a masterful navigator, producing state-of-the-art results across two distinct embodiments, the LoCoBot and Stretch RE-1 robots, and four navigation benchmarks. It breaks through the plateaus of previous work, achieving an unprecedented 85.5% success rate in object goal navigation on the CHORES-S benchmark, a 28.5% absolute improvement. PoliFormer can also be trivially extended to a variety of downstream applications such as object tracking, multi-object navigation, and open-vocabulary navigation with no finetuning.",
    "pdf_url": "https://arxiv.org/pdf/2406.20083v1",
    "github_url": null,
    "published": "2024-06-28T17:51:10+00:00",
    "updated": "2024-06-28T17:51:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.19967v1",
    "title": "Into the Unknown: Generating Geospatial Descriptions for New Environments",
    "authors": [
      "Paz-Argaman",
      "Palowitch",
      "Kulkarni"
    ],
    "summary": "Similar to vision-and-language navigation (VLN) tasks that focus on bridging the gap between vision and language for embodied navigation, the new Rendezvous (RVS) task requires reasoning over allocentric spatial relationships (independent of the observer's viewpoint) using non-sequential navigation instructions and maps. However, performance substantially drops in new environments with no training data. Using opensource descriptions paired with coordinates (e.g., Wikipedia) provides training data but suffers from limited spatially-oriented text resulting in low geolocation resolution. We propose a large-scale augmentation method for generating high-quality synthetic data for new environments using readily available geospatial data. Our method constructs a grounded knowledge-graph, capturing entity relationships. Sampled entities and relations (`shop north of school') generate navigation instructions via (i) generating numerous templates using context-free grammar (CFG) to embed specific entities and relations; (ii) feeding the entities and relation into a large language model (LLM) for instruction generation. A comprehensive evaluation on RVS, showed that our approach improves the 100-meter accuracy by 45.83% on unseen environments. Furthermore, we demonstrate that models trained with CFG-based augmentation achieve superior performance compared with those trained with LLM-based augmentation, both in unseen and seen environments. These findings suggest that the potential advantages of explicitly structuring spatial information for text-based geospatial reasoning in previously unknown, can unlock data-scarce scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2406.19967v1",
    "github_url": null,
    "published": "2024-06-28T14:56:21+00:00",
    "updated": "2024-06-28T14:56:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.19236v3",
    "title": "Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions",
    "authors": [
      "Li",
      "Li",
      "Cheng"
    ],
    "summary": "Vision-and-Language Navigation (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce Human-Aware Vision-and-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN's unique challenges, underscores the need for further research to enhance HA-VLN agents' real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and Sim2Real transfer, paving the way for more realistic and applicable VLN systems in human-populated environments.",
    "pdf_url": "https://arxiv.org/pdf/2406.19236v3",
    "github_url": null,
    "published": "2024-06-27T15:01:42+00:00",
    "updated": "2024-11-02T02:14:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.17960v1",
    "title": "MAGIC: Meta-Ability Guided Interactive Chain-of-Distillation for Effective-and-Efficient Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "He",
      "Shen"
    ],
    "summary": "Despite the remarkable developments of recent large models in Embodied Artificial Intelligence (E-AI), their integration into robotics is hampered by their excessive parameter sizes and computational demands. Towards the Vision-and-Language Navigation (VLN) task, a core task in E-AI, this paper reveals the great potential of using knowledge distillation for obtaining lightweight student models by proposing a Meta-Ability Guided Interactive Chain-of-distillation (MAGIC) method. Specifically, a Meta-Ability Knowledge Distillation (MAKD) framework is proposed for decoupling and refining the necessary meta-abilities of VLN agents. A Meta-Knowledge Randomization Weighting (MKRW) and a Meta-Knowledge Transferable Determination (MKTD) module are incorporated to dynamically adjust aggregation weights at the meta-ability and sample levels, respectively. Move beyond the traditional one-step unidirectional distillation, an Interactive Chain-of-Distillation (ICoD) learning strategy is proposed to allow students to give feedback to teachers, forming a new multi-step teacher-student co-evolution pipeline. Remarkably, on the R2R test unseen public leaderboard, our smallest model, MAGIC-S, with only 5% (11M) of the teacher's size, outperforms all previous methods under the same training data. Additionally, our largest model, MAGIC-L, surpasses the previous state-of-the-art by 5.84% in SPL and 3.18% in SR. Furthermore, a new dataset was collected and annotated from our living environments, where MAGIC-S demonstrated superior performance and real-time efficiency. Our code is publicly available on https://github.com/CrystalSixone/VLN-MAGIC.",
    "pdf_url": "https://arxiv.org/pdf/2406.17960v1",
    "github_url": "https://github.com/CrystalSixone/VLN-MAGIC",
    "published": "2024-06-25T22:33:41+00:00",
    "updated": "2024-06-25T22:33:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.14240v3",
    "title": "CityNav: A Large-Scale Dataset for Real-World Aerial Navigation",
    "authors": [
      "Lee",
      "Miyanishi",
      "Kurita"
    ],
    "summary": "Vision-and-language navigation (VLN) aims to develop agents capable of navigating in realistic environments. While recent cross-modal training approaches have significantly improved navigation performance in both indoor and outdoor scenarios, aerial navigation over real-world cities remains underexplored primarily due to limited datasets and the difficulty of integrating visual and geographic information. To fill this gap, we introduce CityNav, the first large-scale real-world dataset for aerial VLN. Our dataset consists of 32,637 human demonstration trajectories, each paired with a natural language description, covering 4.65 km$^2$ across two real cities: Cambridge and Birmingham. In contrast to existing datasets composed of synthetic scenes such as AerialVLN, our dataset presents a unique challenge because agents must interpret spatial relationships between real-world landmarks and the navigation destination, making CityNav an essential benchmark for advancing aerial VLN. Furthermore, as an initial step toward addressing this challenge, we provide a methodology of creating geographic semantic maps that can be used as an auxiliary modality input during navigation. In our experiments, we compare performance of three representative aerial VLN agents (Seq2seq, CMA and AerialVLN models) and demonstrate that the semantic map representation significantly improves their navigation performance.",
    "pdf_url": "https://arxiv.org/pdf/2406.14240v3",
    "github_url": null,
    "published": "2024-06-20T12:08:27+00:00",
    "updated": "2025-08-02T16:25:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.14103v2",
    "title": "Efficient Strategy Learning by Decoupling Searching and Pathfinding for Object Navigation",
    "authors": [
      "Zheng",
      "Feng",
      "Huang"
    ],
    "summary": "Inspired by human-like behaviors for navigation: first searching to explore unknown areas before discovering the target, and then the pathfinding of moving towards the discovered target, recent studies design parallel submodules to achieve different functions in the searching and pathfinding stages, while ignoring the differences in reward signals between the two stages. As a result, these models often cannot be fully trained or are overfitting on training scenes. Another bottleneck that restricts agents from learning two-stage strategies is spatial perception ability, since the studies used generic visual encoders without considering the depth information of navigation scenes. To release the potential of the model on strategy learning, we propose the Two-Stage Reward Mechanism (TSRM) for object navigation that decouples the searching and pathfinding behaviours in an episode, enabling the agent to explore larger area in searching stage and seek the optimal path in pathfinding stage. Also, we propose a pretraining method Depth Enhanced Masked Autoencoders (DE-MAE) that enables agent to determine explored and unexplored areas during the searching stage, locate target object and plan paths during the pathfinding stage more accurately. In addition, we propose a new metric of Searching Success weighted by Searching Path Length (SSSPL) that assesses agent's searching ability and exploring efficiency. Finally, we evaluated our method on AI2-Thor and RoboTHOR extensively and demonstrated it can outperform the state-of-the-art (SOTA) methods in both the success rate and the navigation efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2406.14103v2",
    "github_url": null,
    "published": "2024-06-20T08:35:10+00:00",
    "updated": "2025-07-22T02:17:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.13636v2",
    "title": "Contrast Sets for Evaluating Language-Guided Robot Policies",
    "authors": [
      "Anwar",
      "Gupta",
      "Thomason"
    ],
    "summary": "Robot evaluations in language-guided, real world settings are time-consuming and often sample only a small space of potential instructions across complex scenes. In this work, we introduce contrast sets for robotics as an approach to make small, but specific, perturbations to otherwise independent, identically distributed (i.i.d.) test instances. We investigate the relationship between experimenter effort to carry out an evaluation and the resulting estimated test performance as well as the insights that can be drawn from performance on perturbed instances. We use the relative performance change of different contrast set perturbations to characterize policies at reduced experimenter effort in both a simulated manipulation task and a physical robot vision-and-language navigation task. We encourage the use of contrast set evaluations as a more informative alternative to small scale, i.i.d. demonstrations on physical robots, and as a scalable alternative to industry-scale real world evaluations.",
    "pdf_url": "https://arxiv.org/pdf/2406.13636v2",
    "github_url": null,
    "published": "2024-06-19T15:31:21+00:00",
    "updated": "2024-10-25T15:23:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.11247v1",
    "title": "STEVE Series: Step-by-Step Construction of Agent Systems in Minecraft",
    "authors": [
      "Zhao",
      "Chai",
      "Wang"
    ],
    "summary": "Building an embodied agent system with a large language model (LLM) as its core is a promising direction. Due to the significant costs and uncontrollable factors associated with deploying and training such agents in the real world, we have decided to begin our exploration within the Minecraft environment. Our STEVE Series agents can complete basic tasks in a virtual environment and more challenging tasks such as navigation and even creative tasks, with an efficiency far exceeding previous state-of-the-art methods by a factor of $2.5\\times$ to $7.3\\times$. We begin our exploration with a vanilla large language model, augmenting it with a vision encoder and an action codebase trained on our collected high-quality dataset STEVE-21K. Subsequently, we enhanced it with a Critic and memory to transform it into a complex system. Finally, we constructed a hierarchical multi-agent system. Our recent work explored how to prune the agent system through knowledge distillation. In the future, we will explore more potential applications of STEVE agents in the real world.",
    "pdf_url": "https://arxiv.org/pdf/2406.11247v1",
    "github_url": null,
    "published": "2024-06-17T06:18:08+00:00",
    "updated": "2024-06-17T06:18:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.11047v1",
    "title": "Enhancing Supermarket Robot Interaction: A Multi-Level LLM Conversational Interface for Handling Diverse Customer Intents",
    "authors": [
      "Nandkumar",
      "Peternel"
    ],
    "summary": "This paper presents the design and evaluation of a novel multi-level LLM interface for supermarket robots to assist customers. The proposed interface allows customers to convey their needs through both generic and specific queries. While state-of-the-art systems like OpenAI's GPTs are highly adaptable and easy to build and deploy, they still face challenges such as increased response times and limitations in strategic control of the underlying model for tailored use-case and cost optimization. Driven by the goal of developing faster and more efficient conversational agents, this paper advocates for using multiple smaller, specialized LLMs fine-tuned to handle different user queries based on their specificity and user intent. We compare this approach to a specialized GPT model powered by GPT-4 Turbo, using the Artificial Social Agent Questionnaire (ASAQ) and qualitative participant feedback in a counterbalanced within-subjects experiment. Our findings show that our multi-LLM chatbot architecture outperformed the benchmarked GPT model across all 13 measured criteria, with statistically significant improvements in four key areas: performance, user satisfaction, user-agent partnership, and self-image enhancement. The paper also presents a method for supermarket robot navigation by mapping the final chatbot response to correct shelf numbers, enabling the robot to sequentially navigate towards the respective products, after which lower-level robot perception, control, and planning can be used for automated object retrieval. We hope this work encourages more efforts into using multiple, specialized smaller models instead of relying on a single powerful, but more expensive and slower model.",
    "pdf_url": "https://arxiv.org/pdf/2406.11047v1",
    "github_url": null,
    "published": "2024-06-16T19:13:01+00:00",
    "updated": "2024-06-16T19:13:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.10721v1",
    "title": "RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics",
    "authors": [
      "Yuan",
      "Duan",
      "Blukis"
    ],
    "summary": "From rearranging objects on a table to putting groceries into shelves, robots must plan precise action points to perform tasks accurately and reliably. In spite of the recent adoption of vision language models (VLMs) to control robot behavior, VLMs struggle to precisely articulate robot actions using language. We introduce an automatic synthetic data generation pipeline that instruction-tunes VLMs to robotic domains and needs. Using the pipeline, we train RoboPoint, a VLM that predicts image keypoint affordances given language instructions. Compared to alternative approaches, our method requires no real-world data collection or human demonstration, making it much more scalable to diverse environments and viewpoints. In addition, RoboPoint is a general model that enables several downstream applications such as robot navigation, manipulation, and augmented reality (AR) assistance. Our experiments demonstrate that RoboPoint outperforms state-of-the-art VLMs (GPT-4o) and visual prompting techniques (PIVOT) by 21.8% in the accuracy of predicting spatial affordance and by 30.5% in the success rate of downstream tasks. Project website: https://robo-point.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2406.10721v1",
    "github_url": null,
    "published": "2024-06-15T19:22:51+00:00",
    "updated": "2024-06-15T19:22:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.09798v3",
    "title": "Sim-to-Real Transfer via 3D Feature Fields for Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "Li",
      "Yang"
    ],
    "summary": "Vision-and-language navigation (VLN) enables the agent to navigate to a remote location in 3D environments following the natural language instruction. In this field, the agent is usually trained and evaluated in the navigation simulators, lacking effective approaches for sim-to-real transfer. The VLN agents with only a monocular camera exhibit extremely limited performance, while the mainstream VLN models trained with panoramic observation, perform better but are difficult to deploy on most monocular robots. For this case, we propose a sim-to-real transfer approach to endow the monocular robots with panoramic traversability perception and panoramic semantic understanding, thus smoothly transferring the high-performance panoramic VLN models to the common monocular robots. In this work, the semantic traversable map is proposed to predict agent-centric navigable waypoints, and the novel view representations of these navigable waypoints are predicted through the 3D feature fields. These methods broaden the limited field of view of the monocular robots and significantly improve navigation performance in the real world. Our VLN system outperforms previous SOTA monocular VLN methods in R2R-CE and RxR-CE benchmarks within the simulation environments and is also validated in real-world environments, providing a practical and high-performance solution for real-world VLN.",
    "pdf_url": "https://arxiv.org/pdf/2406.09798v3",
    "github_url": null,
    "published": "2024-06-14T07:50:09+00:00",
    "updated": "2024-10-14T04:48:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.05881v6",
    "title": "LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning",
    "authors": [
      "Singh",
      "Bhattacharyya",
      "Namboodiri"
    ],
    "summary": "Large language models (LLMs) have shown remarkable abilities in logical reasoning, in-context learning, and code generation. However, translating natural language instructions into effective robotic control policies remains a significant challenge, especially for tasks requiring long-horizon planning and operating under sparse reward conditions. Hierarchical Reinforcement Learning (HRL) provides a natural framework to address this challenge in robotics; however, it typically suffers from non-stationarity caused by the changing behavior of the lower-level policy during training, destabilizing higher-level policy learning. We introduce LGR2, a novel HRL framework that leverages LLMs to generate language-guided reward functions for the higher-level policy. By decoupling high-level reward generation from low-level policy changes, LGR2 fundamentally mitigates the non-stationarity problem in off-policy HRL, enabling stable and efficient learning. To further enhance sample efficiency in sparse environments, we integrate goal-conditioned hindsight experience relabeling. Extensive experiments across simulated and real-world robotic navigation and manipulation tasks demonstrate LGR2 outperforms both hierarchical and non-hierarchical baselines, achieving over 55% success rates on challenging tasks and robust transfer to real robots, without additional fine-tuning.",
    "pdf_url": "https://arxiv.org/pdf/2406.05881v6",
    "github_url": null,
    "published": "2024-06-09T18:40:24+00:00",
    "updated": "2025-08-27T17:57:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.05080v2",
    "title": "I2EDL: Interactive Instruction Error Detection and Localization",
    "authors": [
      "Taioli",
      "Rosa",
      "Castellini"
    ],
    "summary": "In the Vision-and-Language Navigation in Continuous Environments (VLN-CE) task, the human user guides an autonomous agent to reach a target goal via a series of low-level actions following a textual instruction in natural language. However, most existing methods do not address the likely case where users may make mistakes when providing such instruction (e.g. \"turn left\" instead of \"turn right\"). In this work, we address a novel task of Interactive VLN in Continuous Environments (IVLN-CE), which allows the agent to interact with the user during the VLN-CE navigation to verify any doubts regarding the instruction errors. We propose an Interactive Instruction Error Detector and Localizer (I2EDL) that triggers the user-agent interaction upon the detection of instruction errors during the navigation. We leverage a pre-trained module to detect instruction errors and pinpoint them in the instruction by cross-referencing the textual input and past observations. In such way, the agent is able to query the user for a timely correction, without demanding the user's cognitive load, as we locate the probable errors to a precise part of the instruction. We evaluate the proposed I2EDL on a dataset of instructions containing errors, and further devise a novel metric, the Success weighted by Interaction Number (SIN), to reflect both the navigation performance and the interaction effectiveness. We show how the proposed method can ask focused requests for corrections to the user, which in turn increases the navigation success, while minimizing the interactions.",
    "pdf_url": "https://arxiv.org/pdf/2406.05080v2",
    "github_url": null,
    "published": "2024-06-07T16:52:57+00:00",
    "updated": "2024-06-23T22:58:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.02208v1",
    "title": "Why Only Text: Empowering Vision-and-Language Navigation with Multi-modal Prompts",
    "authors": [
      "Hong",
      "Wang",
      "Huang"
    ],
    "summary": "Current Vision-and-Language Navigation (VLN) tasks mainly employ textual instructions to guide agents. However, being inherently abstract, the same textual instruction can be associated with different visual signals, causing severe ambiguity and limiting the transfer of prior knowledge in the vision domain from the user to the agent. To fill this gap, we propose Vision-and-Language Navigation with Multi-modal Prompts (VLN-MP), a novel task augmenting traditional VLN by integrating both natural language and images in instructions. VLN-MP not only maintains backward compatibility by effectively handling text-only prompts but also consistently shows advantages with different quantities and relevance of visual prompts. Possible forms of visual prompts include both exact and similar object images, providing adaptability and versatility in diverse navigation scenarios. To evaluate VLN-MP under a unified framework, we implement a new benchmark that offers: (1) a training-free pipeline to transform textual instructions into multi-modal forms with landmark images; (2) diverse datasets with multi-modal instructions for different downstream tasks; (3) a novel module designed to process various image prompts for seamless integration with state-of-the-art VLN models. Extensive experiments on four VLN benchmarks (R2R, RxR, REVERIE, CVDN) show that incorporating visual prompts significantly boosts navigation performance. While maintaining efficiency with text-only prompts, VLN-MP enables agents to navigate in the pre-explore setting and outperform text-based models, showing its broader applicability.",
    "pdf_url": "https://arxiv.org/pdf/2406.02208v1",
    "github_url": null,
    "published": "2024-06-04T11:06:13+00:00",
    "updated": "2024-06-04T11:06:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.01797v1",
    "title": "The Empirical Impact of Forgetting and Transfer in Continual Visual Odometry",
    "authors": [
      "Cudrano",
      "Luo",
      "Matteucci"
    ],
    "summary": "As robotics continues to advance, the need for adaptive and continuously-learning embodied agents increases, particularly in the realm of assistance robotics. Quick adaptability and long-term information retention are essential to operate in dynamic environments typical of humans' everyday lives. A lifelong learning paradigm is thus required, but it is scarcely addressed by current robotics literature. This study empirically investigates the impact of catastrophic forgetting and the effectiveness of knowledge transfer in neural networks trained continuously in an embodied setting. We focus on the task of visual odometry, which holds primary importance for embodied agents in enabling their self-localization. We experiment on the simple continual scenario of discrete transitions between indoor locations, akin to a robot navigating different apartments. In this regime, we observe initial satisfactory performance with high transferability between environments, followed by a specialization phase where the model prioritizes current environment-specific knowledge at the expense of generalization. Conventional regularization strategies and increased model capacity prove ineffective in mitigating this phenomenon. Rehearsal is instead mildly beneficial but with the addition of a substantial memory cost. Incorporating action information, as commonly done in embodied settings, facilitates quicker convergence but exacerbates specialization, making the model overly reliant on its motion expectations and less adept at correctly interpreting visual cues. These findings emphasize the open challenges of balancing adaptation and memory retention in lifelong robotics and contribute valuable insights into the application of a lifelong paradigm on embodied agents.",
    "pdf_url": "https://arxiv.org/pdf/2406.01797v1",
    "github_url": null,
    "published": "2024-06-03T21:32:50+00:00",
    "updated": "2024-06-03T21:32:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.01256v1",
    "title": "Augmented Commonsense Knowledge for Remote Object Grounding",
    "authors": [
      "Mohammadi",
      "Hong",
      "Qi"
    ],
    "summary": "The vision-and-language navigation (VLN) task necessitates an agent to perceive the surroundings, follow natural language instructions, and act in photo-realistic unseen environments. Most of the existing methods employ the entire image or object features to represent navigable viewpoints. However, these representations are insufficient for proper action prediction, especially for the REVERIE task, which uses concise high-level instructions, such as ''Bring me the blue cushion in the master bedroom''. To address enhancing representation, we propose an augmented commonsense knowledge model (ACK) to leverage commonsense information as a spatio-temporal knowledge graph for improving agent navigation. Specifically, the proposed approach involves constructing a knowledge base by retrieving commonsense information from ConceptNet, followed by a refinement module to remove noisy and irrelevant knowledge. We further present ACK which consists of knowledge graph-aware cross-modal and concept aggregation modules to enhance visual representation and visual-textual data alignment by integrating visible objects, commonsense knowledge, and concept history, which includes object and knowledge temporal information. Moreover, we add a new pipeline for the commonsense-based decision-making process which leads to more accurate local action prediction. Experimental results demonstrate our proposed model noticeably outperforms the baseline and archives the state-of-the-art on the REVERIE benchmark.",
    "pdf_url": "https://arxiv.org/pdf/2406.01256v1",
    "github_url": null,
    "published": "2024-06-03T12:12:33+00:00",
    "updated": "2024-06-03T12:12:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.00375v1",
    "title": "Teledrive: An Embodied AI based Telepresence System",
    "authors": [
      "Banerjee",
      "Paul",
      "Roychoudhury"
    ],
    "summary": "This article presents Teledrive, a telepresence robotic system with embodied AI features that empowers an operator to navigate the telerobot in any unknown remote place with minimal human intervention. We conceive Teledrive in the context of democratizing remote care-giving for elderly citizens as well as for isolated patients, affected by contagious diseases. In particular, this paper focuses on the problem of navigating to a rough target area (like bedroom or kitchen) rather than pre-specified point destinations. This ushers in a unique AreaGoal based navigation feature, which has not been explored in depth in the contemporary solutions. Further, we describe an edge computing-based software system built on a WebRTC-based communication framework to realize the aforementioned scheme through an easy-to-use speech-based human-robot interaction. Moreover, to enhance the ease of operation for the remote caregiver, we incorporate a person following feature, whereby a robot follows a person on the move in its premises as directed by the operator. Moreover, the system presented is loosely coupled with specific robot hardware, unlike the existing solutions. We have evaluated the efficacy of the proposed system through baseline experiments, user study, and real-life deployment.",
    "pdf_url": "https://arxiv.org/pdf/2406.00375v1",
    "github_url": null,
    "published": "2024-06-01T09:27:42+00:00",
    "updated": "2024-06-01T09:27:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.18937v2",
    "title": "Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description",
    "authors": [
      "Ahmed",
      "Fei",
      "Ding"
    ],
    "summary": "In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a challenging task aimed at advancing 3D multimodal learning for fine-grained, part-aware segmentation grounding and detailed explanation of 3D objects. Existing 3D datasets largely focus on either vision-only part segmentation or vision-language scene segmentation, lacking the fine-grained multimodal segmentation needed for robotic navigation and interaction in real-world environments. To address this gap, we present the 3DCoMPaT Grounded Instructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich point cloud descriptions with corresponding part-level segmentation masks. This dataset encompasses extensive samples designed for both PaPGD and fine-grained single-part grounding tasks. To tackle the inherent challenges of grounding objects and generating grounded descriptions at the part level, we propose Kestrel, a part-aware 3D multimodal large language model that integrates an advanced language model for nuanced language comprehension with multi-level point feature propagation and query refinement mechanism to enhance spatial reasoning at the part level. The extensive experiments demonstrate that Kestrel effectively bridges the gap between part-aware language understanding and 3D segmentation grounding, paving the way for more robust and interpretable 3D object comprehension that meets the demands of real-world robotic applications. Project page at https://feielysia.github.io/Kestrel.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2405.18937v2",
    "github_url": null,
    "published": "2024-05-29T09:43:48+00:00",
    "updated": "2025-08-04T13:54:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.18721v2",
    "title": "Correctable Landmark Discovery via Large Models for Vision-Language Navigation",
    "authors": [
      "Lin",
      "Nie",
      "Wei"
    ],
    "summary": "Vision-Language Navigation (VLN) requires the agent to follow language instructions to reach a target position. A key factor for successful navigation is to align the landmarks implied in the instruction with diverse visual observations. However, previous VLN agents fail to perform accurate modality alignment especially in unexplored scenes, since they learn from limited navigation data and lack sufficient open-world alignment knowledge. In this work, we propose a new VLN paradigm, called COrrectable LaNdmark DiScOvery via Large ModEls (CONSOLE). In CONSOLE, we cast VLN as an open-world sequential landmark discovery problem, by introducing a novel correctable landmark discovery scheme based on two large models ChatGPT and CLIP. Specifically, we use ChatGPT to provide rich open-world landmark cooccurrence commonsense, and conduct CLIP-driven landmark discovery based on these commonsense priors. To mitigate the noise in the priors due to the lack of visual constraints, we introduce a learnable cooccurrence scoring module, which corrects the importance of each cooccurrence according to actual observations for accurate landmark discovery. We further design an observation enhancement strategy for an elegant combination of our framework with different VLN agents, where we utilize the corrected landmark features to obtain enhanced observation features for action decision. Extensive experimental results on multiple popular VLN benchmarks (R2R, REVERIE, R4R, RxR) show the significant superiority of CONSOLE over strong baselines. Especially, our CONSOLE establishes the new state-of-the-art results on R2R and R4R in unseen scenarios. Code is available at https://github.com/expectorlin/CONSOLE.",
    "pdf_url": "https://arxiv.org/pdf/2405.18721v2",
    "github_url": "https://github.com/expectorlin/CONSOLE",
    "published": "2024-05-29T03:05:59+00:00",
    "updated": "2024-06-05T09:59:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.16994v1",
    "title": "Vision-and-Language Navigation Generative Pretrained Transformer",
    "authors": [
      "Hanlin"
    ],
    "summary": "In the Vision-and-Language Navigation (VLN) field, agents are tasked with navigating real-world scenes guided by linguistic instructions. Enabling the agent to adhere to instructions throughout the process of navigation represents a significant challenge within the domain of VLN. To address this challenge, common approaches often rely on encoders to explicitly record past locations and actions, increasing model complexity and resource consumption. Our proposal, the Vision-and-Language Navigation Generative Pretrained Transformer (VLN-GPT), adopts a transformer decoder model (GPT2) to model trajectory sequence dependencies, bypassing the need for historical encoding modules. This method allows for direct historical information access through trajectory sequence, enhancing efficiency. Furthermore, our model separates the training process into offline pre-training with imitation learning and online fine-tuning with reinforcement learning. This distinction allows for more focused training objectives and improved performance. Performance assessments on the VLN dataset reveal that VLN-GPT surpasses complex state-of-the-art encoder-based models.",
    "pdf_url": "https://arxiv.org/pdf/2405.16994v1",
    "github_url": null,
    "published": "2024-05-27T09:42:04+00:00",
    "updated": "2024-05-27T09:42:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.16818v1",
    "title": "Advancing Behavior Generation in Mobile Robotics through High-Fidelity Procedural Simulations",
    "authors": [
      "Kich",
      "Bottega",
      "Steinmetz"
    ],
    "summary": "This paper introduces YamaS, a simulator integrating Unity3D Engine with Robotic Operating System for robot navigation research and aims to facilitate the development of both Deep Reinforcement Learning (Deep-RL) and Natural Language Processing (NLP). It supports single and multi-agent configurations with features like procedural environment generation, RGB vision, and dynamic obstacle navigation. Unique to YamaS is its ability to construct single and multi-agent environments, as well as generating agent's behaviour through textual descriptions. The simulator's fidelity is underscored by comparisons with the real-world Yamabiko Beego robot, demonstrating high accuracy in sensor simulations and spatial reasoning. Moreover, YamaS integrates Virtual Reality (VR) to augment Human-Robot Interaction (HRI) studies, providing an immersive platform for developers and researchers. This fusion establishes YamaS as a versatile and valuable tool for the development and testing of autonomous systems, contributing to the fields of robot simulation and AI-driven training methodologies.",
    "pdf_url": "https://arxiv.org/pdf/2405.16818v1",
    "github_url": null,
    "published": "2024-05-27T04:31:55+00:00",
    "updated": "2024-05-27T04:31:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.16439v3",
    "title": "Multi-Agent Inverse Reinforcement Learning in Real World Unstructured Pedestrian Crowds",
    "authors": [
      "Chandra",
      "Karnan",
      "Mehr"
    ],
    "summary": "Social robot navigation in crowded public spaces such as university campuses, restaurants, grocery stores, and hospitals, is an increasingly important area of research. One of the core strategies for achieving this goal is to understand humans' intent--underlying psychological factors that govern their motion--by learning their reward functions, typically via inverse reinforcement learning (IRL). Despite significant progress in IRL, learning reward functions of multiple agents simultaneously in dense unstructured pedestrian crowds has remained intractable due to the nature of the tightly coupled social interactions that occur in these scenarios \\textit{e.g.} passing, intersections, swerving, weaving, etc. In this paper, we present a new multi-agent maximum entropy inverse reinforcement learning algorithm for real world unstructured pedestrian crowds. Key to our approach is a simple, but effective, mathematical trick which we name the so-called tractability-rationality trade-off trick that achieves tractability at the cost of a slight reduction in accuracy. We compare our approach to the classical single-agent MaxEnt IRL as well as state-of-the-art trajectory prediction methods on several datasets including the ETH, UCY, SCAND, JRDB, and a new dataset, called Speedway, collected at a busy intersection on a University campus focusing on dense, complex agent interactions. Our key findings show that, on the dense Speedway dataset, our approach ranks 1st among top 7 baselines with >2X improvement over single-agent IRL, and is competitive with state-of-the-art large transformer-based encoder-decoder models on sparser datasets such as ETH/UCY (ranks 3rd among top 7 baselines).",
    "pdf_url": "https://arxiv.org/pdf/2405.16439v3",
    "github_url": null,
    "published": "2024-05-26T05:48:21+00:00",
    "updated": "2025-03-26T21:19:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.15222v2",
    "title": "Leveraging Unknown Objects to Construct Labeled-Unlabeled Meta-Relationships for Zero-Shot Object Navigation",
    "authors": [
      "Zheng",
      "Li",
      "Lan"
    ],
    "summary": "Zero-shot object navigation (ZSON) addresses situation where an agent navigates to an unseen object that does not present in the training set. Previous works mainly train agent using seen objects with known labels, and ignore the seen objects without labels. In this paper, we introduce seen objects without labels, herein termed as ``unknown objects'', into training procedure to enrich the agent's knowledge base with distinguishable but previously overlooked information. Furthermore, we propose the label-wise meta-correlation module (LWMCM) to harness relationships among objects with and without labels, and obtain enhanced objects information. Specially, we propose target feature generator (TFG) to generate the features representation of the unlabeled target objects. Subsequently, the unlabeled object identifier (UOI) module assesses whether the unlabeled target object appears in the current observation frame captured by the camera and produces an adapted target features representation specific to the observed context. In meta contrastive feature modifier (MCFM), the target features is modified via approaching the features of objects within the observation frame while distancing itself from features of unobserved objects. Finally, the meta object-graph learner (MOGL) module is utilized to calculate the relationships among objects based on the features. Experiments conducted on AI2THOR and RoboTHOR platforms demonstrate the effectiveness of our proposed method.",
    "pdf_url": "https://arxiv.org/pdf/2405.15222v2",
    "github_url": null,
    "published": "2024-05-24T05:26:18+00:00",
    "updated": "2024-05-27T02:39:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.14154v4",
    "title": "Skip-SCAR: Hardware-Friendly High-Quality Embodied Visual Navigation",
    "authors": [
      "Liu",
      "Cao",
      "Zhang"
    ],
    "summary": "In ObjectNav, agents must locate specific objects within unseen environments, requiring effective perception, prediction, localization and planning capabilities. This study finds that state-of-the-art embodied AI agents compete for higher navigation quality, but often compromise the computational efficiency. To address this issue, we introduce \"Skip-SCAR,\" an optimization framework that builds computationally and memory-efficient embodied AI agents to accomplish high-quality visual navigation tasks. Skip-SCAR opportunistically skips the redundant step computations during semantic segmentation and local re-planning without hurting the navigation quality. Skip-SCAR also adopts a novel hybrid sparse and dense network for object prediction, optimizing both the computation and memory footprint. Tested on the HM3D ObjectNav datasets and real-world physical hardware systems, Skip-SCAR not only minimizes hardware resources but also sets new performance benchmarks, demonstrating the benefits of optimizing both navigation quality and computational efficiency for robotics.",
    "pdf_url": "https://arxiv.org/pdf/2405.14154v4",
    "github_url": null,
    "published": "2024-05-23T04:03:39+00:00",
    "updated": "2024-12-07T05:10:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.11410v2",
    "title": "Characterizing the Complexity of Social Robot Navigation Scenarios",
    "authors": [
      "Stratton",
      "Hauser",
      "Mavrogiannis"
    ],
    "summary": "Social robot navigation algorithms are often demonstrated in overly simplified scenarios, prohibiting the extraction of practical insights about their relevance to real-world domains. Our key insight is that an understanding of the inherent complexity of a social robot navigation scenario could help characterize the limitations of existing navigation algorithms and provide actionable directions for improvement. Through an exploration of recent literature, we identify a series of factors contributing to the complexity of a scenario, disambiguating between contextual and robot-related ones. We then conduct a simulation study investigating how manipulations of contextual factors impact the performance of a variety of navigation algorithms. We find that dense and narrow environments correlate most strongly with performance drops, while the heterogeneity of agent policies and directionality of interactions have a less pronounced effect. Our findings motivate a shift towards developing and testing algorithms under higher-complexity settings.",
    "pdf_url": "https://arxiv.org/pdf/2405.11410v2",
    "github_url": null,
    "published": "2024-05-18T23:03:22+00:00",
    "updated": "2024-12-10T16:08:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.10620v2",
    "title": "MC-GPT: Empowering Vision-and-Language Navigation with Memory Map and Reasoning Chains",
    "authors": [
      "Zhan",
      "Yu",
      "Yu"
    ],
    "summary": "In the Vision-and-Language Navigation (VLN) task, the agent is required to navigate to a destination following a natural language instruction. While learning-based approaches have been a major solution to the task, they suffer from high training costs and lack of interpretability. Recently, Large Language Models (LLMs) have emerged as a promising tool for VLN due to their strong generalization capabilities. However, existing LLM-based methods face limitations in memory construction and diversity of navigation strategies. To address these challenges, we propose a suite of techniques. Firstly, we introduce a method to maintain a topological map that stores navigation history, retaining information about viewpoints, objects, and their spatial relationships. This map also serves as a global action space. Additionally, we present a Navigation Chain of Thoughts module, leveraging human navigation examples to enrich navigation strategy diversity. Finally, we establish a pipeline that integrates navigational memory and strategies with perception and action prediction modules. Experimental results on the REVERIE and R2R datasets show that our method effectively enhances the navigation ability of the LLM and improves the interpretability of navigation reasoning.",
    "pdf_url": "https://arxiv.org/pdf/2405.10620v2",
    "github_url": null,
    "published": "2024-05-17T08:33:27+00:00",
    "updated": "2024-08-12T14:07:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.07060v1",
    "title": "Memory-Maze: Scenario Driven Benchmark and Visual Language Navigation Model for Guiding Blind People",
    "authors": [
      "Kuribayashi",
      "Uehara",
      "Wang"
    ],
    "summary": "Visual Language Navigation (VLN) powered navigation robots have the potential to guide blind people by understanding and executing route instructions provided by sighted passersby. This capability allows robots to operate in environments that are often unknown a priori. Existing VLN models are insufficient for the scenario of navigation guidance for blind people, as they need to understand routes described from human memory, which frequently contain stutters, errors, and omission of details as opposed to those obtained by thinking out loud, such as in the Room-to-Room dataset. However, currently, there is no benchmark that simulates instructions that were obtained from human memory in environments where blind people navigate. To this end, we present our benchmark, Memory-Maze, which simulates the scenario of seeking route instructions for guiding blind people. Our benchmark contains a maze-like structured virtual environment and novel route instruction data from human memory. To collect natural language instructions, we conducted two studies from sighted passersby onsite and annotators online. Our analysis demonstrates that instructions data collected onsite were more lengthy and contained more varied wording. Alongside our benchmark, we propose a VLN model better equipped to handle the scenario. Our proposed VLN model uses Large Language Models (LLM) to parse instructions and generate Python codes for robot control. We further show that the existing state-of-the-art model performed suboptimally on our benchmark. In contrast, our proposed method outperformed the state-of-the-art model by a fair margin. We found that future research should exercise caution when considering VLN technology for practical applications, as real-world scenarios have different characteristics than ones collected in traditional settings.",
    "pdf_url": "https://arxiv.org/pdf/2405.07060v1",
    "github_url": null,
    "published": "2024-05-11T17:25:23+00:00",
    "updated": "2024-05-11T17:25:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.05852v1",
    "title": "Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control",
    "authors": [
      "Gupta",
      "Yadav",
      "Gal"
    ],
    "summary": "Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark.",
    "pdf_url": "https://arxiv.org/pdf/2405.05852v1",
    "github_url": null,
    "published": "2024-05-09T15:39:54+00:00",
    "updated": "2024-05-09T15:39:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.05363v1",
    "title": "LOC-ZSON: Language-driven Object-Centric Zero-Shot Object Retrieval and Navigation",
    "authors": [
      "Guan",
      "Yang",
      "Cheng"
    ],
    "summary": "In this paper, we present LOC-ZSON, a novel Language-driven Object-Centric image representation for object navigation task within complex scenes. We propose an object-centric image representation and corresponding losses for visual-language model (VLM) fine-tuning, which can handle complex object-level queries. In addition, we design a novel LLM-based augmentation and prompt templates for stability during training and zero-shot inference. We implement our method on Astro robot and deploy it in both simulated and real-world environments for zero-shot object navigation. We show that our proposed method can achieve an improvement of 1.38 - 13.38% in terms of text-to-image recall on different benchmark settings for the retrieval task. For object navigation, we show the benefit of our approach in simulation and real world, showing 5% and 16.67% improvement in terms of navigation success rate, respectively.",
    "pdf_url": "https://arxiv.org/pdf/2405.05363v1",
    "github_url": null,
    "published": "2024-05-08T18:45:37+00:00",
    "updated": "2024-05-08T18:45:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.04235v2",
    "title": "LTLDoG: Satisfying Temporally-Extended Symbolic Constraints for Safe Diffusion-based Planning",
    "authors": [
      "Feng",
      "Luan",
      "Goyal"
    ],
    "summary": "Operating effectively in complex environments while complying with specified constraints is crucial for the safe and successful deployment of robots that interact with and operate around people. In this work, we focus on generating long-horizon trajectories that adhere to novel static and temporally-extended constraints/instructions at test time. We propose a data-driven diffusion-based framework, LTLDoG, that modifies the inference steps of the reverse process given an instruction specified using finite linear temporal logic ($\\text{LTL}_f$). LTLDoG leverages a satisfaction value function on $\\text{LTL}_f$ and guides the sampling steps using its gradient field. This value function can also be trained to generalize to new instructions not observed during training, enabling flexible test-time adaptability. Experiments in robot navigation and manipulation illustrate that the method is able to generate trajectories that satisfy formulae that specify obstacle avoidance and visitation sequences. Code and supplementary material are available online at https://github.com/clear-nus/ltldog.",
    "pdf_url": "https://arxiv.org/pdf/2405.04235v2",
    "github_url": "https://github.com/clear-nus/ltldog",
    "published": "2024-05-07T11:54:22+00:00",
    "updated": "2024-09-30T08:42:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.16006v1",
    "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI",
    "authors": [
      "Ying",
      "Meng",
      "Wang"
    ],
    "summary": "Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $30$ LVLMs such as the proprietary GPT-4V, GeminiProVision, and open-sourced InternVL-Chat, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.",
    "pdf_url": "https://arxiv.org/pdf/2404.16006v1",
    "github_url": null,
    "published": "2024-04-24T17:37:05+00:00",
    "updated": "2024-04-24T17:37:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.10241v1",
    "title": "Vision-and-Language Navigation via Causal Learning",
    "authors": [
      "Wang",
      "He",
      "Dang"
    ],
    "summary": "In the pursuit of robust and generalizable environment perception and language understanding, the ubiquitous challenge of dataset bias continues to plague vision-and-language navigation (VLN) agents, hindering their performance in unseen environments. This paper introduces the generalized cross-modal causal transformer (GOAT), a pioneering solution rooted in the paradigm of causal inference. By delving into both observable and unobservable confounders within vision, language, and history, we propose the back-door and front-door adjustment causal learning (BACL and FACL) modules to promote unbiased learning by comprehensively mitigating potential spurious correlations. Additionally, to capture global confounder features, we propose a cross-modal feature pooling (CFP) module supervised by contrastive learning, which is also shown to be effective in improving cross-modal representations during pre-training. Extensive experiments across multiple VLN datasets (R2R, REVERIE, RxR, and SOON) underscore the superiority of our proposed method over previous state-of-the-art approaches. Code is available at https://github.com/CrystalSixone/VLN-GOAT.",
    "pdf_url": "https://arxiv.org/pdf/2404.10241v1",
    "github_url": "https://github.com/CrystalSixone/VLN-GOAT",
    "published": "2024-04-16T02:40:35+00:00",
    "updated": "2024-04-16T02:40:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.10220v2",
    "title": "Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V",
    "authors": [
      "Zhi",
      "Zhang",
      "Zhao"
    ],
    "summary": "Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. In this work, we present COME-robot, the first closed-loop robotic system utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios.COME-robot incorporates two key innovative modules: (i) a multi-level open-vocabulary perception and situated reasoning module that enables effective exploration of the 3D environment and target object identification using commonsense knowledge and situated information, and (ii) an iterative closed-loop feedback and restoration mechanism that verifies task feasibility, monitors execution success, and traces failure causes across different modules for robust failure recovery. Through comprehensive experiments involving 8 challenging real-world mobile and tabletop manipulation tasks, COME-robot demonstrates a significant improvement in task success rate (~35%) compared to state-of-the-art methods. We further conduct comprehensive analyses to elucidate how COME-robot's design facilitates failure recovery, free-form instruction following, and long-horizon task planning.",
    "pdf_url": "https://arxiv.org/pdf/2404.10220v2",
    "github_url": null,
    "published": "2024-04-16T02:01:56+00:00",
    "updated": "2025-03-07T05:09:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.10054v1",
    "title": "AIGeN: An Adversarial Approach for Instruction Generation in VLN",
    "authors": [
      "Rawal",
      "Bigazzi",
      "Baraldi"
    ],
    "summary": "In the last few years, the research interest in Vision-and-Language Navigation (VLN) has grown significantly. VLN is a challenging task that involves an agent following human instructions and navigating in a previously unknown environment to reach a specified goal. Recent work in literature focuses on different ways to augment the available datasets of instructions for improving navigation performance by exploiting synthetic training data. In this work, we propose AIGeN, a novel architecture inspired by Generative Adversarial Networks (GANs) that produces meaningful and well-formed synthetic instructions to improve navigation agents' performance. The model is composed of a Transformer decoder (GPT-2) and a Transformer encoder (BERT). During the training phase, the decoder generates sentences for a sequence of images describing the agent's path to a particular point while the encoder discriminates between real and fake instructions. Experimentally, we evaluate the quality of the generated instructions and perform extensive ablation studies. Additionally, we generate synthetic instructions for 217K trajectories using AIGeN on Habitat-Matterport 3D Dataset (HM3D) and show an improvement in the performance of an off-the-shelf VLN method. The validation analysis of our proposal is conducted on REVERIE and R2R and highlights the promising aspects of our proposal, achieving state-of-the-art performance.",
    "pdf_url": "https://arxiv.org/pdf/2404.10054v1",
    "github_url": null,
    "published": "2024-04-15T18:00:30+00:00",
    "updated": "2024-04-15T18:00:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.08353v2",
    "title": "TDANet: Target-Directed Attention Network For Object-Goal Visual Navigation With Zero-Shot Ability",
    "authors": [
      "Lian",
      "Zhang"
    ],
    "summary": "The generalization of the end-to-end deep reinforcement learning (DRL) for object-goal visual navigation is a long-standing challenge since object classes and placements vary in new test environments. Learning domain-independent visual representation is critical for enabling the trained DRL agent with the ability to generalize to unseen scenes and objects. In this letter, a target-directed attention network (TDANet) is proposed to learn the end-to-end object-goal visual navigation policy with zero-shot ability. TDANet features a novel target attention (TA) module that learns both the spatial and semantic relationships among objects to help TDANet focus on the most relevant observed objects to the target. With the Siamese architecture (SA) design, TDANet distinguishes the difference between the current and target states and generates the domain-independent visual representation. To evaluate the navigation performance of TDANet, extensive experiments are conducted in the AI2-THOR embodied AI environment. The simulation results demonstrate a strong generalization ability of TDANet to unseen scenes and target objects, with higher navigation success rate (SR) and success weighted by length (SPL) than other state-of-the-art models. TDANet is finally deployed on a wheeled robot in real scenes, demonstrating satisfactory generalization of TDANet to the real world.",
    "pdf_url": "https://arxiv.org/pdf/2404.08353v2",
    "github_url": null,
    "published": "2024-04-12T09:44:18+00:00",
    "updated": "2024-08-12T07:20:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.07447v1",
    "title": "Interactive-FAR:Interactive, Fast and Adaptable Routing for Navigation Among Movable Obstacles in Complex Unknown Environments",
    "authors": [
      "He",
      "Chen",
      "Wang"
    ],
    "summary": "This paper introduces a real-time algorithm for navigating complex unknown environments cluttered with movable obstacles. Our algorithm achieves fast, adaptable routing by actively attempting to manipulate obstacles during path planning and adjusting the global plan from sensor feedback. The main contributions include an improved dynamic Directed Visibility Graph (DV-graph) for rapid global path searching, a real-time interaction planning method that adapts online from new sensory perceptions, and a comprehensive framework designed for interactive navigation in complex unknown or partially known environments. Our algorithm is capable of replanning the global path in several milliseconds. It can also attempt to move obstacles, update their affordances, and adapt strategies accordingly. Extensive experiments validate that our algorithm reduces the travel time by 33%, achieves up to 49% higher path efficiency, and runs faster than traditional methods by orders of magnitude in complex environments. It has been demonstrated to be the most efficient solution in terms of speed and efficiency for interactive navigation in environments of such complexity. We also open-source our code in the docker demo to facilitate future research.",
    "pdf_url": "https://arxiv.org/pdf/2404.07447v1",
    "github_url": null,
    "published": "2024-04-11T03:04:59+00:00",
    "updated": "2024-04-11T03:04:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.06609v1",
    "title": "GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation",
    "authors": [
      "Khanna",
      "Ramrakhya",
      "Chhablani"
    ],
    "summary": "The Embodied AI community has made significant strides in visual navigation tasks, exploring targets from 3D coordinates, objects, language descriptions, and images. However, these navigation models often handle only a single input modality as the target. With the progress achieved so far, it is time to move towards universal navigation models capable of handling various goal types, enabling more effective user interaction with robots. To facilitate this goal, we propose GOAT-Bench, a benchmark for the universal navigation task referred to as GO to AnyThing (GOAT). In this task, the agent is directed to navigate to a sequence of targets specified by the category name, language description, or image in an open-vocabulary fashion. We benchmark monolithic RL and modular methods on the GOAT task, analyzing their performance across modalities, the role of explicit and implicit scene memories, their robustness to noise in goal specifications, and the impact of memory in lifelong scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2404.06609v1",
    "github_url": null,
    "published": "2024-04-09T20:40:00+00:00",
    "updated": "2024-04-09T20:40:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.05203v1",
    "title": "MeSA-DRL: Memory-Enhanced Deep Reinforcement Learning for Advanced Socially Aware Robot Navigation in Crowded Environments",
    "authors": [
      "Muhammad",
      "Montero"
    ],
    "summary": "Autonomous navigation capabilities play a critical role in service robots operating in environments where human interactions are pivotal, due to the dynamic and unpredictable nature of these environments. However, the variability in human behavior presents a substantial challenge for robots in predicting and anticipating movements, particularly in crowded scenarios. To address this issue, a memory-enabled deep reinforcement learning framework is proposed for autonomous robot navigation in diverse pedestrian scenarios. The proposed framework leverages long-term memory to retain essential information about the surroundings and model sequential dependencies effectively. The importance of human-robot interactions is also encoded to assign higher attention to these interactions. A global planning mechanism is incorporated into the memory-enabled architecture. Additionally, a multi-term reward system is designed to prioritize and encourage long-sighted robot behaviors by incorporating dynamic warning zones. Simultaneously, it promotes smooth trajectories and minimizes the time taken to reach the robot's desired goal. Extensive simulation experiments show that the suggested approach outperforms representative state-of-the-art methods, showcasing its ability to a navigation efficiency and safety in real-world scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2404.05203v1",
    "github_url": null,
    "published": "2024-04-08T05:10:35+00:00",
    "updated": "2024-04-08T05:10:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.04069v2",
    "title": "Bidirectional Human Interactive AI Framework for Social Robot Navigation",
    "authors": [
      "Girgin",
      "Girgin",
      "Yildirim"
    ],
    "summary": "Trustworthiness is a crucial concept in the context of human-robot interaction. Cooperative robots must be transparent regarding their decision-making process, especially when operating in a human-oriented environment. This paper presents a comprehensive end-to-end framework aimed at fostering trustworthy bidirectional human-robot interaction in collaborative environments for the social navigation of mobile robots. In this framework, the robot communicates verbally while the human guides with gestures. Our method enables a mobile robot to predict the trajectory of people and adjust its route in a socially-aware manner. In case of conflict between human and robot decisions, detected through visual examination, the route is dynamically modified based on human preference while verbal communication is maintained. We present our pipeline, framework design, and preliminary experiments that form the foundation of our proposition.",
    "pdf_url": "https://arxiv.org/pdf/2404.04069v2",
    "github_url": null,
    "published": "2024-04-05T12:52:17+00:00",
    "updated": "2024-05-04T14:14:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.03049v1",
    "title": "Language, Environment, and Robotic Navigation",
    "authors": [
      "Avery"
    ],
    "summary": "This paper explores the integration of linguistic inputs within robotic navigation systems, drawing upon the symbol interdependency hypothesis to bridge the divide between symbolic and embodied cognition. It examines previous work incorporating language and semantics into Neural Network (NN) and Simultaneous Localization and Mapping (SLAM) approaches, highlighting how these integrations have advanced the field. By contrasting abstract symbol manipulation with sensory-motor grounding, we propose a unified framework where language functions both as an abstract communicative system and as a grounded representation of perceptual experiences. Our review of cognitive models of distributional semantics and their application to autonomous agents underscores the transformative potential of language-integrated systems.",
    "pdf_url": "https://arxiv.org/pdf/2404.03049v1",
    "github_url": null,
    "published": "2024-04-03T20:30:38+00:00",
    "updated": "2024-04-03T20:30:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.02294v1",
    "title": "Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs",
    "authors": [
      "Lotfi",
      "Faraji",
      "Kakodkar"
    ],
    "summary": "This paper explores leveraging large language models for map-free off-road navigation using generative AI, reducing the need for traditional data collection and annotation. We propose a method where a robot receives verbal instructions, converted to text through Whisper, and a large language model (LLM) model extracts landmarks, preferred terrains, and crucial adverbs translated into speed settings for constrained navigation. A language-driven semantic segmentation model generates text-based masks for identifying landmarks and terrain types in images. By translating 2D image points to the vehicle's motion plane using camera parameters, an MPC controller can guides the vehicle towards the desired terrain. This approach enhances adaptation to diverse environments and facilitates the use of high-level instructions for navigating complex and challenging terrains.",
    "pdf_url": "https://arxiv.org/pdf/2404.02294v1",
    "github_url": null,
    "published": "2024-04-02T20:46:13+00:00",
    "updated": "2024-04-02T20:46:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.01994v1",
    "title": "DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning",
    "authors": [
      "Du",
      "Wu",
      "Zhang"
    ],
    "summary": "Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision. To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations. We also reconstruct a dual-level instruction for adaptation to the dual-level alignment. As the training signals for pre-fusion alignment are extremely limited, self-supervised contrastive learning strategies are employed to enforce the matching between different modalities. Our approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN benchmarks, including R2R, R4R, RxR and CVDN.",
    "pdf_url": "https://arxiv.org/pdf/2404.01994v1",
    "github_url": null,
    "published": "2024-04-02T14:40:04+00:00",
    "updated": "2024-04-02T14:40:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.01943v1",
    "title": "Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation",
    "authors": [
      "Wang",
      "Li",
      "Yang"
    ],
    "summary": "Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. At each navigation step, the agent selects from possible candidate locations and then makes the move. For better navigation planning, the lookahead exploration strategy aims to effectively evaluate the agent's next action by accurately anticipating the future environment of candidate locations. To this end, some existing works predict RGB images for future environments, while this strategy suffers from image distortion and high computational cost. To address these issues, we propose the pre-trained hierarchical neural radiance representation model (HNR) to produce multi-level semantic features for future environments, which are more robust and efficient than pixel-wise RGB reconstruction. Furthermore, with the predicted future environmental representations, our lookahead VLN model is able to construct the navigable future path tree and select the optimal path via efficient parallel evaluation. Extensive experiments on the VLN-CE datasets confirm the effectiveness of our method.",
    "pdf_url": "https://arxiv.org/pdf/2404.01943v1",
    "github_url": null,
    "published": "2024-04-02T13:36:03+00:00",
    "updated": "2024-04-02T13:36:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.01686v1",
    "title": "JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments",
    "authors": [
      "Le",
      "Gou",
      "Datta"
    ],
    "summary": "Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision. Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings. Traditional benchmarks, with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making. As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking benchmark, towards more comprehensive environmental perception. JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition benchmarks, with OSPA-based metrics for evaluation. Extensive evaluation of leading methods shows significant challenges posed by our dataset.",
    "pdf_url": "https://arxiv.org/pdf/2404.01686v1",
    "github_url": null,
    "published": "2024-04-02T06:43:22+00:00",
    "updated": "2024-04-02T06:43:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.19603v1",
    "title": "Semantic Map-based Generation of Navigation Instructions",
    "authors": [
      "Li",
      "Zhang",
      "Teufel"
    ],
    "summary": "We are interested in the generation of navigation instructions, either in their own right or as training material for robotic navigation task. In this paper, we propose a new approach to navigation instruction generation by framing the problem as an image captioning task using semantic maps as visual input. Conventional approaches employ a sequence of panorama images to generate navigation instructions. Semantic maps abstract away from visual details and fuse the information in multiple panorama images into a single top-down representation, thereby reducing computational complexity to process the input. We present a benchmark dataset for instruction generation using semantic maps, propose an initial model and ask human subjects to manually assess the quality of generated instructions. Our initial investigations show promise in using semantic maps for instruction generation instead of a sequence of panorama images, but there is vast scope for improvement. We release the code for data preparation and model training at https://github.com/chengzu-li/VLGen.",
    "pdf_url": "https://arxiv.org/pdf/2403.19603v1",
    "github_url": "https://github.com/chengzu-li/VLGen",
    "published": "2024-03-28T17:27:44+00:00",
    "updated": "2024-03-28T17:27:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.19336v1",
    "title": "IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation",
    "authors": [
      "Huang",
      "Zhang",
      "Zhao"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is a challenging task that requires a robot to navigate in photo-realistic environments with human natural language promptings. Recent studies aim to handle this task by constructing the semantic spatial map representation of the environment, and then leveraging the strong ability of reasoning in large language models for generalizing code for guiding the robot navigation. However, these methods face limitations in instance-level and attribute-level navigation tasks as they cannot distinguish different instances of the same object. To address this challenge, we propose a new method, namely, Instance-aware Visual Language Map (IVLMap), to empower the robot with instance-level and attribute-level semantic mapping, where it is autonomously constructed by fusing the RGBD video data collected from the robot agent with special-designed natural language map indexing in the bird's-in-eye view. Such indexing is instance-level and attribute-level. In particular, when integrated with a large language model, IVLMap demonstrates the capability to i) transform natural language into navigation targets with instance and attribute information, enabling precise localization, and ii) accomplish zero-shot end-to-end navigation tasks based on natural language commands. Extensive navigation experiments are conducted. Simulation results illustrate that our method can achieve an average improvement of 14.4\\% in navigation accuracy. Code and demo are released at https://ivlmap.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2403.19336v1",
    "github_url": null,
    "published": "2024-03-28T11:52:42+00:00",
    "updated": "2024-03-28T11:52:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.18778v1",
    "title": "3P-LLM: Probabilistic Path Planning using Large Language Model for Autonomous Robot Navigation",
    "authors": [
      "Latif"
    ],
    "summary": "Much worldly semantic knowledge can be encoded in large language models (LLMs). Such information could be of great use to robots that want to carry out high-level, temporally extended commands stated in natural language. However, the lack of real-world experience that language models have is a key limitation that makes it challenging to use them for decision-making inside a particular embodiment. This research assesses the feasibility of using LLM (GPT-3.5-turbo chatbot by OpenAI) for robotic path planning. The shortcomings of conventional approaches to managing complex environments and developing trustworthy plans for shifting environmental conditions serve as the driving force behind the research. Due to the sophisticated natural language processing abilities of LLM, the capacity to provide effective and adaptive path-planning algorithms in real-time, great accuracy, and few-shot learning capabilities, GPT-3.5-turbo is well suited for path planning in robotics. In numerous simulated scenarios, the research compares the performance of GPT-3.5-turbo with that of state-of-the-art path planners like Rapidly Exploring Random Tree (RRT) and A*. We observed that GPT-3.5-turbo is able to provide real-time path planning feedback to the robot and outperforms its counterparts. This paper establishes the foundation for LLM-powered path planning for robotic systems.",
    "pdf_url": "https://arxiv.org/pdf/2403.18778v1",
    "github_url": null,
    "published": "2024-03-27T17:26:42+00:00",
    "updated": "2024-03-27T17:26:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.18454v1",
    "title": "Scaling Vision-and-Language Navigation With Offline RL",
    "authors": [
      "Bundele",
      "Bhupati",
      "Banerjee"
    ],
    "summary": "The study of vision-and-language navigation (VLN) has typically relied on expert trajectories, which may not always be available in real-world situations due to the significant effort required to collect them. On the other hand, existing approaches to training VLN agents that go beyond available expert data involve data augmentations or online exploration which can be tedious and risky. In contrast, it is easy to access large repositories of suboptimal offline trajectories. Inspired by research in offline reinforcement learning (ORL), we introduce a new problem setup of VLN-ORL which studies VLN using suboptimal demonstration data. We introduce a simple and effective reward-conditioned approach that can account for dataset suboptimality for training VLN agents, as well as benchmarks to evaluate progress and promote research in this area. We empirically study various noise models for characterizing dataset suboptimality among other unique challenges in VLN-ORL and instantiate it for the VLN$\\circlearrowright$BERT and MTVM architectures in the R2R and RxR environments. Our experiments demonstrate that the proposed reward-conditioned approach leads to significant performance improvements, even in complex and intricate environments.",
    "pdf_url": "https://arxiv.org/pdf/2403.18454v1",
    "github_url": null,
    "published": "2024-03-27T11:13:20+00:00",
    "updated": "2024-03-27T11:13:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.18114v1",
    "title": "Segment Any Medical Model Extended",
    "authors": [
      "Liu",
      "Zhang",
      "Diaz-Pinto"
    ],
    "summary": "The Segment Anything Model (SAM) has drawn significant attention from researchers who work on medical image segmentation because of its generalizability. However, researchers have found that SAM may have limited performance on medical images compared to state-of-the-art non-foundation models. Regardless, the community sees potential in extending, fine-tuning, modifying, and evaluating SAM for analysis of medical imaging. An increasing number of works have been published focusing on the mentioned four directions, where variants of SAM are proposed. To this end, a unified platform helps push the boundary of the foundation model for medical images, facilitating the use, modification, and validation of SAM and its variants in medical image segmentation. In this work, we introduce SAMM Extended (SAMME), a platform that integrates new SAM variant models, adopts faster communication protocols, accommodates new interactive modes, and allows for fine-tuning of subcomponents of the models. These features can expand the potential of foundation models like SAM, and the results can be translated to applications such as image-guided therapy, mixed reality interaction, robotic navigation, and data augmentation.",
    "pdf_url": "https://arxiv.org/pdf/2403.18114v1",
    "github_url": null,
    "published": "2024-03-26T21:37:25+00:00",
    "updated": "2024-03-26T21:37:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.17334v1",
    "title": "OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation",
    "authors": [
      "Zhao",
      "Li",
      "Chen"
    ],
    "summary": "Recent advances in Iterative Vision-and-Language Navigation (IVLN) introduce a more meaningful and practical paradigm of VLN by maintaining the agent's memory across tours of scenes. Although the long-term memory aligns better with the persistent nature of the VLN task, it poses more challenges on how to utilize the highly unstructured navigation memory with extremely sparse supervision. Towards this end, we propose OVER-NAV, which aims to go over and beyond the current arts of IVLN techniques. In particular, we propose to incorporate LLMs and open-vocabulary detectors to distill key information and establish correspondence between multi-modal signals. Such a mechanism introduces reliable cross-modal supervision and enables on-the-fly generalization to unseen scenes without the need of extra annotation and re-training. To fully exploit the interpreted navigation data, we further introduce a structured representation, coded Omnigraph, to effectively integrate multi-modal information along the tour. Accompanied with a novel omnigraph fusion mechanism, OVER-NAV is able to extract the most relevant knowledge from omnigraph for a more accurate navigating action. In addition, OVER-NAV seamlessly supports both discrete and continuous environments under a unified framework. We demonstrate the superiority of OVER-NAV in extensive experiments.",
    "pdf_url": "https://arxiv.org/pdf/2403.17334v1",
    "github_url": null,
    "published": "2024-03-26T02:34:48+00:00",
    "updated": "2024-03-26T02:34:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.17124v2",
    "title": "Grounding Language Plans in Demonstrations Through Counterfactual Perturbations",
    "authors": [
      "Wang",
      "Wang",
      "Mao"
    ],
    "summary": "Grounding the common-sense reasoning of Large Language Models (LLMs) in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://yanweiw.github.io/glide",
    "pdf_url": "https://arxiv.org/pdf/2403.17124v2",
    "github_url": null,
    "published": "2024-03-25T19:04:59+00:00",
    "updated": "2024-04-29T04:34:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.16875v1",
    "title": "TAIL: A Terrain-Aware Multi-Modal SLAM Dataset for Robot Locomotion in Deformable Granular Environments",
    "authors": [
      "Yao",
      "Ge",
      "Shi"
    ],
    "summary": "Terrain-aware perception holds the potential to improve the robustness and accuracy of autonomous robot navigation in the wilds, thereby facilitating effective off-road traversals. However, the lack of multi-modal perception across various motion patterns hinders the solutions of Simultaneous Localization And Mapping (SLAM), especially when confronting non-geometric hazards in demanding landscapes. In this paper, we first propose a Terrain-Aware multI-modaL (TAIL) dataset tailored to deformable and sandy terrains. It incorporates various types of robotic proprioception and distinct ground interactions for the unique challenges and benchmark of multi-sensor fusion SLAM. The versatile sensor suite comprises stereo frame cameras, multiple ground-pointing RGB-D cameras, a rotating 3D LiDAR, an IMU, and an RTK device. This ensemble is hardware-synchronized, well-calibrated, and self-contained. Utilizing both wheeled and quadrupedal locomotion, we efficiently collect comprehensive sequences to capture rich unstructured scenarios. It spans the spectrum of scope, terrain interactions, scene changes, ground-level properties, and dynamic robot characteristics. We benchmark several state-of-the-art SLAM methods against ground truth and provide performance validations. Corresponding challenges and limitations are also reported. All associated resources are accessible upon request at \\url{https://tailrobot.github.io/}.",
    "pdf_url": "https://arxiv.org/pdf/2403.16875v1",
    "github_url": null,
    "published": "2024-03-25T15:39:46+00:00",
    "updated": "2024-03-25T15:39:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.16425v2",
    "title": "Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in Event Cameras",
    "authors": [
      "Nair",
      "Milford",
      "Fischer"
    ],
    "summary": "Event cameras are increasingly popular in robotics due to beneficial features such as low latency, energy efficiency, and high dynamic range. Nevertheless, their downstream task performance is greatly influenced by the optimization of bias parameters. These parameters, for instance, regulate the necessary change in light intensity to trigger an event, which in turn depends on factors such as the environment lighting and camera motion. This paper introduces feedback control algorithms that automatically tune the bias parameters through two interacting methods: 1) An immediate, on-the-fly \\textit{fast} adaptation of the refractory period, which sets the minimum interval between consecutive events, and 2) if the event rate exceeds the specified bounds even after changing the refractory period repeatedly, the controller adapts the pixel bandwidth and event thresholds, which stabilizes after a short period of noise events across all pixels (\\textit{slow} adaptation). Our evaluation focuses on the visual place recognition task, where incoming query images are compared to a given reference database. We conducted comprehensive evaluations of our algorithms' adaptive feedback control in real-time. To do so, we collected the QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366 repeated traversals of a Scout Mini robot navigating through a 100 meter long indoor lab setting (totaling over 35km distance traveled) in varying brightness conditions with ground truth location information. Our proposed feedback controllers result in superior performance when compared to the standard bias settings and prior feedback control methods. Our findings also detail the impact of bias adjustments on task performance and feature ablation studies on the fast and slow adaptation mechanisms.",
    "pdf_url": "https://arxiv.org/pdf/2403.16425v2",
    "github_url": null,
    "published": "2024-03-25T05:10:34+00:00",
    "updated": "2024-08-13T04:16:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.15691v2",
    "title": "Temporal-Spatial Object Relations Modeling for Vision-and-Language Navigation",
    "authors": [
      "Huang",
      "Zheng",
      "Lan"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is a challenging task where an agent is required to navigate to a natural language described location via vision observations. The navigation abilities of the agent can be enhanced by the relations between objects, which are usually learned using internal objects or external datasets. The relationships between internal objects are modeled employing graph convolutional network (GCN) in traditional studies. However, GCN tends to be shallow, limiting its modeling ability. To address this issue, we utilize a cross attention mechanism to learn the connections between objects over a trajectory, which takes temporal continuity into account, termed as Temporal Object Relations (TOR). The external datasets have a gap with the navigation environment, leading to inaccurate modeling of relations. To avoid this problem, we construct object connections based on observations from all viewpoints in the navigational environment, which ensures complete spatial coverage and eliminates the gap, called Spatial Object Relations (SOR). Additionally, we observe that agents may repeatedly visit the same location during navigation, significantly hindering their performance. For resolving this matter, we introduce the Turning Back Penalty (TBP) loss function, which penalizes the agent's repetitive visiting behavior, substantially reducing the navigational distance. Experimental results on the REVERIE, SOON, and R2R datasets demonstrate the effectiveness of the proposed method.",
    "pdf_url": "https://arxiv.org/pdf/2403.15691v2",
    "github_url": null,
    "published": "2024-03-23T02:44:43+00:00",
    "updated": "2024-05-16T07:30:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.15648v3",
    "title": "Unifying Large Language Model and Deep Reinforcement Learning for Human-in-Loop Interactive Socially-aware Navigation",
    "authors": [
      "Wang",
      "Obi",
      "Bera"
    ],
    "summary": "Navigating human-filled spaces is crucial for the interactive social robots to support advanced services, such as cooperative carrying, which enables service provision in complex and crowded environments while adapting behavior based on real-time human language commands or feedback. However, existing social robot navigation planners face two major challenges: managing real-time user inputs and ensuring socially compliant behaviors in unfamiliar, zero-shot environments. In response, we introduce SALM, an interactive, human-in-loop Socially-Aware navigation Large Language Model framework that dynamically integrates deep reinforcement learning (DRL) with large language model (LLM) capabilities. SALM leverages contextual semantic understanding from real-time human-robot interactions to convert high-level user commands into precise, low-level control actions. A high-level LLM module parses user input, guiding the simultaneous generation of navigation commands by both a large language navigation model (LNM) and a DRL-based navigation model (RLNM). A memory mechanism archives temporal data for continuous refinement, while a multi-step graph-of-thoughts inference-based large language feedback model adaptively fuses the strengths of both planning approaches. Experimental evaluations demonstrate that SALM not only enhances navigational precision in crowded, dynamic environments but also significantly improves system adaptability, offering tailored behaviors that align with individual user preferences and real-time feedback. More details and videos about this work are available at: https://sites.google.com/view/navi-salm.",
    "pdf_url": "https://arxiv.org/pdf/2403.15648v3",
    "github_url": null,
    "published": "2024-03-22T23:12:28+00:00",
    "updated": "2025-03-07T20:03:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.15223v1",
    "title": "TriHelper: Zero-Shot Object Navigation with Dynamic Assistance",
    "authors": [
      "Zhang",
      "Zhang",
      "Wang"
    ],
    "summary": "Navigating toward specific objects in unknown environments without additional training, known as Zero-Shot object navigation, poses a significant challenge in the field of robotics, which demands high levels of auxiliary information and strategic planning. Traditional works have focused on holistic solutions, overlooking the specific challenges agents encounter during navigation such as collision, low exploration efficiency, and misidentification of targets. To address these challenges, our work proposes TriHelper, a novel framework designed to assist agents dynamically through three primary navigation challenges: collision, exploration, and detection. Specifically, our framework consists of three innovative components: (i) Collision Helper, (ii) Exploration Helper, and (iii) Detection Helper. These components work collaboratively to solve these challenges throughout the navigation process. Experiments on the Habitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper significantly outperforms all existing baseline methods in Zero-Shot object navigation, showcasing superior success rates and exploration efficiency. Our ablation studies further underscore the effectiveness of each helper in addressing their respective challenges, notably enhancing the agent's navigation capabilities. By proposing TriHelper, we offer a fresh perspective on advancing the object navigation task, paving the way for future research in the domain of Embodied AI and visual-based navigation.",
    "pdf_url": "https://arxiv.org/pdf/2403.15223v1",
    "github_url": null,
    "published": "2024-03-22T14:15:27+00:00",
    "updated": "2024-03-22T14:15:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.15049v3",
    "title": "Continual Vision-and-Language Navigation",
    "authors": [
      "Jeong",
      "Kang",
      "Choi"
    ],
    "summary": "Developing Vision-and-Language Navigation (VLN) agents typically assumes a \\textit{train-once-deploy-once} strategy, which is unrealistic as deployed agents continually encounter novel environments. To address this, we propose the Continual Vision-and-Language Navigation (CVLN) paradigm, where agents learn and adapt incrementally across multiple \\textit{scene domains}. CVLN includes two setups: Initial-instruction based CVLN for instruction-following, and Dialogue-based CVLN for dialogue-guided navigation. We also introduce two simple yet effective baselines for sequential decision-making: Perplexity Replay (PerpR), which replays difficult episodes, and Episodic Self-Replay (ESR), which stores and revisits action logits during training. Experiments show that existing continual learning methods fall short for CVLN, while PerpR and ESR achieve better performance by efficiently utilizing replay memory.",
    "pdf_url": "https://arxiv.org/pdf/2403.15049v3",
    "github_url": null,
    "published": "2024-03-22T09:15:36+00:00",
    "updated": "2025-10-31T01:59:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.14163v1",
    "title": "Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation",
    "authors": [
      "Sun",
      "Kanezaki",
      "Caron"
    ],
    "summary": "Object-goal navigation is a crucial engineering task for the community of embodied navigation; it involves navigating to an instance of a specified object category within unseen environments. Although extensive investigations have been conducted on both end-to-end and modular-based, data-driven approaches, fully enabling an agent to comprehend the environment through perceptual knowledge and perform object-goal navigation as efficiently as humans remains a significant challenge. Recently, large language models have shown potential in this task, thanks to their powerful capabilities for knowledge extraction and integration. In this study, we propose a data-driven, modular-based approach, trained on a dataset that incorporates common-sense knowledge of object-to-room relationships extracted from a large language model. We utilize the multi-channel Swin-Unet architecture to conduct multi-task learning incorporating with multimodal inputs. The results in the Habitat simulator demonstrate that our framework outperforms the baseline by an average of 10.6% in the efficiency metric, Success weighted by Path Length (SPL). The real-world demonstration shows that the proposed approach can efficiently conduct this task by traversing several rooms. For more details and real-world demonstrations, please check our project webpage (https://sunleyuan.github.io/ObjectNav).",
    "pdf_url": "https://arxiv.org/pdf/2403.14163v1",
    "github_url": null,
    "published": "2024-03-21T06:32:36+00:00",
    "updated": "2024-03-21T06:32:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.14158v1",
    "title": "Volumetric Environment Representation for Vision-Language Navigation",
    "authors": [
      "Liu",
      "Wang",
      "Yang"
    ],
    "summary": "Vision-language navigation (VLN) requires an agent to navigate through an 3D environment based on visual observations and natural language instructions. It is clear that the pivotal factor for successful navigation lies in the comprehensive scene understanding. Previous VLN agents employ monocular frameworks to extract 2D features of perspective views directly. Though straightforward, they struggle for capturing 3D geometry and semantics, leading to a partial and incomplete environment representation. To achieve a comprehensive 3D representation with fine-grained details, we introduce a Volumetric Environment Representation (VER), which voxelizes the physical world into structured 3D cells. For each cell, VER aggregates multi-view 2D features into such a unified 3D space via 2D-3D sampling. Through coarse-to-fine feature extraction and multi-task learning for VER, our agent predicts 3D occupancy, 3D room layout, and 3D bounding boxes jointly. Based on online collected VERs, our agent performs volume state estimation and builds episodic memory for predicting the next step. Experimental results show our environment representations from multi-task learning lead to evident performance gains on VLN. Our model achieves state-of-the-art performance across VLN benchmarks (R2R, REVERIE, and R4R).",
    "pdf_url": "https://arxiv.org/pdf/2403.14158v1",
    "github_url": null,
    "published": "2024-03-21T06:14:46+00:00",
    "updated": "2024-03-21T06:14:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.11650v2",
    "title": "Prioritized Semantic Learning for Zero-shot Instance Navigation",
    "authors": [
      "Sun",
      "Liu",
      "Zhi"
    ],
    "summary": "We study zero-shot instance navigation, in which the agent navigates to a specific object without using object annotations for training. Previous object navigation approaches apply the image-goal navigation (ImageNav) task (go to the location of an image) for pretraining, and transfer the agent to achieve object goals using a vision-language model. However, these approaches lead to issues of semantic neglect, where the model fails to learn meaningful semantic alignments. In this paper, we propose a Prioritized Semantic Learning (PSL) method to improve the semantic understanding ability of navigation agents. Specifically, a semantic-enhanced PSL agent is proposed and a prioritized semantic training strategy is introduced to select goal images that exhibit clear semantic supervision and relax the reward function from strict exact view matching. At inference time, a semantic expansion inference scheme is designed to preserve the same granularity level of the goal semantic as training. Furthermore, for the popular HM3D environment, we present an Instance Navigation (InstanceNav) task that requires going to a specific object instance with detailed descriptions, as opposed to the Object Navigation (ObjectNav) task where the goal is defined merely by the object category. Our PSL agent outperforms the previous state-of-the-art by 66% on zero-shot ObjectNav in terms of success rate and is also superior on the new InstanceNav task. Code will be released at https://github.com/XinyuSun/PSL-InstanceNav.",
    "pdf_url": "https://arxiv.org/pdf/2403.11650v2",
    "github_url": "https://github.com/XinyuSun/PSL-InstanceNav",
    "published": "2024-03-18T10:45:50+00:00",
    "updated": "2024-07-16T18:13:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.11541v3",
    "title": "Hierarchical Spatial Proximity Reasoning for Vision-and-Language Navigation",
    "authors": [
      "Xu",
      "Xie"
    ],
    "summary": "Most Vision-and-Language Navigation (VLN) algorithms are prone to making inaccurate decisions due to their lack of visual common sense and limited reasoning capabilities. To address this issue, we propose a Hierarchical Spatial Proximity Reasoning (HSPR) method. First, we introduce a scene understanding auxiliary task to help the agent build a knowledge base of hierarchical spatial proximity. This task utilizes panoramic views and object features to identify types of nodes and uncover the adjacency relationships between nodes, objects, and between nodes and objects. Second, we propose a multi-step reasoning navigation algorithm based on the hierarchical spatial proximity knowledge base, which continuously plans feasible paths to enhance exploration efficiency. Third, we introduce a residual fusion method to improve navigation decision accuracy. Finally, we validate our approach with experiments on publicly available datasets including REVERIE, SOON, R2R, and R4R. Our code is available at https://github.com/iCityLab/HSPR",
    "pdf_url": "https://arxiv.org/pdf/2403.11541v3",
    "github_url": "https://github.com/iCityLab/HSPR",
    "published": "2024-03-18T07:51:22+00:00",
    "updated": "2024-10-06T04:35:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.11487v3",
    "title": "Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis",
    "authors": [
      "Dorbala",
      "Chowdhury",
      "Manocha"
    ],
    "summary": "We present a novel approach to automatically synthesize \"wayfinding instructions\" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we conduct zero-shot navigation with multiple approaches on the REVERIE dataset using the generated instructions, and observe very close correlation with the baseline on standard success metrics (< 1% change in SR), quantifying the viability of generated instructions in replacing human-annotated data. We finally discuss the applicability of our approach in enabling a generalizable evaluation of embodied navigation policies. To the best of our knowledge, ours is the first LLM-driven approach capable of generating \"human-like\" instructions in a platform-agnostic manner, without training.",
    "pdf_url": "https://arxiv.org/pdf/2403.11487v3",
    "github_url": null,
    "published": "2024-03-18T05:38:07+00:00",
    "updated": "2024-04-02T04:27:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.10700v2",
    "title": "Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation",
    "authors": [
      "Taioli",
      "Rosa",
      "Castellini"
    ],
    "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable performance drop (up to -25%) in Success Rate when evaluating the state-of-the-art VLN-CE methods on our benchmark. Moreover, we formally define the task of Instruction Error Detection and Localization, and establish an evaluation protocol on top of our benchmark dataset. We also propose an effective method, based on a cross-modal transformer architecture, that achieves the best performance in error detection and localization, compared to baselines. Surprisingly, our proposed method has revealed errors in the validation set of the two commonly used datasets for VLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in other tasks. Code and dataset available at https://intelligolabs.github.io/R2RIE-CE",
    "pdf_url": "https://arxiv.org/pdf/2403.10700v2",
    "github_url": null,
    "published": "2024-03-15T21:36:15+00:00",
    "updated": "2025-01-15T12:45:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.10105v1",
    "title": "Belief Aided Navigation using Bayesian Reinforcement Learning for Avoiding Humans in Blind Spots",
    "authors": [
      "Kim",
      "Kwak",
      "Rim"
    ],
    "summary": "Recent research on mobile robot navigation has focused on socially aware navigation in crowded environments. However, existing methods do not adequately account for human robot interactions and demand accurate location information from omnidirectional sensors, rendering them unsuitable for practical applications. In response to this need, this study introduces a novel algorithm, BNBRL+, predicated on the partially observable Markov decision process framework to assess risks in unobservable areas and formulate movement strategies under uncertainty. BNBRL+ consolidates belief algorithms with Bayesian neural networks to probabilistically infer beliefs based on the positional data of humans. It further integrates the dynamics between the robot, humans, and inferred beliefs to determine the navigation paths and embeds social norms within the reward function, thereby facilitating socially aware navigation. Through experiments in various risk laden scenarios, this study validates the effectiveness of BNBRL+ in navigating crowded environments with blind spots. The model's ability to navigate effectively in spaces with limited visibility and avoid obstacles dynamically can significantly improve the safety and reliability of autonomous vehicles.",
    "pdf_url": "https://arxiv.org/pdf/2403.10105v1",
    "github_url": null,
    "published": "2024-03-15T08:50:39+00:00",
    "updated": "2024-03-15T08:50:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.10008v1",
    "title": "Language to Map: Topological map generation from natural language path instructions",
    "authors": [
      "Deguchi",
      "Shibata",
      "Taguchi"
    ],
    "summary": "In this paper, a method for generating a map from path information described using natural language (textual path) is proposed. In recent years, robotics research mainly focus on vision-and-language navigation (VLN), a navigation task based on images and textual paths. Although VLN is expected to facilitate user instructions to robots, its current implementation requires users to explain the details of the path for each navigation session, which results in high explanation costs for users. To solve this problem, we proposed a method that creates a map as a topological map from a textual path and automatically creates a new path using this map. We believe that large language models (LLMs) can be used to understand textual path. Therefore, we propose and evaluate two methods, one for storing implicit maps in LLMs, and the other for generating explicit maps using LLMs. The implicit map is in the LLM's memory. It is created using prompts. In the explicit map, a topological map composed of nodes and edges is constructed and the actions at each node are stored. This makes it possible to estimate the path and actions at waypoints on an undescribed path, if enough information is available. Experimental results on path instructions generated in a real environment demonstrate that generating explicit maps achieves significantly higher accuracy than storing implicit maps in the LLMs.",
    "pdf_url": "https://arxiv.org/pdf/2403.10008v1",
    "github_url": null,
    "published": "2024-03-15T04:22:14+00:00",
    "updated": "2024-03-15T04:22:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.09905v4",
    "title": "TAS: A Transit-Aware Strategy for Embodied Navigation with Non-Stationary Targets",
    "authors": [
      "Dorbala",
      "Patel",
      "Bedi"
    ],
    "summary": "Embodied navigation methods commonly operate in static environments with stationary targets. In this work, we present a new algorithm for navigation in dynamic scenarios with non-stationary targets. Our novel Transit-Aware Strategy (TAS) enriches embodied navigation policies with object path information. TAS improves performance in non-stationary environments by rewarding agents for synchronizing their routes with target routes. To evaluate TAS, we further introduce Dynamic Object Maps (DOMs), a dynamic variant of node-attributed topological graphs with structured object transitions. DOMs are inspired by human habits to simulate realistic object routes on a graph. Our experiments show that on average, TAS improves agent Success Rate (SR) by 21.1 in non-stationary environments, while also generalizing better from static environments by 44.5% when measured by Relative Change in Success (RCS). We qualitatively investigate TAS-agent performance on DOMs and draw various inferences to help better model generalist navigation policies. To the best of our knowledge, ours is the first work that quantifies the adaptability of embodied navigation methods in non-stationary environments. Code and data for our benchmark will be made publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2403.09905v4",
    "github_url": null,
    "published": "2024-03-14T22:33:22+00:00",
    "updated": "2025-10-16T18:41:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.09793v3",
    "title": "Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning",
    "authors": [
      "Flgel",
      "Fischer",
      "Rudolf"
    ],
    "summary": "Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep Reinforcement Learning (DRL) approaches are recently used to learn a robot's navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot's exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot's social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, which states that social acting is oriented toward the acting of others. The DRL policy is trained in an environment where other agents interact socially integrated and reward the robot's behavior individually. The simulation results indicate that the proposed socially integrated navigation approach outperforms a socially aware approach in terms of ego navigation performance while significantly reducing the negative impact on all agents within the environment.",
    "pdf_url": "https://arxiv.org/pdf/2403.09793v3",
    "github_url": null,
    "published": "2024-03-14T18:25:40+00:00",
    "updated": "2024-07-26T06:41:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.09285v1",
    "title": "THR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction",
    "authors": [
      "Schreiter",
      "Almeida",
      "Zhu"
    ],
    "summary": "We present a new large dataset of indoor human and robot navigation and interaction, called THR-MAGNI, that is designed to facilitate research on social navigation: e.g., modelling and predicting human motion, analyzing goal-oriented interactions between humans and robots, and investigating visual attention in a social interaction context. THR-MAGNI was created to fill a gap in available datasets for human motion analysis and HRI. This gap is characterized by a lack of comprehensive inclusion of exogenous factors and essential target agent cues, which hinders the development of robust models capable of capturing the relationship between contextual cues and human behavior in different scenarios. Unlike existing datasets, THR-MAGNI includes a broader set of contextual features and offers multiple scenario variations to facilitate factor isolation. The dataset includes many social human-human and human-robot interaction scenarios, rich context annotations, and multi-modal data, such as walking trajectories, gaze tracking data, and lidar and camera streams recorded from a mobile robot. We also provide a set of tools for visualization and processing of the recorded data. THR-MAGNI is, to the best of our knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human-robot interactions.",
    "pdf_url": "https://arxiv.org/pdf/2403.09285v1",
    "github_url": null,
    "published": "2024-03-14T11:12:16+00:00",
    "updated": "2024-03-14T11:12:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.08955v4",
    "title": "Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis",
    "authors": [
      "Liu",
      "Gupta",
      "Noorani"
    ],
    "summary": "Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration efficiency and safety. Risk-sensitive policy gradient methods, which incorporate both expected return and risk measures, have been explored for their ability to yield safe policies, yet their iteration complexity remains largely underexplored. In this work, we conduct a rigorous iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm with an exponential utility function. We establish an iteration complexity of $\\mathcal{O}(^{-2})$ to reach an $$-approximate first-order stationary point (FOSP). Furthermore, we investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutral counterparts. Our analysis indicates that risk-sensitive REINFORCE can potentially converge faster. To validate our analysis, we empirically evaluate the learning performance and convergence efficiency of the risk-neutral and risk-sensitive REINFORCE algorithms in multiple environments: CartPole, MiniGrid, and Robot Navigation. Empirical results confirm that risk-sensitive cases can converge and stabilize faster compared to their risk-neutral counterparts. More details can be found on our website https://anonymous.4open.science/w/riskrl.",
    "pdf_url": "https://arxiv.org/pdf/2403.08955v4",
    "github_url": null,
    "published": "2024-03-13T20:50:49+00:00",
    "updated": "2025-08-29T18:44:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.08282v2",
    "title": "Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation",
    "authors": [
      "Zhao",
      "Chen",
      "Guo"
    ],
    "summary": "Due to the dynamic and unpredictable open-world setting, navigating complex environments in Minecraft poses significant challenges for multi-agent systems. Agents must interact with the environment and coordinate their actions with other agents to achieve common objectives. However, traditional approaches often struggle to efficiently manage inter-agent communication and task distribution, crucial for effective multi-agent navigation. Furthermore, processing and integrating multi-modal information (such as visual, textual, and auditory data) is essential for agents to comprehend their goals and navigate the environment successfully and fully. To address this issue, we design the HAS framework to auto-organize groups of LLM-based agents to complete navigation tasks. In our approach, we devise a hierarchical auto-organizing navigation system, which is characterized by 1) a hierarchical system for multi-agent organization, ensuring centralized planning and decentralized execution; 2) an auto-organizing and intra-communication mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal information platform, facilitating multi-modal perception to perform the three navigation tasks with one system. To assess organizational behavior, we design a series of navigation tasks in the Minecraft environment, which includes searching and exploring. We aim to develop embodied organizations that push the boundaries of embodied AI, moving it towards a more human-like organizational structure.",
    "pdf_url": "https://arxiv.org/pdf/2403.08282v2",
    "github_url": null,
    "published": "2024-03-13T06:22:17+00:00",
    "updated": "2024-03-18T05:03:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.08833v1",
    "title": "TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation",
    "authors": [
      "Li",
      "Chen",
      "Lin"
    ],
    "summary": "Zero-shot navigation is a critical challenge in Vision-Language Navigation (VLN) tasks, where the ability to adapt to unfamiliar instructions and to act in unknown environments is essential. Existing supervised learning-based models, trained using annotated data through reinforcement learning, exhibit limitations in generalization capabilities. Large Language Models (LLMs), with their extensive knowledge and emergent reasoning abilities, present a potential pathway for achieving zero-shot navigation. This paper presents a VLN agent based on LLMs, exploring approaches to the zero-shot navigation problem. To compensate for the shortcomings of LLMs in environmental perception, we propose the Thinking, Interacting, and Action (TINA) framework. TINA enables the agent to scrutinize perceptual information and autonomously query key clues within the environment through an introduced question-answering module, thereby aligning instructions with specific perceptual data. The navigation agent's perceptual abilities are enhanced through the TINA framework, while the explicit thought and query processes also improve the navigational procedure's explainability and transparency. We evaluate the performance of our method on the Room-to-Room dataset. The experiment results indicate that our approach improves the navigation performance of LLM-based agents. Our approach also outperformed some supervised learning-based methods, highlighting its efficacy in zero-shot navigation.",
    "pdf_url": "https://arxiv.org/pdf/2403.08833v1",
    "github_url": null,
    "published": "2024-03-13T05:22:39+00:00",
    "updated": "2024-03-13T05:22:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.08144v1",
    "title": "Prosody for Intuitive Robotic Interface Design: It's Not What You Said, It's How You Said It",
    "authors": [
      "Sanoubari",
      "Iscen",
      "Takayama"
    ],
    "summary": "In this paper, we investigate the use of 'prosody' (the musical elements of speech) as a communicative signal for intuitive human-robot interaction interfaces. Our approach, rooted in Research through Design (RtD), examines the application of prosody in directing a quadruped robot navigation. We involved ten team members in an experiment to command a robot through an obstacle course using natural interaction. A human operator, serving as the robot's sensory and processing proxy, translated human communication into a basic set of navigation commands, effectively simulating an intuitive interface. During our analysis of interaction videos, when lexical and visual cues proved insufficient for accurate command interpretation, we turned to non-verbal auditory cues. Qualitative evidence suggests that participants intuitively relied on prosody to control robot navigation. We highlight specific distinct prosodic constructs that emerged from this preliminary exploration and discuss their pragmatic functions. This work contributes a discussion on the broader potential of prosody as a multifunctional communicative signal for designing future intuitive robotic interfaces, enabling lifelong learning and personalization in human-robot interaction.",
    "pdf_url": "https://arxiv.org/pdf/2403.08144v1",
    "github_url": null,
    "published": "2024-03-13T00:10:18+00:00",
    "updated": "2024-03-13T00:10:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.07376v2",
    "title": "NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning",
    "authors": [
      "Lin",
      "Nie",
      "Wei"
    ],
    "summary": "Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the reasoning from the prior steps. Through constructing formalized labels for training, the LLM can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision. Experimental results across various training settings and popular VLN benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants. Through simple parameter-efficient finetuning, our NavCoT outperforms a recent GPT4-based approach with ~7% relative improvement on the R2R dataset. We believe that NavCoT will help unlock more task-adaptive and scalable LLM-based embodied agents, which are helpful for developing real-world robotics applications. Code is available at https://github.com/expectorlin/NavCoT.",
    "pdf_url": "https://arxiv.org/pdf/2403.07376v2",
    "github_url": "https://github.com/expectorlin/NavCoT",
    "published": "2024-03-12T07:27:02+00:00",
    "updated": "2025-03-22T11:04:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.07076v1",
    "title": "Mapping High-level Semantic Regions in Indoor Environments without Object Recognition",
    "authors": [
      "Bigazzi",
      "Baraldi",
      "Kousik"
    ],
    "summary": "Robots require a semantic understanding of their surroundings to operate in an efficient and explainable way in human environments. In the literature, there has been an extensive focus on object labeling and exhaustive scene graph generation; less effort has been focused on the task of purely identifying and mapping large semantic regions. The present work proposes a method for semantic region mapping via embodied navigation in indoor environments, generating a high-level representation of the knowledge of the agent. To enable region identification, the method uses a vision-to-language model to provide scene information for mapping. By projecting egocentric scene understanding into the global frame, the proposed method generates a semantic map as a distribution over possible region labels at each location. This mapping procedure is paired with a trained navigation policy to enable autonomous map generation. The proposed method significantly outperforms a variety of baselines, including an object-based system and a pretrained scene classifier, in experiments in a photorealistic simulator.",
    "pdf_url": "https://arxiv.org/pdf/2403.07076v1",
    "github_url": null,
    "published": "2024-03-11T18:09:50+00:00",
    "updated": "2024-03-11T18:09:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.05770v1",
    "title": "Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning",
    "authors": [
      "Lin",
      "Long",
      "Zhu"
    ],
    "summary": "Vision-and-language navigation (VLN) asks an agent to follow a given language instruction to navigate through a real 3D environment. Despite significant advances, conventional VLN agents are trained typically under disturbance-free environments and may easily fail in real-world scenarios, since they are unaware of how to deal with various possible disturbances, such as sudden obstacles or human interruptions, which widely exist and may usually cause an unexpected route deviation. In this paper, we present a model-agnostic training paradigm, called Progressive Perturbation-aware Contrastive Learning (PROPER) to enhance the generalization ability of existing VLN agents, by requiring them to learn towards deviation-robust navigation. Specifically, a simple yet effective path perturbation scheme is introduced to implement the route deviation, with which the agent is required to still navigate successfully following the original instruction. Since directly enforcing the agent to learn perturbed trajectories may lead to inefficient training, a progressively perturbed trajectory augmentation strategy is designed, where the agent can self-adaptively learn to navigate under perturbation with the improvement of its navigation performance for each specific trajectory. For encouraging the agent to well capture the difference brought by perturbation, a perturbation-aware contrastive learning mechanism is further developed by contrasting perturbation-free trajectory encodings and perturbation-based counterparts. Extensive experiments on R2R show that PROPER can benefit multiple VLN baselines in perturbation-free scenarios. We further collect the perturbed path data to construct an introspection subset based on the R2R, called Path-Perturbed R2R (PP-R2R). The results on PP-R2R show unsatisfying robustness of popular VLN agents and the capability of PROPER in improving the navigation robustness.",
    "pdf_url": "https://arxiv.org/pdf/2403.05770v1",
    "github_url": null,
    "published": "2024-03-09T02:34:13+00:00",
    "updated": "2024-03-09T02:34:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.03405v1",
    "title": "Causality-based Cross-Modal Representation Learning for Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "He",
      "Dang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) has gained significant research interest in recent years due to its potential applications in real-world scenarios. However, existing VLN methods struggle with the issue of spurious associations, resulting in poor generalization with a significant performance gap between seen and unseen environments. In this paper, we tackle this challenge by proposing a unified framework CausalVLN based on the causal learning paradigm to train a robust navigator capable of learning unbiased feature representations. Specifically, we establish reasonable assumptions about confounders for vision and language in VLN using the structured causal model (SCM). Building upon this, we propose an iterative backdoor-based representation learning (IBRL) method that allows for the adaptive and effective intervention on confounders. Furthermore, we introduce the visual and linguistic backdoor causal encoders to enable unbiased feature expression for multi-modalities during training and validation, enhancing the agent's capability to generalize across different environments. Experiments on three VLN datasets (R2R, RxR, and REVERIE) showcase the superiority of our proposed method over previous state-of-the-art approaches. Moreover, detailed visualization analysis demonstrates the effectiveness of CausalVLN in significantly narrowing down the performance gap between seen and unseen environments, underscoring its strong generalization capability.",
    "pdf_url": "https://arxiv.org/pdf/2403.03405v1",
    "github_url": null,
    "published": "2024-03-06T02:01:38+00:00",
    "updated": "2024-03-06T02:01:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.01542v1",
    "title": "Human Robot Pacing Mismatch",
    "authors": [
      "Sun",
      "Trautman",
      "Murphey"
    ],
    "summary": "A widely accepted explanation for robots planning overcautious or overaggressive trajectories alongside human is that the crowd density exceeds a threshold such that all feasible trajectories are considered unsafe -- the freezing robot problem. However, even with low crowd density, the robot's navigation performance could still drop drastically when in close proximity to human. In this work, we argue that a broader cause of suboptimal navigation performance near human is due to the robot's misjudgement for the human's willingness (flexibility) to share space with others, particularly when the robot assumes the human's flexibility holds constant during interaction, a phenomenon of what we call human robot pacing mismatch. We show that the necessary condition for solving pacing mismatch is to model the evolution of both the robot and the human's flexibility during decision making, a strategy called distribution space modeling. We demonstrate the advantage of distribution space coupling through an anecdotal case study and discuss the future directions of solving human robot pacing mismatch.",
    "pdf_url": "https://arxiv.org/pdf/2403.01542v1",
    "github_url": null,
    "published": "2024-03-03T15:42:56+00:00",
    "updated": "2024-03-03T15:42:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.01450v3",
    "title": "Collision-Free Robot Navigation in Crowded Environments using Learning based Convex Model Predictive Control",
    "authors": [
      "Wen",
      "Dong",
      "Chen"
    ],
    "summary": "Navigating robots safely and efficiently in crowded and complex environments remains a significant challenge. However, due to the dynamic and intricate nature of these settings, planning efficient and collision-free paths for robots to track is particularly difficult. In this paper, we uniquely bridge the robot's perception, decision-making and control processes by utilizing the convex obstacle-free region computed from 2D LiDAR data. The overall pipeline is threefold: (1) We proposes a robot navigation framework that utilizes deep reinforcement learning (DRL), conceptualizing the observation as the convex obstacle-free region, a departure from general reliance on raw sensor inputs. (2) We design the action space, derived from the intersection of the robot's kinematic limits and the convex region, to enable efficient sampling of inherently collision-free reference points. These actions assists in guiding the robot to move towards the goal and interact with other obstacles during navigation. (3) We employ model predictive control (MPC) to track the trajectory formed by the reference points while satisfying constraints imposed by the convex obstacle-free region and the robot's kinodynamic limits. The effectiveness of proposed improvements has been validated through two sets of ablation studies and a comparative experiment against the Timed Elastic Band (TEB), demonstrating improved navigation performance in crowded and complex environments.",
    "pdf_url": "https://arxiv.org/pdf/2403.01450v3",
    "github_url": null,
    "published": "2024-03-03T09:08:07+00:00",
    "updated": "2024-10-19T12:04:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.19432v1",
    "title": "Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation",
    "authors": [
      "Yang",
      "Glossop",
      "Bhorkar"
    ],
    "summary": "Recent years in robotics and imitation learning have shown remarkable progress in training large-scale foundation models by leveraging data across a multitude of embodiments. The success of such policies might lead us to wonder: just how diverse can the robots in the training set be while still facilitating positive transfer? In this work, we study this question in the context of heterogeneous embodiments, examining how even seemingly very different domains, such as robotic navigation and manipulation, can provide benefits when included in the training data for the same model. We train a single goal-conditioned policy that is capable of controlling robotic arms, quadcopters, quadrupeds, and mobile bases. We then investigate the extent to which transfer can occur across navigation and manipulation on these embodiments by framing them as a single goal-reaching task. We find that co-training with navigation data can enhance robustness and performance in goal-conditioned manipulation with a wrist-mounted camera. We then deploy our policy trained only from navigation-only and static manipulation-only data on a mobile manipulator, showing that it can control a novel embodiment in a zero-shot manner. These results provide evidence that large-scale robotic policies can benefit from data collected across various embodiments. Further information and robot videos can be found on our project website http://extreme-cross-embodiment.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2402.19432v1",
    "github_url": null,
    "published": "2024-02-29T18:30:32+00:00",
    "updated": "2024-02-29T18:30:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.19007v2",
    "title": "DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments",
    "authors": [
      "Ma",
      "Dai",
      "Mu"
    ],
    "summary": "Zero-Shot Object Navigation (ZSON) requires agents to autonomously locate and approach unseen objects in unfamiliar environments and has emerged as a particularly challenging task within the domain of Embodied AI. Existing datasets for developing ZSON algorithms lack consideration of dynamic obstacles, object attribute diversity, and scene texts, thus exhibiting noticeable discrepancies from real-world situations. To address these issues, we propose a Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments (DOZE) that comprises ten high-fidelity 3D scenes with over 18k tasks, aiming to mimic complex, dynamic real-world scenarios. Specifically, DOZE scenes feature multiple moving humanoid obstacles, a wide array of open-vocabulary objects, diverse distinct-attribute objects, and valuable textual hints. Besides, different from existing datasets that only provide collision checking between the agent and static obstacles, we enhance DOZE by integrating capabilities for detecting collisions between the agent and moving obstacles. This novel functionality enables the evaluation of the agents' collision avoidance abilities in dynamic environments. We test four representative ZSON methods on DOZE, revealing substantial room for improvement in existing approaches concerning navigation efficiency, safety, and object recognition accuracy. Our dataset can be found at https://DOZE-Dataset.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2402.19007v2",
    "github_url": null,
    "published": "2024-02-29T10:03:57+00:00",
    "updated": "2024-07-08T07:58:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.18065v2",
    "title": "A Probabilistic Motion Model for Skid-Steer Wheeled Mobile Robot Navigation on Off-Road Terrains",
    "authors": [
      "Trivedi",
      "Zolotas",
      "Abbas"
    ],
    "summary": "Skid-Steer Wheeled Mobile Robots (SSWMRs) are increasingly being used for off-road autonomy applications. When turning at high speeds, these robots tend to undergo significant skidding and slipping. In this work, using Gaussian Process Regression (GPR) and Sigma-Point Transforms, we estimate the non-linear effects of tire-terrain interaction on robot velocities in a probabilistic fashion. Using the mean estimates from GPR, we propose a data-driven dynamic motion model that is more accurate at predicting future robot poses than conventional kinematic motion models. By efficiently solving a convex optimization problem based on the history of past robot motion, the GPR augmented motion model generalizes to previously unseen terrain conditions. The output distribution from the proposed motion model can be used for local motion planning approaches, such as stochastic model predictive control, leveraging model uncertainty to make safe decisions. We validate our work on a benchmark real-world multi-terrain SSWMR dataset. Our results show that the model generalizes to three different terrains while significantly reducing errors in linear and angular motion predictions. As shown in the attached video, we perform a separate set of experiments on a physical robot to demonstrate the robustness of the proposed algorithm.",
    "pdf_url": "https://arxiv.org/pdf/2402.18065v2",
    "github_url": null,
    "published": "2024-02-28T05:50:18+00:00",
    "updated": "2024-02-29T15:02:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.17511v1",
    "title": "Rethinking Mutual Information for Language Conditioned Skill Discovery on Imitation Learning",
    "authors": [
      "Ju",
      "Yang",
      "Wang"
    ],
    "summary": "Language-conditioned robot behavior plays a vital role in executing complex tasks by associating human commands or instructions with perception and actions. The ability to compose long-horizon tasks based on unconstrained language instructions necessitates the acquisition of a diverse set of general-purpose skills. However, acquiring inherent primitive skills in a coupled and long-horizon environment without external rewards or human supervision presents significant challenges. In this paper, we evaluate the relationship between skills and language instructions from a mathematical perspective, employing two forms of mutual information within the framework of language-conditioned policy learning. To maximize the mutual information between language and skills in an unsupervised manner, we propose an end-to-end imitation learning approach known as Language Conditioned Skill Discovery (LCSD). Specifically, we utilize vector quantization to learn discrete latent skills and leverage skill sequences of trajectories to reconstruct high-level semantic instructions. Through extensive experiments on language-conditioned robotic navigation and manipulation tasks, encompassing BabyAI, LORel, and CALVIN, we demonstrate the superiority of our method over prior works. Our approach exhibits enhanced generalization capabilities towards unseen tasks, improved skill interpretability, and notably higher rates of task completion success.",
    "pdf_url": "https://arxiv.org/pdf/2402.17511v1",
    "github_url": null,
    "published": "2024-02-27T13:53:52+00:00",
    "updated": "2024-02-27T13:53:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.16482v1",
    "title": "On Languaging a Simulation Engine",
    "authors": [
      "Liu",
      "Li"
    ],
    "summary": "Language model intelligence is revolutionizing the way we program materials simulations. However, the diversity of simulation scenarios renders it challenging to precisely transform human language into a tailored simulator. Here, using three functionalized types of language model, we propose a language-to-simulation (Lang2Sim) framework that enables interactive navigation on languaging a simulation engine, by taking a scenario instance of water sorption in porous matrices. Unlike line-by-line coding of a target simulator, the language models interpret each simulator as an assembly of invariant tool function and its variant input-output pair. Lang2Sim enables the precise transform of textual description by functionalizing and sequentializing the language models of, respectively, rationalizing the tool categorization, customizing its input-output combinations, and distilling the simulator input into executable format. Importantly, depending on its functionalized type, each language model features a distinct processing of chat history to best balance its memory limit and information completeness, thus leveraging the model intelligence to unstructured nature of human request. Overall, this work establishes language model as an intelligent platform to unlock the era of languaging a simulation engine.",
    "pdf_url": "https://arxiv.org/pdf/2402.16482v1",
    "github_url": null,
    "published": "2024-02-26T11:01:54+00:00",
    "updated": "2024-02-26T11:01:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.16117v1",
    "title": "RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis",
    "authors": [
      "Mu",
      "Chen",
      "Zhang"
    ],
    "summary": "Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning. Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one navigation task.",
    "pdf_url": "https://arxiv.org/pdf/2402.16117v1",
    "github_url": null,
    "published": "2024-02-25T15:31:43+00:00",
    "updated": "2024-02-25T15:31:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.15852v7",
    "title": "NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation",
    "authors": [
      "Zhang",
      "Wang",
      "Xu"
    ],
    "summary": "Vision-and-language navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions. In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavor to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometers, or depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision making and instruction following. We train NaVid with 510k navigation samples collected from continuous environments, including action-planning and instruction-reasoning samples, along with 763k large-scale web data. Extensive experiments show that NaVid achieves state-of-the-art performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.",
    "pdf_url": "https://arxiv.org/pdf/2402.15852v7",
    "github_url": null,
    "published": "2024-02-24T16:39:16+00:00",
    "updated": "2024-06-30T11:14:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.14304v2",
    "title": "Vision-Language Navigation with Embodied Intelligence: A Survey",
    "authors": [
      "Gao",
      "Wang",
      "Gao"
    ],
    "summary": "As a long-term vision in the field of artificial intelligence, the core goal of embodied intelligence is to improve the perception, understanding, and interaction capabilities of agents and the environment. Vision-language navigation (VLN), as a critical research path to achieve embodied intelligence, focuses on exploring how agents use natural language to communicate effectively with humans, receive and understand instructions, and ultimately rely on visual information to achieve accurate navigation. VLN integrates artificial intelligence, natural language processing, computer vision, and robotics. This field faces technical challenges but shows potential for application such as human-computer interaction. However, due to the complex process involved from language understanding to action execution, VLN faces the problem of aligning visual information and language instructions, improving generalization ability, and many other challenges. This survey systematically reviews the research progress of VLN and details the research direction of VLN with embodied intelligence. After a detailed summary of its system architecture and research based on methods and commonly used benchmark datasets, we comprehensively analyze the problems and challenges faced by current research and explore the future development direction of this field, aiming to provide a practical reference for researchers.",
    "pdf_url": "https://arxiv.org/pdf/2402.14304v2",
    "github_url": null,
    "published": "2024-02-22T05:45:17+00:00",
    "updated": "2024-03-15T12:31:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.11799v2",
    "title": "Decentralized Multi-Robot Navigation for Autonomous Surface Vehicles with Distributional Reinforcement Learning",
    "authors": [
      "Lin",
      "Huang",
      "Chen"
    ],
    "summary": "Collision avoidance algorithms for Autonomous Surface Vehicles (ASV) that follow the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) have been proposed in recent years. However, it may be difficult and unsafe to follow COLREGs in congested waters, where multiple ASVs are navigating in the presence of static obstacles and strong currents, due to the complex interactions. To address this problem, we propose a decentralized multi-ASV collision avoidance policy based on Distributional Reinforcement Learning, which considers the interactions among ASVs as well as with static obstacles and current flows. We evaluate the performance of the proposed Distributional RL based policy against a traditional RL-based policy and two classical methods, Artificial Potential Fields (APF) and Reciprocal Velocity Obstacles (RVO), in simulation experiments, which show that the proposed policy achieves superior performance in navigation safety, while requiring minimal travel time and energy. A variant of our framework that automatically adapts its risk sensitivity is also demonstrated to improve ASV safety in highly congested environments.",
    "pdf_url": "https://arxiv.org/pdf/2402.11799v2",
    "github_url": null,
    "published": "2024-02-19T03:06:43+00:00",
    "updated": "2024-03-06T19:06:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.10670v2",
    "title": "OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models",
    "authors": [
      "Kuang",
      "Lin",
      "Jiang"
    ],
    "summary": "Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting common sense reasoning on VSSM, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments. Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method's effectiveness. Furthermore, we perform real robot demonstrations to validate our method's open-set-ness and generalizability to real-world environments.",
    "pdf_url": "https://arxiv.org/pdf/2402.10670v2",
    "github_url": null,
    "published": "2024-02-16T13:21:33+00:00",
    "updated": "2024-03-25T02:52:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.08221v1",
    "title": "MetaTra: Meta-Learning for Generalized Trajectory Prediction in Unseen Domain",
    "authors": [
      "Li",
      "Huang",
      "Fan"
    ],
    "summary": "Trajectory prediction has garnered widespread attention in different fields, such as autonomous driving and robotic navigation. However, due to the significant variations in trajectory patterns across different scenarios, models trained in known environments often falter in unseen ones. To learn a generalized model that can directly handle unseen domains without requiring any model updating, we propose a novel meta-learning-based trajectory prediction method called MetaTra. This approach incorporates a Dual Trajectory Transformer (Dual-TT), which enables a thorough exploration of the individual intention and the interactions within group motion patterns in diverse scenarios. Building on this, we propose a meta-learning framework to simulate the generalization process between source and target domains. Furthermore, to enhance the stability of our prediction outcomes, we propose a Serial and Parallel Training (SPT) strategy along with a feature augmentation method named MetaMix. Experimental results on several real-world datasets confirm that MetaTra not only surpasses other state-of-the-art methods but also exhibits plug-and-play capabilities, particularly in the realm of domain generalization.",
    "pdf_url": "https://arxiv.org/pdf/2402.08221v1",
    "github_url": null,
    "published": "2024-02-13T05:25:37+00:00",
    "updated": "2024-02-13T05:25:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.07872v1",
    "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
    "authors": [
      "Nasiriany",
      "Xia",
      "Yu"
    ],
    "summary": "Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data?   In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",
    "pdf_url": "https://arxiv.org/pdf/2402.07872v1",
    "github_url": null,
    "published": "2024-02-12T18:33:47+00:00",
    "updated": "2024-02-12T18:33:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.07049v1",
    "title": "A Factor Graph Model of Trust for a Collaborative Multi-Agent System",
    "authors": [
      "Akbari",
      "Yuan",
      "Wang"
    ],
    "summary": "In the field of Multi-Agent Systems (MAS), known for their openness, dynamism, and cooperative nature, the ability to trust the resources and services of other agents is crucial. Trust, in this setting, is the reliance and confidence an agent has in the information, behaviors, intentions, truthfulness, and capabilities of others within the system. Our paper introduces a new graphical approach that utilizes factor graphs to represent the interdependent behaviors and trustworthiness among agents. This includes modeling the behavior of robots as a trajectory of actions using a Gaussian process factor graph, which accounts for smoothness, obstacle avoidance, and trust-related factors. Our method for evaluating trust is decentralized and considers key interdependent sub-factors such as proximity safety, consistency, and cooperation. The overall system comprises a network of factor graphs that interact through trust-related factors and employs a Bayesian inference method to dynamically assess trust-based decisions with informed consent. The effectiveness of this method is validated via simulations and empirical tests with autonomous robots navigating unsignalized intersections.",
    "pdf_url": "https://arxiv.org/pdf/2402.07049v1",
    "github_url": null,
    "published": "2024-02-10T21:44:28+00:00",
    "updated": "2024-02-10T21:44:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.03561v2",
    "title": "VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation",
    "authors": [
      "Li",
      "Padmakumar",
      "Sukhatme"
    ],
    "summary": "Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with three proxy tasks: Masked Language Modeling, Instruction and Trajectory Matching, and Next Action Prediction, so as to learn temporally-aware and visually-aligned instruction representations. The learned instruction representation is adapted to the state-of-the-art navigator when fine-tuning on the Touchdown dataset. Empirical results demonstrate that VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in task completion rate, achieving a new state-of-the-art on the Touchdown dataset.",
    "pdf_url": "https://arxiv.org/pdf/2402.03561v2",
    "github_url": null,
    "published": "2024-02-05T22:20:19+00:00",
    "updated": "2024-02-07T18:02:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.03494v3",
    "title": "Beyond Text: Utilizing Vocal Cues to Improve Decision Making in LLMs for Robot Navigation Tasks",
    "authors": [
      "Sun",
      "Meng",
      "Chakraborty"
    ],
    "summary": "While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present Beyond Text: an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations.This approach not only achieves a 70.26% winning rate, outperforming existing LLMs by 22.16% to 48.30% (gemini-1.5-pro and gpt-3.5 respectively), but also enhances robustness against token manipulation adversarial attacks, highlighted by a 22.44% less decrease ratio than the text-only language model in winning rate. Beyond Text' marks an advancement in social robot navigation and broader Human-Robot interactions, seamlessly integrating text-based guidance with human-audio-informed language models.",
    "pdf_url": "https://arxiv.org/pdf/2402.03494v3",
    "github_url": null,
    "published": "2024-02-05T20:11:56+00:00",
    "updated": "2024-11-11T04:03:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.02651v3",
    "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning",
    "authors": [
      "Chen",
      "Mees",
      "Kumar"
    ],
    "summary": "Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that encode semantic features of visual observations based on the VLM's internal knowledge and reasoning capabilities, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings from off-the-shelf, general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings. Finally, we show that our approach can use chain-of-thought prompting to produce representations of common-sense semantic reasoning, improving policy performance in novel scenes by 1.5 times.",
    "pdf_url": "https://arxiv.org/pdf/2402.02651v3",
    "github_url": null,
    "published": "2024-02-05T00:48:56+00:00",
    "updated": "2024-05-23T01:04:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.02559v1",
    "title": "NavHint: Vision and Language Navigation Agent with a Hint Generator",
    "authors": [
      "Zhang",
      "Guo",
      "Kordjamshidi"
    ],
    "summary": "Existing work on vision and language navigation mainly relies on navigation-related losses to establish the connection between vision and language modalities, neglecting aspects of helping the navigation agent build a deep understanding of the visual environment. In our work, we provide indirect supervision to the navigation agent through a hint generator that provides detailed visual descriptions. The hint generator assists the navigation agent in developing a global understanding of the visual environment. It directs the agent's attention toward related navigation details, including the relevant sub-instruction, potential challenges in recognition and ambiguities in grounding, and the targeted viewpoint description. To train the hint generator, we construct a synthetic dataset based on landmarks in the instructions and visible and distinctive objects in the visual environment. We evaluate our method on the R2R and R4R datasets and achieve state-of-the-art on several metrics. The experimental results demonstrate that generating hints not only enhances the navigation performance but also helps improve the interpretability of the agent's actions.",
    "pdf_url": "https://arxiv.org/pdf/2402.02559v1",
    "github_url": null,
    "published": "2024-02-04T16:23:16+00:00",
    "updated": "2024-02-04T16:23:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.01183v1",
    "title": "LINGO-Space: Language-Conditioned Incremental Grounding for Space",
    "authors": [
      "Kim",
      "Oh",
      "Hwang"
    ],
    "summary": "We aim to solve the problem of spatially localizing composite instructions referring to space: space grounding. Compared to current instance grounding, space grounding is challenging due to the ill-posedness of identifying locations referred to by discrete expressions and the compositional ambiguity of referring expressions. Therefore, we propose a novel probabilistic space-grounding methodology (LINGO-Space) that accurately identifies a probabilistic distribution of space being referred to and incrementally updates it, given subsequent referring expressions leveraging configurable polar distributions. Our evaluations show that the estimation using polar distributions enables a robot to ground locations successfully through $20$ table-top manipulation benchmark tests. We also show that updating the distribution helps the grounding method accurately narrow the referring space. We finally demonstrate the robustness of the space grounding with simulated manipulation and real quadruped robot navigation tasks. Code and videos are available at https://lingo-space.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2402.01183v1",
    "github_url": null,
    "published": "2024-02-02T06:58:39+00:00",
    "updated": "2024-02-02T06:58:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.00537v2",
    "title": "Robust Path Planning via Learning from Demonstrations for Robotic Catheters in Deformable Environments",
    "authors": [
      "Li",
      "Lambranzi",
      "Wu"
    ],
    "summary": "Objective: Navigation through tortuous and deformable vessels using catheters with limited steering capability underscores the need for reliable path planning. State-of-the-art path planners do not fully account for the deformable nature of the environment. Methods: This work proposes a robust path planner via a learning from demonstrations method, named Curriculum Generative Adversarial Imitation Learning (C-GAIL). This path planning framework takes into account the interaction between steerable catheters and vessel walls and the deformable property of vessels. Results: In-silico comparative experiments show that the proposed network achieves a 38% higher success rate in static environments and 17% higher in dynamic environments compared to a state-of-the-art approach based on GAIL. In-vitro validation experiments indicate that the path generated by the proposed C-GAIL path planner achieves a targeting error of 1.26$\\pm$0.55mm and a tracking error of 5.18$\\pm$3.48mm. These results represent improvements of 41% and 40% over the conventional centerline-following technique for targeting error and tracking error, respectively. Conclusion: The proposed C-GAIL path planner outperforms the state-of-the-art GAIL approach. The in-vitro validation experiments demonstrate that the path generated by the proposed C-GAIL path planner aligns better with the actual steering capability of the pneumatic artificial muscle-driven catheter utilized in this study. Therefore, the proposed approach can provide enhanced support to the user in navigating the catheter towards the target with greater accuracy, effectively meeting clinical accuracy requirements. Significance: The proposed path planning framework exhibits superior performance in managing uncertainty associated with vessel deformation, thereby resulting in lower tracking errors.",
    "pdf_url": "https://arxiv.org/pdf/2402.00537v2",
    "github_url": null,
    "published": "2024-02-01T12:05:53+00:00",
    "updated": "2024-08-31T13:51:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.17914v1",
    "title": "Attention Graph for Multi-Robot Social Navigation with Deep Reinforcement Learning",
    "authors": [
      "Escudie",
      "Matignon",
      "Saraydaryan"
    ],
    "summary": "Learning robot navigation strategies among pedestrian is crucial for domain based applications. Combining perception, planning and prediction allows us to model the interactions between robots and pedestrians, resulting in impressive outcomes especially with recent approaches based on deep reinforcement learning (RL). However, these works do not consider multi-robot scenarios. In this paper, we present MultiSoc, a new method for learning multi-agent socially aware navigation strategies using RL. Inspired by recent works on multi-agent deep RL, our method leverages graph-based representation of agent interactions, combining the positions and fields of view of entities (pedestrians and agents). Each agent uses a model based on two Graph Neural Network combined with attention mechanisms. First an edge-selector produces a sparse graph, then a crowd coordinator applies node attention to produce a graph representing the influence of each entity on the others. This is incorporated into a model-free RL framework to learn multi-agent policies. We evaluate our approach on simulation and provide a series of experiments in a set of various conditions (number of agents / pedestrians). Empirical results show that our method learns faster than social navigation deep RL mono-agent techniques, and enables efficient multi-agent implicit coordination in challenging crowd navigation with multiple heterogeneous humans. Furthermore, by incorporating customizable meta-parameters, we can adjust the neighborhood density to take into account in our navigation strategy.",
    "pdf_url": "https://arxiv.org/pdf/2401.17914v1",
    "github_url": null,
    "published": "2024-01-31T15:24:13+00:00",
    "updated": "2024-01-31T15:24:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.14349v1",
    "title": "Learning to navigate efficiently and precisely in real environments",
    "authors": [
      "Bono",
      "Poirier",
      "Antsfeld"
    ],
    "summary": "In the context of autonomous navigation of terrestrial robots, the creation of realistic models for agent dynamics and sensing is a widespread habit in the robotics literature and in commercial applications, where they are used for model based control and/or for localization and mapping. The more recent Embodied AI literature, on the other hand, focuses on modular or end-to-end agents trained in simulators like Habitat or AI-Thor, where the emphasis is put on photo-realistic rendering and scene diversity, but high-fidelity robot motion is assigned a less privileged role. The resulting sim2real gap significantly impacts transfer of the trained models to real robotic platforms. In this work we explore end-to-end training of agents in simulation in settings which minimize the sim2real gap both, in sensing and in actuation. Our agent directly predicts (discretized) velocity commands, which are maintained through closed-loop control in the real robot. The behavior of the real robot (including the underlying low-level controller) is identified and simulated in a modified Habitat simulator. Noise models for odometry and localization further contribute in lowering the sim2real gap. We evaluate on real navigation scenarios, explore different localization and point goal calculation methods and report significant gains in performance and robustness compared to prior work.",
    "pdf_url": "https://arxiv.org/pdf/2401.14349v1",
    "github_url": null,
    "published": "2024-01-25T17:50:05+00:00",
    "updated": "2024-01-25T17:50:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.13800v1",
    "title": "Multi-Object Navigation in real environments using hybrid policies",
    "authors": [
      "Sadek",
      "Bono",
      "Chidlovskii"
    ],
    "summary": "Navigation has been classically solved in robotics through the combination of SLAM and planning. More recently, beyond waypoint planning, problems involving significant components of (visual) high-level reasoning have been explored in simulated environments, mostly addressed with large-scale machine learning, in particular RL, offline-RL or imitation learning. These methods require the agent to learn various skills like local planning, mapping objects and querying the learned spatial representations. In contrast to simpler tasks like waypoint planning (PointGoal), for these more complex tasks the current state-of-the-art models have been thoroughly evaluated in simulation but, to our best knowledge, not yet in real environments.   In this work we focus on sim2real transfer. We target the challenging Multi-Object Navigation (Multi-ON) task and port it to a physical environment containing real replicas of the originally virtual Multi-ON objects. We introduce a hybrid navigation method, which decomposes the problem into two different skills: (1) waypoint navigation is addressed with classical SLAM combined with a symbolic planner, whereas (2) exploration, semantic mapping and goal retrieval are dealt with deep neural networks trained with a combination of supervised learning and RL. We show the advantages of this approach compared to end-to-end methods both in simulation and a real environment and outperform the SOTA for this task.",
    "pdf_url": "https://arxiv.org/pdf/2401.13800v1",
    "github_url": null,
    "published": "2024-01-24T20:41:25+00:00",
    "updated": "2024-01-24T20:41:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.12275v2",
    "title": "Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation",
    "authors": [
      "Li",
      "Hua",
      "Yao"
    ],
    "summary": "Social robot navigation can be helpful in various contexts of daily life but requires safe human-robot interactions and efficient trajectory planning. While modeling pairwise relations has been widely studied in multi-agent interacting systems, the ability to capture larger-scale group-wise activities is limited. In this paper, we propose a systematic relational reasoning approach with explicit inference of the underlying dynamically evolving relational structures, and we demonstrate its effectiveness for multi-agent trajectory prediction and social robot navigation. In addition to the edges between pairs of nodes (i.e., agents), we propose to infer hyperedges that adaptively connect multiple nodes to enable group-wise reasoning in an unsupervised manner. Our approach infers dynamically evolving relation graphs and hypergraphs to capture the evolution of relations, which the trajectory predictor employs to generate future states. Meanwhile, we propose to regularize the sharpness and sparsity of the learned relations and the smoothness of the relation evolution, which proves to enhance training stability and model performance. The proposed approach is validated on synthetic crowd simulations and real-world benchmark datasets. Experiments demonstrate that the approach infers reasonable relations and achieves state-of-the-art prediction performance. In addition, we present a deep reinforcement learning (DRL) framework for social robot navigation, which incorporates relational reasoning and trajectory prediction systematically. In a group-based crowd simulation, our method outperforms the strongest baseline by a significant margin in terms of safety, efficiency, and social compliance in dense, interactive scenarios. We also demonstrate the practical applicability of our method with real-world robot experiments. The code and videos can be found at https://relational-reasoning-nav.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2401.12275v2",
    "github_url": null,
    "published": "2024-01-22T18:58:22+00:00",
    "updated": "2024-11-11T18:59:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.07314v3",
    "title": "MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation",
    "authors": [
      "Chen",
      "Lin",
      "Xu"
    ],
    "summary": "Embodied agents equipped with GPT as their brains have exhibited extraordinary decision-making and generalization abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt GPT-4 to select potential locations within localized environments, without constructing an effective \"global-view\" for the agent to understand the overall environment. In this work, we present a novel map-guided GPT-based agent, dubbed MapGPT, which introduces an online linguistic-formed map to encourage global exploration. Specifically, we build an online map and incorporate it into the prompts that include node information and topological relationships, to help GPT understand the spatial environment. Benefiting from this design, we further propose an adaptive planning mechanism to assist the agent in performing multi-step path planning based on a map, systematically exploring multiple candidate nodes or sub-goals step by step. Extensive experiments demonstrate that our MapGPT is applicable to both GPT-4 and GPT-4V, achieving state-of-the-art zero-shot performance on R2R and REVERIE simultaneously (~10% and ~12% improvements in SR), and showcasing the newly emergent global thinking and path planning abilities of the GPT.",
    "pdf_url": "https://arxiv.org/pdf/2401.07314v3",
    "github_url": null,
    "published": "2024-01-14T15:34:48+00:00",
    "updated": "2024-06-20T07:23:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.06226v1",
    "title": "Learning Crowd Behaviors in Navigation with Attention-based Spatial-Temporal Graphs",
    "authors": [
      "Zhou",
      "Garcke"
    ],
    "summary": "Safe and efficient navigation in dynamic environments shared with humans remains an open and challenging task for mobile robots. Previous works have shown the efficacy of using reinforcement learning frameworks to train policies for efficient navigation. However, their performance deteriorates when crowd configurations change, i.e. become larger or more complex. Thus, it is crucial to fully understand the complex, dynamic, and sophisticated interactions of the crowd resulting in proactive and foresighted behaviors for robot navigation. In this paper, a novel deep graph learning architecture based on attention mechanisms is proposed, which leverages the spatial-temporal graph to enhance robot navigation. We employ spatial graphs to capture the current spatial interactions, and through the integration with RNN, the temporal graphs utilize past trajectory information to infer the future intentions of each agent. The spatial-temporal graph reasoning ability allows the robot to better understand and interpret the relationships between agents over time and space, thereby making more informed decisions. Compared to previous state-of-the-art methods, our method demonstrates superior robustness in terms of safety, efficiency, and generalization in various challenging scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2401.06226v1",
    "github_url": null,
    "published": "2024-01-11T19:09:41+00:00",
    "updated": "2024-01-11T19:09:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.02695v2",
    "title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
    "authors": [
      "Wu",
      "Mu",
      "Wu"
    ],
    "summary": "In the realm of household robotics, the Zero-Shot Object Navigation (ZSON) task empowers agents to adeptly traverse unfamiliar environments and locate objects from novel categories without prior explicit training. This paper introduces VoroNav, a novel semantic exploration framework that proposes the Reduced Voronoi Graph to extract exploratory paths and planning nodes from a semantic map constructed in real time. By harnessing topological and semantic information, VoroNav designs text-based descriptions of paths and images that are readily interpretable by a large language model (LLM). In particular, our approach presents a synergy of path and farsight descriptions to represent the environmental context, enabling LLM to apply commonsense reasoning to ascertain waypoints for navigation. Extensive evaluation on HM3D and HSSD validates VoroNav surpasses existing benchmarks in both success rate and exploration efficiency (absolute improvement: +2.8% Success and +3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally introduced metrics that evaluate obstacle avoidance proficiency and perceptual efficiency further corroborate the enhancements achieved by our method in ZSON planning. Project page: https://voro-nav.github.io",
    "pdf_url": "https://arxiv.org/pdf/2401.02695v2",
    "github_url": null,
    "published": "2024-01-05T08:05:07+00:00",
    "updated": "2024-02-06T05:15:20+00:00"
  }
]