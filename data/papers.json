[
  {
    "id": "http://arxiv.org/abs/2602.14401v1",
    "title": "pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI",
    "authors": [
      "Yang",
      "Wang",
      "Zhang"
    ],
    "summary": "Vision-Language Navigation VLN requires large-scale trajectory instruction data from private indoor environments, raising significant privacy concerns. Federated Learning FL mitigates this by keeping data on-device, but vanilla FL struggles under VLNs' extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal. This paper proposes pFedNavi, a structure-aware and dynamically adaptive personalized federated learning framework tailored for VLN. Our key idea is to personalize where it matters: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients, and performs fine-grained parameter fusion on the selected components (e.g., the encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization. We evaluate pFedNavi on two standard VLN benchmarks, R2R and RxR, using both ResNet and CLIP visual representations. Across all metrics, pFedNavi consistently outperforms the FedAvg-based VLN baseline, achieving up to 7.5% improvement in navigation success rate and up to 7.8% gain in trajectory fidelity, while converging 1.38x faster under non-IID conditions.",
    "pdf_url": "https://arxiv.org/pdf/2602.14401v1",
    "github_url": null,
    "published": "2026-02-16T02:18:09+00:00",
    "updated": "2026-02-16T02:18:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2602.12159v1",
    "title": "3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting",
    "authors": [
      "Zheng",
      "Chen",
      "Lu"
    ],
    "summary": "Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2602.12159v1",
    "github_url": null,
    "published": "2026-02-12T16:41:26+00:00",
    "updated": "2026-02-12T16:41:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2602.09972v1",
    "title": "Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning",
    "authors": [
      "Wang",
      "Fang",
      "Wang"
    ],
    "summary": "While large vision-language models (VLMs) show promise for object goal navigation, current methods still struggle with low success rates and inefficient localization of unseen objects--failures primarily attributed to weak temporal-spatial reasoning. Meanwhile, recent attempts to inject reasoning into VLM-based agents improve success rates but incur substantial computational overhead. To address both the ineffectiveness and inefficiency of existing approaches, we introduce Hydra-Nav, a unified VLM architecture that adaptively switches between a deliberative slow system for analyzing exploration history and formulating high-level plans, and a reactive fast system for efficient execution. We train Hydra-Nav through a three-stage curriculum: (i) spatial-action alignment to strengthen trajectory planning, (ii) memory-reasoning integration to enhance temporal-spatial reasoning over long-horizon exploration, and (iii) iterative rejection fine-tuning to enable selective reasoning at critical decision points. Extensive experiments demonstrate that Hydra-Nav achieves state-of-the-art performance on the HM3D, MP3D, and OVON benchmarks, outperforming the second-best methods by 11.1%, 17.4%, and 21.2%, respectively. Furthermore, we introduce SOT (Success weighted by Operation Time), a new metric to measure search efficiency across VLMs with varying reasoning intensity. Results show that adaptive reasoning significantly enhances search efficiency over fixed-frequency baselines.",
    "pdf_url": "https://arxiv.org/pdf/2602.09972v1",
    "github_url": null,
    "published": "2026-02-10T17:00:16+00:00",
    "updated": "2026-02-10T17:00:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2602.07629v1",
    "title": "LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation",
    "authors": [
      "Subedi",
      "Haroon",
      "Tetteh"
    ],
    "summary": "We propose LCLA (Language-Conditioned Latent Alignment), a framework for vision-language navigation that learns modular perception-action interfaces by aligning sensory observations to a latent representation of an expert policy. The expert is first trained with privileged state information, inducing a latent space sufficient for control, after which its latent interface and action head are frozen. A lightweight adapter is then trained to map raw visual-language observations, via a frozen vision-language model, into the expert's latent space, reducing the problem of visuomotor learning to supervised latent alignment rather than end-to-end policy optimization. This decoupling enforces a stable contract between perception and control, enabling expert behavior to be reused across sensing modalities and environmental variations. We instantiate LCLA and evaluate it on a vision-language indoor navigation task, where aligned latent spaces yield strong in-distribution performance and robust zero-shot generalization to unseen environments, lighting conditions, and viewpoints while remaining lightweight at inference time.",
    "pdf_url": "https://arxiv.org/pdf/2602.07629v1",
    "github_url": null,
    "published": "2026-02-07T17:20:43+00:00",
    "updated": "2026-02-07T17:20:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2602.07629v2",
    "title": "LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation",
    "authors": [
      "Subedi",
      "Haroon",
      "Tetteh"
    ],
    "summary": "We propose LCLA (Language-Conditioned Latent Alignment), a framework for vision-language navigation that learns modular perception-action interfaces by aligning sensory observations to a latent representation of an expert policy. The expert is first trained with privileged state information, inducing a latent space sufficient for control, after which its latent interface and action head are frozen. A lightweight adapter is then trained to map raw visual-language observations, via a frozen vision-language model, into the expert's latent space, reducing the problem of visuomotor learning to supervised latent alignment rather than end-to-end policy optimization. This decoupling enforces a stable contract between perception and control, enabling expert behavior to be reused across sensing modalities and environmental variations. We instantiate LCLA and evaluate it on a vision-language indoor navigation task, where aligned latent spaces yield strong in-distribution performance and robust zero-shot generalization to unseen environments, lighting conditions, and viewpoints while remaining lightweight at inference time.",
    "pdf_url": "https://arxiv.org/pdf/2602.07629v2",
    "github_url": null,
    "published": "2026-02-07T17:20:43+00:00",
    "updated": "2026-02-10T02:40:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2602.07555v1",
    "title": "VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation",
    "authors": [
      "Taioli",
      "Yang",
      "Raychaudhuri"
    ],
    "summary": "Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer \"Is this the target object?\" and \"Why should I take this action?\" The reasoning process unfolds in three stages: \"think\", \"think summary\", and \"action\", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.",
    "pdf_url": "https://arxiv.org/pdf/2602.07555v1",
    "github_url": null,
    "published": "2026-02-07T14:01:29+00:00",
    "updated": "2026-02-07T14:01:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2602.06459v1",
    "title": "User-Centric Object Navigation: A Benchmark with Integrated User Habits for Personalized Embodied Object Search",
    "authors": [
      "Wang",
      "Zhu",
      "Dong"
    ],
    "summary": "In the evolving field of robotics, the challenge of Object Navigation (ON) in household environments has attracted significant interest. Existing ON benchmarks typically place objects in locations guided by general scene priors, without accounting for the specific placement habits of individual users. This omission limits the adaptability of navigation agents in personalized household environments. To address this, we introduce User-centric Object Navigation (UcON), a new benchmark that incorporates user-specific object placement habits, referred to as user habits. This benchmark requires agents to leverage these user habits for more informed decision-making during navigation. UcON encompasses approximately 22,600 user habits across 489 object categories. UcON is, to our knowledge, the first benchmark that explicitly formalizes and evaluates habit-conditioned object navigation at scale and covers the widest range of target object categories. Additionally, we propose a habit retrieval module to extract and utilize habits related to target objects, enabling agents to infer their likely locations more effectively. Experimental results demonstrate that current SOTA methods exhibit substantial performance degradation under habit-driven object placement, while integrating user habits consistently improves success rates. Code is available at https://github.com/whcpumpkin/User-Centric-Object-Navigation.",
    "pdf_url": "https://arxiv.org/pdf/2602.06459v1",
    "github_url": "https://github.com/whcpumpkin/User-Centric-Object-Navigation",
    "published": "2026-02-06T07:43:05+00:00",
    "updated": "2026-02-06T07:43:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2602.06356v1",
    "title": "Nipping the Drift in the Bud: Retrospective Rectification for Robust Vision-Language Navigation",
    "authors": [
      "He",
      "Liu",
      "Xu"
    ],
    "summary": "Vision-Language Navigation (VLN) requires embodied agents to interpret natural language instructions and navigate through complex continuous 3D environments. However, the dominant imitation learning paradigm suffers from exposure bias, where minor deviations during inference lead to compounding errors. While DAgger-style approaches attempt to mitigate this by correcting error states, we identify a critical limitation: Instruction-State Misalignment. Forcing an agent to learn recovery actions from off-track states often creates supervision signals that semantically conflict with the original instruction. In response to these challenges, we introduce BudVLN, an online framework that learns from on-policy rollouts by constructing supervision to match the current state distribution. BudVLN performs retrospective rectification via counterfactual re-anchoring and decision-conditioned supervision synthesis, using a geodesic oracle to synthesize corrective trajectories that originate from valid historical states, ensuring semantic consistency. Experiments on the standard R2R-CE and RxR-CE benchmarks demonstrate that BudVLN consistently mitigates distribution shift and achieves state-of-the-art performance in both Success Rate and SPL.",
    "pdf_url": "https://arxiv.org/pdf/2602.06356v1",
    "github_url": null,
    "published": "2026-02-06T03:36:27+00:00",
    "updated": "2026-02-06T03:36:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2602.05827v1",
    "title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation",
    "authors": [
      "Zhang",
      "Liang",
      "Chen"
    ],
    "summary": "Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.",
    "pdf_url": "https://arxiv.org/pdf/2602.05827v1",
    "github_url": null,
    "published": "2026-02-05T16:16:13+00:00",
    "updated": "2026-02-05T16:16:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2602.00708v1",
    "title": "USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation",
    "authors": [
      "Gai",
      "Gao",
      "Zhou"
    ],
    "summary": "Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph's semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research.",
    "pdf_url": "https://arxiv.org/pdf/2602.00708v1",
    "github_url": null,
    "published": "2026-01-31T13:10:02+00:00",
    "updated": "2026-01-31T13:10:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2602.00708v2",
    "title": "USS-Nav: Unified Spatio-Semantic Scene Graph for Lightweight UAV Zero-Shot Object Navigation",
    "authors": [
      "Gai",
      "Gao",
      "Zhou"
    ],
    "summary": "Zero-Shot Object Navigation in unknown environments poses significant challenges for Unmanned Aerial Vehicles (UAVs) due to the conflict between high-level semantic reasoning requirements and limited onboard computational resources. To address this, we present USS-Nav, a lightweight framework that incrementally constructs a Unified Spatio-Semantic scene graph and enables efficient Large Language Model (LLM)-augmented Zero-Shot Object Navigation in unknown environments. Specifically, we introduce an incremental Spatial Connectivity Graph generation method utilizing polyhedral expansion to capture global geometric topology, which is dynamically partitioned into semantic regions via graph clustering. Concurrently, open-vocabulary object semantics are instantiated and anchored to this topology to form a hierarchical environmental representation. Leveraging this hierarchical structure, we present a coarse-to-fine exploration strategy: LLM grounded in the scene graph's semantics to determine global target regions, while a local planner optimizes frontier coverage based on information gain. Experimental results demonstrate that our framework outperforms state-of-the-art methods in terms of computational efficiency and real-time update frequency (15 Hz) on a resource-constrained platform. Furthermore, ablation studies confirm the effectiveness of our framework, showing substantial improvements in Success weighted by Path Length (SPL). The source code will be made publicly available to foster further research.",
    "pdf_url": "https://arxiv.org/pdf/2602.00708v2",
    "github_url": null,
    "published": "2026-01-31T13:10:02+00:00",
    "updated": "2026-02-03T05:20:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2602.00222v1",
    "title": "MapDream: Task-Driven Map Learning for Vision-Language Navigation",
    "authors": [
      "Lian",
      "Wang",
      "Wang"
    ],
    "summary": "Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.",
    "pdf_url": "https://arxiv.org/pdf/2602.00222v1",
    "github_url": null,
    "published": "2026-01-30T17:33:16+00:00",
    "updated": "2026-01-30T17:33:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2602.00222v2",
    "title": "MapDream: Task-Driven Map Learning for Vision-Language Navigation",
    "authors": [
      "Lian",
      "Wang",
      "Wang"
    ],
    "summary": "Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.",
    "pdf_url": "https://arxiv.org/pdf/2602.00222v2",
    "github_url": null,
    "published": "2026-01-30T17:33:16+00:00",
    "updated": "2026-02-03T09:27:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2601.21751v1",
    "title": "Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation",
    "authors": [
      "Peng",
      "Guo",
      "Xu"
    ],
    "summary": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a \"Granularity Rigidity\" problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling \"densification on demand\" in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at https://github.com/shannanshouyin/DGNav.",
    "pdf_url": "https://arxiv.org/pdf/2601.21751v1",
    "github_url": "https://github.com/shannanshouyin/DGNav",
    "published": "2026-01-29T14:06:23+00:00",
    "updated": "2026-01-29T14:06:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2601.21409v1",
    "title": "DSCD-Nav: Dual-Stance Cooperative Debate for Object Navigation",
    "authors": [
      "An",
      "Liu",
      "Xu"
    ],
    "summary": "Adaptive navigation in unfamiliar indoor environments is crucial for household service robots. Despite advances in zero-shot perception and reasoning from vision-language models, existing navigation systems still rely on single-pass scoring at the decision layer, leading to overconfident long-horizon errors and redundant exploration. To tackle these problems, we propose Dual-Stance Cooperative Debate Navigation (DSCD-Nav), a decision mechanism that replaces one-shot scoring with stance-based cross-checking and evidence-aware arbitration to improve action reliability under partial observability. Specifically, given the same observation and candidate action set, we explicitly construct two stances by conditioning the evaluation on diverse and complementary objectives: a Task-Scene Understanding (TSU) stance that prioritizes goal progress from scene-layout cues, and a Safety-Information Balancing (SIB) stance that emphasizes risk and information value. The stances conduct a cooperative debate and make policy by cross-checking their top candidates with cue-grounded arguments. Then, a Navigation Consensus Arbitration (NCA) agent is employed to consolidate both sides' reasons and evidence, optionally triggering lightweight micro-probing to verify uncertain choices, preserving NCA's primary intent while disambiguating. Experiments on HM3Dv1, HM3Dv2, and MP3D demonstrate consistent improvements in success and path efficiency while reducing exploration redundancy.",
    "pdf_url": "https://arxiv.org/pdf/2601.21409v1",
    "github_url": null,
    "published": "2026-01-29T08:47:55+00:00",
    "updated": "2026-01-29T08:47:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2601.21409v2",
    "title": "DSCD-Nav: Dual-Stance Cooperative Debate for Object Navigation",
    "authors": [
      "An",
      "Liu",
      "Xu"
    ],
    "summary": "Adaptive navigation in unfamiliar indoor environments is crucial for household service robots. Despite advances in zero-shot perception and reasoning from vision-language models, existing navigation systems still rely on single-pass scoring at the decision layer, leading to overconfident long-horizon errors and redundant exploration. To tackle these problems, we propose Dual-Stance Cooperative Debate Navigation (DSCD-Nav), a decision mechanism that replaces one-shot scoring with stance-based cross-checking and evidence-aware arbitration to improve action reliability under partial observability. Specifically, given the same observation and candidate action set, we explicitly construct two stances by conditioning the evaluation on diverse and complementary objectives: a Task-Scene Understanding (TSU) stance that prioritizes goal progress from scene-layout cues, and a Safety-Information Balancing (SIB) stance that emphasizes risk and information value. The stances conduct a cooperative debate and make policy by cross-checking their top candidates with cue-grounded arguments. Then, a Navigation Consensus Arbitration (NCA) agent is employed to consolidate both sides' reasons and evidence, optionally triggering lightweight micro-probing to verify uncertain choices, preserving NCA's primary intent while disambiguating. Experiments on HM3Dv1, HM3Dv2, and MP3D demonstrate consistent improvements in success and path efficiency while reducing exploration redundancy.",
    "pdf_url": "https://arxiv.org/pdf/2601.21409v2",
    "github_url": null,
    "published": "2026-01-29T08:47:55+00:00",
    "updated": "2026-01-31T05:43:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2601.18188v1",
    "title": "\\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation",
    "authors": [
      "Zhu",
      "Zhang",
      "Wang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \\textsc{NaVIDA} (\\textbf{Nav}igation with \\textbf{I}nverse \\textbf{D}ynamics \\textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \\textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \\textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \\textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.",
    "pdf_url": "https://arxiv.org/pdf/2601.18188v1",
    "github_url": null,
    "published": "2026-01-26T06:16:17+00:00",
    "updated": "2026-01-26T06:16:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2601.13976v1",
    "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
    "authors": [
      "Zuo",
      "Mu",
      "Jiang"
    ],
    "summary": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.",
    "pdf_url": "https://arxiv.org/pdf/2601.13976v1",
    "github_url": null,
    "published": "2026-01-20T13:54:10+00:00",
    "updated": "2026-01-20T13:54:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2601.13976v2",
    "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
    "authors": [
      "Zuo",
      "Mu",
      "Jiang"
    ],
    "summary": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.",
    "pdf_url": "https://arxiv.org/pdf/2601.13976v2",
    "github_url": null,
    "published": "2026-01-20T13:54:10+00:00",
    "updated": "2026-01-23T08:44:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2601.12742v1",
    "title": "AirHunt: Bridging VLM Semantics and Continuous Planning for Efficient Aerial Object Navigation",
    "authors": [
      "Chen",
      "Liu",
      "Ma"
    ],
    "summary": "Recent advances in large Vision-Language Models (VLMs) have provided rich semantic understanding that empowers drones to search for open-set objects via natural language instructions. However, prior systems struggle to integrate VLMs into practical aerial systems due to orders-of-magnitude frequency mismatch between VLM inference and real-time planning, as well as VLMs' limited 3D scene understanding. They also lack a unified mechanism to balance semantic guidance with motion efficiency in large-scale environments. To address these challenges, we present AirHunt, an aerial object navigation system that efficiently locates open-set objects with zero-shot generalization in outdoor environments by seamlessly fusing VLM semantic reasoning with continuous path planning. AirHunt features a dual-pathway asynchronous architecture that establishes a synergistic interface between VLM reasoning and path planning, enabling continuous flight with adaptive semantic guidance that evolves through motion. Moreover, we propose an active dual-task reasoning module that exploits geometric and semantic redundancy to enable selective VLM querying, and a semantic-geometric coherent planning module that dynamically reconciles semantic priorities and motion efficiency in a unified framework, enabling seamless adaptation to environmental heterogeneity. We evaluate AirHunt across diverse object navigation tasks and environments, demonstrating a higher success rate with lower navigation error and reduced flight time compared to state-of-the-art methods. Real-world experiments further validate AirHunt's practical capability in complex and challenging environments. Code and dataset will be made publicly available before publication.",
    "pdf_url": "https://arxiv.org/pdf/2601.12742v1",
    "github_url": null,
    "published": "2026-01-19T05:50:03+00:00",
    "updated": "2026-01-19T05:50:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2601.09111v1",
    "title": "Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning",
    "authors": [
      "Li",
      "Wu",
      "Zhang"
    ],
    "summary": "Vision-Language Navigation aims to enable agents to navigate to a target location based on language instructions. Traditional VLN often follows a close-set assumption, i.e., training and test data share the same style of the input images and instructions. However, the real world is open and filled with various unseen environments, posing enormous difficulties for close-set methods. To this end, we focus on the General Scene Adaptation (GSA-VLN) task, aiming to learn generalized navigation ability by introducing diverse environments and inconsistent intructions.Towards this task, when facing unseen environments and instructions, the challenge mainly lies in how to enable the agent to dynamically produce generalized strategies during the navigation process. Recent research indicates that by means of fast and slow cognition systems, human beings could generate stable policies, which strengthen their adaptation for open world. Inspired by this idea, we propose the slow4fast-VLN, establishing a dynamic interactive fast-slow reasoning framework. The fast-reasoning module, an end-to-end strategy network, outputs actions via real-time input. It accumulates execution records in a history repository to build memory. The slow-reasoning module analyze the memories generated by the fast-reasoning module. Through deep reflection, it extracts experiences that enhance the generalization ability of decision-making. These experiences are structurally stored and used to continuously optimize the fast-reasoning module. Unlike traditional methods that treat fast-slow reasoning as independent mechanisms, our framework enables fast-slow interaction. By leveraging the experiences from slow reasoning. This interaction allows the system to continuously adapt and efficiently execute navigation tasks when facing unseen scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2601.09111v1",
    "github_url": null,
    "published": "2026-01-14T03:22:16+00:00",
    "updated": "2026-01-14T03:22:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.21201v1",
    "title": "Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation",
    "authors": [
      "He",
      "Huang",
      "Liu"
    ],
    "summary": "Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \\textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.",
    "pdf_url": "https://arxiv.org/pdf/2512.21201v1",
    "github_url": null,
    "published": "2025-12-24T14:28:17+00:00",
    "updated": "2025-12-24T14:28:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.20940v1",
    "title": "ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments",
    "authors": [
      "Ye",
      "Mao",
      "Cui"
    ],
    "summary": "Vision-Language Navigation in Continuous Environments (VLN-CE) requires an embodied agent to navigate towards target in continuous environments, following natural language instructions. While current graph-based methods offer an efficient, structured approach by abstracting the environment into a topological map and simplifying the action space to waypoint selection, they lag behind methods based on Large Vision-Language Models (LVLMs) in leveraging large-scale data and advanced training paradigms. In this paper, we try to bridge this gap by introducing ETP-R1, a framework that applies the paradigm of scaling up data and Reinforcement Fine-Tuning (RFT) to a graph-based VLN-CE model. To build a strong foundation, we first construct a high-quality, large-scale pretraining dataset using the Gemini API. This dataset consists of diverse, low-hallucination instructions for topological trajectories, providing rich supervision for our graph-based policy to map language to topological paths. This foundation is further strengthened by unifying data from both R2R and RxR tasks for joint pretraining. Building on this, we introduce a three-stage training paradigm, which culminates in the first application of closed-loop, online RFT to a graph-based VLN-CE model, powered by the Group Relative Policy Optimization (GRPO) algorithm. Extensive experiments demonstrate that our approach is highly effective, establishing new state-of-the-art performance across all major metrics on both the R2R-CE and RxR-CE benchmarks. Our code is available at https://github.com/Cepillar/ETP-R1.",
    "pdf_url": "https://arxiv.org/pdf/2512.20940v1",
    "github_url": "https://github.com/Cepillar/ETP-R1",
    "published": "2025-12-24T04:53:03+00:00",
    "updated": "2025-12-24T04:53:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.19021v1",
    "title": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation",
    "authors": [
      "Lin",
      "Li",
      "Zhao"
    ],
    "summary": "Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting \"ghost\" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.",
    "pdf_url": "https://arxiv.org/pdf/2512.19021v1",
    "github_url": null,
    "published": "2025-12-22T04:27:26+00:00",
    "updated": "2025-12-22T04:27:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.18028v1",
    "title": "Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation",
    "authors": [
      "Sohn",
      "Dillitzer",
      "Corso"
    ],
    "summary": "Vision-language navigation requires agents to reason and act under constraints of embodiment. While vision-language models (VLMs) demonstrate strong generalization, current benchmarks provide limited understanding of how embodiment -- i.e., the choice of physical platform, sensor configuration, and modality alignment -- influences perception, reasoning, and control. We introduce Embodied4C, a closed-loop benchmark designed as a Turing test for embodied reasoning. The benchmark evaluates the core embodied capabilities of VLMs across three heterogeneous embodiments -- autonomous vehicles, aerial drones, and robotic manipulators -- through approximately 1.1K one-shot reasoning questions and 58 goal-directed navigation tasks. These tasks jointly assess four foundational dimensions: semantic, spatial, temporal, and physical reasoning. Each embodiment presents dynamic sensor configurations and environment variations to probe generalization beyond platform-specific adaptation. To prevent embodiment overfitting, Embodied4C integrates domain-far queries targeting abstract and cross-context reasoning. Comprehensive evaluation across ten state-of-the-art VLMs and four embodied control baselines shows that cross-modal alignment and instruction tuning matter more than scale, while spatial and temporal reasoning remains the primary bottleneck for reliable embodied competence.",
    "pdf_url": "https://arxiv.org/pdf/2512.18028v1",
    "github_url": null,
    "published": "2025-12-19T19:47:55+00:00",
    "updated": "2025-12-19T19:47:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.16909v1",
    "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
    "authors": [
      "Ju",
      "Liang",
      "Wang"
    ],
    "summary": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.",
    "pdf_url": "https://arxiv.org/pdf/2512.16909v1",
    "github_url": null,
    "published": "2025-12-18T18:59:03+00:00",
    "updated": "2025-12-18T18:59:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.16755v1",
    "title": "CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?",
    "authors": [
      "Wang",
      "Liang",
      "Gao"
    ],
    "summary": "Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., \"I am thirsty\") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling \"last-mile\" navigation challenges.",
    "pdf_url": "https://arxiv.org/pdf/2512.16755v1",
    "github_url": null,
    "published": "2025-12-18T16:53:12+00:00",
    "updated": "2025-12-18T16:53:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.16514v1",
    "title": "Algorithmic Monetary Policies for Blockchain Participation Games",
    "authors": [
      "Ferraioli",
      "Penna",
      "Schneider"
    ],
    "summary": "A central challenge in blockchain tokenomics is aligning short-term performance incentives with long-term decentralization goals. We propose a framework for algorithmic monetary policies that navigates this tradeoff in repeated participation games. Agents, characterized by type (capability) and stake, choose to participate or abstain at each round; the policy (probabilistically) selects high-type agents for task execution (maximizing throughput) while distributing rewards to sustain decentralization. We analyze equilibria under two agent behaviors: myopic (short-term utility maximization) and foresighted (multi-round planning). For myopic agents, performance-centric policies risk centralization, but foresight enables stable decentralization with some volatility to the token value. We further discuss virtual stake--a hybrid of type and stake--as an alternative approach. We show that the initial virtual stake distribution critically impacts long-term outcomes, suggesting that policies must indirectly manage decentralization.",
    "pdf_url": "https://arxiv.org/pdf/2512.16514v1",
    "github_url": null,
    "published": "2025-12-18T13:28:00+00:00",
    "updated": "2025-12-18T13:28:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.16493v1",
    "title": "YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images",
    "authors": [
      "Hafeez",
      "Garratt",
      "Plested"
    ],
    "summary": "The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.",
    "pdf_url": "https://arxiv.org/pdf/2512.16493v1",
    "github_url": null,
    "published": "2025-12-18T13:00:05+00:00",
    "updated": "2025-12-18T13:00:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.16461v1",
    "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning",
    "authors": [
      "Sohn",
      "Dillitzer",
      "Corso"
    ],
    "summary": "Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.",
    "pdf_url": "https://arxiv.org/pdf/2512.16461v1",
    "github_url": null,
    "published": "2025-12-18T12:27:06+00:00",
    "updated": "2025-12-18T12:27:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.16446v1",
    "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion",
    "authors": [
      "Yalcin",
      "O'Hara",
      "Stamatopoulou"
    ],
    "summary": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.",
    "pdf_url": "https://arxiv.org/pdf/2512.16446v1",
    "github_url": null,
    "published": "2025-12-18T12:08:24+00:00",
    "updated": "2025-12-18T12:08:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.16295v1",
    "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models",
    "authors": [
      "Wu",
      "Xie",
      "Li"
    ],
    "summary": "With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.",
    "pdf_url": "https://arxiv.org/pdf/2512.16295v1",
    "github_url": "https://github.com/numbmelon/OS-Oracle",
    "published": "2025-12-18T08:29:50+00:00",
    "updated": "2025-12-18T08:29:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.16070v1",
    "title": "LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)",
    "authors": [
      "Wang",
      "Li",
      "Ding"
    ],
    "summary": "The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.",
    "pdf_url": "https://arxiv.org/pdf/2512.16070v1",
    "github_url": null,
    "published": "2025-12-18T01:35:30+00:00",
    "updated": "2025-12-18T01:35:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.16027v1",
    "title": "SWIFT-Nav: Stability-Aware Waypoint-Level TD3 with Fuzzy Arbitration for UAV Navigation in Cluttered Environments",
    "authors": [
      "Ji",
      "Bamdad",
      "Cruz"
    ],
    "summary": "Efficient and reliable UAV navigation in cluttered and dynamic environments remains challenging. We propose SWIFT-Nav: Stability-aware Waypoint-level Integration of Fuzzy arbitration and TD3 for Navigation, a TD3-based navigation framework that achieves fast, stable convergence to obstacle-aware paths. The system couples a sensor-driven perception front end with a TD3 waypoint policy: the perception module converts LiDAR ranges into a confidence-weighted safety map and goal cues, while the TD3 policy is trained with Prioritised Experience Replay to focus on high-error transitions and a decaying epsilon-greedy exploration schedule that gradually shifts from exploration to exploitation. A lightweight fuzzy-logic layer computes a safety score from radial measurements and near obstacles, gates mode switching and clamps unsafe actions; in parallel, task-aligned reward shaping combining goal progress, clearance, and switch-economy terms provides dense, well-scaled feedback that accelerates learning. Implemented in Webots with proximity-based collision checking, our approach consistently outperforms baselines in trajectory smoothness and generalization to unseen layouts, while preserving real-time responsiveness. These results show that combining TD3 with replay prioritisation, calibrated exploration, and fuzzy-safety rules yields a robust and deployable solution for UAV navigation in cluttered scenes.",
    "pdf_url": "https://arxiv.org/pdf/2512.16027v1",
    "github_url": null,
    "published": "2025-12-17T23:19:06+00:00",
    "updated": "2025-12-17T23:19:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.16019v1",
    "title": "Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios",
    "authors": [
      "Zhang",
      "Tsoi",
      "Nagib"
    ],
    "summary": "Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.",
    "pdf_url": "https://arxiv.org/pdf/2512.16019v1",
    "github_url": null,
    "published": "2025-12-17T23:06:36+00:00",
    "updated": "2025-12-17T23:06:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15940v1",
    "title": "R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space",
    "authors": [
      "Sohn",
      "Dillitzer",
      "Corso"
    ],
    "summary": "Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.15940v1",
    "github_url": null,
    "published": "2025-12-17T20:08:32+00:00",
    "updated": "2025-12-17T20:08:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15933v1",
    "title": "City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs",
    "authors": [
      "Dalal",
      "Mishra",
      "Ahuja"
    ],
    "summary": "Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/",
    "pdf_url": "https://arxiv.org/pdf/2512.15933v1",
    "github_url": null,
    "published": "2025-12-17T19:59:31+00:00",
    "updated": "2025-12-17T19:59:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15661v1",
    "title": "Prospects for quantum advantage in machine learning from the representability of functions",
    "authors": [
      "Masot-Llima",
      "Gil-Fuster",
      "Bravo-Prieto"
    ],
    "summary": "Demonstrating quantum advantage in machine learning tasks requires navigating a complex landscape of proposed models and algorithms. To bring clarity to this search, we introduce a framework that connects the structure of parametrized quantum circuits to the mathematical nature of the functions they can actually learn. Within this framework, we show how fundamental properties, like circuit depth and non-Clifford gate count, directly determine whether a model's output leads to efficient classical simulation or surrogation. We argue that this analysis uncovers common pathways to dequantization that underlie many existing simulation methods. More importantly, it reveals critical distinctions between models that are fully simulatable, those whose function space is classically tractable, and those that remain robustly quantum. This perspective provides a conceptual map of this landscape, clarifying how different models relate to classical simulability and pointing to where opportunities for quantum advantage may lie.",
    "pdf_url": "https://arxiv.org/pdf/2512.15661v1",
    "github_url": null,
    "published": "2025-12-17T18:14:59+00:00",
    "updated": "2025-12-17T18:14:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15558v1",
    "title": "Deep Reinforcement Learning for EH-Enabled Cognitive-IoT Under Jamming Attacks",
    "authors": [
      "Abdolkhani",
      "Khalek",
      "Hamouda"
    ],
    "summary": "In the evolving landscape of the Internet of Things (IoT), integrating cognitive radio (CR) has become a practical solution to address the challenge of spectrum scarcity, leading to the development of cognitive IoT (CIoT). However, the vulnerability of radio communications makes radio jamming attacks a key concern in CIoT networks. In this paper, we introduce a novel deep reinforcement learning (DRL) approach designed to optimize throughput and extend network lifetime of an energy-constrained CIoT system under jamming attacks. This DRL framework equips a CIoT device with the autonomy to manage energy harvesting (EH) and data transmission, while also regulating its transmit power to respect spectrum-sharing constraints. We formulate the optimization problem under various constraints, and we model the CIoT device's interactions within the channel as a model-free Markov decision process (MDP). The MDP serves as a foundation to develop a double deep Q-network (DDQN), designed to help the CIoT agent learn the optimal communication policy to navigate challenges such as dynamic channel occupancy, jamming attacks, and channel fading while achieving its goal. Additionally, we introduce a variant of the upper confidence bound (UCB) algorithm, named UCB-IA, which enhances the CIoT network's ability to efficiently navigate jamming attacks within the channel. The proposed DRL algorithm does not rely on prior knowledge and uses locally observable information such as channel occupancy, jamming activity, channel gain, and energy arrival to make decisions. Extensive simulations prove that our proposed DRL algorithm that utilizes the UCB-IA strategy surpasses existing benchmarks, allowing for a more adaptive, energy-efficient, and secure spectrum sharing in CIoT networks.",
    "pdf_url": "https://arxiv.org/pdf/2512.15558v1",
    "github_url": null,
    "published": "2025-12-17T16:09:21+00:00",
    "updated": "2025-12-17T16:09:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15557v1",
    "title": "OMCL: Open-vocabulary Monte Carlo Localization",
    "authors": [
      "Kruzhkov",
      "Memmesheimer",
      "Behnke"
    ],
    "summary": "Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.",
    "pdf_url": "https://arxiv.org/pdf/2512.15557v1",
    "github_url": null,
    "published": "2025-12-17T16:08:53+00:00",
    "updated": "2025-12-17T16:08:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15491v1",
    "title": "GazeBlend: Exploring Paired Gaze-Based Input Techniques for Navigation and Selection Tasks on Mobile Devices",
    "authors": [
      "Namnakani",
      "Abdrabou",
      "Grizou"
    ],
    "summary": "The potential of gaze for hands-free mobile interaction is increasingly evident. While each gaze input technique presents distinct advantages and limitations, a combination can amplify strengths and mitigate challenges. We report on the results of a user study (N=24), in which we compared the usability and performance of pairing three popular gaze input techniques: Dwell Time, Pursuits, and Gaze Gestures, for navigation and selection tasks while sitting and walking. Results show that pairing gestures for navigation with either Dwell time or Pursuits for selection improves task completion time and rate compared to using either individually. We discuss the implications of pairing gaze input techniques, such as how Pursuits may negatively impact other techniques, likely due to the visual clutter it adds, how integrating gestures for navigation reduces the chances of unintentional selections, and the impact of motor activity on performance. Our findings provide insights for effective gaze-enabled interfaces.",
    "pdf_url": "https://arxiv.org/pdf/2512.15491v1",
    "github_url": null,
    "published": "2025-12-17T14:40:02+00:00",
    "updated": "2025-12-17T14:40:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15327v1",
    "title": "Vision-based module for accurately reading linear scales in a laboratory",
    "authors": [
      "Saini",
      "Maiti",
      "Rai"
    ],
    "summary": "Capabilities and the number of vision-based models are increasing rapidly. And these vision models are now able to do more tasks like object detection, image classification, instance segmentation etc. with great accuracy. But models which can take accurate quantitative measurements form an image, as a human can do by just looking at it, are rare. For a robot to work with complete autonomy in a Laboratory environment, it needs to have some basic skills like navigation, handling objects, preparing samples etc. to match human-like capabilities in an unstructured environment. Another important capability is to read measurements from instruments and apparatus. Here, we tried to mimic a human inspired approach to read measurements from a linear scale. As a test case we have picked reading level from a syringe and a measuring cylinder. For a randomly oriented syringe we carry out transformations to correct the orientation. To make the system efficient and robust, the area of interest is reduced to just the linear scale containing part of the image. After that, a series of features were extracted like the major makers, the corresponding digits, and the level indicator location, from which the final reading was calculated. Readings obtained using this system were also compared against human read values of the same instances and an accurate correspondence was observed.",
    "pdf_url": "https://arxiv.org/pdf/2512.15327v1",
    "github_url": null,
    "published": "2025-12-17T11:24:22+00:00",
    "updated": "2025-12-17T11:24:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15258v1",
    "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments",
    "authors": [
      "Wu",
      "Zhu",
      "Li"
    ],
    "summary": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.",
    "pdf_url": "https://arxiv.org/pdf/2512.15258v1",
    "github_url": null,
    "published": "2025-12-17T10:02:55+00:00",
    "updated": "2025-12-17T10:02:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15207v1",
    "title": "Remote Magnetic Levitation Using Reduced Attitude Control and Parametric Field Models",
    "authors": [
      "Singh",
      "Zughaibi",
      "Arx"
    ],
    "summary": "Electromagnetic navigation systems (eMNS) are increasingly used in minimally invasive procedures such as endovascular interventions and targeted drug delivery due to their ability to generate fast and precise magnetic fields. In this paper, we utilize the OctoMag eMNS to achieve remote levitation and control of a rigid body across large air gaps which showcases the dynamic capabilities of clinical eMNS. A compact parametric analytical model maps coil currents to the forces and torques acting on the levitating object, eliminating the need for computationally expensive simulations or lookup tables and leading to a levitator agnostic modeling approach. Translational motion is stabilized using linear quadratic regulators. A nonlinear time-invariant controller is used to regulate the reduced attitude accounting for the inherent uncontrollability of rotations about the dipole axis and stabilizing the full five degrees of freedom controllable pose subspace. We analyze key design limitations and evaluate the approach through trajectory tracking experiments. This work demonstrates the dynamic capabilities and potential of feedback control in electromagnetic navigation, which is likely to open up new medical applications.",
    "pdf_url": "https://arxiv.org/pdf/2512.15207v1",
    "github_url": null,
    "published": "2025-12-17T08:58:52+00:00",
    "updated": "2025-12-17T08:58:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15163v1",
    "title": "MCP-SafetyBench: A Benchmark for Safety Evaluation of Large Language Models with Real-World MCP Servers",
    "authors": [
      "Zong",
      "Shen",
      "Wang"
    ],
    "summary": "Large language models (LLMs) are evolving into agentic systems that reason, plan, and operate external tools. The Model Context Protocol (MCP) is a key enabler of this transition, offering a standardized interface for connecting LLMs with heterogeneous tools and services. Yet MCP's openness and multi-server workflows introduce new safety risks that existing benchmarks fail to capture, as they focus on isolated attacks or lack real-world coverage. We present MCP-SafetyBench, a comprehensive benchmark built on real MCP servers that supports realistic multi-turn evaluation across five domains: browser automation, financial analysis, location navigation, repository management, and web search. It incorporates a unified taxonomy of 20 MCP attack types spanning server, host, and user sides, and includes tasks requiring multi-step reasoning and cross-server coordination under uncertainty. Using MCP-SafetyBench, we systematically evaluate leading open- and closed-source LLMs, revealing large disparities in safety performance and escalating vulnerabilities as task horizons and server interactions grow. Our results highlight the urgent need for stronger defenses and establish MCP-SafetyBench as a foundation for diagnosing and mitigating safety risks in real-world MCP deployments.",
    "pdf_url": "https://arxiv.org/pdf/2512.15163v1",
    "github_url": null,
    "published": "2025-12-17T08:00:32+00:00",
    "updated": "2025-12-17T08:00:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15047v1",
    "title": "HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles",
    "authors": [
      "Wang",
      "Feng",
      "Fang"
    ],
    "summary": "3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.",
    "pdf_url": "https://arxiv.org/pdf/2512.15047v1",
    "github_url": null,
    "published": "2025-12-17T03:22:27+00:00",
    "updated": "2025-12-17T03:22:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.14691v2",
    "title": "MMGR: Multi-Modal Generative Reasoning",
    "authors": [
      "Cai",
      "Qiu",
      "Ma"
    ],
    "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.",
    "pdf_url": "https://arxiv.org/pdf/2512.14691v2",
    "github_url": null,
    "published": "2025-12-16T18:58:04+00:00",
    "updated": "2025-12-17T18:42:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.14428v1",
    "title": "Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations",
    "authors": [
      "Kurda",
      "Steuernagel",
      "Jung"
    ],
    "summary": "The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .",
    "pdf_url": "https://arxiv.org/pdf/2512.14428v1",
    "github_url": null,
    "published": "2025-12-16T14:17:00+00:00",
    "updated": "2025-12-16T14:17:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.14411v1",
    "title": "Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids",
    "authors": [
      "Habib",
      "Petruzzelli"
    ],
    "summary": "Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.",
    "pdf_url": "https://arxiv.org/pdf/2512.14411v1",
    "github_url": null,
    "published": "2025-12-16T13:54:34+00:00",
    "updated": "2025-12-16T13:54:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.14377v1",
    "title": "Steering Alternative Realities through Local Quantum Memory Operations",
    "authors": [
      "Ma"
    ],
    "summary": "Quantum measurement resolves a superposition into a definite outcome by correlating it with an observer's memory -- a reality register. While the global quantum state remains coherent, the observer's local reality becomes singular and definite. This work introduces reality steering, a protocol that allows an observer to probabilistically access a different reality already supported by the initial quantum state, without reversing decoherence on the environment. The mechanism relies on locally erasing the 'which-outcome' information stored in the observer's brain. Here, 'local' means operations confined to the observer's memory, excluding the environment, which may be cosmically large. Reality steering nevertheless faces intrinsic constraints: successful navigation requires coherent participation from the observer's counterparts across the relevant branches, and any transition is operationally indistinguishable from non-transition. After arriving in a new reality, all memory records are perfectly consistent with that reality, leaving no internal evidence that a switch occurred. This makes conscious confirmation impossible within standard quantum mechanics. We show that nonlinear operations beyond the standard theory could, in principle, enable verifiable and deliberate navigation. Our results shift multi-reality exploration from philosophical speculation toward a concrete -- though fundamentally constrained -- quantum-informational framework.",
    "pdf_url": "https://arxiv.org/pdf/2512.14377v1",
    "github_url": null,
    "published": "2025-12-16T13:05:55+00:00",
    "updated": "2025-12-16T13:05:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.14340v1",
    "title": "Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments",
    "authors": [
      "Karhunen",
      "Hakala",
      "Karjalainen"
    ],
    "summary": "The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years. While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge. The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight. However, the experiments conducted in the literature and their reporting lack rigor. Very rarely, the density and the difficulty of the test forests are reported, or multiple flights are flown, and the success rate of those flights is reported. The aim of this study was to implement an autonomously flying quadrotor based on a lightweight lidar using openly available algorithms and test its behavior in real forest environments. A set of rigorous experiments was conducted with a quadrotor prototype utilizing the IPC path planner and LTA-OM SLAM algorithm. Based on the results of the first 33 flights, the original system was further enhanced. With the optimized system, 60 flights were performed, resulting in a total of 93 test flights. The optimized system performed significantly better in terms of reliability and flight mission completion times, achieving success rates of 12/15 in a medium-density forest and 15/15 in a dense forest, at a target flight velocity of 1 m/s. At a target flight velocity of 2 m/s, it had a success rate of 12/15 and 5/15, respectively. Furthermore, a standardized testing setup and evaluation criteria were proposed, enabling consistent performance comparisons of autonomous under-canopy UAV systems, enhancing reproducibility, guiding system improvements, and accelerating progress in forest robotics.",
    "pdf_url": "https://arxiv.org/pdf/2512.14340v1",
    "github_url": null,
    "published": "2025-12-16T12:08:12+00:00",
    "updated": "2025-12-16T12:08:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.14302v1",
    "title": "Universal Structure of Nonlocal Operators for Deterministic Navigation and Geometric Locking",
    "authors": [
      "Bao",
      "Guo",
      "Qu"
    ],
    "summary": "We establish a universal geometric framework that transforms the search for optimal nonlocal operators from a combinatorial black box into a deterministic predict-verify operation. We discover that the principal eigenvalue governing nonlocality is rigorously dictated by a low-dimensional manifold parameterized by merely two fundamental angular variables, $θ$ and $φ$, whose symmetry leads to further simplification. This geometric distillation establishes a precise mapping connecting external control parameters directly to optimal measurement configurations. Crucially, a comparative analysis of the geometric angles against the principal eigenvalue spectrum, including its magnitude, susceptibility, and nonlocal gap, reveals a fundamental dichotomy in quantum criticality. While transitions involving symmetry sector rotation manifest as geometric criticality with drastic operator reorientation, transitions dominated by strong anisotropy exhibit geometric locking, where the optimal basis remains robust despite clear signatures of phase transitions in the spectral indicators. This distinction offers a novel structural classification of quantum phase transitions and provides a precision navigation chart for Bell experiments.",
    "pdf_url": "https://arxiv.org/pdf/2512.14302v1",
    "github_url": null,
    "published": "2025-12-16T11:15:47+00:00",
    "updated": "2025-12-16T11:15:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.14222v2",
    "title": "History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation",
    "authors": [
      "Ding",
      "Gao",
      "Pan"
    ],
    "summary": "Aerial Vision-and-Language Navigation (AVLN) requires Unmanned Aerial Vehicle (UAV) agents to localize targets in large-scale urban environments based on linguistic instructions. While successful navigation demands both global environmental reasoning and local scene comprehension, existing UAV agents typically adopt mono-granularity frameworks that struggle to balance these two aspects. To address this limitation, this work proposes a History-Enhanced Two-Stage Transformer (HETT) framework, which integrates the two aspects through a coarse-to-fine navigation pipeline. Specifically, HETT first predicts coarse-grained target positions by fusing spatial landmarks and historical context, then refines actions via fine-grained visual analysis. In addition, a historical grid map is designed to dynamically aggregate visual features into a structured spatial memory, enhancing comprehensive scene awareness. Additionally, the CityNav dataset annotations are manually refined to enhance data quality. Experiments on the refined CityNav dataset show that HETT delivers significant performance gains, while extensive ablation studies further verify the effectiveness of each component.",
    "pdf_url": "https://arxiv.org/pdf/2512.14222v2",
    "github_url": null,
    "published": "2025-12-16T09:16:07+00:00",
    "updated": "2025-12-17T02:51:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.14046v1",
    "title": "E-Navi: Environmental Adaptive Navigation for UAVs on Resource Constrained Platforms",
    "authors": [
      "Li",
      "Jin",
      "Zhao"
    ],
    "summary": "The ability to adapt to changing environments is crucial for the autonomous navigation systems of Unmanned Aerial Vehicles (UAVs). However, existing navigation systems adopt fixed execution configurations without considering environmental dynamics based on available computing resources, e.g., with a high execution frequency and task workload. This static approach causes rigid flight strategies and excessive computations, ultimately degrading flight performance or even leading to failures in UAVs. Despite the necessity for an adaptive system, dynamically adjusting workloads remains challenging, due to difficulties in quantifying environmental complexity and modeling the relationship between environment and system configuration. Aiming at adapting to dynamic environments, this paper proposes E-Navi, an environmental-adaptive navigation system for UAVs that dynamically adjusts task executions on the CPUs in response to environmental changes based on available computational resources. Specifically, the perception-planning pipeline of UAVs navigation system is redesigned through dynamic adaptation of mapping resolution and execution frequency, driven by the quantitative environmental complexity evaluations. In addition, E-Navi supports flexible deployment across hardware platforms with varying levels of computing capability. Extensive Hardware-In-the-Loop and real-world experiments demonstrate that the proposed system significantly outperforms the baseline method across various hardware platforms, achieving up to 53.9% navigation task workload reduction, up to 63.8% flight time savings, and delivering more stable velocity control.",
    "pdf_url": "https://arxiv.org/pdf/2512.14046v1",
    "github_url": null,
    "published": "2025-12-16T03:28:28+00:00",
    "updated": "2025-12-16T03:28:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.14020v1",
    "title": "Deep Learning Perspective of Scene Understanding in Autonomous Robots",
    "authors": [
      "Maham",
      "Tashfa"
    ],
    "summary": "This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.",
    "pdf_url": "https://arxiv.org/pdf/2512.14020v1",
    "github_url": null,
    "published": "2025-12-16T02:31:54+00:00",
    "updated": "2025-12-16T02:31:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.13974v1",
    "title": "Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline",
    "authors": [
      "Naderi",
      "Shojaei",
      "Agee"
    ],
    "summary": "Construction safety inspection remains mostly manual, and automated approaches still rely on task-specific datasets that are hard to maintain in fast-changing construction environments due to frequent retraining. Meanwhile, field inspection with robots still depends on human teleoperation and manual reporting, which are labor-intensive. This paper aims to connect what a robot sees during autonomous navigation to the safety rules that are common in construction sites, automatically generating a safety inspection report. To this end, we proposed a multi-layer framework with two main modules: robotics and AI. On the robotics side, SLAM and autonomous navigation provide repeatable coverage and targeted revisits via waypoints. On AI side, a Vision Language Model (VLM)-based layer produces scene descriptions; a retrieval component powered grounds those descriptions in OSHA and site policies; Another VLM-based layer assesses the safety situation based on rules; and finally Large Language Model (LLM) layer generates safety reports based on previous outputs. The framework is validated with a proof-of-concept implementation and evaluated in a lab environment that simulates common hazards across three scenarios. Results show high recall with competitive precision compared to state-of-the-art closed-source models. This paper contributes a transparent, generalizable pipeline that moves beyond black-box models by exposing intermediate artifacts from each layer and keeping the human in the loop. This work provides a foundation for future extensions to additional tasks and settings within and beyond construction context.",
    "pdf_url": "https://arxiv.org/pdf/2512.13974v1",
    "github_url": null,
    "published": "2025-12-16T00:25:31+00:00",
    "updated": "2025-12-16T00:25:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.13890v1",
    "title": "Group-Theoretic Reinforcement Learning of Dynamical Decoupling Sequences",
    "authors": [
      "Marrder",
      "Sun",
      "Holland"
    ],
    "summary": "Dynamical decoupling seeks to mitigate phase decoherence in qubits by applying a carefully designed sequence of effectively instantaneous electromagnetic pulses. Although analytic solutions exist for pulse timings that are optimal under specific noise regimes, identifying the optimal timings for a realistic noise spectrum remains challenging. We propose a reinforcement learning (RL)-based method for designing pulse sequences on qubits. Our novel action set enables the RL agent to efficiently navigate this inherently non-convex optimization landscape. The action set, derived from Thompson's group $F$, is applicable to a broad class of sequential decision problems whose states can be represented as bounded sequences. We demonstrate that our RL agent can learn pulse sequences that minimize dephasing without requiring explicit knowledge of the underlying noise spectrum. This work opens the possibility for real-time learning of optimal dynamical decoupling sequences on qubits which are dephasing-limited. The model-free nature of our algorithm suggests that the agent may ultimately learn optimal pulse sequences even in the presence of unmodeled physical effects, such as pulse errors or non-Gaussian noise.",
    "pdf_url": "https://arxiv.org/pdf/2512.13890v1",
    "github_url": null,
    "published": "2025-12-15T20:48:08+00:00",
    "updated": "2025-12-15T20:48:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.13644v1",
    "title": "World Models Can Leverage Human Videos for Dexterous Manipulation",
    "authors": [
      "Goswami",
      "Bar",
      "Fan"
    ],
    "summary": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.",
    "pdf_url": "https://arxiv.org/pdf/2512.13644v1",
    "github_url": null,
    "published": "2025-12-15T18:37:12+00:00",
    "updated": "2025-12-15T18:37:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.13514v1",
    "title": "Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM",
    "authors": [
      "Arora",
      "El-Hariry",
      "Olivares-Mendez"
    ],
    "summary": "Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.",
    "pdf_url": "https://arxiv.org/pdf/2512.13514v1",
    "github_url": null,
    "published": "2025-12-15T16:42:48+00:00",
    "updated": "2025-12-15T16:42:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.13477v1",
    "title": "Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model",
    "authors": [
      "Brumfiel",
      "Konda",
      "Elliott"
    ],
    "summary": "To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.",
    "pdf_url": "https://arxiv.org/pdf/2512.13477v1",
    "github_url": null,
    "published": "2025-12-15T16:14:22+00:00",
    "updated": "2025-12-15T16:14:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.13438v1",
    "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents",
    "authors": [
      "Ran",
      "Gong",
      "Guo"
    ],
    "summary": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.",
    "pdf_url": "https://arxiv.org/pdf/2512.13438v1",
    "github_url": null,
    "published": "2025-12-15T15:34:06+00:00",
    "updated": "2025-12-15T15:34:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.13402v1",
    "title": "End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery",
    "authors": [
      "Pettinari",
      "Hadramy",
      "Wehrli"
    ],
    "summary": "Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.",
    "pdf_url": "https://arxiv.org/pdf/2512.13402v1",
    "github_url": null,
    "published": "2025-12-15T14:53:20+00:00",
    "updated": "2025-12-15T14:53:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.14757v1",
    "title": "SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning",
    "authors": [
      "Kawabata",
      "Zhang",
      "Xiao"
    ],
    "summary": "For robots navigating in human-populated environments, safety and social compliance are equally critical, yet prior work has mostly emphasized safety. Socially compliant navigation that accounts for human comfort, social norms, and contextual appropriateness remains underexplored. Vision language models (VLMs) show promise for this task; however, large-scale models incur substantial computational overhead, leading to higher inference latency and energy consumption, which makes them unsuitable for real-time deployment on resource-constrained robotic platforms. To address this issue, we investigate the effectiveness of small VLM and propose SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant navigation with reinforcement fine-tuning (RFT). We further introduce a semantic similarity reward (SSR) to effectively leverage RFT for enhancing the decision-making capabilities. Additionally, we study the effectiveness of different small language model types (Phi, Qwen, and StableLM), routing strategies, and vision encoders (CLIP vs. SigLIP, frozen vs. fine-tuned). Experiments on the SNEI dataset demonstrate that SocialNav-MoE achieves an excellent balance between navigation accuracy and efficiency. The proposed SSR function is more effective than hard-level and character-level rewards. Source code will be released upon acceptance.",
    "pdf_url": "https://arxiv.org/pdf/2512.14757v1",
    "github_url": null,
    "published": "2025-12-15T14:21:15+00:00",
    "updated": "2025-12-15T14:21:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.13293v2",
    "title": "Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration",
    "authors": [
      "Fu",
      "Liu",
      "Zhou"
    ],
    "summary": "This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.",
    "pdf_url": "https://arxiv.org/pdf/2512.13293v2",
    "github_url": "https://github.com/czxhunzi/CEMRRL",
    "published": "2025-12-15T13:03:08+00:00",
    "updated": "2025-12-16T03:34:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.13216v1",
    "title": "Integrating ethical, societal and environmental issues into algorithm design courses",
    "authors": [
      "Bellenguez",
      "Brauner",
      "Solnon"
    ],
    "summary": "This document, intended for computer science teachers, describes a case study that puts into practice a questioning of ethical, societal and environmental issues when designing or implementing a decision support system. This study is based on a very popular application, namely road navigation software that informs users of real-time traffic conditions and suggests routes between a starting point and a destination, taking these conditions into account (such as Waze). The approach proposes to intertwine technical considerations (optimal path algorithms, data needed for location, etc.) with a broader view of the ethical, environmental and societal issues raised by the tools studied. Based on the authors' experience conducting sessions with students over several years, this document discusses the context of such a study, suggests teaching resources for implementing it, describes ways to structure discussions, and shares scenarios in different teaching contexts.",
    "pdf_url": "https://arxiv.org/pdf/2512.13216v1",
    "github_url": null,
    "published": "2025-12-15T11:29:36+00:00",
    "updated": "2025-12-15T11:29:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.13215v1",
    "title": "Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment",
    "authors": [
      "Qu",
      "Li",
      "Zhong"
    ],
    "summary": "Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).",
    "pdf_url": "https://arxiv.org/pdf/2512.13215v1",
    "github_url": null,
    "published": "2025-12-15T11:28:04+00:00",
    "updated": "2025-12-15T11:28:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.13000v1",
    "title": "Legitimizing, Developing, and Sustaining Feminist HCI in East Asia: Challenges and Opportunities",
    "authors": [
      "Zhang",
      "Wan",
      "Jiaqi"
    ],
    "summary": "Feminist HCI has been rapidly developing in East Asian contexts in recent years. The region's unique cultural and political backgrounds have contributed valuable, situated knowledge, revealing topics such as localized digital feminism practices, or women's complex navigation among social expectations. However, the very factors that ground these perspectives also create significant survival challenges for researchers in East Asia. These include a scarcity of dedicated funding, the stigma of being perceived as less valuable than productivity-oriented technologies, and the lack of senior researchers and established, resilient communities. Grounded in these challenges and our prior collective practices, we propose this meet-up with two focused goals: (1) to provide a legitimized channel for Feminist HCI researchers to connect and build community, and (2) to facilitate an action-oriented dialogue on how to legitimize, develop, and sustain Feminist HCI in the East Asian context. The website for this meet-up is: https://feminist-hci.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2512.13000v1",
    "github_url": null,
    "published": "2025-12-15T05:47:05+00:00",
    "updated": "2025-12-15T05:47:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12980v1",
    "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views",
    "authors": [
      "Chen",
      "Fu",
      "Wu"
    ],
    "summary": "Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.   We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.",
    "pdf_url": "https://arxiv.org/pdf/2512.12980v1",
    "github_url": null,
    "published": "2025-12-15T04:49:33+00:00",
    "updated": "2025-12-15T04:49:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12692v1",
    "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
    "authors": [
      "Dihan",
      "Hashem",
      "Ali"
    ],
    "summary": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.",
    "pdf_url": "https://arxiv.org/pdf/2512.12692v1",
    "github_url": null,
    "published": "2025-12-14T13:56:54+00:00",
    "updated": "2025-12-14T13:56:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12622v1",
    "title": "D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation",
    "authors": [
      "Wang",
      "Lee",
      "Dai"
    ],
    "summary": "Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.",
    "pdf_url": "https://arxiv.org/pdf/2512.12622v1",
    "github_url": null,
    "published": "2025-12-14T09:53:15+00:00",
    "updated": "2025-12-14T09:53:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12594v1",
    "title": "ceLLMate: Sandboxing Browser AI Agents",
    "authors": [
      "Meng",
      "Feng",
      "Shumailov"
    ],
    "summary": "Browser-using agents (BUAs) are an emerging class of autonomous agents that interact with web browsers in human-like ways, including clicking, scrolling, filling forms, and navigating across pages. While these agents help automate repetitive online tasks, they are vulnerable to prompt injection attacks that can trick an agent into performing undesired actions, such as leaking private information or issuing state-changing requests. We propose ceLLMate, a browser-level sandboxing framework that restricts the agent's ambient authority and reduces the blast radius of prompt injections. We address two fundamental challenges: (1) The semantic gap challenge in policy enforcement arises because the agent operates through low-level UI observations and manipulations; however, writing and enforcing policies directly over UI-level events is brittle and error-prone. To address this challenge, we introduce an agent sitemap that maps low-level browser behaviors to high-level semantic actions. (2) Policy prediction in BUAs is the norm rather than the exception. BUAs have no app developer to pre-declare sandboxing policies, and thus, ceLLMate pairs website-authored mandatory policies with an automated policy-prediction layer that adapts and instantiates these policies from the user's natural-language task. We implement ceLLMate as an agent-agnostic browser extension and demonstrate how it enables sandboxing policies that effectively block various types of prompt injection attacks with negligible overhead.",
    "pdf_url": "https://arxiv.org/pdf/2512.12594v1",
    "github_url": null,
    "published": "2025-12-14T08:25:31+00:00",
    "updated": "2025-12-14T08:25:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12475v1",
    "title": "Improved Directional State Transition Tensors for Accurate Aerocapture Performance Analysis",
    "authors": [
      "Calkins",
      "McMahon",
      "Woffinden"
    ],
    "summary": "Aerocapture is a unique challenge for semi-analytical propagation because its nonconservative dynamics lead to force magnitudes that vary substantially across the trajectory. State transition tensors (STTs), higher-order Taylor series expansions of the solution flow, have been widely used as a computationally efficient semi-analytical propagation method for orbital scenarios, but have not previously been applied to aerocapture. However, obtaining the higher-order STTs requires integrating exponentially more equations. Directional state transition tensors (DSTTs) mitigate this cost by projecting the state into a reduced-dimension basis. This work develops novel dynamics analysis techniques to identify effective bases for this reduction, including augmented higher-order Cauchy Green tensors tailored to quantities of interest such as apoapsis radius. Results show that DSTTs constructed along these bases significantly reduce computational cost while maintaining accuracy in apoapsis and energy prediction. In particular, certain of these DSTTs outperform traditional DSTTs in nonlinear perturbation propagation for key state subsets and quantities of interest. These results establish STTs and DSTTs as practical tools for aerocapture performance analysis to enable robust guidance and navigation.",
    "pdf_url": "https://arxiv.org/pdf/2512.12475v1",
    "github_url": null,
    "published": "2025-12-13T22:19:34+00:00",
    "updated": "2025-12-13T22:19:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12306v1",
    "title": "From Co-Design to Metacognitive Laziness: Evaluating Generative AI in Vocational Education",
    "authors": [
      "Yunus",
      "Gay",
      "Lee"
    ],
    "summary": "This study examines the development and deployment of a Generative AI proof-of-concept (POC) designed to support lecturers in a vocational education setting in Singapore. Employing a user-centred, mixed-methods design process, we co-developed an AI chatbot with lecturers to address recurring instructional challenges during exam preparation, specifically managing repetitive questions and scaling feedback delivery. The POC achieved its primary operational goals: lecturers reported streamlined workflows, reduced cognitive load, and observed improved student confidence in navigating course content. However, the deployment yielded unexpected insights into student learning behaviours. Despite enhanced teaching processes, performance data revealed no significant improvement in overall student assessment outcomes. Deep analysis of interaction logs identified concerning patterns, including self-efficacy-driven dependency, \"metacognitive laziness\" (cognitive offloading), and divergent usage strategies. While high-ability students leveraged the tool for strategic verification, low-ability students frequently used it to bypass cognitive effort, potentially exacerbating performance gaps. These findings suggest that Generative AI's educational influence extends beyond instructional efficiency to shape cognitive engagement, self-regulation, and learner equity. The study raises consequential design questions regarding how AI tools can be engineered to minimise dependency, scaffold metacognitive development, and calibrate support across varying ability levels. We conclude that while Generative AI can substantially enhance the teaching experience, achieving meaningful learning gains requires rigorous attention to learner behaviour and the equitable design of AI-supported environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.12306v1",
    "github_url": null,
    "published": "2025-12-13T12:26:25+00:00",
    "updated": "2025-12-13T12:26:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12228v1",
    "title": "Semantic Zone based 3D Map Management for Mobile Robot",
    "authors": [
      "Yun",
      "Yoo"
    ],
    "summary": "Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations. However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources. Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments. To address this, we propose a semantic zone-based 3D map management method that shifts the paradigm from geometry-centric to semantics-centric control. Our approach partitions the environment into meaningful spatial units (e.g., lobbies, hallways) and designates these zones as the primary unit for memory management. By dynamically loading only task-relevant zones into Working Memory (WM) and offloading inactive zones to Long-Term Memory (LTM), the system strictly enforces user-defined memory thresholds. Implemented within the RTAB-Map framework, our method demonstrates substantial reductions in unnecessary signature load/unload cycles and cumulative memory utilization compared to standard approaches. The results confirm that semantic zone-based management ensures stable, predictable memory usage while preserving map availability for navigation. Code is available at: https://github.com/huichangs/rtabmap/tree/segment",
    "pdf_url": "https://arxiv.org/pdf/2512.12228v1",
    "github_url": "https://github.com/huichangs/rtabmap",
    "published": "2025-12-13T07:55:40+00:00",
    "updated": "2025-12-13T07:55:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12221v1",
    "title": "Scalable branch-and-bound model selection with non-monotonic criteria including AIC, BIC and Mallows's $\\mathit{C_p}$",
    "authors": [
      "Vanhoefer",
      "Körner",
      "Doresic"
    ],
    "summary": "Model selection is a pivotal process in the quantitative sciences, where researchers must navigate between numerous candidate models of varying complexity. Traditional information criteria, such as the corrected Akaike Information Criterion (AICc), Bayesian Information Criterion (BIC), and Mallows's $\\mathit{C_p}$, are valuable tools for identifying optimal models. However, the exponential increase in candidate models with each additional model parameter renders the evaluation of these criteria for all models -- a strategy known as exhaustive, or brute-force, searches -- computationally prohibitive. Consequently, heuristic approaches like stepwise regression are commonly employed, albeit without guarantees of finding the globally-optimal model.   In this study, we challenge the prevailing notion that non-monotonicity in information criteria precludes bounds on the search space. We introduce a simple but novel bound that enables the development of branch-and-bound algorithms tailored for these non-monotonic functions. We demonstrate that our approach guarantees identification of the optimal model(s) across diverse model classes, sizes, and applications, often with orders of magnitude computational speedups. For instance, in one previously-published model selection task involving $2^{32}$ (approximately 4 billion) candidate models, our method achieves a computational speedup exceeding 6,000. These findings have broad implications for the scalability and effectiveness of model selection in complex scientific domains.",
    "pdf_url": "https://arxiv.org/pdf/2512.12221v1",
    "github_url": null,
    "published": "2025-12-13T07:16:10+00:00",
    "updated": "2025-12-13T07:16:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12203v1",
    "title": "Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion",
    "authors": [
      "Elias",
      "Esswein",
      "How"
    ],
    "summary": "As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.",
    "pdf_url": "https://arxiv.org/pdf/2512.12203v1",
    "github_url": null,
    "published": "2025-12-13T06:24:26+00:00",
    "updated": "2025-12-13T06:24:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12177v1",
    "title": "Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation",
    "authors": [
      "Ayanzadeh",
      "Oates"
    ],
    "summary": "Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.",
    "pdf_url": "https://arxiv.org/pdf/2512.12177v1",
    "github_url": null,
    "published": "2025-12-13T04:49:26+00:00",
    "updated": "2025-12-13T04:49:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12164v1",
    "title": "Random Combinatorial Libraries and Automated Nanoindentation for High-Throughput Structural Materials Discovery",
    "authors": [
      "Chawla",
      "Penumadu",
      "Kalinin"
    ],
    "summary": "Accelerating the discovery of structural materials is essential for applications in hard and refractory alloys, hypersonic platforms, nuclear systems, and other extreme environment technologies. Progress is often constrained by slow synthesis and characterization cycles and the need for extensive mechanical testing across large compositional spaces. Here, we propose a rapid screening strategy based on random material libraries, in which thousands of distinct compositions are embedded within a single specimen, mapped by EDS, and subsequently characterized. Using nanoindentation as a representative case, we show that such libraries enable dense composition property mapping while reducing the number of samples required to explore high dimensional composition spaces compared to traditional synthesis and test workflows. An experimentally calibrated Monte Carlo framework is developed to quantify practical limits, including particle size, EDS noise and resolution, positional accuracy, and nanoindenter motion costs. The simulations identify regimes where random libraries provide orders of magnitude acceleration over classical workflows. Finally, we demonstrate experimental navigation of these libraries using automated indentation. Together, these results establish random libraries as a general route to high throughput characterization in structurally critical material systems.",
    "pdf_url": "https://arxiv.org/pdf/2512.12164v1",
    "github_url": null,
    "published": "2025-12-13T04:14:13+00:00",
    "updated": "2025-12-13T04:14:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12106v1",
    "title": "DreamRAM: A Fine-Grained Configurable Design Space Modeling Tool for Custom 3D Die-Stacked DRAM",
    "authors": [
      "Cai",
      "Zhou",
      "Do"
    ],
    "summary": "3D die-stacked DRAM has emerged as a key technology for delivering high bandwidth and high density for applications such as high-performance computing, graphics, and machine learning. However, different applications place diverse and sometimes diverging demands on power, performance, and area that cannot be universally satisfied with fixed commodity DRAM designs. Die stacking creates the opportunity for a large DRAM design space through 3D integration and expanded total die area. To open and navigate this expansive design space of customized memory architectures that cater to application-specific needs, we introduce DreamRAM, a configurable bandwidth, capacity, energy, latency, and area modeling tool for custom 3D die-stacked DRAM designs. DreamRAM exposes fine-grained design customization parameters at the MAT, subarray, bank, and inter-bank levels, including extensions of partial page and subarray parallelism proposals found in the literature, to open a large previously-unexplored design space. DreamRAM analytically models wire pitch, width, length, capacitance, and scaling parameters to capture the performance tradeoffs of physical layout and routing design choices. Routing awareness enables DreamRAM to model a custom MAT-level routing scheme, Dataline-Over-MAT (DLOMAT), to facilitate better bandwidth tradeoffs. DreamRAM is calibrated and validated against published industry HBM3 and HBM2E designs. Within DreamRAM's rich design space, we identify designs that achieve each of 66% higher bandwidth, 100% higher capacity, and 45% lower power and energy per bit compared to the baseline design, each on an iso-bandwidth, iso-capacity, and iso-power basis.",
    "pdf_url": "https://arxiv.org/pdf/2512.12106v1",
    "github_url": null,
    "published": "2025-12-13T00:42:48+00:00",
    "updated": "2025-12-13T00:42:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12086v1",
    "title": "CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation",
    "authors": [
      "Yang",
      "Ardakanian"
    ],
    "summary": "Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.",
    "pdf_url": "https://arxiv.org/pdf/2512.12086v1",
    "github_url": null,
    "published": "2025-12-12T23:30:43+00:00",
    "updated": "2025-12-12T23:30:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.12046v1",
    "title": "Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning",
    "authors": [
      "Giammarino",
      "Qureshi"
    ],
    "summary": "Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.",
    "pdf_url": "https://arxiv.org/pdf/2512.12046v1",
    "github_url": null,
    "published": "2025-12-12T21:37:11+00:00",
    "updated": "2025-12-12T21:37:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11736v1",
    "title": "Bench-Push: Benchmarking Pushing-based Navigation and Manipulation Tasks for Mobile Robots",
    "authors": [
      "Zhong",
      "Caro",
      "Ramesh"
    ],
    "summary": "Mobile robots are increasingly deployed in cluttered environments with movable objects, posing challenges for traditional methods that prohibit interaction. In such settings, the mobile robot must go beyond traditional obstacle avoidance, leveraging pushing or nudging strategies to accomplish its goals. While research in pushing-based robotics is growing, evaluations rely on ad hoc setups, limiting reproducibility and cross-comparison. To address this, we present Bench-Push, the first unified benchmark for pushing-based mobile robot navigation and manipulation tasks. Bench-Push includes multiple components: 1) a comprehensive range of simulated environments that capture the fundamental challenges in pushing-based tasks, including navigating a maze with movable obstacles, autonomous ship navigation in ice-covered waters, box delivery, and area clearing, each with varying levels of complexity; 2) novel evaluation metrics to capture efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-Push to evaluate example implementations of established baselines across environments. Bench-Push is open-sourced as a Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.",
    "pdf_url": "https://arxiv.org/pdf/2512.11736v1",
    "github_url": "https://github.com/IvanIZ/BenchNPIN",
    "published": "2025-12-12T17:25:32+00:00",
    "updated": "2025-12-12T17:25:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11510v1",
    "title": "Reconstruction as a Bridge for Event-Based Visual Question Answering",
    "authors": [
      "Lou",
      "Zhou",
      "Zhang"
    ],
    "summary": "Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.",
    "pdf_url": "https://arxiv.org/pdf/2512.11510v1",
    "github_url": null,
    "published": "2025-12-12T12:16:45+00:00",
    "updated": "2025-12-12T12:16:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11943v1",
    "title": "How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism",
    "authors": [
      "Liu",
      "Li",
      "Dou"
    ],
    "summary": "Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent \"AI optimism\"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.",
    "pdf_url": "https://arxiv.org/pdf/2512.11943v1",
    "github_url": null,
    "published": "2025-12-12T12:14:48+00:00",
    "updated": "2025-12-12T12:14:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11300v2",
    "title": "Distributed Quantum Magnetic Sensing for Infrastructure-free Geo-localization",
    "authors": [
      "Le",
      "Guo",
      "Liu"
    ],
    "summary": "Modern navigation systems rely heavily on Global Navigation Satellite Systems (GNSS), whose weak spaceborne signals are vulnerable to jamming, spoofing, and line-of-sight blockage. As an alternative, the Earth's magnetic field entails location information and is found critical to many animals' cognitive and navigation behavior. However, the practical use of the Earth's magnetic field for geo-localization hinges on an ultra-sensitive magnetometer. This work investigates how quantum magnetic sensing can be used for this purpose. We theoretically derive the Cramér-Rao lower bound (CRLB) for the estimation error of quantum sensing when using a nitrogen-vacancy (NV) center and prove the quantum advantage over classical magnetometers. Moreover, we employ a practical distributed quantum sensing protocol to saturate CRLB. Based on the estimated magnetic field and the earth's magnetic field map, we formulate geo-localization as a map-matching problem and introduce a coarse-to-fine Mahalanobis distance search in both gradient space (local field derivatives) and corner space (raw field samples). We simulate the proposed quantum sensing-based geo-localization framework over four cities in the United States and Canada. The results report that in high-gradient regions, gradient-space Mahalanobis search achieves sub-kilometer median localization error; while in magnetically smoother areas, corner-space search provides better accuracy and a $4-8\\times$ reduction in runtime.",
    "pdf_url": "https://arxiv.org/pdf/2512.11300v2",
    "github_url": null,
    "published": "2025-12-12T05:52:49+00:00",
    "updated": "2025-12-15T01:37:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.15756v1",
    "title": "ReactorFold: Generative discovery of nuclear reactor cores via emergent physical reasoning",
    "authors": [
      "Lee"
    ],
    "summary": "Designing nuclear reactor cores requires navigating large discrete design spaces governed by complex neutronic interactions. Traditional deterministic, metaheuristic, and machine-learning-assisted methods search within fixed, human-defined configuration spaces, limiting their ability to discover fundamentally new design topologies. Here we introduce ReactorFold, a generative framework that reformulates fuel-assembly design as a sequence modeling problem for language models. Using Monte Carlo data, parameter-efficient fine-tuning, and Direct Preference Optimization (DPO), the model learns the latent structure of a pressurized-water-reactor assembly and generates candidate layouts in a single forward pass. Notably, the DPO-aligned model exhibits emergent design-space expansion: despite being trained exclusively on configurations with a fixed number of gadolinium burnable absorber (Gd) rods, it autonomously adjusts Gd inventory to satisfy strict power-peaking constraints. The model also discovers high-performing asymmetric configurations that challenge conventional symmetric loading heuristics, accessing design regimes inaccessible to conventional search methods and demonstrating that language models can internalize causal physical relationships and transcend human-imposed design constraints.",
    "pdf_url": "https://arxiv.org/pdf/2512.15756v1",
    "github_url": null,
    "published": "2025-12-12T02:26:19+00:00",
    "updated": "2025-12-12T02:26:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11173v1",
    "title": "Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance",
    "authors": [
      "Lee",
      "Mahmudova",
      "Desingh"
    ],
    "summary": "Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/",
    "pdf_url": "https://arxiv.org/pdf/2512.11173v1",
    "github_url": null,
    "published": "2025-12-11T23:35:05+00:00",
    "updated": "2025-12-11T23:35:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11105v1",
    "title": "Supporting Medicinal Chemists in Iterative Hypothesis Generation for Drug Target Identification",
    "authors": [
      "Jeon",
      "Hwang",
      "Li"
    ],
    "summary": "While drug discovery is vital for human health, the process remains inefficient. Medicinal chemists must navigate a vast protein space to identify target proteins that meet three criteria: physical and functional interactions, therapeutic impact, and docking potential. Prior approaches have provided fragmented support for each criterion, limiting the generation of promising hypotheses for wet-lab experiments. We present HAPPIER, an AI-powered tool that supports hypothesis generation with integrated multi-criteria support for target identification. HAPPIER enables medicinal chemists to 1) efficiently explore and verify proteins in a single integrated graph component showing multi-criteria satisfaction and 2) validate AI suggestions with domain knowledge. These capabilities facilitate iterative cycles of divergent and convergent thinking, essential for hypothesis generation. We evaluated HAPPIER with ten medicinal chemists, finding that it increased the number of high-confidence hypotheses and support for the iterative cycle, and further demonstrated the relationship between engaging in such cycles and confidence in outputs.",
    "pdf_url": "https://arxiv.org/pdf/2512.11105v1",
    "github_url": null,
    "published": "2025-12-11T20:39:34+00:00",
    "updated": "2025-12-11T20:39:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11086v1",
    "title": "An Open Source Realtime GPU Beamformer for Row-Column and Top Orthogonal to Bottom Electrode (TOBE) Arrays",
    "authors": [
      "Palamar",
      "Dahunsi",
      "Henry"
    ],
    "summary": "Research ultrasound platforms have enabled many next-generation imaging sequences but have lacked realtime navigation capabilities for emerging 2D arrays such as row-column arrays (RCAs). We present an open-source, GPU-accelerated reconstruction and rendering software suite integrated with a programmable ultrasound platform and novel electrostrictive Top-Orthogonal-to-Bottom-Electrode (TOBE) arrays. The system supports advanced real-time modes, including cross-plane aperture-encoded synthetic-aperture imaging and aperture-encoded volumetric scanning. TOBE-enabled methods demonstrate improved image quality and expanded field of view compared with conventional RCA techniques. The software implements beamforming and rendering kernels using OpenGL compute shaders and is designed for maximum data throughput helping to minimize stalls and latency. Accompanying sample datasets and example scripts for offline reconstruction are provided to facilitate external testing.",
    "pdf_url": "https://arxiv.org/pdf/2512.11086v1",
    "github_url": null,
    "published": "2025-12-11T19:58:25+00:00",
    "updated": "2025-12-11T19:58:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11083v1",
    "title": "Integrating Regional Ice Charts and Copernicus Sea Ice Products for Navigation Risk in Alaskan Waters",
    "authors": [
      "Peel",
      "Gedikli"
    ],
    "summary": "As climate change continues to reshape marginal ice zones in the Arctic, accurate and reliable sea ice data are critical for ensuring maritime safety. This study compares regional ice charts from the Alaska Sea Ice Program with satellite derived Copernicus sea ice concentration data to evaluate spatial and temporal discrepancies in ice representation across Alaskan waters from January 2010 to March 2025. Daily Arctic Sea Ice Program polygons were aligned with Copernicus grid points in a common UTM framework, and residuals were computed to quantify systematic differences. Results show that Copernicus consistently underestimates ice concentration relative to Arctic Sea Ice Program, particularly in nearshore and marginal ice zones affected by land-spillover and mixed-pixel effects such as those observed in Cook Inlet. Empirical Orthogonal Function analysis shows that both datasets capture the same dominant physical modes of sea ice variability, with the first mode representing the annual freeze thaw cycle and the second reflecting marginal ice-zone dynamics. To assess operational implications, vessel Automatic Identification System data were combined with Alaska Sea Ice Program ice charts using the IACS POLARIS Risk Index Outcome framework. Approximately 36 percent of AIS observations within ice affected waters corresponded to negative Risk Index Outcome values, indicating that vessels frequently operated under elevated-risk conditions. These findings demonstrate that regional charts and Copernicus provide complementary capabilities that together enable more accurate and operationally meaningful Arctic navigation and risk assessments.",
    "pdf_url": "https://arxiv.org/pdf/2512.11083v1",
    "github_url": null,
    "published": "2025-12-11T19:55:03+00:00",
    "updated": "2025-12-11T19:55:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11064v1",
    "title": "An Open Benchmark of One Million High-Fidelity Cislunar Trajectories",
    "authors": [
      "Yeager",
      "Higgins",
      "Mcgill"
    ],
    "summary": "Cislunar space spans from geosynchronous altitudes to beyond the Moon and will underpin future exploration, science, and security operations. We describe and release an open dataset of one million numerically propagated cislunar trajectories generated with the open-source Space Situational Awareness Python package (SSAPy). The model includes high-degree Earth/Moon gravity, solar gravity, and Earth/Sun radiation pressure; other planetary gravities are omitted by design for computational efficiency. Initial conditions uniformly sample commonly used osculating-element ranges, and each trajectory is propagated for up to six years under a single, fixed start epoch. The dataset is intended as a reusable benchmark for method development (e.g., space domain awareness, navigation, and machine learning pipelines), a reference library for statistical studies of orbit families, and a starting point for community-driven extensions (e.g., alternative epochs). We report empirically observed stability trends (e.g., a band near 5 GEO and persistence of some co-orbital classes including L4/L5 librators) as dataset descriptors rather than new dynamical results. The chief contribution is the scale, fidelity, organization (CSV/HDF5 with full state time series and metadata), and open availability, which together lower the barrier to comparative and data-driven studies in the cislunar regime.",
    "pdf_url": "https://arxiv.org/pdf/2512.11064v1",
    "github_url": null,
    "published": "2025-12-11T19:33:16+00:00",
    "updated": "2025-12-11T19:33:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10956v1",
    "title": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision",
    "authors": [
      "Zhou",
      "Chen",
      "Rajagopal"
    ],
    "summary": "The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.   We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.",
    "pdf_url": "https://arxiv.org/pdf/2512.10956v1",
    "github_url": null,
    "published": "2025-12-11T18:59:56+00:00",
    "updated": "2025-12-11T18:59:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10934v1",
    "title": "Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit",
    "authors": [
      "Mari",
      "Pasquet",
      "Seinturier"
    ],
    "summary": "Autonomous drone navigation in confined tubular environments remains a major challenge due to the constraining geometry of the conduits, the proximity of the walls, and the perceptual limitations inherent to such scenarios. We propose a reinforcement learning approach enabling a drone to navigate unknown three-dimensional tubes without any prior knowledge of their geometry, relying solely on local observations from LiDAR and a conditional visual detection of the tube center. In contrast, the Pure Pursuit algorithm, used as a deterministic baseline, benefits from explicit access to the centerline, creating an information asymmetry designed to assess the ability of RL to compensate for the absence of a geometric model. The agent is trained through a progressive Curriculum Learning strategy that gradually exposes it to increasingly curved geometries, where the tube center frequently disappears from the visual field. A turning-negotiation mechanism, based on the combination of direct visibility, directional memory, and LiDAR symmetry cues, proves essential for ensuring stable navigation under such partial observability conditions. Experiments show that the PPO policy acquires robust and generalizable behavior, consistently outperforming the deterministic controller despite its limited access to geometric information. Validation in a high-fidelity 3D environment further confirms the transferability of the learned behavior to a continuous physical dynamics.   The proposed approach thus provides a complete framework for autonomous navigation in unknown tubular environments and opens perspectives for industrial, underground, or medical applications where progressing through narrow and weakly perceptive conduits represents a central challenge.",
    "pdf_url": "https://arxiv.org/pdf/2512.10934v1",
    "github_url": null,
    "published": "2025-12-11T18:57:29+00:00",
    "updated": "2025-12-11T18:57:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10925v1",
    "title": "Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation",
    "authors": [
      "Mari",
      "Nawaf",
      "Drap"
    ],
    "summary": "Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.",
    "pdf_url": "https://arxiv.org/pdf/2512.10925v1",
    "github_url": null,
    "published": "2025-12-11T18:52:42+00:00",
    "updated": "2025-12-11T18:52:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10660v1",
    "title": "NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation",
    "authors": [
      "Wu",
      "Steiner",
      "Schmidt"
    ],
    "summary": "The complexity of autonomous driving scenarios requires robust models that can interpret high-level navigation commands and generate safe trajectories. While traditional rule-based systems can react to these commands, they often struggle in dynamic environments, and end-to-end methods face challenges in complying with explicit navigation commands. To address this, we present NaviHydra, a controllable navigation-guided end-to-end model distilled from an existing rule-based simulator. Our framework accepts high-level navigation commands as control signals, generating trajectories that align with specified intentions. We utilize a Bird's Eye View (BEV) based trajectory gathering method to enhance the trajectory feature extraction. Additionally, we introduce a novel navigation compliance metric to evaluate adherence to intended route, improving controllability and navigation safety. To comprehensively assess our model's controllability, we design a test that evaluates its response to various navigation commands. Our method significantly outperforms baseline models, achieving state-of-the-art results in the NAVSIM benchmark, demonstrating its effectiveness in advancing autonomous driving.",
    "pdf_url": "https://arxiv.org/pdf/2512.10660v1",
    "github_url": null,
    "published": "2025-12-11T14:05:18+00:00",
    "updated": "2025-12-11T14:05:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11918v1",
    "title": "Financial Management Challenges in Enterprises Employing Remote and Hybrid Workforces",
    "authors": [
      "Ćwiąkała",
      "Wojak",
      "Baran"
    ],
    "summary": "The paper examines financial management challenges faced by organizations operating under remote and hybrid work models. It investigates how these flexible arrangements influence budgeting, reporting, and financial transparency in distributed teams. Using a quantitative survey of managers, HR staff, and finance professionals, the study analyzes the role of digital tools, communication, and organizational practices in shaping financial outcomes. Results indicate that remote and hybrid work can improve budget control and process transparency through the use of ERP systems and digital workflows. However, forecasting accuracy and interdepartmental communication remain major challenges, particularly in organizations with insufficient digital integration. Respondents also reported lower stress levels and improved work-life balance, suggesting potential well-being and productivity benefits. The paper recommends that companies enhance digital infrastructure, adopt advanced analytics for forecasting, and develop clear communication frameworks supported by employee well-being programs. The study contributes original empirical evidence on financial management in flexible work environments, offering practical insights for leaders navigating the digital transformation of finance.",
    "pdf_url": "https://arxiv.org/pdf/2512.11918v1",
    "github_url": null,
    "published": "2025-12-11T13:56:26+00:00",
    "updated": "2025-12-11T13:56:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10480v1",
    "title": "Seamless Outdoor-Indoor Pedestrian Positioning System with GNSS/UWB/IMU Fusion: A Comparison of EKF, FGO, and PF",
    "authors": [
      "Zhang",
      "Yu",
      "Ha"
    ],
    "summary": "Accurate and continuous pedestrian positioning across outdoor-indoor environments remains challenging because GNSS, UWB, and inertial PDR are complementary yet individually fragile under signal blockage, multipath, and drift. This paper presents a unified GNSS/UWB/IMU fusion framework for seamless pedestrian localization and provides a controlled comparison of three probabilistic back-ends: an error-state extended Kalman filter, sliding-window factor graph optimization, and a particle filter. The system uses chest-mounted IMU-based PDR as the motion backbone and integrates absolute updates from GNSS outdoors and UWB indoors. To enhance transition robustness and mitigate urban GNSS degradation, we introduce a lightweight map-based feasibility constraint derived from OpenStreetMap building footprints, treating most building interiors as non-navigable while allowing motion inside a designated UWB-instrumented building. The framework is implemented in ROS 2 and runs in real time on a wearable platform, with visualization in Foxglove. We evaluate three scenarios: indoor (UWB+PDR), outdoor (GNSS+PDR), and seamless outdoor-indoor (GNSS+UWB+PDR). Results show that the ESKF provides the most consistent overall performance in our implementation.",
    "pdf_url": "https://arxiv.org/pdf/2512.10480v1",
    "github_url": null,
    "published": "2025-12-11T09:59:03+00:00",
    "updated": "2025-12-11T09:59:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10428v1",
    "title": "Design and Implementation of a High-Precision Wind-Estimation UAV with Onboard Sensors",
    "authors": [
      "Yu",
      "Fan",
      "Liu"
    ],
    "summary": "Accurate real-time wind vector estimation is essential for enhancing the safety, navigation accuracy, and energy efficiency of unmanned aerial vehicles (UAVs). Traditional approaches rely on external sensors or simplify vehicle dynamics, which limits their applicability during agile flight or in resource-constrained platforms. This paper proposes a real-time wind estimation method based solely on onboard sensors. The approach first estimates external aerodynamic forces using a disturbance observer (DOB), and then maps these forces to wind vectors using a thin-plate spline (TPS) model. A custom-designed wind barrel mounted on the UAV enhances aerodynamic sensitivity, further improving estimation accuracy. The system is validated through comprehensive experiments in wind tunnels, indoor and outdoor flights. Experimental results demonstrate that the proposed method achieves consistently high-accuracy wind estimation across controlled and real-world conditions, with speed RMSEs as low as \\SI{0.06}{m/s} in wind tunnel tests, \\SI{0.22}{m/s} during outdoor hover, and below \\SI{0.38}{m/s} in indoor and outdoor dynamic flights, and direction RMSEs under \\ang{7.3} across all scenarios, outperforming existing baselines. Moreover, the method provides vertical wind estimates -- unavailable in baselines -- with RMSEs below \\SI{0.17}{m/s} even during fast indoor translations.",
    "pdf_url": "https://arxiv.org/pdf/2512.10428v1",
    "github_url": null,
    "published": "2025-12-11T08:39:55+00:00",
    "updated": "2025-12-11T08:39:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10392v1",
    "title": "Collision-Aware Density-Driven Control of Multi-Agent Systems via Control Barrier Functions",
    "authors": [
      "Seo",
      "Lee"
    ],
    "summary": "This paper tackles the problem of safe and efficient area coverage using a multi-agent system operating in environments with obstacles. Applications such as environmental monitoring and search and rescue require robot swarms to cover large domains under resource constraints, making both coverage efficiency and safety essential. To address the efficiency aspect, we adopt the Density-Driven Control (D$^2$C) framework, which uses optimal transport theory to steer agents according to a reference distribution that encodes spatial coverage priorities. To ensure safety, we incorporate Control Barrier Functions (CBFs) into the framework. While CBFs are commonly used for collision avoidance, we extend their applicability by introducing obstacle-specific formulations for both circular and rectangular shapes. In particular, we analytically derive a unit normal vector based on the agent's position relative to the nearest face of a rectangular obstacle, improving safety enforcement in environments with non-smooth boundaries. Additionally, a velocity-dependent term is incorporated into the CBF to enhance collision avoidance. Simulation results validate the proposed method by demonstrating smoother navigation near obstacles and more efficient area coverage than the existing method, while still ensuring collision-free operation.",
    "pdf_url": "https://arxiv.org/pdf/2512.10392v1",
    "github_url": null,
    "published": "2025-12-11T07:57:47+00:00",
    "updated": "2025-12-11T07:57:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10363v1",
    "title": "Point to Span: Zero-Shot Moment Retrieval for Navigating Unseen Hour-Long Videos",
    "authors": [
      "Jeon",
      "Yang",
      "Han"
    ],
    "summary": "Zero-shot Long Video Moment Retrieval (ZLVMR) is the task of identifying temporal segments in hour-long videos using a natural language query without task-specific training. The core technical challenge of LVMR stems from the computational infeasibility of processing entire lengthy videos in a single pass. This limitation has established a 'Search-then-Refine' approach, where candidates are rapidly narrowed down, and only those portions are analyzed, as the dominant paradigm for LVMR. However, existing approaches to this paradigm face severe limitations. Conventional supervised learning suffers from limited scalability and poor generalization, despite substantial resource consumption. Yet, existing zero-shot methods also fail, facing a dual challenge: (1) their heuristic strategies cause a 'search' phase candidate explosion, and (2) the 'refine' phase, which is vulnerable to semantic discrepancy, requires high-cost VLMs for verification, incurring significant computational overhead. We propose \\textbf{P}oint-\\textbf{to}-\\textbf{S}pan (P2S), a novel training-free framework to overcome this challenge of inefficient 'search' and costly 'refine' phases. P2S overcomes these challenges with two key innovations: an 'Adaptive Span Generator' to prevent the search phase candidate explosion, and 'Query Decomposition' to refine candidates without relying on high-cost VLM verification. To our knowledge, P2S is the first zero-shot framework capable of temporal grounding in hour-long videos, outperforming supervised state-of-the-art methods by a significant margin (e.g., +3.7\\% on R5@0.1 on MAD).",
    "pdf_url": "https://arxiv.org/pdf/2512.10363v1",
    "github_url": null,
    "published": "2025-12-11T07:25:48+00:00",
    "updated": "2025-12-11T07:25:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10360v1",
    "title": "CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "He",
      "Li"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps. While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks. To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR). RSMP adopts a causal-learning-based dual-branch architecture to enhance generalization, while RLMR leverages panoramic visual prompting with chain-of-thought reasoning to support interpretable spatial understanding and navigation. We further introduce an uncertainty-aware collaboration mechanism (UCM) that adaptively fuses decisions from both models. For obstacle avoidance, in simulation, we replace the rule-based controller with a fully learnable point-goal policy, and in real-world deployment, we design a LiDAR-based clustering module for generating navigable waypoints and pair it with an online SLAM-based local controller. CLASH achieves state-of-the-art (SoTA) results (ranking 1-st) on the VLN-CE leaderboard, significantly improving SR and SPL on the test-unseen set over the previous SoTA methods. Real-world experiments demonstrate CLASH's strong robustness, validating its effectiveness in both simulation and deployment scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2512.10360v1",
    "github_url": null,
    "published": "2025-12-11T07:20:06+00:00",
    "updated": "2025-12-11T07:20:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10342v1",
    "title": "CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates",
    "authors": [
      "Grover",
      "Pathak",
      "Kumar"
    ],
    "summary": "Large-scale Vision-Language Models (VLMs) exhibit impressive complex reasoning capabilities but remain largely unexplored in visual sequential planning, i.e., executing multi-step actions towards a goal. Additionally, practical sequential planning often involves non-optimal (erroneous) steps, challenging VLMs to detect and correct such steps. We propose Corrective Sequential Planning Benchmark (CoSPlan) to evaluate VLMs in error-prone, vision-based sequential planning tasks across 4 domains: maze navigation, block rearrangement, image reconstruction,and object reorganization. CoSPlan assesses two key abilities: Error Detection (identifying non-optimal action) and Step Completion (correcting and completing action sequences to reach the goal). Despite using state-of-the-art reasoning techniques such as Chain-of-Thought and Scene Graphs, VLMs (e.g. Intern-VLM and Qwen2) struggle on CoSPlan, failing to leverage contextual cues to reach goals. Addressing this, we propose a novel training-free method, Scene Graph Incremental updates (SGI), which introduces intermediate reasoning steps between the initial and goal states. SGI helps VLMs reason about sequences, yielding an average performance gain of 5.2%. In addition to enhancing reliability in corrective sequential planning, SGI generalizes to traditional planning tasks such as Plan-Bench and VQA.",
    "pdf_url": "https://arxiv.org/pdf/2512.10342v1",
    "github_url": null,
    "published": "2025-12-11T06:46:51+00:00",
    "updated": "2025-12-11T06:46:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10322v1",
    "title": "User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation",
    "authors": [
      "Yu",
      "Li",
      "Mahmood"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.",
    "pdf_url": "https://arxiv.org/pdf/2512.10322v1",
    "github_url": null,
    "published": "2025-12-11T06:11:45+00:00",
    "updated": "2025-12-11T06:11:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10319v1",
    "title": "Design of a six wheel suspension and a three-axis linear actuation mechanism for a laser weeding robot",
    "authors": [
      "Usama",
      "Khan",
      "Hasan"
    ],
    "summary": "Mobile robots are increasingly utilized in agriculture to automate labor-intensive tasks such as weeding, sowing, harvesting and soil analysis. Recently, agricultural robots have been developed to detect and remove weeds using mechanical tools or precise herbicide sprays. Mechanical weeding is inefficient over large fields, and herbicides harm the soil ecosystem. Laser weeding with mobile robots has emerged as a sustainable alternative in precision farming. In this paper, we present an autonomous weeding robot that uses controlled exposure to a low energy laser beam for weed removal. The proposed robot is six-wheeled with a novel double four-bar suspension for higher stability. The laser is guided towards the detected weeds by a three-dimensional linear actuation mechanism. Field tests have demonstrated the robot's capability to navigate agricultural terrains effectively by overcoming obstacles up to 15 cm in height. At an optimal speed of 42.5 cm/s, the robot achieves a weed detection rate of 86.2\\% and operating time of 87 seconds per meter. The laser actuation mechanism maintains a minimal mean positional error of 1.54 mm, combined with a high hit rate of 97\\%, ensuring effective and accurate weed removal. This combination of speed, accuracy, and efficiency highlights the robot's potential for significantly enhancing precision farming practices.",
    "pdf_url": "https://arxiv.org/pdf/2512.10319v1",
    "github_url": null,
    "published": "2025-12-11T06:11:05+00:00",
    "updated": "2025-12-11T06:11:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10310v1",
    "title": "Efficient-VLN: A Training-Efficient Vision-Language Navigation Model",
    "authors": [
      "Zheng",
      "Huang",
      "Li"
    ],
    "summary": "Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.",
    "pdf_url": "https://arxiv.org/pdf/2512.10310v1",
    "github_url": null,
    "published": "2025-12-11T05:57:48+00:00",
    "updated": "2025-12-11T05:57:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10046v1",
    "title": "SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration",
    "authors": [
      "Zhuang",
      "Ren",
      "Ye"
    ],
    "summary": "Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.10046v1",
    "github_url": null,
    "published": "2025-12-10T20:04:08+00:00",
    "updated": "2025-12-10T20:04:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09929v1",
    "title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning",
    "authors": [
      "Parthasarathy",
      "Kalra",
      "Agrawal"
    ],
    "summary": "World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.",
    "pdf_url": "https://arxiv.org/pdf/2512.09929v1",
    "github_url": null,
    "published": "2025-12-10T18:59:45+00:00",
    "updated": "2025-12-10T18:59:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09920v1",
    "title": "LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating",
    "authors": [
      "Chen",
      "Li",
      "Jiang"
    ],
    "summary": "Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/",
    "pdf_url": "https://arxiv.org/pdf/2512.09920v1",
    "github_url": null,
    "published": "2025-12-10T18:54:30+00:00",
    "updated": "2025-12-10T18:54:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09903v1",
    "title": "YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos",
    "authors": [
      "Meegan",
      "D'Souza",
      "Cao"
    ],
    "summary": "Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.",
    "pdf_url": "https://arxiv.org/pdf/2512.09903v1",
    "github_url": null,
    "published": "2025-12-10T18:32:38+00:00",
    "updated": "2025-12-10T18:32:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09898v1",
    "title": "Visual Heading Prediction for Autonomous Aerial Vehicles",
    "authors": [
      "Ahmari",
      "Mohammadi",
      "Hemmati"
    ],
    "summary": "The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506° and a root mean squared error of 0.1957°, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration",
    "pdf_url": "https://arxiv.org/pdf/2512.09898v1",
    "github_url": "https://github.com/Kooroshraf/UAV-UGV-Integration",
    "published": "2025-12-10T18:27:37+00:00",
    "updated": "2025-12-10T18:27:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10999v1",
    "title": "KBQA-R1: Reinforcing Large Language Models for Knowledge Base Question Answering",
    "authors": [
      "Sun",
      "Chen",
      "Zheng"
    ],
    "summary": "Knowledge Base Question Answering (KBQA) challenges models to bridge the gap between natural language and strict knowledge graph schemas by generating executable logical forms. While Large Language Models (LLMs) have advanced this field, current approaches often struggle with a dichotomy of failure: they either generate hallucinated queries without verifying schema existence or exhibit rigid, template-based reasoning that mimics synthesized traces without true comprehension of the environment. To address these limitations, we present \\textbf{KBQA-R1}, a framework that shifts the paradigm from text imitation to interaction optimization via Reinforcement Learning. Treating KBQA as a multi-turn decision process, our model learns to navigate the knowledge base using a list of actions, leveraging Group Relative Policy Optimization (GRPO) to refine its strategies based on concrete execution feedback rather than static supervision. Furthermore, we introduce \\textbf{Referenced Rejection Sampling (RRS)}, a data synthesis method that resolves cold-start challenges by strictly aligning reasoning traces with ground-truth action sequences. Extensive experiments on WebQSP, GrailQA, and GraphQuestions demonstrate that KBQA-R1 achieves state-of-the-art performance, effectively grounding LLM reasoning in verifiable execution.",
    "pdf_url": "https://arxiv.org/pdf/2512.10999v1",
    "github_url": null,
    "published": "2025-12-10T17:45:42+00:00",
    "updated": "2025-12-10T17:45:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09840v1",
    "title": "Multimodal motion and behavior switching of multistable ciliary walkers",
    "authors": [
      "Mohanty",
      "Baconnier",
      "Schomaker"
    ],
    "summary": "The collective motion of arrays of cilia - tiny, hairlike protrusions - drives the locomotion of numerous microorganisms, enabling multimodal motion and autonomous switching between gaits to navigate complex environments. To endow minimalist centimeter-scale robots with similarly rich dynamics, we introduce millimeter-scale flexible cilia that buckle under the robots weight, coupling multistability and actuation within a single physical mechanism. When placed on a vibrating surface, these ciliary walkers select their propulsion direction through the buckled states of their cilia, allowing multimodal motion and switching between modes in response to perturbations. We first show that bimodal walkers with left-right symmetric cilia can autonomously reverse direction upon encountering obstacles. Next, we demonstrate that walkers with isotropic cilia exhibit both translational and rotational motion and switch between them in response to environmental interactions. At increasing densities, swarms of such walkers collectively transition from predominantly spinning to translational motion. Finally, we show that the shape, placement and number of cilia controls the modes of motion of the walkers. Our results establish a rational, physically grounded strategy for designing minimalist soft robots where complex behaviors emerge from feedback between internal mechanical states and environmental interactions, laying the foundation for autonomous robotic collectives without the need for centralized control.",
    "pdf_url": "https://arxiv.org/pdf/2512.09840v1",
    "github_url": null,
    "published": "2025-12-10T17:22:17+00:00",
    "updated": "2025-12-10T17:22:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09798v1",
    "title": "High-Resolution Water Sampling via a Solar-Powered Autonomous Surface Vehicle",
    "authors": [
      "Mamani",
      "Fernandez",
      "Luna"
    ],
    "summary": "Accurate water quality assessment requires spatially resolved sampling, yet most unmanned surface vehicles (USVs) can collect only a limited number of samples or rely on single-point sensors with poor representativeness. This work presents a solar-powered, fully autonomous USV featuring a novel syringe-based sampling architecture capable of acquiring 72 discrete, contamination-minimized water samples per mission. The vehicle incorporates a ROS 2 autonomy stack with GPS-RTK navigation, LiDAR and stereo-vision obstacle detection, Nav2-based mission planning, and long-range LoRa supervision, enabling dependable execution of sampling routes in unstructured environments. The platform integrates a behavior-tree autonomy architecture adapted from Nav2, enabling mission-level reasoning and perception-aware navigation. A modular 6x12 sampling system, controlled by distributed micro-ROS nodes, provides deterministic actuation, fault isolation, and rapid module replacement, achieving spatial coverage beyond previously reported USV-based samplers. Field trials in Achocalla Lagoon (La Paz, Bolivia) demonstrated 87% waypoint accuracy, stable autonomous navigation, and accurate physicochemical measurements (temperature, pH, conductivity, total dissolved solids) comparable to manually collected references. These results demonstrate that the platform enables reliable high-resolution sampling and autonomous mission execution, providing a scalable solution for aquatic monitoring in remote environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.09798v1",
    "github_url": null,
    "published": "2025-12-10T16:12:59+00:00",
    "updated": "2025-12-10T16:12:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09972v2",
    "title": "BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization",
    "authors": [
      "Chen",
      "Luo",
      "Zhu"
    ],
    "summary": "Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the \"curse of dimensionality,\" rendering the search space computationally intractable. To resolve this dichotomy, we propose BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework that automatically constructs the LLM Pareto set. BAMBO renders the search tractable by introducing a Hybrid Optimal Block Partitioning strategy. Formulated as a 1D clustering problem, this strategy leverages a dynamic programming approach to optimally balance intra-block homogeneity and inter-block information distribution, thereby dramatically reducing dimensionality without sacrificing critical granularity. The entire process is automated within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function. Experiments demonstrate that BAMBO discovers a superior and more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/xin8coder/BAMBO.",
    "pdf_url": "https://arxiv.org/pdf/2512.09972v2",
    "github_url": "https://github.com/xin8coder/BAMBO",
    "published": "2025-12-10T15:32:56+00:00",
    "updated": "2025-12-12T05:23:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11903v1",
    "title": "Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics",
    "authors": [
      "Catalano",
      "Montijano",
      "Civera"
    ],
    "summary": "Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.11903v1",
    "github_url": null,
    "published": "2025-12-10T15:13:30+00:00",
    "updated": "2025-12-10T15:13:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09646v1",
    "title": "VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification",
    "authors": [
      "Zhang",
      "Foo",
      "Beeler"
    ],
    "summary": "Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.",
    "pdf_url": "https://arxiv.org/pdf/2512.09646v1",
    "github_url": null,
    "published": "2025-12-10T13:40:24+00:00",
    "updated": "2025-12-10T13:40:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09607v1",
    "title": "UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories",
    "authors": [
      "Mei",
      "Yang",
      "Guo"
    ],
    "summary": "Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.",
    "pdf_url": "https://arxiv.org/pdf/2512.09607v1",
    "github_url": null,
    "published": "2025-12-10T12:54:04+00:00",
    "updated": "2025-12-10T12:54:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09577v1",
    "title": "Auto-BenchmarkCard: Automated Synthesis of Benchmark Documentation",
    "authors": [
      "Hofmann",
      "Vejsbjerg",
      "Salwala"
    ],
    "summary": "We present Auto-BenchmarkCard, a workflow for generating validated descriptions of AI benchmarks. Benchmark documentation is often incomplete or inconsistent, making it difficult to interpret and compare benchmarks across tasks or domains. Auto-BenchmarkCard addresses this gap by combining multi-agent data extraction from heterogeneous sources (e.g., Hugging Face, Unitxt, academic papers) with LLM-driven synthesis. A validation phase evaluates factual accuracy through atomic entailment scoring using the FactReasoner tool. This workflow has the potential to promote transparency, comparability, and reusability in AI benchmark reporting, enabling researchers and practitioners to better navigate and evaluate benchmark choices.",
    "pdf_url": "https://arxiv.org/pdf/2512.09577v1",
    "github_url": null,
    "published": "2025-12-10T12:09:44+00:00",
    "updated": "2025-12-10T12:09:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09566v2",
    "title": "Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search",
    "authors": [
      "Ji",
      "Yang",
      "Xu"
    ],
    "summary": "Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold. By combining generalization, plausibility, and interpretability, Trio establishes a closed-loop generative paradigm that redefines how chemical space can be navigated, offering a transformative foundation for the next era of AI-driven drug discovery.",
    "pdf_url": "https://arxiv.org/pdf/2512.09566v2",
    "github_url": null,
    "published": "2025-12-10T11:59:42+00:00",
    "updated": "2025-12-18T10:53:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09537v1",
    "title": "REASAN: Learning Reactive Safe Navigation for Legged Robots",
    "authors": [
      "Yuan",
      "Cao",
      "Cao"
    ],
    "summary": "We present a novel modularized end-to-end framework for legged reactive navigation in complex dynamic environments using a single light detection and ranging (LiDAR) sensor. The system comprises four simulation-trained modules: three reinforcement-learning (RL) policies for locomotion, safety shielding, and navigation, and a transformer-based exteroceptive estimator that processes raw point-cloud inputs. This modular decomposition of complex legged motor-control tasks enables lightweight neural networks with simple architectures, trained using standard RL practices with targeted reward shaping and curriculum design, without reliance on heuristics or sophisticated policy-switching mechanisms. We conduct comprehensive ablations to validate our design choices and demonstrate improved robustness compared to existing approaches in challenging navigation tasks. The resulting reactive safe navigation (REASAN) system achieves fully onboard and real-time reactive navigation across both single- and multi-robot settings in complex environments. We release our training and deployment code at https://github.com/ASIG-X/REASAN.",
    "pdf_url": "https://arxiv.org/pdf/2512.09537v1",
    "github_url": "https://github.com/ASIG-X/REASAN",
    "published": "2025-12-10T11:23:32+00:00",
    "updated": "2025-12-10T11:23:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09463v1",
    "title": "Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing",
    "authors": [
      "Coninck",
      "Gamba",
      "Doninck"
    ],
    "summary": "The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.",
    "pdf_url": "https://arxiv.org/pdf/2512.09463v1",
    "github_url": null,
    "published": "2025-12-10T09:33:03+00:00",
    "updated": "2025-12-10T09:33:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09431v1",
    "title": "A Hierarchical, Model-Based System for High-Performance Humanoid Soccer",
    "authors": [
      "Wang",
      "Zhu",
      "Hou"
    ],
    "summary": "The development of athletic humanoid robots has gained significant attention as advances in actuation, sensing, and control enable increasingly dynamic, real-world capabilities. RoboCup, an international competition of fully autonomous humanoid robots, provides a uniquely challenging benchmark for such systems, culminating in the long-term goal of competing against human soccer players by 2050. This paper presents the hardware and software innovations underlying our team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition. On the hardware side, we introduce an adult-sized humanoid platform built with lightweight structural components, high-torque quasi-direct-drive actuators, and a specialized foot design that enables powerful in-gait kicks while preserving locomotion robustness. On the software side, we develop an integrated perception and localization framework that combines stereo vision, object detection, and landmark-based fusion to provide reliable estimates of the ball, goals, teammates, and opponents. A mid-level navigation stack then generates collision-aware, dynamically feasible trajectories, while a centralized behavior manager coordinates high-level decision making, role selection, and kick execution based on the evolving game state. The seamless integration of these subsystems results in fast, precise, and tactically effective gameplay, enabling robust performance under the dynamic and adversarial conditions of real matches. This paper presents the design principles, system architecture, and experimental results that contributed to ARTEMIS's success as the 2024 Adult-Sized Humanoid Soccer champion.",
    "pdf_url": "https://arxiv.org/pdf/2512.09431v1",
    "github_url": null,
    "published": "2025-12-10T08:58:37+00:00",
    "updated": "2025-12-10T08:58:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09097v1",
    "title": "Characterizing Human Feedback-Based Control in Naturalistic Driving Interactions via Gaussian Process Regression with Linear Feedback",
    "authors": [
      "DiPirro",
      "Devonport",
      "Calderone"
    ],
    "summary": "Understanding driver interactions is critical to designing autonomous vehicles to interoperate safely with human-driven cars. We consider the impact of these interactions on the policies drivers employ when navigating unsigned intersections in a driving simulator. The simulator allows the collection of naturalistic decision-making and behavior data in a controlled environment. Using these data, we model the human driver responses as state-based feedback controllers learned via Gaussian Process regression methods. We compute the feedback gain of the controller using a weighted combination of linear and nonlinear priors. We then analyze how the individual gains are reflected in driver behavior. We also assess differences in these controllers across populations of drivers. Our work in data-driven analyses of how drivers determine their policies can facilitate future work in the design of socially responsive autonomy for vehicles.",
    "pdf_url": "https://arxiv.org/pdf/2512.09097v1",
    "github_url": null,
    "published": "2025-12-09T20:21:22+00:00",
    "updated": "2025-12-09T20:21:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09085v1",
    "title": "Mental Models of Autonomy and Sentience Shape Reactions to AI",
    "authors": [
      "Pauketat",
      "Shank",
      "Manoli"
    ],
    "summary": "Narratives about artificial intelligence (AI) entangle autonomy, the capacity to self-govern, with sentience, the capacity to sense and feel. AI agents that perform tasks autonomously and companions that recognize and express emotions may activate mental models of autonomy and sentience, respectively, provoking distinct reactions. To examine this possibility, we conducted three pilot studies (N = 374) and four preregistered vignette experiments describing an AI as autonomous, sentient, both, or neither (N = 2,702). Activating a mental model of sentience increased general mind perception (cognition and emotion) and moral consideration more than autonomy, but autonomy increased perceived threat more than sentience. Sentience also increased perceived autonomy more than vice versa. Based on a within-paper meta-analysis, sentience changed reactions more than autonomy on average. By disentangling different mental models of AI, we can study human-AI interaction with more precision to better navigate the detailed design of anthropomorphized AI and prompting interfaces.",
    "pdf_url": "https://arxiv.org/pdf/2512.09085v1",
    "github_url": null,
    "published": "2025-12-09T19:56:52+00:00",
    "updated": "2025-12-09T19:56:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09083v1",
    "title": "Reactive Vehicle Guidance using Dynamic Maneuvering Cue",
    "authors": [
      "Moll",
      "Weintraub"
    ],
    "summary": "Recent approaches for navigating among dynamic threat regions (i.e., weapon engagement zones) have focused on planning entire trajectories. Moreover, the allowance for penetration into these threat regions was based on heuristic measurements of risk. This paper offers an approach for a more reactive (i.e., feedback-based) guidance that is based on closed-form analytical expressions and thereby suitable for onboard, real-time execution. In addition, a risk measurement is formulated based upon the concept of Dynamic Maneuvering Cue (DMC) which measures the amount of turn a vehicle would need to take in its current state in order to put itself outside the threat region. This approach is then extended to handle multiple threat regions simultaneously (with minimal additional computational complexity). Finally, the DMC constraint is applied to a simple feedback controller as well as a model predictive controller (MPC). The MPC shows better performance but at the cost of having to solve an optimization problem online versus the meager computational burden associated with the simple controller. This approach, which is based on assuming the threats are adversarial, may be used as a conservative method for collision avoidance and deconfliction.",
    "pdf_url": "https://arxiv.org/pdf/2512.09083v1",
    "github_url": null,
    "published": "2025-12-09T19:55:57+00:00",
    "updated": "2025-12-09T19:55:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09071v1",
    "title": "Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics",
    "authors": [
      "Trinh",
      "Lyons"
    ],
    "summary": "Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications. This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic. Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves. However, for a robot implementation, determining which matches are sufficiently good is often reduced to a manually set threshold. And it is difficult to manually select a threshold that will work for a variety of visual scenarios. This paper addresses the problem of automatically selecting a threshold for VPR by looking at the 'negative' Gaussian mixture statistics for a place - image statistics indicating not this place. We show that this approach can be used to select thresholds that work well for a variety of image databases and image descriptors.",
    "pdf_url": "https://arxiv.org/pdf/2512.09071v1",
    "github_url": null,
    "published": "2025-12-09T19:34:43+00:00",
    "updated": "2025-12-09T19:34:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09065v1",
    "title": "ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors",
    "authors": [
      "Agrawal",
      "Brawer",
      "Naik"
    ],
    "summary": "Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.",
    "pdf_url": "https://arxiv.org/pdf/2512.09065v1",
    "github_url": null,
    "published": "2025-12-09T19:33:19+00:00",
    "updated": "2025-12-09T19:33:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08766v1",
    "title": "Optimal navigation in two-dimensional regular and turbulent flows",
    "authors": [
      "Parfenyev"
    ],
    "summary": "Zermelo's navigation problem seeks the trajectory of minimal travel time between two points in a fluid flow. We address this problem for an agent -- such as a micro-robot or active particle -- that is advected by a two-dimensional flow, self-propels at a fixed speed smaller than or comparable to the characteristic flow velocity, and can steer its direction. The flows considered span increasing levels of complexity, from steady solid-body rotation to the Taylor-Green flow and fully developed turbulence in the inverse cascade regime. Although optimal control theory provides time-minimizing trajectories, these solutions become unstable in chaotic regimes realized for complex background flows. To design robust navigation strategies under such conditions, we apply reinforcement learning. Both action-value (Q-learning) and policy-gradient (one-step actor-critic) methods achieve successful navigation with comparable performance. Crucially, we show that agents trained on coarse-grained flows -- retaining only large-scale features -- generalize effectively to the full turbulent field. This robustness to incomplete flow information is essential for practical navigation in real-world oceanic and atmospheric environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.08766v1",
    "github_url": null,
    "published": "2025-12-09T16:14:07+00:00",
    "updated": "2025-12-09T16:14:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08688v1",
    "title": "Non Normalized Shared-Constraint Dynamic Games for Human-Robot Collaboration with Asymmetric Responsibility",
    "authors": [
      "Pustilnik",
      "Borrelli"
    ],
    "summary": "This paper proposes a dynamic game formulation for cooperative human-robot navigation in shared workspaces with obstacles, where the human and robot jointly satisfy shared safety constraints while pursuing a common task. A key contribution is the introduction of a non-normalized equilibrium structure for the shared constraints. This structure allows the two agents to contribute different levels of effort towards enforcing safety requirements such as collision avoidance and inter-players spacing. We embed this non-normalized equilibrium into a receding-horizon optimal control scheme.",
    "pdf_url": "https://arxiv.org/pdf/2512.08688v1",
    "github_url": null,
    "published": "2025-12-09T15:08:02+00:00",
    "updated": "2025-12-09T15:08:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08639v1",
    "title": "Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning",
    "authors": [
      "Xu",
      "Liu",
      "Luomei"
    ],
    "summary": "Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.",
    "pdf_url": "https://arxiv.org/pdf/2512.08639v1",
    "github_url": null,
    "published": "2025-12-09T14:25:24+00:00",
    "updated": "2025-12-09T14:25:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08626v1",
    "title": "Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR",
    "authors": [
      "Bari",
      "Veciana",
      "Zhou"
    ],
    "summary": "Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.   In this paper, we introduce the \\textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.   We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings.",
    "pdf_url": "https://arxiv.org/pdf/2512.08626v1",
    "github_url": null,
    "published": "2025-12-09T14:10:41+00:00",
    "updated": "2025-12-09T14:10:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08608v1",
    "title": "NLoS Localization with Single Base Station Based on Radio Map",
    "authors": [
      "Xu",
      "Guo",
      "Wang"
    ],
    "summary": "Accurate outdoor localization in Non-Line-of-Sight (NLoS) environments remains a critical challenge for wireless communication and sensing systems. Existing methods, including positioning based on the Global Navigation Satellite System (GNSS) and triple Base Stations (BSs) techniques, cannot provide reliable performance under NLoS conditions, particularly in dense urban areas with strong multipath effects. To address this limitation, we propose a single BS localization framework that integrates sequential signal measurements with prior radio information embedded in the Radio Map (RM). Using temporal measurement features and matching them with radio maps, the proposed method effectively mitigates the adverse impact of multipath propagation and reduces the dependence on LoS paths. Simulation experiments further evaluate the impact of different radio map construction strategies and the varying lengths of the measurement sequence on localization accuracy. Results demonstrate that the proposed scheme achieves sub-meter positioning accuracy in typical NLoS environments, highlighting its potential as a practical and robust solution for single-base-station deployment.",
    "pdf_url": "https://arxiv.org/pdf/2512.08608v1",
    "github_url": null,
    "published": "2025-12-09T13:53:28+00:00",
    "updated": "2025-12-09T13:53:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08574v1",
    "title": "RVC-NMPC: Nonlinear Model Predictive Control with Reciprocal Velocity Constraints for Mutual Collision Avoidance in Agile UAV Flight",
    "authors": [
      "Kratky",
      "Penicka",
      "Gupta"
    ],
    "summary": "This paper presents an approach to mutual collision avoidance based on Nonlinear Model Predictive Control (NMPC) with time-dependent Reciprocal Velocity Constraints (RVCs). Unlike most existing methods, the proposed approach relies solely on observable information about other robots, eliminating the necessity of excessive communication use. The computationally efficient algorithm for computing RVCs, together with the direct integration of these constraints into NMPC problem formulation on a controller level, allows the whole pipeline to run at 100 Hz. This high processing rate, combined with modeled nonlinear dynamics of the controlled Uncrewed Aerial Vehicles (UAVs), is a key feature that facilitates the use of the proposed approach for an agile UAV flight. The proposed approach was evaluated through extensive simulations emulating real-world conditions in scenarios involving up to 10 UAVs and velocities of up to 25 m/s, and in real-world experiments with accelerations up to 30 m/s$^2$. Comparison with state of the art shows 31% improvement in terms of flight time reduction in challenging scenarios, while maintaining a collision-free navigation in all trials.",
    "pdf_url": "https://arxiv.org/pdf/2512.08574v1",
    "github_url": null,
    "published": "2025-12-09T13:13:27+00:00",
    "updated": "2025-12-09T13:13:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08492v1",
    "title": "Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance",
    "authors": [
      "Kaliutau"
    ],
    "summary": "Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the \"Semantic Trap\" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.",
    "pdf_url": "https://arxiv.org/pdf/2512.08492v1",
    "github_url": null,
    "published": "2025-12-09T11:11:37+00:00",
    "updated": "2025-12-09T11:11:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08476v1",
    "title": "A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems",
    "authors": [
      "Shih",
      "Wang",
      "Li"
    ],
    "summary": "Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems.",
    "pdf_url": "https://arxiv.org/pdf/2512.08476v1",
    "github_url": null,
    "published": "2025-12-09T10:50:19+00:00",
    "updated": "2025-12-09T10:50:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08437v1",
    "title": "Time and Money Matters for Sustainability: Insights on User Preferences on Renewable Energy for Electric Vehicle Charging Stations",
    "authors": [
      "Du",
      "Vavouris",
      "Veisi"
    ],
    "summary": "Charging electric vehicles (EVs) with renewable energy can lessen their environmental impact. However, the fluctuating availability of renewable energy affects the sustainability of public EV charging stations. Nearby public charging stations may utilize differing energy sources due to their microgrid connections - ranging from exclusively renewable to non-renewable or a combination of both - highlighting the substantial variability in energy supply types within short distances. This study investigates the near-future scenario of integrating dynamic renewable energy availability in charging station navigation to impact the choices of EV users towards renewable sources. We conducted a within-subjects design survey with 50 car users and semi-structured interviews with 10 EV users from rural, suburban, and urban areas. The results show that when choosing EV charging stations, drivers often prioritize either time savings or money savings based on the driving scenarios that influence drivers' consumer value. Notably, EV users tend to select renewable-powered stations when they align with their main priority, be it saving money or time. This study offers end-user insights into the front-end graphic user interface and the development of the back-end ranking algorithm for navigation recommender systems that integrate dynamic renewable energy availability for the sustainable use of electric vehicles.",
    "pdf_url": "https://arxiv.org/pdf/2512.08437v1",
    "github_url": null,
    "published": "2025-12-09T10:07:00+00:00",
    "updated": "2025-12-09T10:07:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08296v2",
    "title": "Towards a Science of Scaling Agent Systems",
    "authors": [
      "Kim",
      "Gu",
      "Park"
    ],
    "summary": "Agents, language model-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored. We address this by deriving quantitative scaling principles for agent systems. We first formalize a definition for agentic evaluation and characterize scaling laws as the interplay between agent quantity, coordination structure, model capability, and task properties. We evaluate this across four benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. With five canonical agent architectures (Single-Agent and four Multi-Agent Systems: Independent, Centralized, Decentralized, Hybrid), instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations. We derive a predictive model using coordination metrics, that achieves cross-validated R^2=0.524, enabling prediction on unseen task domains. We identify three effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.8% on parallelizable tasks, while decentralized coordination excels on web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, every multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations. Out-of-sample validation on GPT-5.2, achieves MAE=0.071 and confirms four of five scaling principles generalize to unseen frontier models.",
    "pdf_url": "https://arxiv.org/pdf/2512.08296v2",
    "github_url": null,
    "published": "2025-12-09T06:52:21+00:00",
    "updated": "2025-12-17T02:41:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08286v1",
    "title": "Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework",
    "authors": [
      "Hu",
      "Wu",
      "Qi"
    ],
    "summary": "The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.",
    "pdf_url": "https://arxiv.org/pdf/2512.08286v1",
    "github_url": null,
    "published": "2025-12-09T06:34:28+00:00",
    "updated": "2025-12-09T06:34:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11886v1",
    "title": "Enabling Autonomous Navigation in a Snake Robot through Visual-Inertial Odometry and Closed-Loop Trajectory Tracking Control",
    "authors": [
      "Ali"
    ],
    "summary": "Snake robots offer exceptional mobility across extreme terrain inaccessible to conventional rovers, yet their highly articulated bodies present fundamental challenges for autonomous navigation in environments lacking external tracking infrastructure. This thesis develops a complete autonomy pipeline for COBRA, an 11 degree-of-freedom modular snake robot designed for planetary exploration. While the robot's biologically inspired serpentine gaits achieve impressive mobility, prior work has relied entirely on open-loop teleoperation. This approach integrates onboard visual-inertial SLAM, reduced-order state estimation, and closed-loop trajectory tracking to enable autonomous waypoint navigation. A depth camera paired with edge computing performs real-time localization during dynamic locomotion, validated against motion-capture ground truth to characterize drift behavior and failure modes unique to snake robot platforms. A reduced-order framework estimates Center-of-Mass pose, driving a closed-loop controller that modulates CPG gait parameters through distance-dependent yaw error blending. Physical experiments validate the complete system, demonstrating accurate multi-waypoint tracking and establishing foundations for autonomous snake robot navigation.",
    "pdf_url": "https://arxiv.org/pdf/2512.11886v1",
    "github_url": null,
    "published": "2025-12-09T05:17:42+00:00",
    "updated": "2025-12-09T05:17:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08248v1",
    "title": "Learning Spatiotemporal Tubes for Temporal Reach-Avoid-Stay Tasks using Physics-Informed Neural Networks",
    "authors": [
      "Basu",
      "Das",
      "Jagtap"
    ],
    "summary": "This paper presents a Spatiotemporal Tube (STT)-based control framework for general control-affine MIMO nonlinear pure-feedback systems with unknown dynamics to satisfy prescribed time reach-avoid-stay tasks under external disturbances. The STT is defined as a time-varying ball, whose center and radius are jointly approximated by a Physics-Informed Neural Network (PINN). The constraints governing the STT are first formulated as loss functions of the PINN, and a training algorithm is proposed to minimize the overall violation. The PINN being trained on certain collocation points, we propose a Lipschitz-based validity condition to formally verify that the learned PINN satisfies the conditions over the continuous time horizon. Building on the learned STT representation, an approximation-free closed-form controller is defined to guarantee satisfaction of the T-RAS specification. Finally, the effectiveness and scalability of the framework are validated through two case studies involving a mobile robot and an aerial vehicle navigating through cluttered environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.08248v1",
    "github_url": null,
    "published": "2025-12-09T05:08:52+00:00",
    "updated": "2025-12-09T05:08:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08229v1",
    "title": "Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems",
    "authors": [
      "Salloom",
      "Zhou",
      "Sun"
    ],
    "summary": "Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.",
    "pdf_url": "https://arxiv.org/pdf/2512.08229v1",
    "github_url": null,
    "published": "2025-12-09T04:14:05+00:00",
    "updated": "2025-12-09T04:14:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08186v1",
    "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
    "authors": [
      "Wei",
      "Wan",
      "Peng"
    ],
    "summary": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, \"grounds slowly\" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, \"moves fast\" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.08186v1",
    "github_url": null,
    "published": "2025-12-09T02:29:36+00:00",
    "updated": "2025-12-09T02:29:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08139v1",
    "title": "Robust Agents in Open-Ended Worlds",
    "authors": [
      "Samvelyan"
    ],
    "summary": "The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.",
    "pdf_url": "https://arxiv.org/pdf/2512.08139v1",
    "github_url": null,
    "published": "2025-12-09T00:30:33+00:00",
    "updated": "2025-12-09T00:30:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08122v1",
    "title": "Evolutionary perspective of large language models on shaping research insights into healthcare disparities",
    "authors": [
      "An"
    ],
    "summary": "Introduction. Advances in large language models (LLMs) offer a chance to act as scientific assistants, helping people grasp complex research areas. This study examines how LLMs evolve in healthcare disparities research, with attention to public access to relevant information. Methods. We studied three well-known LLMs: ChatGPT, Copilot, and Gemini. Each week, we asked them a consistent prompt about research themes in healthcare disparities and tracked how their answers changed over a one-month period. Analysis. The themes produced by the LLMs were categorized and cross-checked against H-index values from the Web of Science to verify relevance. This dual approach shows how the outputs of LLMs develop over time and how such progress could help researchers navigate trends. Results. The outputs aligned with actual scientific impact and trends in the field, indicating that LLMs can help people understand the healthcare disparities landscape. Time-series comparisons showed differences among the models in how broadly and deeply they identified and classified themes. Conclusion. The study offers a framework that uses the evolution of multiple LLMs to illuminate AI tools for studying healthcare disparities, informing future research and public engagement strategies.",
    "pdf_url": "https://arxiv.org/pdf/2512.08122v1",
    "github_url": null,
    "published": "2025-12-08T23:58:58+00:00",
    "updated": "2025-12-08T23:58:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08084v1",
    "title": "Bayesian Co-Navigation of a Computational Physical Model and AFM Experiment to Autonomously Survey a Combinatorial Materials Library",
    "authors": [
      "Slautin",
      "Barakati",
      "Liu"
    ],
    "summary": "Building autonomous experiment workflows requires transcending beyond the data-driven surrogate models to incorporate and dynamically refine physical theory during exploration. Here we demonstrate the first fully automated experimental realization of Bayesian co-navigation - a framework in which an autonomous agent simultaneously runs a physical experiment and a computationally expensive physical model. Using an automated AFM platform coupled to a kinetic Monte Carlo (kMC) model of thin-film growth, the system infers a set of effective bond energies for the (CrTaWV)x-Mo(1-x) pseudo-binary combinatorial library, progressively adjusting the kMC parameters to decrease the epistemic disparity between simulation and experiment. This real-time theoretical refinement enables the kMC model to capture the behavior of the specific materials system and reveals the mechanistic role of hetero-bonding in governing surface diffusion. Together, these results establish co-navigation as a general strategy for tightly integrating physical models with autonomous experimental platforms to produce interpretable and continually self-correcting theoretical modelling of complex materials systems.",
    "pdf_url": "https://arxiv.org/pdf/2512.08084v1",
    "github_url": null,
    "published": "2025-12-08T22:29:08+00:00",
    "updated": "2025-12-08T22:29:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08028v1",
    "title": "Optimized Area Coverage in Disaster Response Utilizing Autonomous UAV Swarm Formations",
    "authors": [
      "Papakostas",
      "Geladaris",
      "Mastrogeorgiou"
    ],
    "summary": "This paper presents a UAV swarm system designed to assist first responders in disaster scenarios like wildfires. By distributing sensors across multiple agents, the system extends flight duration and enhances data availability, reducing the risk of mission failure due to collisions. To mitigate this risk further, we introduce an autonomous navigation framework that utilizes a local Euclidean Signed Distance Field (ESDF) map for obstacle avoidance while maintaining swarm formation with minimal path deviation. Additionally, we incorporate a Traveling Salesman Problem (TSP) variant to optimize area coverage, prioritizing Points of Interest (POIs) based on preassigned values derived from environmental behavior and critical infrastructure. The proposed system is validated through simulations with varying swarm sizes, demonstrating its ability to maximize coverage while ensuring collision avoidance between UAVs and obstacles.",
    "pdf_url": "https://arxiv.org/pdf/2512.08028v1",
    "github_url": null,
    "published": "2025-12-08T20:41:08+00:00",
    "updated": "2025-12-08T20:41:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08025v1",
    "title": "\"Your Privacy is Your Responsibility\": Understanding How Users Collectively Navigate the Complexity of Privacy on Quora",
    "authors": [
      "Shiri",
      "Xiong",
      "Guo"
    ],
    "summary": "In the current technology environment, users are often in a vulnerable position when it comes to protecting their privacy. Previous efforts to promote privacy protection have largely focused on top-down approaches such as regulation and technology design, missing opportunities to understand how to empower users through bottom-up, collective approaches. Our paper addresses this by analyzing what and how privacy-related topics are discussed on Quora. We identified a wide range of interconnected privacy topics brought up by the users, including privacy risks and dangers, protection strategies, organizational practices, and existing laws and regulations. Our results highlight the interplay among the individual, technological, organizational, and societal factors affecting users' privacy attitudes. Moreover, we provide implications for designing community-based tools to better support users' collective efforts in navigating privacy, tools that incorporate users' diverse privacy-related behaviors and preferences, simplify information access and sharing, and connect designers and developers with the user community.",
    "pdf_url": "https://arxiv.org/pdf/2512.08025v1",
    "github_url": null,
    "published": "2025-12-08T20:35:25+00:00",
    "updated": "2025-12-08T20:35:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07976v1",
    "title": "VLD: Visual Language Goal Distance for Reinforcement Learning Navigation",
    "authors": [
      "Milikic",
      "Patel",
      "Frey"
    ],
    "summary": "Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-\"where to go\"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies.",
    "pdf_url": "https://arxiv.org/pdf/2512.07976v1",
    "github_url": null,
    "published": "2025-12-08T19:05:51+00:00",
    "updated": "2025-12-08T19:05:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07926v1",
    "title": "Can AI autonomously build, operate, and use the entire data stack?",
    "authors": [
      "Agarwal",
      "Amini",
      "Mehta"
    ],
    "summary": "Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.",
    "pdf_url": "https://arxiv.org/pdf/2512.07926v1",
    "github_url": null,
    "published": "2025-12-08T18:59:01+00:00",
    "updated": "2025-12-08T18:59:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07813v1",
    "title": "Inchworm-Inspired Soft Robot with Groove-Guided Locomotion",
    "authors": [
      "Thanabalan",
      "Bengtsson",
      "Lafont"
    ],
    "summary": "Soft robots require directional control to navigate complex terrains. However, achieving such control often requires multiple actuators, which increases mechanical complexity, complicates control systems, and raises energy consumption. Here, we introduce an inchworm-inspired soft robot whose locomotion direction is controlled passively by patterned substrates. The robot employs a single rolled dielectric elastomer actuator, while groove patterns on a 3D-printed substrate guide its alignment and trajectory. Through systematic experiments, we demonstrate that varying groove angles enables precise control of locomotion direction without the need for complex actuation strategies. This groove-guided approach reduces energy consumption, simplifies robot design, and expands the applicability of bio-inspired soft robots in fields such as search and rescue, pipe inspection, and planetary exploration.",
    "pdf_url": "https://arxiv.org/pdf/2512.07813v1",
    "github_url": null,
    "published": "2025-12-08T18:47:04+00:00",
    "updated": "2025-12-08T18:47:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07807v1",
    "title": "Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes",
    "authors": [
      "Krakovsky",
      "Fiebelman",
      "Benaim"
    ],
    "summary": "Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2512.07807v1",
    "github_url": null,
    "published": "2025-12-08T18:39:58+00:00",
    "updated": "2025-12-08T18:39:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07668v1",
    "title": "EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset",
    "authors": [
      "John",
      "Kesari",
      "DiMatteo"
    ],
    "summary": "We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .",
    "pdf_url": "https://arxiv.org/pdf/2512.07668v1",
    "github_url": "https://github.com/ComputerVisionRutgers/EgoCampus",
    "published": "2025-12-08T16:03:21+00:00",
    "updated": "2025-12-08T16:03:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07609v2",
    "title": "Obstacle Avoidance of UAV in Dynamic Environments Using Direction and Velocity-Adaptive Artificial Potential Field",
    "authors": [
      "Pavle",
      "Rajneesh",
      "Sahoo"
    ],
    "summary": "The conventional Artificial Potential Field (APF) is fundamentally limited by the local minima issue and its inability to account for the kinematics of moving obstacles. This paper addresses the critical challenge of autonomous collision avoidance for Unmanned Aerial Vehicles (UAVs) operating in dynamic and cluttered airspace by proposing a novel Direction and Relative Velocity Weighted Artificial Potential Field (APF). In this approach, a bounded weighting function, $ω(θ,v_{e})$, is introduced to dynamically scale the repulsive potential based on the direction and velocity of the obstacle relative to the UAV. This robust APF formulation is integrated within a Model Predictive Control (MPC) framework to generate collision-free trajectories while adhering to kinematic constraints. Simulation results demonstrate that the proposed method effectively resolves local minima and significantly enhances safety by enabling smooth, predictive avoidance maneuvers. The system ensures superior path integrity and reliable performance, confirming its viability for autonomous navigation in complex environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.07609v2",
    "github_url": null,
    "published": "2025-12-08T14:57:01+00:00",
    "updated": "2025-12-09T17:04:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07574v1",
    "title": "Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework",
    "authors": [
      "Li",
      "Jia",
      "Sharipov"
    ],
    "summary": "Accurate three-dimensional delineation of liver tumors on contrast-enhanced CT is a prerequisite for treatment planning, navigation and response assessment, yet manual contouring is slow, observer-dependent and difficult to standardise across centres. Automatic segmentation is complicated by low lesion-parenchyma contrast, blurred or incomplete boundaries, heterogeneous enhancement patterns, and confounding structures such as vessels and adjacent organs. We propose a hybrid framework that couples an attention-enhanced cascaded U-Net with handcrafted radiomics and voxel-wise 3D CNN refinement for joint liver and liver-tumor segmentation. First, a 2.5D two-stage network with a densely connected encoder, sub-pixel convolution decoders and multi-scale attention gates produces initial liver and tumor probability maps from short stacks of axial slices. Inter-slice temporal consistency is then enforced by a simple three-slice refinement rule along the cranio-caudal direction, which restores thin and tiny lesions while suppressing isolated noise. Next, 728 radiomic descriptors spanning intensity, texture, shape, boundary and wavelet feature groups are extracted from candidate lesions and reduced to 20 stable, highly informative features via multi-strategy feature selection; a random forest classifier uses these features to reject false-positive regions. Finally, a compact 3D patch-based CNN derived from AlexNet operates in a narrow band around the tumor boundary to perform voxel-level relabelling and contour smoothing.",
    "pdf_url": "https://arxiv.org/pdf/2512.07574v1",
    "github_url": null,
    "published": "2025-12-08T14:09:21+00:00",
    "updated": "2025-12-08T14:09:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07512v1",
    "title": "Dictionary-Based Contrastive Learning for GNSS Jamming Detection",
    "authors": [
      "Hussain",
      "Majal",
      "Chughtai"
    ],
    "summary": "Global Navigation Satellite System (GNSS) signals are fundamental in applications across navigation, transportation, and industrial networks. However, their extremely low received power makes them highly vulnerable to radio-frequency interference (RFI) and intentional jamming. Modern data-driven methods offer powerful representational power for such applications, however real-time and reliable jamming detection on resource-limited embedded receivers remains a key challenge due to the high computational and memory demands of the conventional learning paradigm. To address these challenges, this work presents a dictionary-based contrastive learning (DBCL) framework for GNSS jamming detection that integrates transfer learning, contrastive representation learning, and model compression techniques. The framework combines tuned contrastive and dictionary-based loss functions to enhance feature separability under low-data conditions and applies structured pruning and knowledge distillation to reduce model complexity while maintaining high accuracy. Extensive evaluation across varying data regimes demonstrate that the proposed algorithm consistently outperforms modern CNN, MobileViT, and ResNet-18 architectures. The framework achieves a substantial reduction in memory footprint and inference latency, confirming its suitability for real-time, low-power GNSS interference detection on embedded platforms.",
    "pdf_url": "https://arxiv.org/pdf/2512.07512v1",
    "github_url": null,
    "published": "2025-12-08T12:47:39+00:00",
    "updated": "2025-12-08T12:47:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07483v1",
    "title": "SemanticTours: A Conceptual Framework for Non-Linear, Knowledge Graph-Driven Data Tours",
    "authors": [
      "Fürst",
      "Haar",
      "El-Assady"
    ],
    "summary": "Interactive tours help users explore datasets and provide onboarding. They rely on a linear sequence of views, showing a curated set of relevant data selections and introduce user interfaces. Existing frameworks of tours, however, often do not allow for branching and refining hypotheses outside of a rigid sequence, which is important in knowledge-centric domains such as law. For example, lawyers performing analytical case analysis need to iteratively weigh up different legal norms and construct strings of arguments. To address this gap, we propose SemanticTours, a semantic, graph-based model of tours that shifts from a sequence-based towards a graph-based navigation. Our model constructs a domain-specific knowledge graph that connects data elements based on user-definable semantic relationships. These relationships enable non-linear graph navigation that defines tours. We apply SemanticTours to the domain of law and conceptualize a visual analytics design and interaction concept for analytical reasoning in legal case analysis. Our concept accounts for the inherent complexity of graph-based tours using aggregated graph nodes and supporting navigation with a semantic lens. During an evaluation with six domain experts from law, they suggest that graph-based tours better support their analytical reasoning than sequences. Our work opens research opportunities for such tours to support analytical reasoning in law and other knowledge-centric domains.",
    "pdf_url": "https://arxiv.org/pdf/2512.07483v1",
    "github_url": null,
    "published": "2025-12-08T12:10:07+00:00",
    "updated": "2025-12-08T12:10:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07342v2",
    "title": "PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning",
    "authors": [
      "Gong",
      "Liu",
      "Li"
    ],
    "summary": "Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.   To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.",
    "pdf_url": "https://arxiv.org/pdf/2512.07342v2",
    "github_url": null,
    "published": "2025-12-08T09:29:24+00:00",
    "updated": "2025-12-16T02:17:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07309v1",
    "title": "Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals",
    "authors": [
      "Wang",
      "Wang",
      "Yang"
    ],
    "summary": "Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining framework couples a large localization model (LM) with a neural radio-frequency radiance field (RF-NeRF) in an asymmetrical autoencoder architecture. In this design, the LM encodes received RF spectra into latent, position-relevant representations, while the RF-NeRF decodes them to reconstruct the original spectra. This alignment between input and output enables effective representation learning using large-scale, unlabeled RF data, which can be collected continuously with minimal effort. To this end, we collected RF samples at 7,327,321 positions across 100 diverse scenes using four common wireless technologies--RFID, BLE, WiFi, and IIoT. Data from 75 scenes were used for training, and the remaining 25 for evaluation. Experimental results show that the RFRP-pretrained LM reduces localization error by over 40% compared to non-pretrained models and by 21% compared to those pretrained using supervised learning.",
    "pdf_url": "https://arxiv.org/pdf/2512.07309v1",
    "github_url": null,
    "published": "2025-12-08T08:52:08+00:00",
    "updated": "2025-12-08T08:52:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07266v1",
    "title": "SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks",
    "authors": [
      "Tretter",
      "Flögel",
      "Vasilache"
    ],
    "summary": "Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.",
    "pdf_url": "https://arxiv.org/pdf/2512.07266v1",
    "github_url": null,
    "published": "2025-12-08T08:06:40+00:00",
    "updated": "2025-12-08T08:06:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07910v1",
    "title": "The interstellar signature: A computational framework for open source interstellar tracking",
    "authors": [
      "Sahu"
    ],
    "summary": "Interstellar objects such as 1I/'Oumuamua and 2I/Borisov offer a unique window into the formation and evolution of other star systems, yet the tracking and analysis of their trajectories remain limited to specialized research institutions. Existing interstellar and solar system datasets are large, complex, and difficult to navigate, reducing accessibility for developers, researchers, and enthusiasts. To address this, we present The Interstellar Signature: a computational framework for open-source interstellar tracking, implemented through a web-based platform.   Interstellar Signature bridges raw astronomical data and an intuitive, developer-friendly interface. The framework integrates live data streams from public repositories and APIs with physics-based simulation methods to model and visualize the motion of interstellar and solar system objects in real time. The platform supports interactive visualizations, comparative orbital analysis, and modular tools that allow users to explore and extend the system for research, experimentation, or development.   As an open-source project, the framework encourages collaboration and hands-on engagement with complex datasets. It exists within NexusCosmos, an ecosystem envisioned as a \"Linux for the space race,\" aimed at democratizing access to space science tools and data. By transforming large datasets into visual, interactive, and customizable simulations, Interstellar Signature expands participation in interstellar research and observation.   Future extensions will add AI-driven trajectory prediction, anomaly detection, and advanced visualization. By combining open-source accessibility with computational rigor, this framework lowers the barrier to interstellar analysis and serves as a step toward bridging professional astronomy and public scientific engagement.",
    "pdf_url": "https://arxiv.org/pdf/2512.07910v1",
    "github_url": null,
    "published": "2025-12-08T06:41:36+00:00",
    "updated": "2025-12-08T06:41:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07043v1",
    "title": "Set-based Optimal, Robust, and Resilient Control with Applications to Autonomous Precision Landing",
    "authors": [
      "Kamath",
      "Vinod",
      "Elango"
    ],
    "summary": "We present a real-time-capable set-based framework for closed-loop predictive control of autonomous systems using tools from computational geometry, dynamic programming, and convex optimization. The control architecture relies on the offline precomputation of the controllable tube, i.e, a time-indexed sequence of controllable sets. Sets are represented using constrained zonotopes (CZs), which are efficient encodings of convex polytopes that support fast set operations and enable tractable dynamic programming in high dimensions. Online, we obtain a globally optimal control profile by solving a series of one-step optimal control problems. Our key contributions are: (1) free-final-time optimality: we devise an optimal horizon computation algorithm to achieve global optimality; (2) robustness: we handle stochastic uncertainty in both the state and control, with probabilistic guarantees, by constructing bounded disturbance sets; (3) resilience: we develop (i) an optimization-free approach to computing the instantaneous reachable set, i.e., the reachable set from the current state, to enable, for example, large/maximal divert maneuvers, and (ii) an approach to achieving maximal decision-deferral, i.e., maintaining reachability/divert-feasibility to multiple targets for as long as possible. By means of an autonomous precision landing case study, we demonstrate globally optimal free-final-time guidance, robustness to navigation and actuation uncertainties, instantaneous divert envelope computation, and maximal decision-deferral.",
    "pdf_url": "https://arxiv.org/pdf/2512.07043v1",
    "github_url": null,
    "published": "2025-12-07T23:37:56+00:00",
    "updated": "2025-12-07T23:37:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06951v1",
    "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge",
    "authors": [
      "Larchenko",
      "Zarin",
      "Karnatak"
    ],
    "summary": "We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.   Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.   Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.",
    "pdf_url": "https://arxiv.org/pdf/2512.06951v1",
    "github_url": null,
    "published": "2025-12-07T18:08:45+00:00",
    "updated": "2025-12-07T18:08:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06944v1",
    "title": "A Unifying Human-Centered AI Fairness Framework",
    "authors": [
      "Rahman",
      "Pan",
      "Foulds"
    ],
    "summary": "The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.",
    "pdf_url": "https://arxiv.org/pdf/2512.06944v1",
    "github_url": null,
    "published": "2025-12-07T17:52:38+00:00",
    "updated": "2025-12-07T17:52:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06934v1",
    "title": "Visual Function Profiles via Multi-Path Aggregation Reveal Neuron-Level Responses in the Drosophila Brain",
    "authors": [
      "Xie",
      "Ren",
      "Zhou"
    ],
    "summary": "Accurately predicting individual neurons' responses and spatial functional properties in complex visual tasks remains a key challenge in understanding neural computation. Existing whole-brain connectome models of Drosophila often rely on parameter assumptions or deep learning approaches, yet remain limited in their ability to reliably predict dynamic neuronal responses. We introduce a Multi-Path Aggregation (MPA) framework, based on neural network steady-state theory, to build a whole-brain Visual Function Profiles (VFP) of Drosophila neurons and predict their responses under diverse visual tasks. Unlike conventional methods relying on redundant parameters, MPA combines visual input features with the whole-brain connectome topology. It uses adjacency matrix powers and finite-path optimization to efficiently predict neuronal function, including ON/OFF polarity, direction selectivity, and responses to complex visual stimuli. Our model achieves a Pearson correlation of 0.84+/-0.12 for ON/OFF responses, outperforming existing methods (0.33+/-0.59), and accurately captures neuron functional properties, including luminance and direction preferences, while allowing single-neuron or population-level blockade simulations. Replacing CNN modules with VFP-derived Lobula Columnar(LC) population responses in a Drosophila simulation enables successful navigation and obstacle avoidance, demonstrating the model's effectiveness in guiding embodied behavior. This study establishes a \"connectome-functional profile-behavior\" framework, offering a whole-brain quantitative tool to study Drosophila visual computation and a neuron-level guide for brain-inspired intelligence.",
    "pdf_url": "https://arxiv.org/pdf/2512.06934v1",
    "github_url": null,
    "published": "2025-12-07T17:29:56+00:00",
    "updated": "2025-12-07T17:29:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11876v1",
    "title": "Traversability Aware Autonomous Navigation for Multi-Modal Mobility Morphobot (M4)",
    "authors": [
      "Suryawanshi"
    ],
    "summary": "Autonomous navigation in unstructured environments requires robots to assess terrain difficulty in real-time and plan paths that balance efficiency with safety. This thesis presents a traversability-aware navigation framework for the M4 robot platform that uses learned terrain analysis to generate energy-efficient paths avoiding difficult terrain.Our approach uses FAST-LIO for real-time localization, generating 2.5D elevation maps from LiDAR point clouds. A CNN-based model processes these elevation maps to estimate traversability scores, which are converted into navigation costs for path planning. A custom A* planner incorporates these costs alongside geometric distance and energy consumption to find paths that trade modest distance increases for substantial terrain quality improvements. Before system development, a platform-agnostic study compared LiDAR-based and camera-based SLAM using OptiTrack ground truth. Point cloud comparison through ICP alignment and cloud-to-mesh distance analysis demonstrated that LiDAR-based mapping achieves centimeter-level precision essential for elevation mapping, while camera-based approaches exhibited significantly higher geometric error. These findings directly resulted in the selection of LiDAR as the primary sensor to generate elevation maps. The complete pipeline integrates FAST-LIO localization, GPU-accelerated elevation mapping, CNN-based traversability estimation, and Nav2 navigation with a custom traversability-aware planner. Experimental results demonstrate that the system successfully avoids low traversability regions and accepts a few longer paths to achieve a reduction in terrain cost. This work establishes a foundation for intelligent terrain-aware navigation applicable to multi-modal robotic platforms.",
    "pdf_url": "https://arxiv.org/pdf/2512.11876v1",
    "github_url": null,
    "published": "2025-12-07T17:28:34+00:00",
    "updated": "2025-12-07T17:28:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06922v1",
    "title": "Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI",
    "authors": [
      "Mikros"
    ],
    "summary": "Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.",
    "pdf_url": "https://arxiv.org/pdf/2512.06922v1",
    "github_url": null,
    "published": "2025-12-07T17:05:31+00:00",
    "updated": "2025-12-07T17:05:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06912v2",
    "title": "Khalasi: Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields",
    "authors": [
      "Gadhvi",
      "Manjanna"
    ],
    "summary": "For centuries, khalasi (Gujarati for sailor) have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.",
    "pdf_url": "https://arxiv.org/pdf/2512.06912v2",
    "github_url": null,
    "published": "2025-12-07T16:36:31+00:00",
    "updated": "2025-12-09T03:48:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06754v1",
    "title": "Model-Less Feedback Control of Space-based Continuum Manipulators using Backbone Tension Optimization",
    "authors": [
      "Rajneesh",
      "Pavle",
      "Sahoo"
    ],
    "summary": "Continuum manipulators offer intrinsic dexterity and safe geometric compliance for navigation within confined and obstacle-rich environments. However, their infinite-dimensional backbone deformation, unmodeled internal friction, and configuration-dependent stiffness fundamentally limit the reliability of model-based kinematic formulations, resulting in inaccurate Jacobian predictions, artificial singularities, and unstable actuation behavior. Motivated by these limitations, this work presents a complete model-less control framework that bypasses kinematic modeling by using an empirically initialized Jacobian refined online through differential convex updates. Tip motion is generated via a real-time quadratic program that computes actuator increments while enforcing tendon slack avoidance and geometric limits. A backbone tension optimization term is introduced in this paper to regulate axial loading and suppress co-activation compression. The framework is validated across circular, pentagonal, and square trajectories, demonstrating smooth convergence, stable tension evolution, and sub-millimeter steady-state accuracy without any model calibration or parameter identification. These results establish the proposed controller as a scalable alternative to model-dependent continuum manipulation in a constrained environment.",
    "pdf_url": "https://arxiv.org/pdf/2512.06754v1",
    "github_url": null,
    "published": "2025-12-07T09:34:28+00:00",
    "updated": "2025-12-07T09:34:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06636v1",
    "title": "Distribution-Aware Exploration for Adaptive HNSW Search",
    "authors": [
      "Zhang",
      "Miller"
    ],
    "summary": "Hierarchical Navigable Small World (HNSW) is widely adopted for approximate nearest neighbor search (ANNS) for its ability to deliver high recall with low latency on large-scale, high-dimensional embeddings. The exploration factor, commonly referred to as ef, is a key parameter in HNSW-based vector search that balances accuracy and efficiency. However, existing systems typically rely on manually and statically configured ef values that are uniformly applied across all queries. This results in a distribution-agnostic configuration that fails to account for the non-uniform and skewed nature of real-world embedding data and query workloads. As a consequence, HNSW-based systems suffer from two key practical issues: (i) the absence of recall guarantees, and (ii) inefficient ANNS performance due to over- or under-searching. In this paper, we propose Adaptive-ef (Ada-ef), a data-driven, update-friendly, query-adaptive approach that dynamically configures ef for each query at runtime to approximately meet a declarative target recall with minimal computation. The core of our approach is a theoretically grounded statistical model that captures the similarity distribution between each query and the database vectors. Based on this foundation, we design a query scoring mechanism that distinguishes between queries requiring only small ef and those that need larger ef to meet a target recall, and accordingly assigns an appropriate ef to each query. Experimental results on real-world embeddings produced by state-of-the-art Transformer models from OpenAI and Cohere show that, compared with state-of-the-art learning-based adaptive approaches, our method achieves the target recall while avoiding both over- and under-searching, reducing online query latency by up to 4x, offline computation time by 50x, and offline memory usage by 100x.",
    "pdf_url": "https://arxiv.org/pdf/2512.06636v1",
    "github_url": null,
    "published": "2025-12-07T02:55:56+00:00",
    "updated": "2025-12-07T02:55:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06608v1",
    "title": "A New Trajectory-Oriented Approach to Enhancing Comprehensive Crowd Navigation Performance",
    "authors": [
      "Zhou",
      "Piao",
      "Gao"
    ],
    "summary": "Crowd navigation has garnered considerable research interest in recent years, especially with the proliferating application of deep reinforcement learning (DRL) techniques. Many studies, however, do not sufficiently analyze the relative priorities among evaluation metrics, which compromises the fair assessment of methods with divergent objectives. Furthermore, trajectory-continuity metrics, specifically those requiring $C^2$ smoothness, are rarely incorporated. Current DRL approaches generally prioritize efficiency and proximal comfort, often neglecting trajectory optimization or addressing it only through simplistic, unvalidated smoothness reward. Nevertheless, effective trajectory optimization is essential to ensure naturalness, enhance comfort, and maximize the energy efficiency of any navigation system. To address these gaps, this paper proposes a unified framework that enables the fair and transparent assessment of navigation methods by examining the prioritization and joint evaluation of multiple optimization objectives. We further propose a novel reward-shaping strategy that explicitly emphasizes trajectory-curvature optimization. The resulting trajectory quality and adaptability are significantly enhanced across multi-scale scenarios. Through extensive 2D and 3D experiments, we demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches.",
    "pdf_url": "https://arxiv.org/pdf/2512.06608v1",
    "github_url": null,
    "published": "2025-12-07T00:52:07+00:00",
    "updated": "2025-12-07T00:52:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06535v1",
    "title": "The E-Rocket: Low-cost Testbed for TVC Rocket GNC Validation",
    "authors": [
      "Santos",
      "Fonte",
      "Martins"
    ],
    "summary": "This paper presents the E-Rocket, an electric-powered, low-cost rocket prototype for validation of Guidance, Navigation & Control (GNC) algorithms based on Thrust Vector Control (TVC). Relying on commercially available components and 3D printed parts, a pair of contra-rotating DC brushless motors is assembled on a servo-actuated gimbal mechanism that provides thrust vectoring capability. A custom avionics hardware and software stack is developed considering a dual computer setup which leverages the capabilities of the PX4 autopilot and the modularity of ROS 2 to accommodate for tailored GNC algorithms. The platform is validated in an indoor motion-capture arena using a baseline PID-based trajectory tracking controller. Results demonstrate accurate trajectory tracking and confirm the suitability of the E-Rocket as a versatile testbed for rocket GNC algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2512.06535v1",
    "github_url": null,
    "published": "2025-12-06T19:00:13+00:00",
    "updated": "2025-12-06T19:00:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06452v1",
    "title": "Trajectory Optimization for Cellular-Connected UAV in Complex Environment with Partial CKM",
    "authors": [
      "Song",
      "Lu",
      "Zhang"
    ],
    "summary": "Cellular-connected unmanned aerial vehicles (UAVs) are expected to play an increasingly important role in future wireless networks. To facilitate the reliable navigation for cellular-connected UAVs, channel knowledge map (CKM) is considered a promising approach capable of tackling the non-negligible co-channel interference resulting from the high line-of-sight (LoS) probability of air-ground (AG) channels. Nevertheless, due to measurement constraints and the aging of information, CKM is usually incomplete and needs to be regularly updated to capture the dynamic nature of complex environments. In this paper, we propose a novel trajectory design strategy in which UAV navigation and CKM completion are incorporated into a common framework, enabling mutual benefits for both tasks. Specifically, a cellular-connected UAV deployed in an urban environment measures the radio information during its flight and completes the CKM with Kriging interpolation. Based on the method of grid discretization and spherical approximation, a mixed-integer multi-objective optimization problem is formulated. The problem falls into the category of combinatorial mathematics and is essentially equivalent to determining an optimum sequence of grid points to traverse. Through proper mathematical manipulation, the problem is reformulated as variants of two classic models in graph theory, namely the shortest-path problem (SPP) and the traveling salesman problem (TSP). Two navigation strategies based on the two different models are proposed and thoroughly compared based on numerical results to provide implementable methods for engineering practice and reveal the trade-offs between UAV navigation and CKM completion. Simulation results reveal that the proposed navigation strategies can quickly expand the Pareto boundary of the problem and approach the performance of fully-known CKM.",
    "pdf_url": "https://arxiv.org/pdf/2512.06452v1",
    "github_url": null,
    "published": "2025-12-06T14:28:43+00:00",
    "updated": "2025-12-06T14:28:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08987v1",
    "title": "3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization",
    "authors": [
      "Hao",
      "Zhu",
      "Yang"
    ],
    "summary": "Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.",
    "pdf_url": "https://arxiv.org/pdf/2512.08987v1",
    "github_url": null,
    "published": "2025-12-06T13:09:03+00:00",
    "updated": "2025-12-06T13:09:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06250v1",
    "title": "Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning",
    "authors": [
      "Tava"
    ],
    "summary": "Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\\times$16 to 128$\\times$128 $\\times$ 10 unique mazes $\\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\\% threshold baselines. Results show 23-55\\% improvements in completion time, 83\\% reduction in runtime variance, and 71\\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\\% improvement for 16$\\times$16 mazes, 34\\% for 32$\\times$32, and 55\\% for 64$\\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.",
    "pdf_url": "https://arxiv.org/pdf/2512.06250v1",
    "github_url": null,
    "published": "2025-12-06T02:50:32+00:00",
    "updated": "2025-12-06T02:50:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06232v1",
    "title": "Opinion: Learning Intuitive Physics May Require More than Visual Data",
    "authors": [
      "Su",
      "Legris",
      "Gureckis"
    ],
    "summary": "Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.",
    "pdf_url": "https://arxiv.org/pdf/2512.06232v1",
    "github_url": null,
    "published": "2025-12-06T00:49:41+00:00",
    "updated": "2025-12-06T00:49:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06207v1",
    "title": "Where to Fly, What to Send: Communication-Aware Aerial Support for Ground Robots",
    "authors": [
      "Suthar",
      "Maity"
    ],
    "summary": "In this work we consider a multi-robot team operating in an unknown environment where one aerial agent is tasked to map the environment and transmit (a portion of) the mapped environment to a group of ground agents that are trying to reach their goals. The entire operation takes place over a bandwidth-limited communication channel, which motivates the problem of determining what and how much information the assisting agent should transmit and when while simultaneously performing exploration/mapping. The proposed framework enables the assisting aerial agent to decide what information to transmit based on the Value-of-Information (VoI), how much to transmit using a Mixed-Integer Linear Programming (MILP), and how to acquire additional information through an utility score-based environment exploration strategy. We perform a communication-motion trade-off analysis between the total amount of map data communicated by the aerial agent and the navigation cost incurred by the ground agents.",
    "pdf_url": "https://arxiv.org/pdf/2512.06207v1",
    "github_url": null,
    "published": "2025-12-05T23:00:12+00:00",
    "updated": "2025-12-05T23:00:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06200v1",
    "title": "How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?",
    "authors": [
      "Yamashita",
      "Amagata",
      "Matsui"
    ],
    "summary": "Approximate Nearest Neighbor Search (ANNS) has recently gained significant attention due to its many applications, such as Retrieval-Augmented Generation. Such applications require ANNS algorithms that support dynamic data, so the ANNS problem on dynamic data has attracted considerable interest. However, a comprehensive evaluation methodology for data deletion in ANNS has yet to be established. This study proposes an experimental framework and comprehensive evaluation metrics to assess the efficiency of data deletion for ANNS indexes under practical use cases. Specifically, we categorize data deletion methods in graph-based ANNS into three approaches and formalize them mathematically. The performance is assessed in terms of accuracy, query speed, and other relevant metrics. Finally, we apply the proposed evaluation framework to Hierarchical Navigable Small World, one of the state-of-the-art ANNS methods, to analyze the effects of data deletion, and propose Deletion Control, a method which dynamically selects the appropriate deletion method under a required search accuracy.",
    "pdf_url": "https://arxiv.org/pdf/2512.06200v1",
    "github_url": null,
    "published": "2025-12-05T22:53:45+00:00",
    "updated": "2025-12-05T22:53:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06198v1",
    "title": "Cascaded Tightly-Coupled Observer Design for Single-Range-Aided Inertial Navigation",
    "authors": [
      "Sifour",
      "Berkane",
      "Tayebi"
    ],
    "summary": "This work introduces a single-range-aided navigation observer that reconstructs the full state of a rigid body using only an Inertial Measurement Unit (IMU), a body-frame vector measurement (e.g., magnetometer), and a distance measurement from a fixed anchor point. The design first formulates an extended linear time-varying (LTV) system to estimate body-frame position, body-frame velocity, and the gravity direction. The recovered gravity direction, combined with the body-frame vector measurement, is then used to reconstruct the full orientation on $\\mathrm{SO}(3)$, resulting in a cascaded observer architecture. Almost Global Asymptotic Stability (AGAS) of the cascaded design is established under a uniform observability condition, ensuring robustness to sensor noise and trajectory variations. Simulation studies on three-dimensional trajectories demonstrate accurate estimation of position, velocity, and orientation, highlighting single-range aiding as a lightweight and effective modality for autonomous navigation.",
    "pdf_url": "https://arxiv.org/pdf/2512.06198v1",
    "github_url": null,
    "published": "2025-12-05T22:44:22+00:00",
    "updated": "2025-12-05T22:44:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06151v1",
    "title": "Real-Time Spatiotemporal Tubes for Dynamic Unsafe Sets",
    "authors": [
      "Das",
      "Upadhyay",
      "Jagtap"
    ],
    "summary": "This paper presents a real-time control framework for nonlinear pure-feedback systems with unknown dynamics to satisfy reach-avoid-stay tasks within a prescribed time in dynamic environments. To achieve this, we introduce a real-time spatiotemporal tube (STT) framework. An STT is defined as a time-varying ball in the state space whose center and radius adapt online using only real-time sensory input. A closed-form, approximation-free control law is then derived to constrain the system output within the STT, ensuring safety and task satisfaction. We provide formal guarantees for obstacle avoidance and on-time task completion. The effectiveness and scalability of the framework are demonstrated through simulations and hardware experiments on a mobile robot and an aerial vehicle, navigating in cluttered dynamic environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.06151v1",
    "github_url": null,
    "published": "2025-12-05T21:00:43+00:00",
    "updated": "2025-12-05T21:00:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06147v1",
    "title": "GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers",
    "authors": [
      "Hwang",
      "Yang",
      "Monon"
    ],
    "summary": "While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\\&M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.",
    "pdf_url": "https://arxiv.org/pdf/2512.06147v1",
    "github_url": null,
    "published": "2025-12-05T20:57:48+00:00",
    "updated": "2025-12-05T20:57:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06133v1",
    "title": "A Nonlinear Observer for Air-Velocity and Attitude Estimation Using Pitot and Barometric Measurements",
    "authors": [
      "Tchonkeu",
      "Berkane",
      "Hamel"
    ],
    "summary": "This paper addresses the problem of estimating air velocity and full attitude for unmanned aerial vehicles (UAVs) in GNSS-denied environments using minimal onboard sensing-an interesting and practically relevant challenge for UAV navigation. The contribution of the paper is twofold: (i) an observability analysis establishing the conditions for uniform observability, which are useful for trajectory planning and motion control of the UAV; and (ii) the design of a nonlinear observer on SO3R3R that incorporates pitot-tube, barometric altitude, and magnetometer measurements as outputs, with IMU data used as inputs, within a unified framework. Simulation results are presented to confirm the convergence and robustness of the proposed design, including under minimally excited trajectories.",
    "pdf_url": "https://arxiv.org/pdf/2512.06133v1",
    "github_url": null,
    "published": "2025-12-05T20:26:00+00:00",
    "updated": "2025-12-05T20:26:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06095v1",
    "title": "Comparative Analysis of Autonomous and Systematic Control Strategies for Hole-Doped Hubbard Clusters: Reinforcement Learning versus Physics-Guided Design",
    "authors": [
      "Dwivedi",
      "Palandage"
    ],
    "summary": "Engineering electron correlations in quantum dot arrays demands navigation of high-dimensional, non-convex parameter spaces where hole doping fundamentally alters the physics. We present a comparative study of two control paradigms for the one-hole, half-filled Hubbard model: (i) systematic physics-guided design and (ii) autonomous deep reinforcement learning with geometry-aware neural architectures. While systematic analysis reveals key design principles, such as field-induced localization for trapping the mobile hole, it becomes computationally intractable for optimization. We show that an autonomous RL agent, benchmarked across five 3D lattices from tetrahedron to FCC, achieves human-competitive accuracy (R^2 > 0.97) and 95.5 percent success on held-out tasks. The agent is 3-4 orders of magnitude more sample-efficient than grid search and outperforms other black-box optimization methods. Transfer learning yields 91 percent few-shot generalization to unseen geometries. This work establishes autonomous RL as a viable and highly efficient framework for rapid optimization and non-obvious strategy discovery in complex quantum systems.",
    "pdf_url": "https://arxiv.org/pdf/2512.06095v1",
    "github_url": null,
    "published": "2025-12-05T19:04:28+00:00",
    "updated": "2025-12-05T19:04:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.05929v1",
    "title": "LLM Harms: A Taxonomy and Discussion",
    "authors": [
      "Chen",
      "Afroogh",
      "Murali"
    ],
    "summary": "This study addresses categories of harm surrounding Large Language Models (LLMs) in the field of artificial intelligence. It addresses five categories of harms addressed before, during, and after development of AI applications: pre-development, direct output, Misuse and Malicious Application, and downstream application. By underscoring the need to define risks of the current landscape to ensure accountability, transparency and navigating bias when adapting LLMs for practical applications. It proposes mitigation strategies and future directions for specific domains and a dynamic auditing system guiding responsible development and integration of LLMs in a standardized proposal.",
    "pdf_url": "https://arxiv.org/pdf/2512.05929v1",
    "github_url": null,
    "published": "2025-12-05T18:12:21+00:00",
    "updated": "2025-12-05T18:12:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.05808v1",
    "title": "Real-time Remote Tracking and Autonomous Planning for Whale Rendezvous using Robots",
    "authors": [
      "Bhattacharya",
      "Jadhav",
      "Izhar"
    ],
    "summary": "We introduce a system for real-time sperm whale rendezvous at sea using an autonomous uncrewed aerial vehicle. Our system employs model-based reinforcement learning that combines in situ sensor data with an empirical whale dive model to guide navigation decisions. Key challenges include (i) real-time acoustic tracking in the presence of multiple whales, (ii) distributed communication and decision-making for robot deployments, and (iii) on-board signal processing and long-range detection from fish-trackers. We evaluate our system by conducting rendezvous with sperm whales at sea in Dominica, performing hardware experiments on land, and running simulations using whale trajectories interpolated from marine biologists' surface observations.",
    "pdf_url": "https://arxiv.org/pdf/2512.05808v1",
    "github_url": null,
    "published": "2025-12-05T15:27:58+00:00",
    "updated": "2025-12-05T15:27:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.05731v1",
    "title": "DeeDeeExperiment: Building an infrastructure for integrating and managing omics data analysis results in R/Bioconductor",
    "authors": [
      "Abassi",
      "Schwarz",
      "Filippi"
    ],
    "summary": "Summary: Modern omics experiments now involve multiple conditions and complex designs, producing an increasingly large set of differential expression and functional enrichment analysis results. However, no standardized data structure exists to store and contextualize these results together with their metadata, leaving researchers with an unmanageable and potentially non-reproducible collection of results that are difficult to navigate and/or share. Here we introduce DeeDeeExperiment, a new S4 class for managing and storing omics data analysis results, implemented within the Bioconductor ecosystem, which promotes interoperability, reproducibility and good documentation. This class extends the widely used SingleCellExperiment object by introducing dedicated slots for Differential Expression (DEA) and Functional Enrichment Analysis (FEA) results, allowing users to organize, store, and retrieve information on multiple contrasts and associated metadata within a single data object, ultimately streamlining the management and interpretation of many omics datasets. Availability and implementation: DeeDeeExperiment is available on Bioconductor under the MIT license (https://bioconductor.org/packages/DeeDeeExperiment), with its development version also available on Github (https://github.com/imbeimainz/DeeDeeExperiment).",
    "pdf_url": "https://arxiv.org/pdf/2512.05731v1",
    "github_url": "https://github.com/imbeimainz/DeeDeeExperiment",
    "published": "2025-12-05T14:11:28+00:00",
    "updated": "2025-12-05T14:11:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.06048v1",
    "title": "The Road of Adaptive AI for Precision in Cybersecurity",
    "authors": [
      "Garg"
    ],
    "summary": "Cybersecurity's evolving complexity presents unique challenges and opportunities for AI research and practice. This paper shares key lessons and insights from designing, building, and operating production-grade GenAI pipelines in cybersecurity, with a focus on the continual adaptation required to keep pace with ever-shifting knowledge bases, tooling, and threats. Our goal is to provide an actionable perspective for AI practitioners and industry stakeholders navigating the frontier of GenAI for cybersecurity, with particular attention to how different adaptation mechanisms complement each other in end-to-end systems. We present practical guidance derived from real-world deployments, propose best practices for leveraging retrieval- and model-level adaptation, and highlight open research directions for making GenAI more robust, precise, and auditable in cyber defense.",
    "pdf_url": "https://arxiv.org/pdf/2512.06048v1",
    "github_url": null,
    "published": "2025-12-05T10:16:45+00:00",
    "updated": "2025-12-05T10:16:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.05533v1",
    "title": "From Challenge to Change: Design Principles for AI Transformations",
    "authors": [
      "Tavantzis",
      "Lambiase",
      "Russo"
    ],
    "summary": "The rapid rise of Artificial Intelligence (AI) is reshaping Software Engineering (SE), creating new opportunities while introducing human-centered challenges. Although prior work notes behavioral and other non-technical factors in AI integration, most studies still emphasize technical concerns and offer limited insight into how teams adapt to and trust AI. This paper proposes a Behavioral Software Engineering (BSE)-informed, human-centric framework to support SE organizations during early AI adoption. Using a mixed-methods approach, we built and refined the framework through a literature review of organizational change models and thematic analysis of interview data, producing concrete, actionable steps. The framework comprises nine dimensions: AI Strategy Design, AI Strategy Evaluation, Collaboration, Communication, Governance and Ethics, Leadership, Organizational Culture, Organizational Dynamics, and Up-skilling, each supported by design principles and actions. To gather preliminary practitioner input, we conducted a survey (N=105) and two expert workshops (N=4). Survey results show that Up-skilling (15.2%) and AI Strategy Design (15.1%) received the highest $100-method allocations, underscoring their perceived importance in early AI initiatives. Findings indicate that organizations currently prioritize procedural elements such as strategy design, while human-centered guardrails remain less developed. Workshop feedback reinforced these patterns and emphasized the need to ground the framework in real-world practice. By identifying key behavioral dimensions and offering actionable guidance, this work provides a pragmatic roadmap for navigating the socio-technical complexity of early AI adoption and highlights future research directions for human-centric AI in SE.",
    "pdf_url": "https://arxiv.org/pdf/2512.05533v1",
    "github_url": null,
    "published": "2025-12-05T08:45:14+00:00",
    "updated": "2025-12-05T08:45:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.05509v1",
    "title": "Hertz-Integral-Linewidth Lasers based on Portable Solid-state Microresonators",
    "authors": [
      "Jin",
      "Zhang",
      "Zhang"
    ],
    "summary": "Optical reference resonators serve as a cornerstone in various scientific fields. In recent years, there has been an increasing demand for compact ultrastable reference resonators capable of operating in ambient environments, enabling applications beyond the laboratory, such as navigation, portable optical clocks, and remote sensing. Here, we present a compact ultrastable whispering-gallery-mode \\ce{MgF2} reference resonator with a high loaded quality factor of $2.24\\times 10^9$. The device is packaged in a compact form of 50$\\times$77$\\times$90 mm and supports stable optical coupling with polarization-maintaining fiber, which enables robust operation under ambient conditions. Laser stabilization using this resonator yields a phase noise of -105 dBc/Hz at a 10 kHz offset frequency, an integral linewidth of 4 Hz, and a fractional frequency stability of $2.5\\times 10^{-14}$ at a 10 ms averaging time. With the high performance and rapid manufacturability, our work offers a promising solution for ultrastable optical frequency references beyond laboratory settings.",
    "pdf_url": "https://arxiv.org/pdf/2512.05509v1",
    "github_url": null,
    "published": "2025-12-05T08:11:39+00:00",
    "updated": "2025-12-05T08:11:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.11859v1",
    "title": "Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion",
    "authors": [
      "Chertkov"
    ],
    "summary": "We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.   We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.",
    "pdf_url": "https://arxiv.org/pdf/2512.11859v1",
    "github_url": null,
    "published": "2025-12-05T05:18:15+00:00",
    "updated": "2025-12-05T05:18:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.05418v1",
    "title": "Performance Evaluation of Deep Learning for Tree Branch Segmentation in Autonomous Forestry Systems",
    "authors": [
      "Lin",
      "Xue",
      "Zhang"
    ],
    "summary": "UAV-based autonomous forestry operations require rapid and precise tree branch segmentation for safe navigation and automated pruning across varying pixel resolutions and operational conditions. We evaluate different deep learning methods at three resolutions (256x256, 512x512, 1024x1024) using the Urban Street Tree Dataset, employing standard metrics (IoU, Dice) and specialized measures including Thin Structure IoU (TS-IoU) and Connectivity Preservation Rate (CPR). Among 22 configurations tested, U-Net with MiT-B4 backbone achieves strong performance at 256x256. At 512x512, MiT-B4 leads in IoU, Dice, TS-IoU, and Boundary-F1. At 1024x1024, U-Net+MiT-B3 shows the best validation performance for IoU/Dice and precision, while U-Net++ excels in boundary quality. PSPNet provides the most efficient option (2.36/9.43/37.74 GFLOPs) with 25.7/19.6/11.8 percentage point IoU reductions compared to top performers at respective resolutions. These results establish multi-resolution benchmarks for accuracy-efficiency trade-offs in embedded forestry systems. Implementation is available at https://github.com/BennyLinntu/PerformanceTreeBranchSegmentation.",
    "pdf_url": "https://arxiv.org/pdf/2512.05418v1",
    "github_url": "https://github.com/BennyLinntu/PerformanceTreeBranchSegmentation",
    "published": "2025-12-05T04:31:58+00:00",
    "updated": "2025-12-05T04:31:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.05211v1",
    "title": "Wake Vectoring for Efficient Morphing Flight",
    "authors": [
      "Mandralis",
      "Schumacher",
      "Gharib"
    ],
    "summary": "Morphing aerial robots have the potential to transform autonomous flight, enabling navigation through cluttered environments, perching, and seamless transitions between aerial and terrestrial locomotion. Yet mid-flight reconfiguration presents a critical aerodynamic challenge: tilting propulsors to achieve shape change reduces vertical thrust, undermining stability and control authority. Here, we introduce a passive wake vectoring mechanism that recovers lost thrust during morphing. Integrated into a novel robotic system, Aerially Transforming Morphobot (ATMO), internal deflectors intercept and redirect rotor wake downward, passively steering airflow momentum that would otherwise be wasted. This electronics-free solution achieves up to a 40% recovery of vertical thrust in configurations where no useful thrust would otherwise be produced, substantially extending hover and maneuvering capabilities during transformation. Our findings highlight a new direction for morphing aerial robot design, where passive aerodynamic structures, inspired by thrust vectoring in rockets and aircraft, enable efficient, agile flight without added mechanical complexity.",
    "pdf_url": "https://arxiv.org/pdf/2512.05211v1",
    "github_url": null,
    "published": "2025-12-04T19:31:17+00:00",
    "updated": "2025-12-04T19:31:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10982v1",
    "title": "Rosetta Stone of Neural Mass Models",
    "authors": [
      "Castaldo",
      "Aristides",
      "Clusella"
    ],
    "summary": "Brain dynamics dominate every level of neural organization -- from single-neuron spiking to the macroscopic waves captured by fMRI, MEG, and EEG -- yet the mathematical tools used to interrogate those dynamics remain scattered across a patchwork of traditions. Neural mass models (NMMs) (aggregate neural models) provide one of the most popular gateways into this landscape, but their sheer variety -- spanning lumped parameter models, firing-rate equations, and multi-layer generators -- demands a unifying framework that situates diverse architectures along a continuum of abstraction and biological detail. Here, we start from the idea that oscillations originate from a simple push-pull interaction between two or more neural populations. We build from the undamped harmonic oscillator and, guided by a simple push-pull motif between excitatory and inhibitory populations, climb a systematic ladder of detail. Each rung is presented first in isolation, next under forcing, and then within a coupled network, reflecting the progression from single-node to whole-brain modeling. By transforming a repertoire of disparate formalisms into a navigable ladder, we hope to turn NMM choice from a subjective act into a principled design decision, helping both theorists and experimentalists translate between scales, modalities, and interventions. In doing so, we offer a \\emph{Rosetta Stone} for brain oscillation models -- one that lets the field speak a common dynamical language while preserving the dialectical richness that fuels discovery.",
    "pdf_url": "https://arxiv.org/pdf/2512.10982v1",
    "github_url": null,
    "published": "2025-12-04T18:37:14+00:00",
    "updated": "2025-12-04T18:37:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.04832v1",
    "title": "Tokenizing Buildings: A Transformer for Layout Synthesis",
    "authors": [
      "Guevara",
      "Rhee",
      "Bidgoli"
    ],
    "summary": "We introduce Small Building Model (SBM), a Transformer-based architecture for layout synthesis in Building Information Modeling (BIM) scenes. We address the question of how to tokenize buildings by unifying heterogeneous feature sets of architectural elements into sequences while preserving compositional structure. Such feature sets are represented as a sparse attribute-feature matrix that captures room properties. We then design a unified embedding module that learns joint representations of categorical and possibly correlated continuous feature groups. Lastly, we train a single Transformer backbone in two modes: an encoder-only pathway that yields high-fidelity room embeddings, and an encoder-decoder pipeline for autoregressive prediction of room entities, referred to as Data-Driven Entity Prediction (DDEP). Experiments across retrieval and generative layout synthesis show that SBM learns compact room embeddings that reliably cluster by type and topology, enabling strong semantic retrieval. In DDEP mode, SBM produces functionally sound layouts, with fewer collisions and boundary violations and improved navigability.",
    "pdf_url": "https://arxiv.org/pdf/2512.04832v1",
    "github_url": null,
    "published": "2025-12-04T14:16:09+00:00",
    "updated": "2025-12-04T14:16:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.04772v1",
    "title": "TEMPO-VINE: A Multi-Temporal Sensor Fusion Dataset for Localization and Mapping in Vineyards",
    "authors": [
      "Martini",
      "Ambrosio",
      "Vilella-Cantos"
    ],
    "summary": "In recent years, precision agriculture has been introducing groundbreaking innovations in the field, with a strong focus on automation. However, research studies in robotics and autonomous navigation often rely on controlled simulations or isolated field trials. The absence of a realistic common benchmark represents a significant limitation for the diffusion of robust autonomous systems under real complex agricultural conditions. Vineyards pose significant challenges due to their dynamic nature, and they are increasingly drawing attention from both academic and industrial stakeholders interested in automation. In this context, we introduce the TEMPO-VINE dataset, a large-scale multi-temporal dataset specifically designed for evaluating sensor fusion, simultaneous localization and mapping (SLAM), and place recognition techniques within operational vineyard environments. TEMPO-VINE is the first multi-modal public dataset that brings together data from heterogeneous LiDARs of different price levels, AHRS, RTK-GPS, and cameras in real trellis and pergola vineyards, with multiple rows exceeding 100 m in length. In this work, we address a critical gap in the landscape of agricultural datasets by providing researchers with a comprehensive data collection and ground truth trajectories in different seasons, vegetation growth stages, terrain and weather conditions. The sequence paths with multiple runs and revisits will foster the development of sensor fusion, localization, mapping and place recognition solutions for agricultural fields. The dataset, the processing tools and the benchmarking results will be available at the dedicated webpage upon acceptance.",
    "pdf_url": "https://arxiv.org/pdf/2512.04772v1",
    "github_url": null,
    "published": "2025-12-04T13:20:41+00:00",
    "updated": "2025-12-04T13:20:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08977v1",
    "title": "Hacia una moderna \"republica de las ideas\" via un nuevo ecosistema de comunicacion cientifica (Toward a modern \"Republic of Ideas\" via a new Ecosystem of Scientific Communication)",
    "authors": [
      "Lopez-Gonzalez"
    ],
    "summary": "The contemporary academic ecosystem, heir to the Enlightenment's \"Republic of Letters,\" finds itself in a state of profound and unsustainable crisis. That order, based on the free flow of correspondence and the disinterested pursuit of knowledge, has been supplanted by a system teetering under the weight of its own contradictions. This work embarks on a fundamental redesign to articulate an innovative and coherent framework for scientific communication. To this end, four distinct but complementary schools of thought are synthesized: From Ordoliberalism, we take the rigor of designing an \"economic constitution\" that prevents the concentration of power and fosters fair competition. From Humanistic Economics, we extract the telos, or normative purpose (human flourishing and shared prosperity). From Digital Humanism, we derive the technological ethos, ensuring that the infrastructure serves human dignity. Finally, from Decentralized Science (DeSci), we take the set of architectural tools (smart contracts, DAOs, tokens) capable of building this new order from the ground up. The narrative arc is deliberate: Part 1 offers an anatomy of decay, using the Uddin demand as a scalpel to dissect the economic, institutional, and epistemic pathologies of the current system. Part 2 articulates the philosophical constitution of the new republic. Part 3 details the architectural blueprint and technological infrastructure of this new order. Part 4 subjects this design to rigorous stress tests to forge its resilience and antifragility. Part 5 charts a strategic roadmap for the transition, a plausible path to navigate from the status quo to full community sovereignty. Finally, Part 6 brings the book full circle, returning to the foundational vision. This is a call to action to build the New Republic of Ideas that the pursuit of truth deserves in the 21st century.",
    "pdf_url": "https://arxiv.org/pdf/2512.08977v1",
    "github_url": null,
    "published": "2025-12-04T11:15:53+00:00",
    "updated": "2025-12-04T11:15:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.04597v1",
    "title": "When Robots Should Say \"I Don't Know\": Benchmarking Abstention in Embodied Question Answering",
    "authors": [
      "Wu",
      "Zhou",
      "Zhao"
    ],
    "summary": "Embodied Question Answering (EQA) requires an agent to interpret language, perceive its environment, and navigate within 3D scenes to produce responses. Existing EQA benchmarks assume that every question must be answered, but embodied agents should know when they do not have sufficient information to answer. In this work, we focus on a minimal requirement for EQA agents, abstention: knowing when to withhold an answer. From an initial study of 500 human queries, we find that 32.4% contain missing or underspecified context. Drawing on this initial study and cognitive theories of human communication errors, we derive five representative categories requiring abstention: actionability limitation, referential underspecification, preference dependence, information unavailability, and false presupposition. We augment OpenEQA by having annotators transform well-posed questions into ambiguous variants outlined by these categories. The resulting dataset, AbstainEQA, comprises 1,636 annotated abstention cases paired with 1,636 original OpenEQA instances for balanced evaluation. Evaluating on AbstainEQA, we find that even the best frontier model only attains 42.79% abstention recall, while humans achieve 91.17%. We also find that scaling, prompting, and reasoning only yield marginal gains, and that fine-tuned models overfit to textual cues. Together, these results position abstention as a fundamental prerequisite for reliable interaction in embodied settings and as a necessary basis for effective clarification.",
    "pdf_url": "https://arxiv.org/pdf/2512.04597v1",
    "github_url": null,
    "published": "2025-12-04T09:17:40+00:00",
    "updated": "2025-12-04T09:17:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.04404v1",
    "title": "Bridging Probabilistic Inference and Behavior Trees: An Interactive Framework for Adaptive Multi-Robot Cooperation",
    "authors": [
      "Wang",
      "Sun",
      "Zhang"
    ],
    "summary": "This paper proposes an Interactive Inference Behavior Tree (IIBT) framework that integrates behavior trees (BTs) with active inference under the free energy principle for distributed multi-robot decision-making. The proposed IIBT node extends conventional BTs with probabilistic reasoning, enabling online joint planning and execution across multiple robots. It remains fully compatible with standard BT architectures, allowing seamless integration into existing multi-robot control systems. Within this framework, multi-robot cooperation is formulated as a free-energy minimization process, where each robot dynamically updates its preference matrix based on perceptual inputs and peer intentions, thereby achieving adaptive coordination in partially observable and dynamic environments. The proposed approach is validated through both simulation and real-world experiments, including a multi-robot maze navigation and a collaborative manipulation task, compared against traditional BTs(https://youtu.be/KX_oT3IDTf4). Experimental results demonstrate that the IIBT framework reduces BT node complexity by over 70%, while maintaining robust, interpretable, and adaptive cooperative behavior under environmental uncertainty.",
    "pdf_url": "https://arxiv.org/pdf/2512.04404v1",
    "github_url": null,
    "published": "2025-12-04T03:08:55+00:00",
    "updated": "2025-12-04T03:08:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.04382v1",
    "title": "Toward Enhanced Inertial Sensing via Dynamically Soft Topological States in Piezoelectric Microacoustic Metamaterials",
    "authors": [
      "Kaya",
      "Pantuso",
      "Galli"
    ],
    "summary": "In recent decades, microelectromechanical systems (MEMS)-based gyroscopes have been employed to meet positioning and navigation demands of a plethora of commercially available devices. Most of such gyroscopes rely on electrostatic actuators with nanometer-scale air gaps$\\unicode{x2013}$an architecture that enables large particle velocities in a proof mass and, consequently, high Coriolis-force sensitivity to angular velocity$\\unicode{x2013}$but is inherently susceptible to damage under shock and vibration. This vulnerability is typically mitigated by purposely reducing gyroscopic sensitivity, thereby compromising readout accuracy. Microacoustic gyroscopes, by contrast, offer greater resilience to shock and vibration but currently exhibit significantly lower sensitivities. This limitation stems from the low dynamic compliance of the modes they employ$\\unicode{x2013}$typically Lamb or Rayleigh modes$\\unicode{x2013}$which restricts their maximum achievable particle velocity. This work presents a piezoelectric microacoustic device that overcomes this fundamental constraint by harnessing a topological interface state at the boundary between two microscale metamaterial structures. We theoretically and experimentally show that this state exhibits much higher modal compliance than Lamb or Rayleigh modes. This enables record-high particle velocities (>51 m/s) never reached, due to material limits, by any previously demonstrated piezoelectric gyroscope.",
    "pdf_url": "https://arxiv.org/pdf/2512.04382v1",
    "github_url": null,
    "published": "2025-12-04T02:12:45+00:00",
    "updated": "2025-12-04T02:12:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.04381v1",
    "title": "FALCON: Actively Decoupled Visuomotor Policies for Loco-Manipulation with Foundation-Model-Based Coordination",
    "authors": [
      "He",
      "Sun",
      "Bai"
    ],
    "summary": "We present FoundAtion-model-guided decoupled LoCO-maNipulation visuomotor policies (FALCON), a framework for loco-manipulation that combines modular diffusion policies with a vision-language foundation model as the coordinator. Our approach explicitly decouples locomotion and manipulation into two specialized visuomotor policies, allowing each subsystem to rely on its own observations. This mitigates the performance degradation that arise when a single policy is forced to fuse heterogeneous, potentially mismatched observations from locomotion and manipulation. Our key innovation lies in restoring coordination between these two independent policies through a vision-language foundation model, which encodes global observations and language instructions into a shared latent embedding conditioning both diffusion policies. On top of this backbone, we introduce a phase-progress head that uses textual descriptions of task stages to infer discrete phase and continuous progress estimates without manual phase labels. To further structure the latent space, we incorporate a coordination-aware contrastive loss that explicitly encodes cross-subsystem compatibility between arm and base actions. We evaluate FALCON on two challenging loco-manipulation tasks requiring navigation, precise end-effector placement, and tight base-arm coordination. Results show that it surpasses centralized and decentralized baselines while exhibiting improved robustness and generalization to out-of-distribution scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2512.04381v1",
    "github_url": null,
    "published": "2025-12-04T02:04:26+00:00",
    "updated": "2025-12-04T02:04:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.04232v1",
    "title": "Decentralized Social Media and Artificial Intelligence in Digital Public Health Monitoring",
    "authors": [
      "Salathé",
      "Mohanty"
    ],
    "summary": "Digital public health monitoring has long relied on data from major social media platforms. Twitter was once an indispensable resource for tracking disease outbreaks and public sentiment in real time. Researchers used Twitter to monitor everything from influenza spread to vaccine hesitancy, demonstrating that social media data can serve as an early-warning system for emerging health threats. However, recent shifts in the social media landscape have challenged this data-driven paradigm. Platform policy changes, exemplified by Twitter's withdrawal of free data access, now restrict the very data that fueled a decade of digital public health research. At the same time, advances in artificial intelligence, particularly large language models (LLMs), have dramatically expanded our capacity to analyze large-scale textual data across languages and contexts. This presents a paradox: we possess powerful new AI tools to extract insights from social media, but face dwindling access to the data. In this viewpoint, we examine how digital public health monitoring is navigating these countervailing trends. We discuss the rise of decentralized social networks like Mastodon and Bluesky as alternative data sources, weighing their openness and ethical alignment with research against their smaller scale and potential biases. Ultimately, we argue that digital public health surveillance must adapt by embracing new platforms and methodologies, focusing on common diseases and broad signals that remain detectable, while advocating for policies that preserve researchers' access to public data in privacy-respective ways.",
    "pdf_url": "https://arxiv.org/pdf/2512.04232v1",
    "github_url": null,
    "published": "2025-12-03T19:54:59+00:00",
    "updated": "2025-12-03T19:54:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.04036v1",
    "title": "The Loss Landscape of Powder X-Ray Diffraction-Based Structure Optimization Is Too Rough for Gradient Descent",
    "authors": [
      "Segal",
      "Subramanian",
      "Li"
    ],
    "summary": "Solving crystal structures from powder X-ray diffraction (XRD) is a central challenge in materials characterization. In this work, we study the powder XRD-to-structure mapping using gradient descent optimization, with the goal of recovering the correct structure from moderately distorted initial states based solely on XRD similarity. We show that commonly used XRD similarity metrics result in a highly non-convex landscape, complicating direct optimization. Constraining the optimization to the ground-truth crystal family significantly improves recovery, yielding higher match rates and increased mutual information and correlation scores between structural similarity and XRD similarity. Nevertheless, the landscape may remain non-convex along certain symmetry axes. These findings suggest that symmetry-aware inductive biases could play a meaningful role in helping learning models navigate the inverse mapping from diffraction to structure.",
    "pdf_url": "https://arxiv.org/pdf/2512.04036v1",
    "github_url": null,
    "published": "2025-12-03T18:21:50+00:00",
    "updated": "2025-12-03T18:21:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03958v2",
    "title": "MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation",
    "authors": [
      "Zhao",
      "Lyu",
      "Chen"
    ],
    "summary": "Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extended Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction. Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception. To bridge this gap, we present the method of Agricultural Vision-and-Language Navigation with Monocular Depth Estimation (MDE-AgriVLN), in which we propose the MDE module generating depth features from RGB images, to assist the decision-maker on multimodal reasoning. When evaluated on the A2A benchmark, our MDE-AgriVLN method successfully increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, demonstrating the state-of-the-art performance in the agricultural VLN domain. Code: https://github.com/AlexTraveling/MDE-AgriVLN.",
    "pdf_url": "https://arxiv.org/pdf/2512.03958v2",
    "github_url": "https://github.com/AlexTraveling/MDE-AgriVLN",
    "published": "2025-12-03T16:52:07+00:00",
    "updated": "2025-12-15T05:12:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03886v1",
    "title": "A Modular Architecture Design for Autonomous Driving Racing in Controlled Environments",
    "authors": [
      "Fontan-Costas",
      "Diaz-Cacho",
      "Fernandez-Boullon"
    ],
    "summary": "This paper presents an Autonomous System (AS) architecture for vehicles in a closed circuit. The AS performs precision tasks including computer vision for environment perception, positioning and mapping for accurate localization, path planning for optimal trajectory generation, and control for precise vehicle actuation. Each subsystem operates independently while connecting data through a cohesive pipeline architecture. The system implements a modular design that combines state-of-the-art technologies for real-time autonomous navigation in controlled environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.03886v1",
    "github_url": null,
    "published": "2025-12-03T15:36:46+00:00",
    "updated": "2025-12-03T15:36:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03756v1",
    "title": "Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models",
    "authors": [
      "Steiner",
      "Wagner",
      "Tas"
    ],
    "summary": "Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle's intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.",
    "pdf_url": "https://arxiv.org/pdf/2512.03756v1",
    "github_url": "https://github.com/KIT-MRT/future-motion",
    "published": "2025-12-03T12:57:03+00:00",
    "updated": "2025-12-03T12:57:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03755v1",
    "title": "Origin-Conditional Trajectory Encoding: Measuring Urban Configurational Asymmetries through Neural Decomposition",
    "authors": [
      "Law",
      "Yang",
      "Chen"
    ],
    "summary": "Urban analytics increasingly relies on AI-driven trajectory analysis, yet current approaches suffer from methodological fragmentation: trajectory learning captures movement patterns but ignores spatial context, while spatial embedding methods encode street networks but miss temporal dynamics. Three gaps persist: (1) lack of joint training that integrates spatial and temporal representations, (2) origin-agnostic treatment that ignores directional asymmetries in navigation ($A \\to B \\ne B \\to A$), and (3) over-reliance on auxiliary data (POIs, imagery) rather than fundamental geometric properties of urban space. We introduce a conditional trajectory encoder that jointly learns spatial and movement representations while preserving origin-dependent asymmetries using geometric features. This framework decomposes urban navigation into shared cognitive patterns and origin-specific spatial narratives, enabling quantitative measurement of cognitive asymmetries across starting locations. Our bidirectional LSTM processes visibility ratio and curvature features conditioned on learnable origin embeddings, decomposing representations into shared urban patterns and origin-specific signatures through contrastive learning. Results from six synthetic cities and real-world validation on Beijing's Xicheng District demonstrate that urban morphology creates systematic cognitive inequalities. This provides urban planners quantitative tools for assessing experiential equity, offers architects insights into layout decisions' cognitive impacts, and enables origin-aware analytics for navigation systems.",
    "pdf_url": "https://arxiv.org/pdf/2512.03755v1",
    "github_url": null,
    "published": "2025-12-03T12:54:16+00:00",
    "updated": "2025-12-03T12:54:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03736v1",
    "title": "Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control",
    "authors": [
      "Stewart",
      "Chapin",
      "Leontie"
    ],
    "summary": "Reinforcement learning (RL) offers transformative potential for robotic control in space. We present the first on-orbit demonstration of RL-based autonomous control of a free-flying robot, the NASA Astrobee, aboard the International Space Station (ISS). Using NVIDIA's Omniverse physics simulator and curriculum learning, we trained a deep neural network to replace Astrobee's standard attitude and translation control, enabling it to navigate in microgravity. Our results validate a novel training pipeline that bridges the simulation-to-reality (Sim2Real) gap, utilizing a GPU-accelerated, scientific-grade simulation environment for efficient Monte Carlo RL training. This successful deployment demonstrates the feasibility of training RL policies terrestrially and transferring them to space-based applications. This paves the way for future work in In-Space Servicing, Assembly, and Manufacturing (ISAM), enabling rapid on-orbit adaptation to dynamic mission requirements.",
    "pdf_url": "https://arxiv.org/pdf/2512.03736v1",
    "github_url": null,
    "published": "2025-12-03T12:33:35+00:00",
    "updated": "2025-12-03T12:33:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03639v1",
    "title": "Context-Triggered Contingency Games for Strategic Multi-Agent Interaction",
    "authors": [
      "Schweppe",
      "Schmuck"
    ],
    "summary": "We address the challenge of reliable and efficient interaction in autonomous multi-agent systems, where agents must balance long-term strategic objectives with short-term dynamic adaptation. We propose context-triggered contingency games, a novel integration of strategic games derived from temporal logic specifications with dynamic contingency games solved in real time. Our two-layered architecture leverages strategy templates to guarantee satisfaction of high-level objectives, while a new factor-graph-based solver enables scalable, real-time model predictive control of dynamic interactions. The resulting framework ensures both safety and progress in uncertain, interactive environments. We validate our approach through simulations and hardware experiments in autonomous driving and robotic navigation, demonstrating efficient, reliable, and adaptive multi-agent interaction.",
    "pdf_url": "https://arxiv.org/pdf/2512.03639v1",
    "github_url": null,
    "published": "2025-12-03T10:19:36+00:00",
    "updated": "2025-12-03T10:19:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03606v1",
    "title": "Observation-driven correction of numerical weather prediction for marine winds",
    "authors": [
      "Peduto",
      "Yang",
      "Giezendanner"
    ],
    "summary": "Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.",
    "pdf_url": "https://arxiv.org/pdf/2512.03606v1",
    "github_url": null,
    "published": "2025-12-03T09:39:44+00:00",
    "updated": "2025-12-03T09:39:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03568v1",
    "title": "Synthetic Cognitive Walkthrough: Aligning Large Language Model Performance with Human Cognitive Walkthrough",
    "authors": [
      "Zhong",
      "McDonald",
      "Hsieh"
    ],
    "summary": "Conducting usability testing like cognitive walkthrough (CW) can be costly. Recent developments in large language models (LLMs), with visual reasoning and UI navigation capabilities, present opportunities to automate CW. We explored whether LLMs (GPT-4 and Gemini-2.5-pro) can simulate human behavior in CW by comparing their walkthroughs with human participants. While LLMs could navigate interfaces and provide reasonable rationales, their behavior differed from humans. LLM-prompted CW achieved higher task completion rates than humans and followed more optimal navigation paths, while identifying fewer potential failure points. However, follow-up studies demonstrated that with additional prompting, LLMs can predict human-identified failure points, aligning their performance with human participants. Our work highlights that while LLMs may not replicate human behaviors exactly, they can be leveraged for scaling usability walkthroughs and providing UI insights, offering a valuable complement to traditional usability testing.",
    "pdf_url": "https://arxiv.org/pdf/2512.03568v1",
    "github_url": null,
    "published": "2025-12-03T08:45:47+00:00",
    "updated": "2025-12-03T08:45:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03558v1",
    "title": "CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding",
    "authors": [
      "Ung",
      "Habault",
      "Nishimura"
    ],
    "summary": "The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git",
    "pdf_url": "https://arxiv.org/pdf/2512.03558v1",
    "github_url": "https://github.com/ungquanghuy-kddi/CartoMapQA",
    "published": "2025-12-03T08:25:22+00:00",
    "updated": "2025-12-03T08:25:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03541v1",
    "title": "Toward AI-Ready Medical Imaging Data",
    "authors": [
      "Nikolov",
      "Amorim",
      "Caufield"
    ],
    "summary": "Medical imaging data plays a vital role in disease diagnosis, monitoring, and clinical research discovery. Biomedical data managers and clinical researchers must navigate a complex landscape of medical imaging infrastructure, input/output tools and data reliability workflow configurations taking months to operationalize.   While standard formats exist for medical imaging data, standard operating procedures (SOPs) for data management are lacking. These data management SOPs are key for developing Findable, Accessible, Interoperable, and Reusable (FAIR) data, a prerequisite for AI-ready datasets.   The National Institutes of Health (NIH) Bridge to Artificial Intelligence (Bridge2AI) Standards Working Group members and domain-expert stakeholders from the Bridge2AI Grand Challenges teams developed data management SOPs for the Digital Imaging and Communications in Medicine (DICOM) format.   We describe novel SOPs applying to both static and cutting edge video imaging modalities. We emphasize steps required for centralized data aggregation, validation, and de-identification, including a review of new defacing methods for facial DICOM scans, anticipating adversarial AI/ML data re-identification methods. Data management vignettes based on Bridge2AI datasets include example parameters for efficient capture of a wide modality spectrum, including datasets from new ophthalmology retinal scans DICOM modalities.",
    "pdf_url": "https://arxiv.org/pdf/2512.03541v1",
    "github_url": null,
    "published": "2025-12-03T08:02:13+00:00",
    "updated": "2025-12-03T08:02:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03500v1",
    "title": "EEA: Exploration-Exploitation Agent for Long Video Understanding",
    "authors": [
      "Yang",
      "Zhu",
      "Wang"
    ],
    "summary": "Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.",
    "pdf_url": "https://arxiv.org/pdf/2512.03500v1",
    "github_url": null,
    "published": "2025-12-03T06:48:36+00:00",
    "updated": "2025-12-03T06:48:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03429v1",
    "title": "World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations",
    "authors": [
      "Steinmetz",
      "Rosa",
      "Kich"
    ],
    "summary": "Autonomous navigation of terrestrial robots using Reinforcement Learning (RL) from LIDAR observations remains challenging due to the high dimensionality of sensor data and the sample inefficiency of model-free approaches. Conventional policy networks struggle to process full-resolution LIDAR inputs, forcing prior works to rely on simplified observations that reduce spatial awareness and navigation robustness. This paper presents a novel model-based RL framework built on top of the DreamerV3 algorithm, integrating a Multi-Layer Perceptron Variational Autoencoder (MLP-VAE) within a world model to encode high-dimensional LIDAR readings into compact latent representations. These latent features, combined with a learned dynamics predictor, enable efficient imagination-based policy optimization. Experiments on simulated TurtleBot3 navigation tasks demonstrate that the proposed architecture achieves faster convergence and higher success rate compared to model-free baselines such as SAC, DDPG, and TD3. It is worth emphasizing that the DreamerV3-based agent attains a 100% success rate across all evaluated environments when using the full dataset of the Turtlebot3 LIDAR (360 readings), while model-free methods plateaued below 85%. These findings demonstrate that integrating predictive world models with learned latent representations enables more efficient and robust navigation from high-dimensional sensory data.",
    "pdf_url": "https://arxiv.org/pdf/2512.03429v1",
    "github_url": null,
    "published": "2025-12-03T04:15:31+00:00",
    "updated": "2025-12-03T04:15:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03422v1",
    "title": "What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models",
    "authors": [
      "Deng",
      "Pan",
      "Yuan"
    ],
    "summary": "In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.",
    "pdf_url": "https://arxiv.org/pdf/2512.03422v1",
    "github_url": null,
    "published": "2025-12-03T03:57:01+00:00",
    "updated": "2025-12-03T03:57:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03389v1",
    "title": "Continuous Prompts: LLM-Augmented Pipeline Processing over Unstructured Streams",
    "authors": [
      "Chen",
      "Raghavan",
      "Çetintemel"
    ],
    "summary": "Monitoring unstructured streams increasingly requires persistent, semantics-aware computation, yet today's LLM frameworks remain stateless and one-shot, limiting their usefulness for long-running analytics. We introduce Continuous Prompts (CPs), the first framework that brings LLM reasoning into continuous stream processing. CPs extend RAG to streaming settings, define continuous semantic operators, and provide multiple implementations, primarily focusing on LLM-based approaches but also reporting one embedding-based variants. Furthermore, we study two LLM-centric optimizations, tuple batching and operator fusion, to significantly improve efficiency while managing accuracy loss.   Because these optimizations inherently trade accuracy for speed, we present a dynamic optimization framework that uses lightweight shadow executions and cost-aware multi-objective Bayesian optimization (MOBO) to learn throughput-accuracy frontiers and adapt plans under probing budgets.   We implement CPs in the VectraFlow stream processing system. Using operator-level microbenchmarks and streaming pipelines on real datasets, we show that VectraFlow can adapt to workload dynamics, navigate accuracy-efficiency trade-offs, and sustain persistent semantic queries over evolving unstructured streams.",
    "pdf_url": "https://arxiv.org/pdf/2512.03389v1",
    "github_url": null,
    "published": "2025-12-03T02:41:45+00:00",
    "updated": "2025-12-03T02:41:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.10976v1",
    "title": "The Homological Brain: Parity Principle and Amortized Inference",
    "authors": [
      "Li"
    ],
    "summary": "Biological intelligence emerges from substrates that are slow, noisy, and energetically constrained, yet it performs rapid and coherent inference in open-ended environments. Classical computational theories, built around vector-space transformations and instantaneous error minimization, struggle to reconcile the slow timescale of synaptic plasticity with the fast timescale of perceptual synthesis. We propose a unifying framework based on algebraic topology, the Homological Brain, in which neural computation is understood as the construction and navigation of topological structure. Central to this view is the Parity Principle, a homological partition between even-dimensional scaffolds encoding stable content ($Φ$) and odd-dimensional flows encoding dynamic context ($Ψ$). Transient contextual flows are resolved through a three-stage topological trinity transformation: Search (open-chain exploration), Closure (topological cycle formation), and Condensation (collapse of validated flows into new scaffold). This process converts high-complexity recursive search (formally modeled by Savitch's Theorem in NPSPACE) into low-complexity navigation over a learned manifold (analogous to memoized Dynamic Programming in P). In this framework, topological condensation is the mechanism that transforms a ``search problem'' into a ``navigation task'', allowing the brain to amortize past inference and achieve rapid perceptual integration. This perspective unifies the Wake-Sleep cycle, episodic-to-semantic consolidation, and dual-process theories (System 1-vs-System 2), revealing the brain as a homology engine that minimizes topological complexity to transmute high-entropy sensory flux into low-entropy, invariant cognitive structure.",
    "pdf_url": "https://arxiv.org/pdf/2512.10976v1",
    "github_url": null,
    "published": "2025-12-03T00:51:25+00:00",
    "updated": "2025-12-03T00:51:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03317v1",
    "title": "NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction",
    "authors": [
      "Monninger",
      "Zhang",
      "Staab"
    ],
    "summary": "Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion",
    "pdf_url": "https://arxiv.org/pdf/2512.03317v1",
    "github_url": "https://github.com/tmonnin/navmapfusion",
    "published": "2025-12-03T00:10:47+00:00",
    "updated": "2025-12-03T00:10:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03293v1",
    "title": "Prior preferences in active inference agents: soft, hard, and goal shaping",
    "authors": [
      "Torresan",
      "Kanai",
      "Baltieri"
    ],
    "summary": "Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).",
    "pdf_url": "https://arxiv.org/pdf/2512.03293v1",
    "github_url": null,
    "published": "2025-12-02T23:07:24+00:00",
    "updated": "2025-12-02T23:07:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03204v1",
    "title": "Scaling Internal-State Policy-Gradient Methods for POMDPs",
    "authors": [
      "Aberdeen",
      "Baxter"
    ],
    "summary": "Policy-gradient methods have received increased attention recently as a mechanism for learning to act in partially observable environments. They have shown promise for problems admitting memoryless policies but have been less successful when memory is required. In this paper we develop several improved algorithms for learning policies with memory in an infinite-horizon setting -- directly when a known model of the environment is available, and via simulation otherwise. We compare these algorithms on some large POMDPs, including noisy robot navigation and multi-agent problems.",
    "pdf_url": "https://arxiv.org/pdf/2512.03204v1",
    "github_url": null,
    "published": "2025-12-02T20:03:35+00:00",
    "updated": "2025-12-02T20:03:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.03040v2",
    "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
    "authors": [
      "Xiao",
      "Zhao",
      "Li"
    ],
    "summary": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
    "pdf_url": "https://arxiv.org/pdf/2512.03040v2",
    "github_url": null,
    "published": "2025-12-02T18:59:44+00:00",
    "updated": "2025-12-11T08:18:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02994v1",
    "title": "GNSS Array-Based Multipath Detection Employing UKF on Manifolds",
    "authors": [
      "Ahmed",
      "Ballal",
      "Liu"
    ],
    "summary": "Global Navigation Satellite Systems (GNSS) applications are often hindered by various sources of error, with multipath interference being one of the most challenging, particularly in urban environments. In this work, we build on previous research by implementing a GNSS array-based multipath detection algorithm, incorporating real-time attitude estimation for dynamic scenarios. The method fuses GNSS and IMU data using an Unscented Kalman Filter (UKF) on a manifold, enabling continuous attitude tracking. The proposed approach utilizes attitude information from satellite combinations to identify and exclude multipath-affected satellites, improving the accuracy of both positioning and attitude determination. To address computational challenges associated with evaluating large numbers of satellite combinations, we propose the use of the Random Sample Consensus (RANSAC) algorithm, which reduces the number of combinations assessed while maintaining high detection performance. Performance evaluations are conducted using trajectories and IMU readings from the KITTI dataset. GNSS observations are simulated based on ground truth positions and satellite ephemeris. The results demonstrate the effectiveness of the proposed approach in detecting satellites affected by multipath interference. Significant improvements in positioning accuracy are observed, particularly in scenarios where a large portion of the visible satellites are contaminated by severe multipath.",
    "pdf_url": "https://arxiv.org/pdf/2512.02994v1",
    "github_url": null,
    "published": "2025-12-02T18:18:24+00:00",
    "updated": "2025-12-02T18:18:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02875v1",
    "title": "SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures",
    "authors": [
      "Tirelli",
      "Ferretti",
      "Pozzi"
    ],
    "summary": "Coarse-Grain Reconfigurable Arrays (CGRAs) are emerging low-power architectures aimed at accelerating compute-intensive application loops. The acceleration that a CGRA can ultimately provide, however, heavily depends on the quality of the mapping, i.e. on how effectively the loop is compiled onto the given platform. State of the Art compilation techniques achieve mapping through modulo scheduling, a strategy which attempts to minimize the II (Iteration Interval) needed to execute a loop, and they do so usually through well known graph algorithms, such as Max-Clique Enumeration.   We address the mapping problem through a SAT formulation, instead, and thus explore the solution space more effectively than current SoA tools. To formulate the SAT problem, we introduce an ad-hoc schedule called the \\textit{kernel mobility schedule} (KMS), which we use in conjunction with the data-flow graph and the architectural information of the CGRA in order to create a set of boolean statements that describe all constraints to be obeyed by the mapping for a given II. We then let the SAT solver efficiently navigate this complex space. As in other SoA techniques, the process is iterative: if a valid mapping does not exist for the given II, the II is increased and a new KMS and set of constraints is generated and solved.   Our experimental results show that SAT-MapIt obtains better results compared to SoA alternatives in $47.72\\%$ of the benchmarks explored: sometimes finding a lower II, and others even finding a valid mapping when none could previously be found.",
    "pdf_url": "https://arxiv.org/pdf/2512.02875v1",
    "github_url": null,
    "published": "2025-12-02T15:36:19+00:00",
    "updated": "2025-12-02T15:36:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02851v3",
    "title": "SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots",
    "authors": [
      "Zhura",
      "Karaf",
      "Batool"
    ],
    "summary": "Visual traversability estimation is critical for autonomous navigation, but existing VLM-based methods rely on hand-crafted prompts, generalize poorly across embodiments, and output only traversability maps, leaving trajectory generation to slow external planners. We propose SwarmDiffusion, a lightweight end-to-end diffusion model that jointly predicts traversability and generates a feasible trajectory from a single RGB image. To remove the need for annotated or planner-produced paths, we introduce a planner-free trajectory construction pipeline based on randomized waypoint sampling, Bezier smoothing, and regularization enforcing connectivity, safety, directionality, and path thinness. This enables learning stable motion priors without demonstrations. SwarmDiffusion leverages VLM-derived supervision without prompt engineering and conditions the diffusion process on a compact embodiment state, producing physically consistent, traversable paths that transfer across different robot platforms. Across indoor environments and two embodiments (quadruped and aerial), the method achieves 80-100% navigation success and 0.09s inference, and adapts to a new robot using only-500 additional visual samples. It generalizes reliably to unseen environments in simulation and real-world trials, offering a scalable, prompt-free approach to unified traversability reasoning and trajectory generation.",
    "pdf_url": "https://arxiv.org/pdf/2512.02851v3",
    "github_url": null,
    "published": "2025-12-02T15:09:19+00:00",
    "updated": "2025-12-08T14:25:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02826v2",
    "title": "From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity",
    "authors": [
      "Liu",
      "Liu",
      "Li"
    ],
    "summary": "Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.",
    "pdf_url": "https://arxiv.org/pdf/2512.02826v2",
    "github_url": null,
    "published": "2025-12-02T14:34:10+00:00",
    "updated": "2025-12-17T08:02:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02774v1",
    "title": "AI-Driven Document Redaction in UK Public Authorities: Implementation Gaps, Regulatory Challenges, and the Human Oversight Imperative",
    "authors": [
      "Chen"
    ],
    "summary": "Document redaction in public authorities faces critical challenges as traditional manual approaches struggle to balance growing transparency demands with increasingly stringent data protection requirements. This study investigates the implementation of AI-driven document redaction within UK public authorities through Freedom of Information (FOI) requests. While AI technologies offer potential solutions to redaction challenges, their actual implementation within public sector organizations remains underexplored. Based on responses from 44 public authorities across healthcare, government, and higher education sectors, this study reveals significant gaps between technological possibilities and organizational realities. Findings show highly limited AI adoption (only one authority reported using AI tools), widespread absence of formal redaction policies (50 percent reported \"information not held\"), and deficiencies in staff training. The study identifies three key barriers to effective AI implementation: poor record-keeping practices, lack of standardized redaction guidelines, and insufficient specialized training for human oversight. These findings highlight the need for a socio-technical approach that balances technological automation with meaningful human expertise. This research provides the first empirical assessment of AI redaction practices in UK public authorities and contributes evidence to support policymakers navigating the complex interplay between transparency obligations, data protection requirements, and emerging AI technologies in public administration.",
    "pdf_url": "https://arxiv.org/pdf/2512.02774v1",
    "github_url": null,
    "published": "2025-12-02T13:52:10+00:00",
    "updated": "2025-12-02T13:52:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02631v1",
    "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization",
    "authors": [
      "Wang",
      "Lin",
      "Yang"
    ],
    "summary": "Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.",
    "pdf_url": "https://arxiv.org/pdf/2512.02631v1",
    "github_url": null,
    "published": "2025-12-02T10:40:46+00:00",
    "updated": "2025-12-02T10:40:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02544v1",
    "title": "A Human-centric Framework for Debating the Ethics of AI Consciousness Under Uncertainty",
    "authors": [
      "Ziheng",
      "Dai",
      "Ling"
    ],
    "summary": "As AI systems become increasingly sophisticated, questions about machine consciousness and its ethical implications have moved from fringe speculation to mainstream academic debate. Current ethical frameworks in this domain often implicitly rely on contested functionalist assumptions, prioritize speculative AI welfare over concrete human interests, and lack coherent theoretical foundations. We address these limitations through a structured three-level framework grounded in philosophical uncertainty. At the foundational level, we establish five factual determinations about AI consciousness alongside human-centralism as our meta-ethical stance. These foundations logically entail three operational principles: presumption of no consciousness (placing the burden of proof on consciousness claims), risk prudence (prioritizing human welfare under uncertainty), and transparent reasoning (enabling systematic evaluation and adaptation). At the application level, the third component of our framework, we derive default positions on pressing ethical questions through a transparent logical process where each position can be explicitly traced back to our foundational commitments. Our approach balances philosophical rigor with practical guidance, distinguishes consciousness from anthropomorphism, and creates pathways for responsible evolution as scientific understanding advances, providing a human-centric foundation for navigating these profound ethical challenges.",
    "pdf_url": "https://arxiv.org/pdf/2512.02544v1",
    "github_url": null,
    "published": "2025-12-02T09:15:01+00:00",
    "updated": "2025-12-02T09:15:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02473v1",
    "title": "WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling",
    "authors": [
      "Oshima",
      "Iwasawa",
      "Suzuki"
    ],
    "summary": "Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions. Temporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs. In this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length. Our compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high context efficiency, and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning. Our performance is evaluated with LoopNav, a benchmark on Minecraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outperforms strong state-of-the-art models.",
    "pdf_url": "https://arxiv.org/pdf/2512.02473v1",
    "github_url": null,
    "published": "2025-12-02T07:06:23+00:00",
    "updated": "2025-12-02T07:06:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02458v1",
    "title": "Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration",
    "authors": [
      "Cai",
      "Du",
      "Wang"
    ],
    "summary": "Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.",
    "pdf_url": "https://arxiv.org/pdf/2512.02458v1",
    "github_url": null,
    "published": "2025-12-02T06:35:30+00:00",
    "updated": "2025-12-02T06:35:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02424v1",
    "title": "Optimal Comprehensible Targeting",
    "authors": [
      "Zhang"
    ],
    "summary": "Developments in machine learning and big data allow firms to fully personalize and target their marketing mix. However, data and privacy regulations, such as those in the European Union (GDPR), incorporate a \"right to explanation,\" which is fulfilled when targeting policies are comprehensible to customers. This paper provides a framework for firms to navigate right-to-explanation legislation. First, I construct a class of comprehensible targeting policies that is represented by a sentence. Second, I show how to optimize over this class of policies to find the profit-maximizing comprehensible policy. I further demonstrate that it is optimal to estimate the comprehensible policy directly from the data, rather than projecting down the black box policy into a comprehensible policy. Third, I find the optimal black box targeting policy and compare it to the optimal comprehensible policy. I then empirically apply my framework using data from a price promotion field experiment from a durable goods retailer. I quantify the cost of explanation, which I define as the difference in expected profits between the optimal black box and comprehensible targeting policies. Compared to the black box benchmark, the comprehensible targeting policy reduces profits by 7.5% or 23 cents per customer.",
    "pdf_url": "https://arxiv.org/pdf/2512.02424v1",
    "github_url": null,
    "published": "2025-12-02T05:11:26+00:00",
    "updated": "2025-12-02T05:11:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02423v1",
    "title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning",
    "authors": [
      "Yan",
      "Shen",
      "Huang"
    ],
    "summary": "With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.",
    "pdf_url": "https://arxiv.org/pdf/2512.02423v1",
    "github_url": null,
    "published": "2025-12-02T05:11:23+00:00",
    "updated": "2025-12-02T05:11:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02400v1",
    "title": "Nav-$R^2$ Dual-Relation Reasoning for Generalizable Open-Vocabulary Object-Goal Navigation",
    "authors": [
      "Xiang",
      "Zhang",
      "Yang"
    ],
    "summary": "Object-goal navigation in open-vocabulary settings requires agents to locate novel objects in unseen environments, yet existing approaches suffer from opaque decision-making processes and low success rate on locating unseen objects. To address these challenges, we propose Nav-$R^2$, a framework that explicitly models two critical types of relationships, target-environment modeling and environment-action planning, through structured Chain-of-Thought (CoT) reasoning coupled with a Similarity-Aware Memory. We construct a Nav$R^2$-CoT dataset that teaches the model to perceive the environment, focus on target-related objects in the surrounding context and finally make future action plans. Our SA-Mem preserves the most target-relevant and current observation-relevant features from both temporal and semantic perspectives by compressing video frames and fusing historical observations, while introducing no additional parameters. Compared to previous methods, Nav-R^2 achieves state-of-the-art performance in localizing unseen objects through a streamlined and efficient pipeline, avoiding overfitting to seen object categories while maintaining real-time inference at 2Hz. Resources will be made publicly available at \\href{https://github.com/AMAP-EAI/Nav-R2}{github link}.",
    "pdf_url": "https://arxiv.org/pdf/2512.02400v1",
    "github_url": "https://github.com/AMAP-EAI/Nav-R2",
    "published": "2025-12-02T04:21:02+00:00",
    "updated": "2025-12-02T04:21:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02009v1",
    "title": "AirSim360: A Panoramic Simulation Platform within Drone View",
    "authors": [
      "Ge",
      "Pan",
      "Zhang"
    ],
    "summary": "The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.",
    "pdf_url": "https://arxiv.org/pdf/2512.02009v1",
    "github_url": null,
    "published": "2025-12-01T18:59:30+00:00",
    "updated": "2025-12-01T18:59:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01962v1",
    "title": "A framework for disentangling spatial and visual neural representations",
    "authors": [
      "Morimoto",
      "Fournier",
      "Saleem"
    ],
    "summary": "Neurons in cortical areas often integrate signals from different origins. In the primary visual cortex (V1), neural responses are modulated by non-visual context such as the animal's position. However, the spatial profile of these position signals across the environment remains unknown. Here, we propose a new framework to disentangle visual and spatial contributions in virtual reality. This method relies on two principles: 1) a virtual corridor design that decorrelates vision and space through targeted cue repetitions and manipulations and 2) a Generalized Linear Model (GLM) that explicitly estimates visual contributions in retinotopic rather than environmental coordinates. In simulations, we demonstrate that this framework is highly specific (recovering spatial modulation only when present) and effectively captures the profile and weight of spatial gain fields across the environment. When applied to V1 recordings from mice navigating the virtual corridor, the model isolated significant spatial components in a substantial fraction of V1 neurons. The recovered spatial components exhibited heterogeneous, often multi-peaked, profiles. Application of this framework to large-scale recordings may provide a robust approach to characterize the nature of spatial signals modulating sensory processing across brain areas.",
    "pdf_url": "https://arxiv.org/pdf/2512.01962v1",
    "github_url": null,
    "published": "2025-12-01T18:19:15+00:00",
    "updated": "2025-12-01T18:19:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01952v1",
    "title": "GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment",
    "authors": [
      "He",
      "Patrikar",
      "Kim"
    ],
    "summary": "Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.01952v1",
    "github_url": null,
    "published": "2025-12-01T18:03:29+00:00",
    "updated": "2025-12-01T18:03:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01897v1",
    "title": "NeuroHJR: Hamilton-Jacobi Reachability-based Obstacle Avoidance in Complex Environments with Physics-Informed Neural Networks",
    "authors": [
      "Halder",
      "Majumder",
      "R"
    ],
    "summary": "Autonomous ground vehicles (AGVs) must navigate safely in cluttered environments while accounting for complex dynamics and environmental uncertainty. Hamilton-Jacobi Reachability (HJR) offers formal safety guarantees through the computation of forward and backward reachable sets, but its application is hindered by poor scalability in environments with numerous obstacles. In this paper, we present a novel framework called NeuroHJR that leverages Physics-Informed Neural Networks (PINNs) to approximate the HJR solution for real-time obstacle avoidance. By embedding system dynamics and safety constraints directly into the neural network loss function, our method bypasses the need for grid-based discretization and enables efficient estimation of reachable sets in continuous state spaces. We demonstrate the effectiveness of our approach through simulation results in densely cluttered scenarios, showing that it achieves safety performance comparable to that of classical HJR solvers while significantly reducing the computational cost. This work provides a new step toward real-time, scalable deployment of reachability-based obstacle avoidance in robotics.",
    "pdf_url": "https://arxiv.org/pdf/2512.01897v1",
    "github_url": null,
    "published": "2025-12-01T17:18:24+00:00",
    "updated": "2025-12-01T17:18:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01763v1",
    "title": "HiconAgent: History Context-aware Policy Optimization for GUI Agents",
    "authors": [
      "Zhou",
      "Chen",
      "Xie"
    ],
    "summary": "Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.",
    "pdf_url": "https://arxiv.org/pdf/2512.01763v1",
    "github_url": null,
    "published": "2025-12-01T15:06:45+00:00",
    "updated": "2025-12-01T15:06:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01753v1",
    "title": "AgriLiRa4D: A Multi-Sensor UAV Dataset for Robust SLAM in Challenging Agricultural Fields",
    "authors": [
      "Zhan",
      "Ming",
      "Li"
    ],
    "summary": "Multi-sensor Simultaneous Localization and Mapping (SLAM) is essential for Unmanned Aerial Vehicles (UAVs) performing agricultural tasks such as spraying, surveying, and inspection. However, real-world, multi-modal agricultural UAV datasets that enable research on robust operation remain scarce. To address this gap, we present AgriLiRa4D, a multi-modal UAV dataset designed for challenging outdoor agricultural environments. AgriLiRa4D spans three representative farmland types-flat, hilly, and terraced-and includes both boundary and coverage operation modes, resulting in six flight sequence groups. The dataset provides high-accuracy ground-truth trajectories from a Fiber Optic Inertial Navigation System with Real-Time Kinematic capability (FINS_RTK), along with synchronized measurements from a 3D LiDAR, a 4D Radar, and an Inertial Measurement Unit (IMU), accompanied by complete intrinsic and extrinsic calibrations. Leveraging its comprehensive sensor suite and diverse real-world scenarios, AgriLiRa4D supports diverse SLAM and localization studies and enables rigorous robustness evaluation against low-texture crops, repetitive patterns, dynamic vegetation, and other challenges of real agricultural environments. To further demonstrate its utility, we benchmark four state-of-the-art multi-sensor SLAM algorithms across different sensor combinations, highlighting the difficulty of the proposed sequences and the necessity of multi-modal approaches for reliable UAV localization. By filling a critical gap in agricultural SLAM datasets, AgriLiRa4D provides a valuable benchmark for the research community and contributes to advancing autonomous navigation technologies for agricultural UAVs. The dataset can be downloaded from: https://zhan994.github.io/AgriLiRa4D.",
    "pdf_url": "https://arxiv.org/pdf/2512.01753v1",
    "github_url": null,
    "published": "2025-12-01T14:56:56+00:00",
    "updated": "2025-12-01T14:56:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01668v1",
    "title": "Dynamic Log-Gaussian Process Control Barrier Function for Safe Robotic Navigation in Dynamic Environments",
    "authors": [
      "Yin",
      "Liang",
      "Guo"
    ],
    "summary": "Control Barrier Functions (CBFs) have emerged as efficient tools to address the safe navigation problem for robot applications. However, synthesizing informative and obstacle motion-aware CBFs online using real-time sensor data remains challenging, particularly in unknown and dynamic scenarios. Motived by this challenge, this paper aims to propose a novel Gaussian Process-based formulation of CBF, termed the Dynamic Log Gaussian Process Control Barrier Function (DLGP-CBF), to enable real-time construction of CBF which are both spatially informative and responsive to obstacle motion. Firstly, the DLGP-CBF leverages a logarithmic transformation of GP regression to generate smooth and informative barrier values and gradients, even in sparse-data regions. Secondly, by explicitly modeling the DLGP-CBF as a function of obstacle positions, the derived safety constraint integrates predicted obstacle velocities, allowing the controller to proactively respond to dynamic obstacles' motion. Simulation results demonstrate significant improvements in obstacle avoidance performance, including increased safety margins, smoother trajectories, and enhanced responsiveness compared to baseline methods.",
    "pdf_url": "https://arxiv.org/pdf/2512.01668v1",
    "github_url": null,
    "published": "2025-12-01T13:38:43+00:00",
    "updated": "2025-12-01T13:38:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01637v1",
    "title": "Exploring Scavenging Strategies and Cognitive Problem-Solving in Indian Free-Ranging Dogs",
    "authors": [
      "Pal",
      "Nandi",
      "Gope"
    ],
    "summary": "Animals employ strategic decision-making while carefully weighing nutritional benefits against the risks presented by aversive or harmful stimuli in their natural environment, to maximize foraging efficiency, In India, free-ranging dogs subsist predominantly on human-generated waste, where they often encounter food contaminated with unpalatable or noxious substances such as lemon juice while scavenging. The strategies these dogs use to navigate such challenges remain poorly understood, yet are critical for understanding their ecological adaptability and survival in human-dominated environments. A total of 156 randomly encountered free-ranging adult dogs were tested across 15 sites in Nadia district, West Bengal. Each individual was exposed to a single food source containing chicken placed in either lemon juice, diluted lemon solution, or water. All trials were video-recorded, and the behavioural sequences of the dogs, including sniffing, licking, eating, and food manipulation were coded and analysed to quantify strategic foraging responses under unpalatable conditions. They were found to use a flexible, multi-pronged strategy to manipulate the comparatively less palatable food option, and typically avoid the most unpalatable one, to maximize their acquiring options. Overall, this study revealed a hierarchically structured and context-dependent foraging strategy of free-ranging dogs, propelled by sensory evaluation, risk-reward balancing, and behavioural flexibility. These findings demonstrated how urban scavengers dynamically adapt to aversive conditions while scavenging, underscoring the cognitive mechanisms that support their survival in human-dominated environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.01637v1",
    "github_url": null,
    "published": "2025-12-01T13:06:41+00:00",
    "updated": "2025-12-01T13:06:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01624v2",
    "title": "A Platform for Evanescently Trapping Rb-87 Using Silicon Nitride Strip Waveguides Buried in Silica",
    "authors": [
      "Harding",
      "Weidner"
    ],
    "summary": "Cold-atom systems have emerged as a highly promising avenue for quantum-enhanced position, navigation, and timing applications. However, their wider adoption is currently hampered in part by the large footprint of the systems. In leveraging the miniaturisation possible through photonic integrated circuits, cold-atom sensors would be able to reach much wider commercial adoption. In this paper, we introduce a platform for evanescently trapping 87Rb using strip silicon nitride waveguides buried in silica using red- and blue-detuned fundamental and higher-order modes, providing a three-dimensional adjustable trap for BEC-based, chip-scale work in quantum science and technologies.",
    "pdf_url": "https://arxiv.org/pdf/2512.01624v2",
    "github_url": null,
    "published": "2025-12-01T12:44:54+00:00",
    "updated": "2025-12-04T16:34:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01608v1",
    "title": "Integrated YOLOP Perception and Lyapunov-based Control for Autonomous Mobile Robot Navigation on Track",
    "authors": [
      "Chen"
    ],
    "summary": "This work presents a real-time autonomous track navigation framework for nonholonomic differential-drive mobile robots by jointly integrating multi-task visual perception and a provably stable tracking controller. The perception pipeline reconstructs lane centerlines using 2D-to-3D camera projection, arc-length based uniform point resampling, and cubic polynomial fitting solved via robust QR least-squares optimization. The controller regulates robot linear and angular velocities through a Lyapunov-stability grounded design, ensuring bounded error dynamics and asymptotic convergence of position and heading deviations even in dynamic and partially perceived lane scenarios, without relying on HD prior maps or global satellite localization. Real-world experiments on embedded platforms verify system fidelity, real-time execution, trajectory smoothness, and closed-loop stability for reliable autonomous navigation.",
    "pdf_url": "https://arxiv.org/pdf/2512.01608v1",
    "github_url": null,
    "published": "2025-12-01T12:29:02+00:00",
    "updated": "2025-12-01T12:29:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01550v1",
    "title": "NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction",
    "authors": [
      "Liu",
      "Xie",
      "Luo"
    ],
    "summary": "Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.",
    "pdf_url": "https://arxiv.org/pdf/2512.01550v1",
    "github_url": null,
    "published": "2025-12-01T11:24:16+00:00",
    "updated": "2025-12-01T11:24:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01445v1",
    "title": "Mathematical and numerical study of a model for navigation in stratified waters",
    "authors": [
      "Rammal",
      "Brachet",
      "Rousseaux"
    ],
    "summary": "We derive a linear model of navigation in a two-layer fluid with a variable velocity of the ship. A spectral version of the model including a Rayleigh damping term is analyzed. We prove that the Cauchy problem has a unique solution if the velocity and if the initial data are sufficiently regular. The case of a constant speed is thoroughly investigated and the importance of a critical speed which separates two types of regimes is pointed out. We propose a numerical scheme based on the discrete Fourier transform for the space discretization and on an exponential integrator for the time discretization. We prove an error estimate for the exponential integrator. Numerical experiments in one and two space dimensions complete the theoretical results.",
    "pdf_url": "https://arxiv.org/pdf/2512.01445v1",
    "github_url": null,
    "published": "2025-12-01T09:28:53+00:00",
    "updated": "2025-12-01T09:28:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01438v1",
    "title": "Competition among seaports through Mean Field Games and real-world data",
    "authors": [
      "Lehalle",
      "Livieri"
    ],
    "summary": "This paper presents a Mean Field Game (MFG) model for maritime traffic flow, treating the navigation of ships between seaports as a large-scale stochastic control problem. The MFG framework enables the modeling of agents at a microscopic level as rational decision-makers who seek to optimize their utility, thereby translating complex microscopic behaviors into macroscopic models. We build upon this MFG framework to develop a mesoscopic-scale MFG model that defines the payoff and cost functions for a coordinator at each seaport considered in our study. The coordinator determines the routes taken by ships transporting goods between ports by evaluating several key factors: transportation costs, expected profit margins from loading specific goods at the seaports and unloading them at various destinations, and a congestion term that reflects the costs associated with accessing the destination port. We derive an explicit solution for the stationary version of the model under certain approximations and establish conditions necessary to ensure the uniqueness of the corresponding Mean Field Equilibrium (MFE). Furthermore, we introduce a statistical methodology to infer the parameters of the game from real-world data, specifically focusing on costs and the components of expected commercial margins. To validate our model in a real-world context, we analyze the ShipFix dataset of daily ''Dry Coal'' shipments worldwide from 2015 to 2025. Our discussion highlights the influence of empirical traffic flow on various components of costs. We believe that this research represents a significant advancement in the application of MFGs for effective maritime traffic management and offers valuable insights for practitioners in the field.",
    "pdf_url": "https://arxiv.org/pdf/2512.01438v1",
    "github_url": null,
    "published": "2025-12-01T09:23:09+00:00",
    "updated": "2025-12-01T09:23:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01228v1",
    "title": "On the Tension Between Optimality and Adversarial Robustness in Policy Optimization",
    "authors": [
      "Li",
      "Lv",
      "Han"
    ],
    "summary": "Achieving optimality and adversarial robustness in deep reinforcement learning has long been regarded as conflicting goals. Nonetheless, recent theoretical insights presented in CAR suggest a potential alignment, raising the important question of how to realize this in practice. This paper first identifies a key gap between theory and practice by comparing standard policy optimization (SPO) and adversarially robust policy optimization (ARPO). Although they share theoretical consistency, a fundamental tension between robustness and optimality arises in practical policy gradient methods. SPO tends toward convergence to vulnerable first-order stationary policies (FOSPs) with strong natural performance, whereas ARPO typically favors more robust FOSPs at the expense of reduced returns. Furthermore, we attribute this tradeoff to the reshaping effect of the strongest adversary in ARPO, which significantly complicates the global landscape by inducing deceptive sticky FOSPs. This improves robustness but makes navigation more challenging. To alleviate this, we develop the BARPO, a bilevel framework unifying SPO and ARPO by modulating adversary strength, thereby facilitating navigability while preserving global optima. Extensive empirical results demonstrate that BARPO consistently outperforms vanilla ARPO, providing a practical approach to reconcile theoretical and empirical performance.",
    "pdf_url": "https://arxiv.org/pdf/2512.01228v1",
    "github_url": null,
    "published": "2025-12-01T03:14:17+00:00",
    "updated": "2025-12-01T03:14:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01194v1",
    "title": "RoboLoc: A Benchmark Dataset for Point Place Recognition and Localization in Indoor-Outdoor Integrated Environments",
    "authors": [
      "Jeon",
      "Ryoo",
      "Lee"
    ],
    "summary": "Robust place recognition is essential for reliable localization in robotics, particularly in complex environments with frequent indoor-outdoor transitions. However, existing LiDAR-based datasets often focus on outdoor scenarios and lack seamless domain shifts. In this paper, we propose RoboLoc, a benchmark dataset designed for GPS-free place recognition in indoor-outdoor environments with floor transitions. RoboLoc features real-world robot trajectories, diverse elevation profiles, and transitions between structured indoor and unstructured outdoor domains. We benchmark a variety of state-of-the-art models, point-based, voxel-based, and BEV-based architectures, highlighting their generalizability domain shifts. RoboLoc provides a realistic testbed for developing multi-domain localization systems in robotics and autonomous navigation",
    "pdf_url": "https://arxiv.org/pdf/2512.01194v1",
    "github_url": null,
    "published": "2025-12-01T02:20:14+00:00",
    "updated": "2025-12-01T02:20:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01182v1",
    "title": "Autonomous Navigation and Station-Keeping on Near-Rectilinear Halo Orbits",
    "authors": [
      "Shimane",
      "Berntorp",
      "Cairano"
    ],
    "summary": "This article develops an optical navigation (OPNAV) and station-keeping pipeline for the near-rectilinear halo orbit (NRHO) in high-fidelity ephemeris model dynamics. The pipeline involves synthetic images used by the non-iterative horizon-based OPNAV algorithm, fed into an extended Kalman filter. The state estimate is used by a controller to maintain the spacecraft's motion within the vicinity of a reference NRHO. We study differential correction-based and minimization-based implementations of the x-axis crossing control scheme, and propose an improved targeting prediction scheme by incorporating the filter's state covariance with an unscented transform. We also introduce a hysteresis mechanism, which improves station-keeping cost and provides insight into the difference in performance between the differential correction-based and minimization-based approaches. We perform Monte-Carlo experiments to assess the pipeline's tracking and ΔV performances. We report several key findings, including the variability of the filter performance with the sensor field of view and measurement locations, station-keeping cost reduction achieved by the unscented transform-based prediction and hysteresis, as well as variability of the cumulative ΔV as a function of maneuver location due to the periodic structure in the OPNAV-based filter's estimation accuracy.",
    "pdf_url": "https://arxiv.org/pdf/2512.01182v1",
    "github_url": null,
    "published": "2025-12-01T01:46:33+00:00",
    "updated": "2025-12-01T01:46:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01155v2",
    "title": "Beyond Greenfield: The D3 Framework for AI-Driven Productivity in Brownfield Engineering",
    "authors": [
      "Sharma"
    ],
    "summary": "Brownfield engineering work involving legacy systems, incomplete documentation, and fragmented architectural knowledge poses unique challenges for the effective use of large language models (LLMs). Prior research has largely focused on greenfield or synthetic tasks, leaving a gap in structured workflows for complex, context-heavy environments. This paper introduces the Discover-Define-Deliver (D3) Framework, a disciplined LLM-assisted workflow that combines role-separated prompting strategies with applied best practices for navigating ambiguity in brownfield systems. The framework incorporates a dual-agent prompting architecture in which a Builder model generates candidate outputs and a Reviewer model provides structured critique to improve reliability. I conducted an exploratory survey study with 52 software practitioners who applied the D3 workflow to real-world engineering tasks such as legacy system exploration, documentation reconstruction, and architectural refactoring. Respondents reported perceived improvements in task clarity, documentation quality, and cognitive load, along with self-estimated productivity gains. In this exploratory study, participants reported a weighted average productivity improvement of 26.9%, reduced cognitive load for approximately 77% of participants, and 83% of participants spent less time fixing or rewriting code due to better initial planning with AI. As these findings are self-reported and not derived from controlled experiments, they should be interpreted as preliminary evidence of practitioner sentiment rather than causal effects. The results highlight both the potential and limitations of structured LLM workflows for legacy engineering systems and motivate future controlled evaluations.",
    "pdf_url": "https://arxiv.org/pdf/2512.01155v2",
    "github_url": null,
    "published": "2025-12-01T00:26:41+00:00",
    "updated": "2025-12-02T10:47:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01112v2",
    "title": "Autodeleveraging: Impossibilities and Optimization",
    "authors": [
      "Chitra"
    ],
    "summary": "Autodeleveraging (ADL) is a last-resort loss socialization mechanism for perpetual futures venues. It is triggered when solvency-preserving liquidations fail. Despite the dominance of perpetual futures in the crypto derivatives market, with over \\$60 trillion of volume in 2024, there has been no formal study of ADL. In this paper, we provide the first rigorous model of ADL. We prove that ADL mechanisms face a fundamental \\emph{trilemma}: no policy can simultaneously satisfy exchange \\emph{solvency}, \\emph{revenue}, and \\emph{fairness} to traders. This impossibility theorem implies that as participation scales, a novel form of \\emph{moral hazard} grows asymptotically, rendering `zero-loss' socialization impossible. Constructively, we show that three classes of ADL mechanisms can optimally navigate this trilemma to provide fairness, robustness to price shocks, and maximal exchange revenue. We analyze these mechanisms on the Hyperliquid dataset from October 10, 2025, when ADL was used repeatedly to close \\$2.1 billion of positions in 12 minutes. By comparing our ADL mechanisms to the standard approaches used in practice, we demonstrate empirically that Hyperliquid's production queue overutilized ADL by $\\approx 28\\times$ relative to our optimal policy, imposing roughly \\$653 million of unnecessary haircuts on winning traders. This comparison also suggests that Binance overutilized ADL far more than Hyperliquid. Our results both theoretically and empirically demonstrate that optimized ADL mechanisms can dramatically reduce the loss of trader profits while maintaining exchange solvency.",
    "pdf_url": "https://arxiv.org/pdf/2512.01112v2",
    "github_url": null,
    "published": "2025-11-30T22:17:49+00:00",
    "updated": "2025-12-08T03:31:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01109v1",
    "title": "How do we measure privacy in text? A survey of text anonymization metrics",
    "authors": [
      "Ren",
      "Ramesh",
      "Yao"
    ],
    "summary": "In this work, we aim to clarify and reconcile metrics for evaluating privacy protection in text through a systematic survey. Although text anonymization is essential for enabling NLP research and model development in domains with sensitive data, evaluating whether anonymization methods sufficiently protect privacy remains an open challenge. In manually reviewing 47 papers that report privacy metrics, we identify and compare six distinct privacy notions, and analyze how the associated metrics capture different aspects of privacy risk. We then assess how well these notions align with legal privacy standards (HIPAA and GDPR), as well as user-centered expectations grounded in HCI studies. Our analysis offers practical guidance on navigating the landscape of privacy evaluation approaches further and highlights gaps in current practices. Ultimately, we aim to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.",
    "pdf_url": "https://arxiv.org/pdf/2512.01109v1",
    "github_url": null,
    "published": "2025-11-30T22:12:30+00:00",
    "updated": "2025-11-30T22:12:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01066v1",
    "title": "Reinforcement Learning for Gliding Projectile Guidance and Control",
    "authors": [
      "Cahn",
      "Thomas",
      "Pastor"
    ],
    "summary": "This paper presents the development of a control law, which is intended to be implemented on an optical guided glider. This guiding law follows an innovative approach, the reinforcement learning. This control law is used to make navigation more flexible and autonomous in a dynamic environment. The final objective is to track a target detected with the camera and then guide the glider to this point with high precision. Already applied on quad-copter drones, we wish by this study to demonstrate the applicability of reinforcement learning for fixed-wing aircraft on all of its axis.",
    "pdf_url": "https://arxiv.org/pdf/2512.01066v1",
    "github_url": null,
    "published": "2025-11-30T20:25:43+00:00",
    "updated": "2025-11-30T20:25:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01052v1",
    "title": "Autonomous Grasping On Quadruped Robot With Task Level Interaction",
    "authors": [
      "Muhtadin",
      "Rusydiansyah",
      "Purnomo"
    ],
    "summary": "Quadruped robots are increasingly used in various applications due to their high mobility and ability to operate in diverse terrains. However, most available quadruped robots are primarily focused on mobility without object manipulation capabilities. Equipping a quadruped robot with a robotic arm and gripper introduces a challenge in manual control, especially in remote scenarios that require complex commands. This research aims to develop an autonomous grasping system on a quadruped robot using a task-level interaction approach. The system includes hardware integration of a robotic arm and gripper onto the quadruped robot's body, a layered control system designed using ROS, and a web-based interface for human-robot interaction. The robot is capable of autonomously performing tasks such as navigation, object detection, and grasping using GraspNet. Testing was conducted through real-world scenarios to evaluate navigation, object selection and grasping, and user experience. The results show that the robot can perform tasks accurately and consistently, achieving a grasping success rate of 75 % from 12 trials. Therefore, the system demonstrates significant potential in enhancing the capabilities of quadruped robots as service robots in real-world environments.",
    "pdf_url": "https://arxiv.org/pdf/2512.01052v1",
    "github_url": null,
    "published": "2025-11-30T19:54:35+00:00",
    "updated": "2025-11-30T19:54:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.01009v1",
    "title": "FOM-Nav: Frontier-Object Maps for Object Goal Navigation",
    "authors": [
      "Chabal",
      "Chen",
      "Ponce"
    ],
    "summary": "This paper addresses the Object Goal Navigation problem, where a robot must efficiently find a target object in an unknown environment. Existing implicit memory-based methods struggle with long-term memory retention and planning, while explicit map-based approaches lack rich semantic information. To address these challenges, we propose FOM-Nav, a modular framework that enhances exploration efficiency through Frontier-Object Maps and vision-language models. Our Frontier-Object Maps are built online and jointly encode spatial frontiers and fine-grained object information. Using this representation, a vision-language model performs multimodal scene understanding and high-level goal prediction, which is executed by a low-level planner for efficient trajectory generation. To train FOM-Nav, we automatically construct large-scale navigation datasets from real-world scanned environments. Extensive experiments validate the effectiveness of our model design and constructed dataset. FOM-Nav achieves state-of-the-art performance on the MP3D and HM3D benchmarks, particularly in navigation efficiency metric SPL, and yields promising results on a real robot.",
    "pdf_url": "https://arxiv.org/pdf/2512.01009v1",
    "github_url": null,
    "published": "2025-11-30T18:16:09+00:00",
    "updated": "2025-11-30T18:16:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00883v1",
    "title": "Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound",
    "authors": [
      "Wang",
      "Yan",
      "Zheng"
    ],
    "summary": "World models simulate environmental dynamics to enable agents to plan and reason about future states. While existing approaches have primarily focused on visual observations, real-world perception inherently involves multiple sensory modalities. Audio provides crucial spatial and temporal cues such as sound source localization and acoustic scene properties, yet its integration into world models remains largely unexplored. No prior work has formally defined what constitutes an audio-visual world model or how to jointly capture binaural spatial audio and visual dynamics under precise action control with task reward prediction. This work presents the first formal framework for Audio-Visual World Models (AVWM), formulating multimodal environment simulation as a partially observable Markov decision process with synchronized audio-visual observations, fine-grained actions, and task rewards. To address the lack of suitable training data, we construct AVW-4k, a dataset comprising 30 hours of binaural audio-visual trajectories with action annotations and reward signals across 76 indoor environments. We propose AV-CDiT, an Audio-Visual Conditional Diffusion Transformer with a novel modality expert architecture that balances visual and auditory learning, optimized through a three-stage training strategy for effective multimodal integration. Extensive experiments demonstrate that AV-CDiT achieves high-fidelity multimodal prediction across visual and auditory modalities with reward. Furthermore, we validate its practical utility in continuous audio-visual navigation tasks, where AVWM significantly enhances the agent's performance.",
    "pdf_url": "https://arxiv.org/pdf/2512.00883v1",
    "github_url": null,
    "published": "2025-11-30T13:11:56+00:00",
    "updated": "2025-11-30T13:11:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00867v1",
    "title": "The AI Attribution Paradox: Transparency as Social Strategy in Open-Source Software Development",
    "authors": [
      "Kraishan"
    ],
    "summary": "AI coding assistants have transformed software development, raising questions about transparency and attribution practices. We examine the \"AI attribution paradox\": how developers strategically balance acknowledging AI assistance with managing community scrutiny. Analyzing 14,300 GitHub commits across 7,393 repositories from 2023-2025, we investigated attribution strategies and community responses across eight major AI tools. Results reveal widespread AI usage (95.2% of commits) but strategic attribution: only 29.5% employ explicit disclosure, with dramatic tool variation (Claude 80.5% versus Copilot 9.0%). Explicit attribution triggers modest scrutiny (23% more questions and 21% more comments) but tool choice matters 20-30 times more for predicting reception. Community sentiment remains neutral regardless of attribution type, suggesting curiosity rather than hostility. Temporal analyses show rapid norm evolution: explicit attribution increased from near-zero in early 2024 to 40% by late 2025, indicating community adaptation. These findings illuminate attribution as strategic communication rather than simple transparency, advancing understanding of algorithmic accountability and norm formation during technological transitions. We discuss implications for developers navigating disclosure decisions, platforms designing attribution mechanisms, and researchers studying emergent practices in AI-augmented collaborative work.",
    "pdf_url": "https://arxiv.org/pdf/2512.00867v1",
    "github_url": null,
    "published": "2025-11-30T12:30:55+00:00",
    "updated": "2025-11-30T12:30:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00850v1",
    "title": "Smol-GS: Compact Representations for Abstract 3D Gaussian Splatting",
    "authors": [
      "Wang",
      "Vali",
      "Solin"
    ],
    "summary": "We present Smol-GS, a novel method for learning compact representations for 3D Gaussian Splatting (3DGS). Our approach learns highly efficient encodings in 3D space that integrate both spatial and semantic information. The model captures the coordinates of the splats through a recursive voxel hierarchy, while splat-wise features store abstracted cues, including color, opacity, transformation, and material properties. This design allows the model to compress 3D scenes by orders of magnitude without loss of flexibility. Smol-GS achieves state-of-the-art compression on standard benchmarks while maintaining high rendering quality. Beyond visual fidelity, the discrete representations could potentially serve as a foundation for downstream tasks such as navigation, planning, and broader 3D scene understanding.",
    "pdf_url": "https://arxiv.org/pdf/2512.00850v1",
    "github_url": null,
    "published": "2025-11-30T11:42:00+00:00",
    "updated": "2025-11-30T11:42:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00847v1",
    "title": "Planar Diffractive Neural Networks Empowered Communications: A Spatial Modulation Scheme",
    "authors": [
      "Teng",
      "Ren",
      "Chen"
    ],
    "summary": "Diffractive neural networks, where signal processing is embedded into wave propagation, promise light-speed and energy-efficient computation. However, existing three-dimensional structures, such as stacked intelligent metasurfaces (SIMs), face critical challenges in implementation and integration. In contrast, this work pioneers planar diffractive neural networks (PDNNs) empowered communications, a novel architecture that performs signal processing as signals propagate through artificially designed planar circuits. To demonstrate the capability of PDNN, we propose a PDNN-based space-shift-keying (PDNN-SSK) communication system with a single radio-frequency (RF) chain and a maximum power detector. In this system, PDNNs are deployed at both the transmitter and receiver to jointly execute modulation, beamforming, and detection. We conduct theoretical analyses to provide the maximization condition of correct detection probability and derive the closed-form expression of the symbol error rate (SER) for the proposed system. To approach these theoretical benchmarks, the phase shift parameters of PDNNs are optimized using a surrogate model-based training approach, which effectively navigates the high-dimensional, non-convex optimization landscape. Extensive simulations verify the theoretical analysis framework and uncover fundamental design principles for the PDNN architecture, highlighting its potential to revolutionize RF front-ends by replacing conventional digital baseband modules with this integrable RF computing platform.",
    "pdf_url": "https://arxiv.org/pdf/2512.00847v1",
    "github_url": null,
    "published": "2025-11-30T11:33:04+00:00",
    "updated": "2025-11-30T11:33:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00771v1",
    "title": "EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes",
    "authors": [
      "Wu",
      "Yu",
      "Lyu"
    ],
    "summary": "Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.",
    "pdf_url": "https://arxiv.org/pdf/2512.00771v1",
    "github_url": null,
    "published": "2025-11-30T08:05:28+00:00",
    "updated": "2025-11-30T08:05:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00736v1",
    "title": "REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories",
    "authors": [
      "Thompson",
      "Garcia-Lopez",
      "Bisk"
    ],
    "summary": "Humans build viewpoint-independent cognitive maps through navigation, enabling intuitive reasoning about object permanence and spatial relations. We argue that multimodal large language models (MLLMs), despite extensive video training, lack this fundamental spatial reasoning capability, a critical limitation for embodied applications. To demonstrate these limitations and drive research, we introduce REM (Reasoning over Embodied Multi-Frame Trajectories), a benchmark using controllable 3D environments for long-horizon embodied spatial reasoning. REM systematically evaluates key aspects like object permanence/distinction, spatial relationships, and numerical tracking across dynamic embodied viewpoints. Our evaluation shows that the best-performing current models exhibit promising overall performance, but become increasingly unreliable at even moderate complexity levels easily handled by humans. These findings highlight challenges MLLMs face in developing robust spatial representations from sequential visual input. Consequently, REM provides targeted metrics and diagnostics to foster improved spatial understanding in future models.",
    "pdf_url": "https://arxiv.org/pdf/2512.00736v1",
    "github_url": null,
    "published": "2025-11-30T05:20:22+00:00",
    "updated": "2025-11-30T05:20:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00592v1",
    "title": "HAVEN: Hierarchical Adversary-aware Visibility-Enabled Navigation with Cover Utilization using Deep Transformer Q-Networks",
    "authors": [
      "Chauhan",
      "Conover",
      "Bera"
    ],
    "summary": "Autonomous navigation in partially observable environments requires agents to reason beyond immediate sensor input, exploit occlusion, and ensure safety while progressing toward a goal. These challenges arise in many robotics domains, from urban driving and warehouse automation to defense and surveillance. Classical path planning approaches and memoryless reinforcement learning often fail under limited fields of view (FoVs) and occlusions, committing to unsafe or inefficient maneuvers. We propose a hierarchical navigation framework that integrates a Deep Transformer Q-Network (DTQN) as a high-level subgoal selector with a modular low-level controller for waypoint execution. The DTQN consumes short histories of task-aware features, encoding odometry, goal direction, obstacle proximity, and visibility cues, and outputs Q-values to rank candidate subgoals. Visibility-aware candidate generation introduces masking and exposure penalties, rewarding the use of cover and anticipatory safety. A low-level potential field controller then tracks the selected subgoal, ensuring smooth short-horizon obstacle avoidance. We validate our approach in 2D simulation and extend it directly to a 3D Unity-ROS environment by projecting point-cloud perception into the same feature schema, enabling transfer without architectural changes. Results show consistent improvements over classical planners and RL baselines in success rate, safety margins, and time to goal, with ablations confirming the value of temporal memory and visibility-aware candidate design. These findings highlight a generalizable framework for safe navigation under uncertainty, with broad relevance across robotic platforms.",
    "pdf_url": "https://arxiv.org/pdf/2512.00592v1",
    "github_url": null,
    "published": "2025-11-29T18:46:18+00:00",
    "updated": "2025-11-29T18:46:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00313v1",
    "title": "Evolving Paradigms in Task-Based Search and Learning: A Comparative Analysis of Traditional Search Engine with LLM-Enhanced Conversational Search System",
    "authors": [
      "Guan",
      "Wang"
    ],
    "summary": "Large Language Models (LLMs) are rapidly reshaping information retrieval by enabling interactive, generative, and inference-driven search. While traditional keyword-based search remains central to web and academic information access, it often struggles to support multi-step reasoning and exploratory learning tasks. LLM-powered search interfaces, such as ChatGPT and Claude, introduce new capabilities that may influence how users formulate queries, navigate information, and construct knowledge. However, empirical understanding of these effects is still limited. This study compares search behavior and learning outcomes in two environments: a standard search engine and an LLM-powered search system. We investigate (1) how search strategies, query formulation, and evaluation behaviors differ across systems, and (2) how LLM use affects comprehension, knowledge integration, and critical thinking during search-based learning tasks. Findings offer insight into how generative AI shapes information-seeking processes and contribute to ongoing discussions in information retrieval, human-AI interaction, and technology-supported learning.",
    "pdf_url": "https://arxiv.org/pdf/2512.00313v1",
    "github_url": null,
    "published": "2025-11-29T04:14:14+00:00",
    "updated": "2025-11-29T04:14:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00226v1",
    "title": "DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation",
    "authors": [
      "Wang",
      "Zhang"
    ],
    "summary": "3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language tasks.In this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.",
    "pdf_url": "https://arxiv.org/pdf/2512.00226v1",
    "github_url": null,
    "published": "2025-11-28T22:02:10+00:00",
    "updated": "2025-11-28T22:02:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.23354v1",
    "title": "Functional Program Synthesis with Higher-Order Functions and Recursion Schemes",
    "authors": [
      "Fernandes"
    ],
    "summary": "Program synthesis is the process of generating a computer program following a set of specifications, such as a set of input-output examples. It can be modeled as a search problem in which the search space is the set of all valid programs. As the search space is vast, brute force is usually not feasible, and search heuristics, such as genetic programming, also have difficulty navigating it without guidance. This text presents 2 novel GP algorithms that synthesize pure, typed, and functional programs: HOTGP and Origami. HOTGP uses strong types and a functional grammar, synthesizing Haskell code, with support for higher-order functions, $λ$-functions, and parametric polymorphism. Experimental results show that HOTGP is competitive with the state of the art. Additionally, Origami is an algorithm that tackles the challenge of effectively handling loops and recursion by exploring Recursion Schemes, in which the programs are composed of well-defined templates with only a few parts that need to be synthesized. The first implementation of Origami can synthesize solutions in several Recursion Schemes and data structures, being competitive with other GP methods in the literature, as well as LLMs. The latest version of Origami employs a novel procedure, called AC/DC, designed to improve the search-space exploration. It achieves considerable improvement over its previous version by raising success rates on every problem. Compared to similar methods in the literature, it has the highest count of problems solved with success rates of $100\\%$, $\\geq 75\\%$, and $\\geq 25\\%$ across all benchmarks. In $18\\%$ of all benchmark problems, it stands as the only method to reach $100\\%$ success rate, being the first known approach to achieve it on any problem in PSB2. It also demonstrates competitive performance to LLMs, achieving the highest overall win-rate against Copilot among all GP methods.",
    "pdf_url": "https://arxiv.org/pdf/2511.23354v1",
    "github_url": null,
    "published": "2025-11-28T17:02:01+00:00",
    "updated": "2025-11-28T17:02:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00140v1",
    "title": "The Geometry of Certainty: Recursive Topological Condensation and the Limits of Inference",
    "authors": [
      "Li"
    ],
    "summary": "Computation fundamentally separates time from space: nondeterministic search is exponential in time but polynomially simulable in space (Savitch's Theorem). We propose that the brain physically instantiates a biological variant of this theorem through Memory-Amortized Inference (MAI), creating a geometry of certainty from the chaos of exploration. We formalize the cortical algorithm as a recursive topological transformation of flow into scaffold:$H_{odd}^{(k)} \\xrightarrow{\\text{Condense}} H_{even}^{(k+1)}$, where a stable, high-frequency cycle ($β_1$) at level $k$ is collapsed into a static atomic unit ($β_0$) at level $k+1$. Through this Topological Trinity (Search $\\to$ Closure $\\to$ Condensation), the system amortizes the thermodynamic cost of inference. By reducing complex homological loops into zero-dimensional defects (memory granules), the cortex converts high-entropy parallel search into low-entropy serial navigation. This mechanism builds a ``Tower of Scaffolds'' that achieves structural parity with the environment, allowing linear cortical growth to yield exponential representational reach. However, this efficiency imposes a strict limit: the same metric contraction that enables \\emph{generalization} (valid manifold folding) inevitably risks \\emph{hallucination} (homological collapse). We conclude that intelligence is the art of navigating this trade-off, where the ``Geometry of Certainty'' is defined by the precise threshold between necessary abstraction and topological error.",
    "pdf_url": "https://arxiv.org/pdf/2512.00140v1",
    "github_url": null,
    "published": "2025-11-28T16:25:01+00:00",
    "updated": "2025-11-28T16:25:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.23254v1",
    "title": "Unrepeated White Rabbit Time Synchronisation over a 300 km Optical Fibre Link",
    "authors": [
      "Amies-King",
      "Lucamarini"
    ],
    "summary": "White Rabbit (WR) technology provides a commercially-available off-the-shelf solution for time synchronisation with sub-nanosecond accuracy and picosecond-level precision over optical fibre links typically spanning tens of kilometres. Such high-performance time dissemination can support a variety of applications, including position, navigation and timing (PNT), financial transactions, metrology, as well as entanglement and quantum key distribution (QKD). Demonstrations of WR over significantly longer distances remain few and far between, particularly in scenarios where intermediate amplification is unavailable, such as stretches of long-haul underwater fibre. In this work, we report the longest unrepeated deployment of WR to date, achieving time synchronisation over a 300 km (51.34 dB) single-span optical fibre link, even in highly asymmetrical configurations, with 99.86% uptime, whilst maintaining picosecond-level precision and sub-nanosecond accuracy. This was achieved through careful selection and optimisation of the components deployed at the link's end points. By leveraging standard telecom fibre and off-the-shelf hardware, our results pave the way for a scalable and standardised timing backbone for large-scale quantum networks, offering a practical route toward time distribution in future heterogeneous quantum communication systems.",
    "pdf_url": "https://arxiv.org/pdf/2511.23254v1",
    "github_url": null,
    "published": "2025-11-28T15:02:46+00:00",
    "updated": "2025-11-28T15:02:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.23140v1",
    "title": "Multi-fidelity Bayesian Optimization Framework for CFD-Based Non-Premixed Burner Design",
    "authors": [
      "Lima",
      "Reis",
      "Santos"
    ],
    "summary": "We propose a multi-fidelity Bayesian optimization (MF-BO) framework that integrates computational fluid dynamics (CFD) evaluations with Gaussian-process surrogates to efficiently navigate the accuracy-cost trade-off induced by mesh resolution. The design vector x = [h, l, s] (height, length, and mesh element size) defines a continuous fidelity index Z(h, l, s), enabling the optimizer to adaptively combine low- and high-resolution simulations. This framework is applied to a non-premixed burner configuration targeting improved thermal efficiency under hydrogen-enriched fuels. A calibrated runtime model t_hat(h, l, s) penalizes computationally expensive queries, while a constrained noisy expected improvement (qNEI) guides sampling under an emissions cap of 2e-6 for NOx.   Surrogates trained on CFD data exhibit stable hyperparameters and physically consistent sensitivities: mean temperature increases with reactor length and fidelity and is weakly negative with height; NOx grows with temperature yet tends to decrease with length. The best design achieves T_bar approx 2.0e3 K while satisfying the NOx limit.   Relative to a hypothetical single-fidelity campaign (Z = 1), the MF-BO achieves comparable convergence with about 57 percent lower total wall time by learning the design landscape through fast low-Z evaluations and reserving high-Z CFD for promising candidates. Overall, the methodology offers a generalizable and computationally affordable path for optimizing reacting-flow systems in which mesh-driven fidelity inherently couples accuracy, cost, and emissions. This highlights its potential to accelerate design cycles and reduce resource requirements in industrial burner development and other high-cost CFD-driven applications.",
    "pdf_url": "https://arxiv.org/pdf/2511.23140v1",
    "github_url": null,
    "published": "2025-11-28T12:45:51+00:00",
    "updated": "2025-11-28T12:45:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.23017v1",
    "title": "Adaptive Factor Graph-Based Tightly Coupled GNSS/IMU Fusion for Robust Positionin",
    "authors": [
      "Ahmadi",
      "Olama",
      "Välisuo"
    ],
    "summary": "Reliable positioning in GNSS-challenged environments remains a critical challenge for navigation systems. Tightly coupled GNSS/IMU fusion improves robustness but remains vulnerable to non-Gaussian noise and outliers. We present a robust and adaptive factor graph-based fusion framework that directly integrates GNSS pseudorange measurements with IMU preintegration factors and incorporates the Barron loss, a general robust loss function that unifies several m-estimators through a single tunable parameter. By adaptively down weighting unreliable GNSS measurements, our approach improves resilience positioning. The method is implemented in an extended GTSAM framework and evaluated on the UrbanNav dataset. The proposed solution reduces positioning errors by up to 41% relative to standard FGO, and achieves even larger improvements over extended Kalman filter (EKF) baselines in urban canyon environments. These results highlight the benefits of Barron loss in enhancing the resilience of GNSS/IMU-based navigation in urban and signal-compromised environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.23017v1",
    "github_url": null,
    "published": "2025-11-28T09:33:13+00:00",
    "updated": "2025-11-28T09:33:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.23016v1",
    "title": "Maritime Activities Observed Through Open-Access Positioning Data: Moving and Stationary Vessels in the Baltic Sea",
    "authors": [
      "Hütten"
    ],
    "summary": "Understanding past and present maritime activity patterns is critical for navigation safety, environmental assessment, and commercial operations. An increasing number of services now openly provide positioning data from the Automatic Identification System (AIS) via ground-based receivers. We show that coastal vessel activity can be reconstructed from open access data with high accuracy, even with limited data quality and incomplete receiver coverage. For three months of open AIS data in the Baltic Sea from August to October 2024, we present (i) cleansing and reconstruction methods to improve the data quality, and (ii) a journey model that converts AIS message data into vessel counts, traffic estimates, and spatially resolved vessel density at a resolution of $\\sim$400 m. Vessel counts are provided, along with their uncertainties, for both moving and stationary activity. Vessel density maps also enable the identification of port locations, and we infer the most crowded and busiest coastal areas in the Baltic Sea. We find that on average, $\\gtrsim$4000 vessels simultaneously operate in the Baltic Sea, and more than 300 vessels enter or leave the area each day. Our results agree within 20\\% with previous studies relying on proprietary data.",
    "pdf_url": "https://arxiv.org/pdf/2511.23016v1",
    "github_url": null,
    "published": "2025-11-28T09:30:33+00:00",
    "updated": "2025-11-28T09:30:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.04105v1",
    "title": "LegalWebAgent: Empowering Access to Justice via LLM-Based Web Agents",
    "authors": [
      "Tan",
      "Benyekhlef"
    ],
    "summary": "Access to justice remains a global challenge, with many citizens still finding it difficult to seek help from the justice system when facing legal issues. Although the internet provides abundant legal information and services, navigating complex websites, understanding legal terminology, and filling out procedural forms continue to pose barriers to accessing justice. This paper introduces the LegalWebAgent framework that employs a web agent powered by multimodal large language models to bridge the gap in access to justice for ordinary citizens. The framework combines the natural language understanding capabilities of large language models with multimodal perception, enabling a complete process from user query to concrete action. It operates in three stages: the Ask Module understands user needs through natural language processing; the Browse Module autonomously navigates webpages, interacts with page elements (including forms and calendars), and extracts information from HTML structures and webpage screenshots; the Act Module synthesizes information for users or performs direct actions like form completion and schedule booking. To evaluate its effectiveness, we designed a benchmark test covering 15 real-world tasks, simulating typical legal service processes relevant to Québec civil law users, from problem identification to procedural operations. Evaluation results show LegalWebAgent achieved a peak success rate of 86.7%, with an average of 84.4% across all tested models, demonstrating high autonomy in complex real-world scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2512.04105v1",
    "github_url": null,
    "published": "2025-11-28T07:26:29+00:00",
    "updated": "2025-11-28T07:26:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22728v1",
    "title": "Optimal Singular Perturbation-based Model Reduction for Heterogeneous Power Systems",
    "authors": [
      "Huang",
      "Sapkota",
      "Singh"
    ],
    "summary": "Power systems are globally experiencing an unprecedented growth in size and complexity due to the advent of nonconventional generation and consumption technologies. To navigate computational complexity, power system dynamic models are often reduced using techniques based on singular perturbation. However, several technical assumptions enabling traditional approaches are being challenged due to the heterogeneous, and often black-box, nature of modern power system component models. This work proposes two singular perturbation approaches that aim to optimally identify fast states that shall be reduced, without prior knowledge about the physical meaning of system states. After presenting a timescale-agnostic formulation for singular perturbation, the first approach uses greedy optimization to sequentially select states to be reduced. The second approach relies on a nonlinear optimization routine allowing state transformations while obtaining an optimally reduced model. Numerical studies on a test system featuring synchronous machines, inverters, and line dynamics demonstrate the generalizability and accuracy of the developed approaches.",
    "pdf_url": "https://arxiv.org/pdf/2511.22728v1",
    "github_url": null,
    "published": "2025-11-27T19:17:53+00:00",
    "updated": "2025-11-27T19:17:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22685v1",
    "title": "Deadlock-Free Hybrid RL-MAPF Framework for Zero-Shot Multi-Robot Navigation",
    "authors": [
      "Wang",
      "Luo",
      "Kantaros"
    ],
    "summary": "Multi-robot navigation in cluttered environments presents fundamental challenges in balancing reactive collision avoidance with long-range goal achievement. When navigating through narrow passages   or confined spaces, deadlocks frequently emerge that prevent agents from reaching their destinations, particularly when Reinforcement Learning (RL) control policies encounter novel configurations out of learning distribution. Existing RL-based approaches suffer from limited generalization capability in unseen environments. We propose a hybrid framework that seamlessly integrates RL-based reactive navigation with on-demand Multi-Agent Path Finding (MAPF) to explicitly resolve topological deadlocks. Our approach integrates a safety layer that monitors agent progress to detect deadlocks and, when detected, triggers a coordination controller for affected agents. The framework constructs globally feasible trajectories via MAPF and regulates waypoint progression to reduce inter-agent conflicts during navigation.   Extensive evaluation on dense multi-agent benchmarks shows that our method boosts task completion from marginal to near-universal success, markedly reducing deadlocks and collisions. When integrated with hierarchical task planning, it enables coordinated navigation for heterogeneous robots, demonstrating that coupling reactive RL navigation with selective MAPF intervention yields a robust, zero-shot performance.",
    "pdf_url": "https://arxiv.org/pdf/2511.22685v1",
    "github_url": null,
    "published": "2025-11-27T18:39:38+00:00",
    "updated": "2025-11-27T18:39:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22609v1",
    "title": "MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory",
    "authors": [
      "Wang",
      "Lin",
      "Liu"
    ],
    "summary": "We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.",
    "pdf_url": "https://arxiv.org/pdf/2511.22609v1",
    "github_url": null,
    "published": "2025-11-27T16:43:21+00:00",
    "updated": "2025-11-27T16:43:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22368v1",
    "title": "Distributed Koopman Operator Learning for Perception and Safe Navigation",
    "authors": [
      "Azarbahram",
      "Liu",
      "Incremona"
    ],
    "summary": "This paper presents a unified and scalable framework for predictive and safe autonomous navigation in dynamic transportation environments by integrating model predictive control (MPC) with distributed Koopman operator learning. High-dimensional sensory data are employed to model and forecast the motion of surrounding dynamic obstacles. A consensus-based distributed Koopman learning algorithm enables multiple computational agents or sensing units to collaboratively estimate the Koopman operator without centralized data aggregation, thereby supporting large-scale and communication-efficient learning across a networked system. The learned operator predicts future spatial densities of obstacles, which are subsequently represented through Gaussian mixture models. Their confidence ellipses are approximated by convex polytopes and embedded as linear constraints in the MPC formulation to guarantee safe and collision-free navigation. The proposed approach not only ensures obstacle avoidance but also scales efficiently with the number of sensing or computational nodes, aligning with cooperative perception principles in intelligent transportation system (ITS) applications. Theoretical convergence guarantees and predictive constraint formulations are established, and extensive simulations demonstrate reliable, safe, and computationally efficient navigation performance in complex environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.22368v1",
    "github_url": null,
    "published": "2025-11-27T12:06:49+00:00",
    "updated": "2025-11-27T12:06:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22364v1",
    "title": "BINDER: Instantly Adaptive Mobile Manipulation with Open-Vocabulary Commands",
    "authors": [
      "Cho",
      "Ahn",
      "Shin"
    ],
    "summary": "Open-vocabulary mobile manipulation (OVMM) requires robots to follow language instructions, navigate, and manipulate while updating their world representation under dynamic environmental changes. However, most prior approaches update their world representation only at discrete update points such as navigation targets, waypoints, or the end of an action step, leaving robots blind between updates and causing cascading failures: overlooked objects, late error detection, and delayed replanning. To address this limitation, we propose BINDER (Bridging INstant and DEliberative Reasoning), a dual process framework that decouples strategic planning from continuous environment monitoring. Specifically, BINDER integrates a Deliberative Response Module (DRM, a multimodal LLM for task planning) with an Instant Response Module (IRM, a VideoLLM for continuous monitoring). The two modules play complementary roles: the DRM performs strategic planning with structured 3D scene updates and guides what the IRM attends to, while the IRM analyzes video streams to update memory, correct ongoing actions, and trigger replanning when necessary. Through this bidirectional coordination, the modules address the trade off between maintaining awareness and avoiding costly updates, enabling robust adaptation under dynamic conditions. Evaluated in three real world environments with dynamic object placement, BINDER achieves substantially higher success and efficiency than SoTA baselines, demonstrating its effectiveness for real world deployment.",
    "pdf_url": "https://arxiv.org/pdf/2511.22364v1",
    "github_url": null,
    "published": "2025-11-27T12:03:31+00:00",
    "updated": "2025-11-27T12:03:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22353v1",
    "title": "A Bio-Inspired Whisker Sensor toward Underwater Flow Sensing in Darkness and Turbidity",
    "authors": [
      "Hang",
      "Xiong",
      "Xie"
    ],
    "summary": "Underwater flow sensing is critical for unmanned underwater vehicles (UUVs) and environmental monitoring, yet existing sensors often suffer from low responsiveness, high detection thresholds, limited directional discrimination, complex packaging, and poor long-term stability, especially for navigation and target perception in turbid and cluttered waters. Previous solutions based on traditional strain gauges with limited detection accuracy or doped silicon sensors with limited detection height have shown feasibility but still face challenges in scalability, robustness under harsh aquatic conditions, and calibration complexity. This work presents a bio-inspired whisker sensor that provides a balanced solution by embedding high-gauge-factor silicon strain gauges into a flexible PDMS base, mimicking seal whiskers to offer both high sensitivity and simplified packaging. The device exhibits a linear force-resistance response with a limit of detection of 0.27 mN, maintains stability after 10,000 loading cycles, and shows minimal offset drift of less than 2 percent. It also demonstrates frequency matching in underwater dipole tests with clear longitudinal and transverse spatial response patterns. These results indicate a robust and scalable route for underwater flow sensing on UUV platforms in practical deployments.",
    "pdf_url": "https://arxiv.org/pdf/2511.22353v1",
    "github_url": null,
    "published": "2025-11-27T11:45:32+00:00",
    "updated": "2025-11-27T11:45:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22311v1",
    "title": "Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation",
    "authors": [
      "Wang",
      "Lee",
      "Kaplan"
    ],
    "summary": "Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.",
    "pdf_url": "https://arxiv.org/pdf/2511.22311v1",
    "github_url": null,
    "published": "2025-11-27T10:42:52+00:00",
    "updated": "2025-11-27T10:42:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22217v1",
    "title": "Optimizing NetGPT via Routing-Based Synergy and Reinforcement Learning",
    "authors": [
      "Chen",
      "Li",
      "Chen"
    ],
    "summary": "Large language model (LLM) agents at the network edge offer low-latency execution for routine queries. In contrast, complex requests often require the superior capability of cloud models, incurring higher latency and cost. To navigate this quality-cost trade-off under dynamic network conditions, we propose a cloud-edge synergy for NetGPT that integrates network-aware routing with on-edge self-improvement. Specifically, our framework routes structured tool-calling requests to cloud or edge agents via a novel scoring policy. We prove that, under mild regularity assumptions, the optimal routing rule admits a unique fallback threshold with monotone dependence on bandwidth and round-trip time (RTT). Concurrently, based on the dataset collected from requests routed to the cloud and corresponding responses, we instantiate a schema-preserving reinforcement learning (RL) to improve the capability of the edge agent. We analyze a supervised finetuning (SFT)-anchored composite objective that combines a reverse-KL trust-region step with a forward-KL realignment toward the SFT prior, explaining stability and constraining policy drift. Both the network-aware routing policy and the edge agent are updated coherently. Experiments across controlled network states and pricing schedules demonstrate smooth quality-cost frontiers, consistent gains of dynamic fallback thresholds over fixed policies, and sustained reductions in offloading while maintaining task success and schema-correct outputs.",
    "pdf_url": "https://arxiv.org/pdf/2511.22217v1",
    "github_url": null,
    "published": "2025-11-27T08:40:08+00:00",
    "updated": "2025-11-27T08:40:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22172v1",
    "title": "Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning",
    "authors": [
      "Wei",
      "Ding",
      "Hao"
    ],
    "summary": "Models capable of \"thinking with images\" by dynamically grounding their reasoning in visual evidence represent a major leap in multimodal AI. However, replicating and advancing this ability is non-trivial, with current methods often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT). This leads to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes. To navigate this dilemma, we introduce GRiP (Guided Reasoning and Perception), a novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model's perceptual focus and logical pathways. GRiP's core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward that incentivizes the model to prioritize the localization of mission-critical objects over trivial distractors, and (2) a Multi-Heuristic Reward that encourages cognitive flexibility by rewarding diverse yet logically valid reasoning pathways. Initialized from the Qwen2.5-VL-7B model, GRiP demonstrates significant performance gains across multiple challenging benchmarks. It achieves state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench, proving its effectiveness in complex visual reasoning. Our work demonstrates that moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence. The code will be made publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2511.22172v1",
    "github_url": null,
    "published": "2025-11-27T07:18:25+00:00",
    "updated": "2025-11-27T07:18:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22130v1",
    "title": "Benchmarking In-context Experiential Learning Through Repeated Product Recommendations",
    "authors": [
      "Yang",
      "Chen",
      "Yen"
    ],
    "summary": "To reliably navigate ever-shifting real-world environments, agents must grapple with incomplete knowledge and adapt their behavior through experience. However, current evaluations largely focus on tasks that leave no ambiguity, and do not measure agents' ability to adaptively learn and reason through the experiences they accrued. We exemplify the need for this in-context experiential learning in a product recommendation context, where agents must navigate shifting customer preferences and product landscapes through natural language dialogue. We curate a benchmark for experiential learning and active exploration (BELA) that combines (1) rich real-world products from Amazon, (2) a diverse collection of user personas to represent heterogeneous yet latent preferences, and (3) a LLM user simulator powered by the persona to create rich interactive trajectories. We observe that current frontier models struggle to meaningfully improve across episodes, underscoring the need for agentic systems with strong in-context learning capabilities.",
    "pdf_url": "https://arxiv.org/pdf/2511.22130v1",
    "github_url": null,
    "published": "2025-11-27T05:48:51+00:00",
    "updated": "2025-11-27T05:48:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22043v1",
    "title": "SwordRiding: A Unified Navigation Framework for Quadrotors in Unknown Complex Environments via Online Guiding Vector Fields",
    "authors": [
      "Liu",
      "Li",
      "Xin"
    ],
    "summary": "Although quadrotor navigation has achieved high performance in trajectory planning and control, real-time adaptability in unknown complex environments remains a core challenge. This difficulty mainly arises because most existing planning frameworks operate in an open-loop manner, making it hard to cope with environmental uncertainties such as wind disturbances or external perturbations. This paper presents a unified real-time navigation framework for quadrotors in unknown complex environments, based on the online construction of guiding vector fields (GVFs) from discrete reference path points. In the framework, onboard perception modules build a Euclidean Signed Distance Field (ESDF) representation of the environment, which enables obstacle awareness and path distance evaluation. The system first generates discrete, collision-free path points using a global planner, and then parameterizes them via uniform B-splines to produce a smooth and physically feasible reference trajectory. An adaptive GVF is then synthesized from the ESDF and the optimized B-spline trajectory. Unlike conventional approaches, the method adopts a closed-loop navigation paradigm, which significantly enhances robustness under external disturbances. Compared with conventional GVF methods, the proposed approach directly accommodates discretized paths and maintains compatibility with standard planning algorithms. Extensive simulations and real-world experiments demonstrate improved robustness against external disturbances and superior real-time performance.",
    "pdf_url": "https://arxiv.org/pdf/2511.22043v1",
    "github_url": null,
    "published": "2025-11-27T02:55:26+00:00",
    "updated": "2025-11-27T02:55:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22018v1",
    "title": "MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis",
    "authors": [
      "Zhu",
      "Lin",
      "Chen"
    ],
    "summary": "Accurate medical diagnosis often involves progressive visual focusing and iterative reasoning, characteristics commonly observed in clinical workflows. While recent vision-language models demonstrate promising chain-of-thought (CoT) reasoning capabilities via reinforcement learning with verifiable rewards (RLVR), their purely on-policy learning paradigm tends to reinforce superficially coherent but clinically inaccurate reasoning paths. We propose MedEyes, a novel reinforcement learning framework that dynamically models clinician-style diagnostic reasoning by progressively attending to and interpreting relevant medical image regions. By incorporating off-policy expert guidance, MedEyes converts expert visual search trajectories into structured external behavioral signals, guiding the model toward clinically aligned visual reasoning. We design the Gaze-guided Reasoning Navigator (GRN) to emulate the diagnostic process through a dual-mode exploration strategy, scanning for systematic abnormality localization and drilling for detailed regional analysis. To balance expert imitation and autonomous discovery, we introduce the Confidence Value Sampler (CVS), which employs nucleus sampling and adaptive termination to create diverse yet credible exploration paths. Finally, the dual-stream GRPO optimization framework decouples on-policy and off-policy learning signals, mitigating reward assimilation and entropy collapse. Experiments demonstrate that MedEyes achieves an average performance improvement of +8.5\\% across multiple medical VQA benchmarks, validating MedEyes's potential in building interpretable medical AI systems.",
    "pdf_url": "https://arxiv.org/pdf/2511.22018v1",
    "github_url": null,
    "published": "2025-11-27T01:47:43+00:00",
    "updated": "2025-11-27T01:47:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.22006v1",
    "title": "All-Optical Photonic Crystal Bolometer with Ultra-Low Heat Capacity for Scalable Thermal Imaging",
    "authors": [
      "Follet",
      "Goldstein",
      "Panuski"
    ],
    "summary": "High-speed thermal imaging in the long-wave infrared (LWIR) is critical for applications from autonomous navigation to medical screening, yet existing uncooled detectors are fundamentally constrained. Resistive bolometers are limited by electronic noise and the parasitic thermal load of wired readouts, while state-of-the-art nanomechanical resonators require vacuum packaging to maintain the mechanical $Q$ needed for sensitivity, preventing simultaneous fast and high-sensitivity operation. Here, we introduce and demonstrate an uncooled thermal detector that addresses these challenges via an all-optical transduction mechanism. The heterogeneously integrated pixel is engineered for minimal thermal mass, combining pyrolytic carbon absorbers for broadband LWIR absorption, hollow zirconia structures for ultra-low-conductance thermal isolation -- leaving gas-mediated convection as the dominant tunable pathway -- and a silicon photonic crystal cavity that serves as a high-$Q$ optical thermometer. Operating at ambient temperature and pressure, we measure a specific detectivity of $1.1\\times10^{7}$ Jones and a thermal time constant of $34~μ\\mathrm{s}$, corresponding to a speed that surpasses typical high-sensitivity uncooled technologies by an order of magnitude. While the demonstrated sensitivity is instrument-limited, our analysis shows that the device architecture itself supports a >25-fold performance enhancement toward its fundamental thermorefractive-noise-limited value ($3.1\\times10^{8}$ Jones). Crucially, because the optical readout is pressure-insensitive, the operating point can be tuned along the speed-sensitivity trade-off. We expect this architecture to provide a general route toward scalable, high-performance thermal imaging systems.",
    "pdf_url": "https://arxiv.org/pdf/2511.22006v1",
    "github_url": null,
    "published": "2025-11-27T01:14:39+00:00",
    "updated": "2025-11-27T01:14:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21955v1",
    "title": "Photonic Generation and Free-Space Distribution of Millimeter Waves for Portable Optical Clocks",
    "authors": [
      "Meyer",
      "Lind",
      "Groman"
    ],
    "summary": "Robust and portable optical clocks promise to bring sub-picosecond timing instability to smaller form factors, offering possible performance improvements and new scenarios for positioning and navigation, radar technologies, and experiments probing fundamental physics. However, there are currently limited methods suitable for broadly disseminating the sub-picosecond timing signals or performing frequency comparison of these clocks--particularly over open-air paths. Established microwave time transfer techniques only offer nanosecond level time synchronization, whereas optical techniques have challenging pointing requirements and lack the capability of all-weather operation. In this paper, we explore optically derived millimeter-wave carriers as a time-frequency link for full utilization of the next generation of portable optical clocks. We introduce an architecture that synthesizes 90 GHz millimeter waves with a one second residual instability of 2x10^-15, averaging into the 10^-17 range. In addition, we demonstrate a first-of-its-kind 110 m phase-stabilized free-space frequency comparison link over a millimeter-wave band with a one second instability in the 10^-14 region. Technical and systematic uncertainties are investigated and characterized, providing a foundation for future time and frequency transfer experiments among distributed portable optical clocks.",
    "pdf_url": "https://arxiv.org/pdf/2511.21955v1",
    "github_url": null,
    "published": "2025-11-26T22:30:34+00:00",
    "updated": "2025-11-26T22:30:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21912v1",
    "title": "Tracing How Annotators Think: Augmenting Preference Judgments with Reading Processes",
    "authors": [
      "Langis",
      "Walker",
      "Le"
    ],
    "summary": "We propose an annotation approach that captures not only labels but also the reading process underlying annotators' decisions, e.g., what parts of the text they focus on, re-read or skim. Using this framework, we conduct a case study on the preference annotation task, creating a dataset PreferRead that contains fine-grained annotator reading behaviors obtained from mouse tracking. PreferRead enables detailed analysis of how annotators navigate between a prompt and two candidate responses before selecting their preference. We find that annotators re-read a response in roughly half of all trials, most often revisiting the option they ultimately choose, and rarely revisit the prompt. Reading behaviors are also significantly related to annotation outcomes: re-reading is associated with higher inter-annotator agreement, whereas long reading paths and times are associated with lower agreement. These results demonstrate that reading processes provide a complementary cognitive dimension for understanding annotator reliability, decision-making and disagreement in complex, subjective NLP tasks. Our code and data are publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2511.21912v1",
    "github_url": null,
    "published": "2025-11-26T21:07:02+00:00",
    "updated": "2025-11-26T21:07:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21902v1",
    "title": "PathReasoning: A multimodal reasoning agent for query-based ROI navigation on whole-slide images",
    "authors": [
      "Zhang",
      "Xu",
      "Wang"
    ],
    "summary": "Deciphering tumor microenvironment from Whole Slide Images (WSIs) is intriguing as it is key to cancer diagnosis, prognosis and treatment response. While these gigapixel images on one hand offer a comprehensive portrait of cancer, on the other hand, the extremely large size, as much as more than 10 billion pixels, make it challenging and time-consuming to navigate to corresponding regions to support diverse clinical inspection. Inspired by pathologists who conducted navigation on WSIs with a combination of sampling, reasoning and self-reflection, we proposed \"PathReasoning\", a multi-modal reasoning agent that iteratively navigates across WSIs through multiple rounds of reasoning and refinements. Specifically, starting with randomly sampled candidate regions, PathReasoning reviews current selections with self-reflection, reasoning over the correspondence between visual observations and clinical questions, and concludes by proposing new regions to explore. Across rounds, PathReasoning builds a reasoning chain that gradually directs attention to diagnostically relevant areas. PathReasoning turns each whole slide into a sequence of question-guided views, allowing the model to efficiently find informative ROIs within a fixed number of steps, without the need for dense pixel-level annotations. PathReasoning can substantially outperform strong ROI-selection approaches by 6.7% and 3.1% of AUROC on subtyping and longitudinal analysis tasks. The high-quality ROIs further support accurate report generation on breast cancer, significantly outperforming the standard GPT-4o by 10% in accuracy. PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, supporting efficient slide review, consistent diagnostic interpretations, comprehensive reporting, and evidence traceability in digital pathology.",
    "pdf_url": "https://arxiv.org/pdf/2511.21902v1",
    "github_url": null,
    "published": "2025-11-26T20:44:17+00:00",
    "updated": "2025-11-26T20:44:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21855v1",
    "title": "Automated Enumeration of Reconfigurable Architectures for Thermal Management Systems in Battery Electric Vehicles",
    "authors": [
      "Jahedan",
      "Peddada",
      "Jennings"
    ],
    "summary": "As the automotive industry moves towards vehicle electrification, designing and optimizing thermal management systems (TMSs) for Battery Electric Vehicles (BEVs) has become a critical focus in recent years. The dependence of battery performance on operating temperature, the lack of waste combustion heat, and the significant effect of TMS energy consumption on driving range make the design of BEV TMSs highly complicated compared to conventional vehicles. Although prior research has focused on optimizing the configuration of thermal systems for varying ambient conditions, a holistic approach to studying the full potential of reconfigurable TMS architectures has not yet been fully explored. The complex design landscape of multi-mode reconfigurable systems is difficult to navigate. Relying solely on expert intuition and creativity to identify new architectures both restricts progress and leaves significant performance improvements unrealized. In this study, using graph modelling of TMS architectures, we propose a systematic method to automatically enumerate and simulate reconfigurable architectures for a TMS, given the desired operating modes, along with a framework to conduct transient performance analysis and optimization-based trade-off studies among system performance, energy consumption, and complexity. We explored more than 150 operating mode sequences, retaining 39 unique architectures for further evaluation. MATLAB Simscape models of these architectures were automatically created and their performance evaluated. The multi-objective optimization results provide decision support for selecting the best architecture based on user priorities.",
    "pdf_url": "https://arxiv.org/pdf/2511.21855v1",
    "github_url": null,
    "published": "2025-11-26T19:27:46+00:00",
    "updated": "2025-11-26T19:27:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21633v1",
    "title": "Bang-Bang Evasion: Its Stochastic Optimality and a Terminal-Set-Based Implementation",
    "authors": [
      "Mudrik",
      "Oshman"
    ],
    "summary": "We address the problem of optimal evasion in a planar endgame engagement, where a target with bounded lateral acceleration seeks to avoid interception by a missile guided by a linear feedback law. Contrary to existing approaches, that assume perfect information or use heuristic maneuver models in stochastic settings, we formulate the problem in an inherently stochastic framework involving imperfect information and bounded controls. Complying with the generalized separation theorem, the control law factors in the posterior distribution of the state. Extending the well-known optimality of bang-bang evasion maneuvers in deterministic settings to the realm of realistic, stochastic evasion scenarios, we firstly prove that an optimal evasion strategy always exists, and that the set of optimal solutions includes at least one bang-bang policy, rendering the resulting optimal control problem finite-dimensional. Leveraging this structure, we secondly propose the closed-loop terminal-set-based evasion (TSE) strategy, and demonstrate its effectiveness in simulation against a proportional navigation pursuer. Monte Carlo simulations show that the TSE strategy outperforms traditional stochastic evasion strategies based on random telegraph, Singer, and weaving models.",
    "pdf_url": "https://arxiv.org/pdf/2511.21633v1",
    "github_url": null,
    "published": "2025-11-26T18:01:49+00:00",
    "updated": "2025-11-26T18:01:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21570v1",
    "title": "From Prediction to Foresight: The Role of AI in Designing Responsible Futures",
    "authors": [
      "Perez-Ortiz"
    ],
    "summary": "In an era marked by rapid technological advancements and complex global challenges, responsible foresight has emerged as an essential framework for policymakers aiming to navigate future uncertainties and shape the future. Responsible foresight entails the ethical anticipation of emerging opportunities and risks, with a focus on fostering proactive, sustainable, and accountable future design. This paper coins the term \"responsible computational foresight\", examining the role of human-centric artificial intelligence and computational modeling in advancing responsible foresight, establishing a set of foundational principles for this new field and presenting a suite of AI-driven foresight tools currently shaping it. AI, particularly in conjunction with simulations and scenario analysis, enhances policymakers' ability to address uncertainty, evaluate risks, and devise strategies geared toward sustainable, resilient futures. However, responsible foresight extends beyond mere technical forecasting; it demands a nuanced understanding of the interdependencies within social, environmental, economic and political systems, alongside a commitment to ethical, long-term decision-making that supports human intelligence. We argue that AI will play a role as a supportive tool in responsible, human-centered foresight, complementing rather than substituting policymaker judgment to enable the proactive shaping of resilient and ethically sound futures. This paper advocates for the thoughtful integration of AI into foresight practices to empower policymakers and communities as they confront the grand challenges of the 21st century.",
    "pdf_url": "https://arxiv.org/pdf/2511.21570v1",
    "github_url": null,
    "published": "2025-11-26T16:42:10+00:00",
    "updated": "2025-11-26T16:42:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21432v2",
    "title": "Multi-Hypotheses Navigation in Collaborative Localization subject to Cyber Attacks",
    "authors": [
      "Karstensen",
      "Galeazzi"
    ],
    "summary": "This paper addresses resilient collaborative localization in multi-agent systems exposed to spoofed radio frequency measurements. Each agent maintains multiple hypotheses of its own state and exchanges selected information with neighbors using covariance intersection. Geometric reductions based on distance tests and convex hull structure limit the number of hypotheses transmitted, controlling the spread of hypotheses through the network. The method enables agents to separate spoofed and truthful measurements and to recover consistent estimates once the correct hypothesis is identified. Numerical results demonstrate the ability of the approach to contain the effect of adversarial measurements, while also highlighting the impact of conservative fusion on detection speed. The framework provides a foundation for resilient multi-agent navigation and can be extended with coordinated hypothesis selection across the network.",
    "pdf_url": "https://arxiv.org/pdf/2511.21432v2",
    "github_url": null,
    "published": "2025-11-26T14:22:41+00:00",
    "updated": "2025-11-27T17:47:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21398v1",
    "title": "Prune4Web: DOM Tree Pruning Programming for Web Agent",
    "authors": [
      "Zhang",
      "Chen",
      "Lu"
    ],
    "summary": "Web automation employs intelligent agents to execute high-level tasks by mimicking human interactions with web interfaces. Despite the capabilities of recent Large Language Model (LLM)-based web agents, navigating complex, real-world webpages efficiently remains a significant hurdle due to the prohibitively large size of Document Object Model (DOM) structures, often ranging from 10,000 to 100,000 tokens. Existing strategies typically rely on crude DOM truncation -- risking the loss of critical information -- or employ inefficient heuristics and separate ranking models, failing to achieve an optimal balance between precision and scalability. To address these challenges, we introduce Prune4Web, a novel paradigm that shifts DOM processing from resource-intensive LLM reading to efficient programmatic pruning. Central to our approach is DOM Tree Pruning Programming, where an LLM generates executable Python scoring scripts to dynamically filter DOM elements based on semantic cues from decomposed sub-tasks. This mechanism eliminates the need for LLMs to ingest raw, massive DOMs, instead delegating traversal and scoring to lightweight, interpretable programs. This methodology achieves a 25x to 50x reduction in candidate elements for grounding, thereby facilitating precise action localization while mitigating attention dilution. Furthermore, we propose a specialized data annotation pipeline and a two-turn dialogue training strategy that jointly optimizes the Planner, Programmatic Filter, and Grounder within a unified framework. Extensive experiments demonstrate state-of-the-art performance. Notably, on our low-level grounding task, Prune4Web dramatically improves accuracy from 46.8% to 88.28%, underscoring its efficacy in real-world web automation.",
    "pdf_url": "https://arxiv.org/pdf/2511.21398v1",
    "github_url": null,
    "published": "2025-11-26T13:49:39+00:00",
    "updated": "2025-11-26T13:49:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21312v1",
    "title": "Neural NMPC through Signed Distance Field Encoding for Collision Avoidance",
    "authors": [
      "Jacquet",
      "Harms",
      "Alexis"
    ],
    "summary": "This paper introduces a neural Nonlinear Model Predictive Control (NMPC) framework for mapless, collision-free navigation in unknown environments with Aerial Robots, using onboard range sensing. We leverage deep neural networks to encode a single range image, capturing all the available information about the environment, into a Signed Distance Function (SDF). The proposed neural architecture consists of two cascaded networks: a convolutional encoder that compresses the input image into a low-dimensional latent vector, and a Multi-Layer Perceptron that approximates the corresponding spatial SDF. This latter network parametrizes an explicit position constraint used for collision avoidance, which is embedded in a velocity-tracking NMPC that outputs thrust and attitude commands to the robot. First, a theoretical analysis of the contributed NMPC is conducted, verifying recursive feasibility and stability properties under fixed observations. Subsequently, we evaluate the open-loop performance of the learning-based components as well as the closed-loop performance of the controller in simulations and experiments. The simulation study includes an ablation study, comparisons with two state-of-the-art local navigation methods, and an assessment of the resilience to drifting odometry. The real-world experiments are conducted in forest environments, demonstrating that the neural NMPC effectively performs collision avoidance in cluttered settings against an adversarial reference velocity input and drifting position estimates.",
    "pdf_url": "https://arxiv.org/pdf/2511.21312v1",
    "github_url": null,
    "published": "2025-11-26T11:59:09+00:00",
    "updated": "2025-11-26T11:59:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21297v1",
    "title": "Active Learning Driven Materials Discovery for Low Thermal Conductivity Rare-Earth Pyrochlore for Thermal Barrier Coatings",
    "authors": [
      "Chowdhury",
      "Romero",
      "Figueredo"
    ],
    "summary": "High-Entropy/multicomponent rare-earth oxides (HECs and MCCs) show promise as alternative materials for thermal barrier coatings (TBC) with the ability to tailor properties based on the combination of rare-earth elements present. By enabling the substitution of scarce or supply-risk rare-earths with more readily available alternatives while maintaining comparable material performance, HECs and MCCs offer a valuable path towards alternative TBC material design. However, navigating this search space of compositionally complex materials is both time and resource intensive. In this study, an active learning (AL) framework was employed to identify HEC/MCC materials with a pyrochlore structure, with acceptable thermal conductivity (TC) for TBC applications. The AL framework was applied through a Bayesian optimisation (BO) strategy, coupled with a random forest surrogate model. TC was selected as the optimisation criterion as that is the most basic requirement of TBC materials. Over two iterations of the AL cycle, four compositions were generated and synthesized in the lab for experimental evaluation. The first iteration yielded two single-phase pyrochlores, $(La_{0.29}Nd_{0.36}Gd_{0.36})_2Zr_2O_7$ and $(La_{0.333}Nd_{0.26}Gd_{0.15}Ho_{0.15}Yb_{0.111})_2Zr_2O_7$, with measured thermal conductivities of 2.03 and 1.90 $W/mK$, respectively. The surrogate model predicted a TC of 2.009 $W/mK$ for both compositions, demonstrating it's accuracy for completely new compositions. The second iteration compositions showed dual-phase when synthesized, highlighting the need to take into account phase formation in the AL framework.",
    "pdf_url": "https://arxiv.org/pdf/2511.21297v1",
    "github_url": null,
    "published": "2025-11-26T11:39:53+00:00",
    "updated": "2025-11-26T11:39:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21135v1",
    "title": "SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation",
    "authors": [
      "Chen",
      "Guo",
      "Chu"
    ],
    "summary": "Embodied navigation that adheres to social norms remains an open research challenge. Our \\textbf{SocialNav} is a foundational model for socially-aware navigation with a hierarchical \"brain-action\" architecture, capable of understanding high-level social norms and generating low-level, socially compliant trajectories. To enable such dual capabilities, we construct the SocNav Dataset, a large-scale collection of 7 million samples, comprising (1) a Cognitive Activation Dataset providing social reasoning signals such as chain-of-thought explanations and social traversability prediction, and (2) an Expert Trajectories Pyramid aggregating diverse navigation demonstrations from internet videos, simulated environments, and real-world robots. A multi-stage training pipeline is proposed to gradually inject and refine navigation intelligence: we first inject general navigation skills and social norms understanding into the model via imitation learning, and then refine such skills through a deliberately designed Socially-Aware Flow Exploration GRPO (SAFE-GRPO), the first flow-based reinforcement learning framework for embodied navigation that explicitly rewards socially compliant behaviors. SocialNav achieves +38% success rate and +46% social compliance rate compared to the state-of-the-art method, demonstrating strong gains in both navigation performance and social compliance. Our project page: https://amap-eai.github.io/SocialNav/",
    "pdf_url": "https://arxiv.org/pdf/2511.21135v1",
    "github_url": null,
    "published": "2025-11-26T07:36:01+00:00",
    "updated": "2025-11-26T07:36:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21083v1",
    "title": "Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry",
    "authors": [
      "Pan",
      "Zheng",
      "Yin"
    ],
    "summary": "Visual-Inertial Odometry (VIO) is a critical component for robust ego-motion estimation, enabling foundational capabilities such as autonomous navigation in robotics and real-time 6-DoF tracking for augmented reality. Existing methods face a well-known trade-off: filter-based approaches are efficient but prone to drift, while optimization-based methods, though accurate, rely on computationally prohibitive Visual-Inertial Bundle Adjustment (VIBA) that is difficult to run on resource-constrained platforms. Rather than removing VIBA altogether, we aim to reduce how often and how heavily it must be invoked. To this end, we cast two key design choices in modern VIO, when to run the visual frontend and how strongly to trust its output, as sequential decision problems, and solve them with lightweight reinforcement learning (RL) agents. Our framework introduces a lightweight, dual-pronged RL policy that serves as our core contribution: (1) a Select Agent intelligently gates the entire VO pipeline based only on high-frequency IMU data; and (2) a composite Fusion Agent that first estimates a robust velocity state via a supervised network, before an RL policy adaptively fuses the full (p, v, q) state. Experiments on the EuRoC MAV and TUM-VI datasets show that, in our unified evaluation, the proposed method achieves a more favorable accuracy-efficiency-memory trade-off than prior GPU-based VO/VIO systems: it attains the best average ATE while running up to 1.77 times faster and using less GPU memory. Compared to classical optimization-based VIO systems, our approach maintains competitive trajectory accuracy while substantially reducing computational load.",
    "pdf_url": "https://arxiv.org/pdf/2511.21083v1",
    "github_url": null,
    "published": "2025-11-26T06:03:03+00:00",
    "updated": "2025-11-26T06:03:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.21037v1",
    "title": "LOOM: Personalized Learning Informed by Daily LLM Conversations Toward Long-Term Mastery via a Dynamic Learner Memory Graph",
    "authors": [
      "Cui",
      "Pu",
      "Grossman"
    ],
    "summary": "Foundation models are increasingly used to personalize learning, yet many systems still assume fixed curricula or coarse progress signals, limiting alignment with learners' day-to-day needs. At the other extreme, lightweight incidental systems offer flexible, in-the-moment content but rarely guide learners toward mastery. Prior work privileges either continuity (maintaining a plan across sessions) or initiative (reacting to the moment), not both, leaving learners to navigate the trade-off between recency and trajectory-immediate relevance versus cumulative, goal-aligned progress. We present LOOM, an agentic pipeline that infers evolving learner needs from recent LLM conversations and a dynamic learner memory graph, then assembles coherent learning materials personalized to the learner's current needs, priorities, and understanding. These materials link adjacent concepts and surface gaps as tightly scoped modules that cumulatively advance broader goals, providing guidance and sustained progress while remaining responsive to new interests. We describe LOOM's end-to-end architecture and working prototype, including conversation summarization, topic planning, course generation, and graph-based progress tracking. In a formative study with ten participants, users reported that LOOM's generated lessons felt relevant to their recent activities and helped them recognize knowledge gaps, though they also highlighted needs for greater consistency and control. We conclude with design implications for more robust, mixed-initiative learning pipelines that integrate structured learner modelling with everyday LLM interactions.",
    "pdf_url": "https://arxiv.org/pdf/2511.21037v1",
    "github_url": null,
    "published": "2025-11-26T04:14:24+00:00",
    "updated": "2025-11-26T04:14:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20894v1",
    "title": "Efficient Greedy Algorithms for Feature Selection in Robot Visual Localization",
    "authors": [
      "Pandey",
      "Mollaei",
      "Motee"
    ],
    "summary": "Robot localization is a fundamental component of autonomous navigation in unknown environments. Among various sensing modalities, visual input from cameras plays a central role, enabling robots to estimate their position by tracking point features across image frames. However, image frames often contain a large number of features, many of which are redundant or uninformative for localization. Processing all features can introduce significant computational latency and inefficiency. This motivates the need for intelligent feature selection, identifying a subset of features that are most informative for localization over a prediction horizon. In this work, we propose two fast and memory-efficient feature selection algorithms that enable robots to actively evaluate the utility of visual features in real time. Unlike existing approaches with high computational and memory demands, the proposed methods are explicitly designed to reduce both time and memory complexity while achieving a favorable trade-off between computational efficiency and localization accuracy.",
    "pdf_url": "https://arxiv.org/pdf/2511.20894v1",
    "github_url": null,
    "published": "2025-11-25T22:21:27+00:00",
    "updated": "2025-11-25T22:21:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20878v1",
    "title": "Supporting Students in Navigating LLM-Generated Insecure Code",
    "authors": [
      "Park",
      "Lim",
      "Park"
    ],
    "summary": "The advent of Artificial Intelligence (AI), particularly large language models (LLMs), has revolutionized software development by enabling developers to specify tasks in natural language and receive corresponding code, boosting productivity. However, this shift also introduces security risks, as LLMs may generate insecure code that can be exploited by adversaries. Current educational approaches emphasize efficiency while overlooking these risks, leaving students underprepared to identify and mitigate security issues in AI-assisted workflows.   To address this gap, we present Bifröst, an educational framework that cultivates security awareness in AI-augmented development. Bifröst integrates (1) a Visual Studio Code extension simulating realistic environments, (2) adversarially configured LLMs that generate insecure code, and (3) a feedback system highlighting vulnerabilities. By immersing students in tasks with compromised LLMs and providing targeted security analysis, Bifröst cultivates critical evaluation skills; classroom deployments (n=61) show vulnerability to insecure code, while a post-intervention survey (n=21) indicates increased skepticism toward LLM outputs.",
    "pdf_url": "https://arxiv.org/pdf/2511.20878v1",
    "github_url": null,
    "published": "2025-11-25T21:49:36+00:00",
    "updated": "2025-11-25T21:49:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.02043v1",
    "title": "Mirror, Mirror on the Wall -- Which is the Best Model of Them All?",
    "authors": [
      "Sayed",
      "Schuldt"
    ],
    "summary": "Large Language Models (LLMs) have become one of the most transformative tools across many applications, as they have significantly boosted productivity and achieved impressive results in various domains such as finance, healthcare, education, telecommunications, and law, among others. Typically, state-of-the-art (SOTA) foundation models are developed by large corporations based on large data collections and substantial computational and financial resources required to pretrain such models from scratch. These foundation models then serve as the basis for further development and domain adaptation for specific use cases or tasks. However, given the dynamic and fast-paced nature of launching new foundation models, the process of selecting the most suitable model for a particular use case, application, or domain becomes increasingly complex. We argue that there are two main dimensions that need to be taken into consideration when selecting a model for further training: a qualitative dimension (which model is best suited for a task based on information, for instance, taken from model cards) and a quantitative dimension (which is the best performing model). The quantitative performance of models is assessed through leaderboards, which rank models based on standardized benchmarks and provide a consistent framework for comparing different LLMs. In this work, we address the analysis of the quantitative dimension by exploring the current leaderboards and benchmarks. To illustrate this analysis, we focus on the medical domain as a case study, demonstrating the evolution, current landscape, and practical significance of this quantitative evaluation dimension. Finally, we propose a Model Selection Methodology (MSM), a systematic approach designed to guide the navigation, prioritization, and selection of the model that best aligns with a given use case.",
    "pdf_url": "https://arxiv.org/pdf/2512.02043v1",
    "github_url": null,
    "published": "2025-11-25T20:52:45+00:00",
    "updated": "2025-11-25T20:52:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20823v1",
    "title": "RefTr: Recurrent Refinement of Confluent Trajectories for 3D Vascular Tree Centerline Graphs",
    "authors": [
      "Naeem",
      "Hagerman",
      "Alvén"
    ],
    "summary": "Tubular trees, such as blood vessels and lung airways, are essential for material transport within the human body. Accurately detecting their centerlines with correct tree topology is critical for clinical tasks such as diagnosis, treatment planning, and surgical navigation. In these applications, maintaining high recall is crucial, as missing small branches can result in fatal mistakes caused by incomplete assessments or undetected abnormalities. We present RefTr, a 3D image-to-graph model for centerline generation of vascular trees via recurrent refinement of confluent trajectories. RefTr uses a Producer-Refiner architecture based on a Transformer decoder, where the Producer proposes a set of initial confluent trajectories that are recurrently refined by the Refiner to produce final trajectories, which forms the centerline graph. The confluent trajectory representation enables refinement of complete trajectories while explicitly enforcing a valid tree topology. The recurrent refinement scheme improves precision and reuses the same Refiner block across multiple steps, yielding a 2.4x reduction in decoder parameters compared to previous SOTA. We also introduce an efficient non-maximum suppression algorithm for spatial tree graphs to merge duplicate branches and boost precision. Across multiple public centerline datasets, RefTr achieves superior recall and comparable precision to previous SOTA, while offering faster inference and substantially fewer parameters, demonstrating its potential as a new state-of-the-art framework for vascular tree analysis in 3D medical imaging.",
    "pdf_url": "https://arxiv.org/pdf/2511.20823v1",
    "github_url": null,
    "published": "2025-11-25T20:22:57+00:00",
    "updated": "2025-11-25T20:22:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20620v1",
    "title": "Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI",
    "authors": [
      "Liu",
      "Li",
      "Deng"
    ],
    "summary": "Reproducible closed-loop evaluation remains a major bottleneck in Embodied AI such as visual navigation. A promising path forward is high-fidelity simulation that combines photorealistic sensor rendering with geometrically grounded interaction in complex, open-world urban environments. Although recent video-3DGS methods ease open-world scene capturing, they are still unsuitable for benchmarking due to large visual and geometric sim-to-real gaps. To address these challenges, we introduce Wanderland, a real-to-sim framework that features multi-sensor capture, reliable reconstruction, accurate geometry, and robust view synthesis. Using this pipeline, we curate a diverse dataset of indoor-outdoor urban scenes and systematically demonstrate how image-only pipelines scale poorly, how geometry quality impacts novel view synthesis, and how all of these adversely affect navigation policy learning and evaluation reliability. Beyond serving as a trusted testbed for embodied navigation, Wanderland's rich raw sensor data further allows benchmarking of 3D reconstruction and novel view synthesis models. Our work establishes a new foundation for reproducible research in open-world embodied AI. Project website is at https://ai4ce.github.io/wanderland/.",
    "pdf_url": "https://arxiv.org/pdf/2511.20620v1",
    "github_url": null,
    "published": "2025-11-25T18:43:55+00:00",
    "updated": "2025-11-25T18:43:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20504v1",
    "title": "Ultralow noise microwaves with free-running frequency combs and electrical feedforward",
    "authors": [
      "Nakamura",
      "Groman",
      "Ji"
    ],
    "summary": "Optically generated microwave signals exhibit some of the lowest phase noise and timing jitter of any microwave-generating technology to date. The success of octave-spanning optical frequency combs in down-converting ultrastable optical frequency references has motivated the development of compact, robust and highly manufacturable optical systems that maintain the ultralow microwave phase noise of their tabletop counterparts. Two-point optical frequency division using chip-scale components and ~1 THz-spanning microcombs has been quite successful, but with stringent requirements on the comb source's free-running noise and feedback control dynamics. Here we introduce a major simplification of this architecture that replaces feedback control of the frequency comb in favor of electronic feedforward noise cancelation that significantly relaxes the comb requirements. Demonstrated with both a high repetition rate solid-state mode-locked laser and a microcomb, feedforward on a 10 GHz carrier results in more robust operation with phase noise as low as -153 dBc/Hz at offsets >10 kHz, femtosecond timing jitter, and elimination of the large \"servo bump\" noise increase at high offset frequency. The system's compatibility with a variety of highly manufacturable mode-locked laser designs and its resilience and straightforward implementation represents an important step forward towards a fully chip-scale implementation of optically generated microwaves, with applications in radar, sensing, and position, navigation and timing.",
    "pdf_url": "https://arxiv.org/pdf/2511.20504v1",
    "github_url": null,
    "published": "2025-11-25T17:12:48+00:00",
    "updated": "2025-11-25T17:12:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20467v1",
    "title": "Power-Efficient Autonomous Mobile Robots",
    "authors": [
      "Liu",
      "Shi",
      "Shin"
    ],
    "summary": "This paper presents pNav, a novel power-management system that significantly enhances the power/energy-efficiency of Autonomous Mobile Robots (AMRs) by jointly optimizing their physical/mechanical and cyber subsystems. By profiling AMRs' power consumption, we identify three challenges in achieving CPS (cyber-physical system) power-efficiency that involve both cyber (C) and physical (P) subsystems: (1) variabilities of system power consumption breakdown, (2) environment-aware navigation locality, and (3) coordination of C and P subsystems. pNav takes a multi-faceted approach to achieve power-efficiency of AMRs. First, it integrates millisecond-level power consumption prediction for both C and P subsystems. Second, it includes novel real-time modeling and monitoring of spatial and temporal navigation localities for AMRs. Third, it supports dynamic coordination of AMR software (navigation, detection) and hardware (motors, DVFS driver) configurations. pNav is prototyped using the Robot Operating System (ROS) Navigation Stack, 2D LiDAR, and camera. Our in-depth evaluation with a real robot and Gazebo environments demonstrates a >96% accuracy in predicting power consumption and a 38.1% reduction in power consumption without compromising navigation accuracy and safety.",
    "pdf_url": "https://arxiv.org/pdf/2511.20467v1",
    "github_url": null,
    "published": "2025-11-25T16:31:43+00:00",
    "updated": "2025-11-25T16:31:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20394v1",
    "title": "Improved adaptive wind driven optimization algorithm for real-time path planning",
    "authors": [
      "Liu",
      "Zain",
      "Mao"
    ],
    "summary": "Recently, path planning has achieved remarkable progress in enhancing global search capability and convergence accuracy through heuristic and learning-inspired optimization frameworks. However, real-time adaptability in dynamic environments remains a critical challenge for autonomous navigation, particularly when robots must generate collision-free, smooth, and efficient trajectories under complex constraints. By analyzing the difficulties in dynamic path planning, the Wind Driven Optimization (WDO) algorithm emerges as a promising framework owing to its physically interpretable search dynamics. Motivated by these observations, this work revisits the WDO principle and proposes a variant formulation, Multi-hierarchical adaptive wind driven optimization(MAWDO), that improves adaptability and robustness in time-varying environments. To mitigate instability and premature convergence, a hierarchical-guidance mechanism divides the population into multiple groups guided by individual, regional, and global leaders to balance exploration and exploitation. Extensive evaluations on sixteen benchmark functions show that MAWDO achieves superior optimization accuracy, convergence stability, and adaptability over state-of-the art metaheuristics. In dynamic path planning, MAWDO shortens the path length to 469.28 pixels, improving over Multi-strategy ensemble wind driven optimization(MEWDO), Adaptive wind driven optimization(AWDO) and WDO by 3.51\\%, 11.63\\% and 14.93\\%, and achieves the smallest optimality gap (1.01) with smoothness 0.71 versus 13.50 and 15.67 for AWDO and WDO, leading to smoother, shorter, and collision-free trajectories that confirm its effectiveness for real-time path planning in complex environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.20394v1",
    "github_url": null,
    "published": "2025-11-25T15:19:45+00:00",
    "updated": "2025-11-25T15:19:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.07845v1",
    "title": "AudioScene: Integrating Object-Event Audio into 3D Scenes",
    "authors": [
      "Yuan",
      "Wen",
      "Shafique"
    ],
    "summary": "The rapid advances in audio analysis underscore its vast potential for humancomputer interaction, environmental monitoring, and public safety; yet, existing audioonly datasets often lack spatial context. To address this gap, we present two novel audiospatial scene datasets, AudioScanNet and AudioRoboTHOR, designed to explore audioconditioned tasks within 3D environments. By integrating audio clips with spatially aligned 3D scenes, our datasets enable research on how audio signals interact with spatial context. To associate audio events with corresponding spatial information, we leverage the common sense reasoning ability of large language models and supplement them with rigorous human verification, This approach offers greater scalability compared to purely manual annotation while maintaining high standards of accuracy, completeness, and diversity, quantified through inter annotator agreement and performance on two benchmark tasks audio based 3D visual grounding and audio based robotic zeroshot navigation. The results highlight the limitations of current audiocentric methods and underscore the practical challenges and significance of our datasets in advancing audio guided spatial learning.",
    "pdf_url": "https://arxiv.org/pdf/2512.07845v1",
    "github_url": null,
    "published": "2025-11-25T14:28:13+00:00",
    "updated": "2025-11-25T14:28:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20216v1",
    "title": "CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents",
    "authors": [
      "Seong",
      "Kim",
      "Kim"
    ],
    "summary": "Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \\emph{CostNav}, a \\textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \\textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\\% SLA compliance but is \\emph{not} commercially viable: yielding a loss of \\$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.",
    "pdf_url": "https://arxiv.org/pdf/2511.20216v1",
    "github_url": null,
    "published": "2025-11-25T11:42:28+00:00",
    "updated": "2025-11-25T11:42:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.05974v1",
    "title": "The rationality of radical pair mechanism in real biological systems",
    "authors": [
      "Chen",
      "Liu",
      "Cai"
    ],
    "summary": "The radical pair mechanism (RPM) in the chemical magnetic compass model is considered to be one of the most promising candidates for the avian magnetic navigation, and quantum needle phenomenon further boosts the navigation precision to a new high level. It is well known that there are also a variety of methods in the field of magnetic field sensing in laboratory, e.g. Ramsey protocol of NV centers in diamond. Here, we compare the RPM model and Ramsey-like model under laboratory conditions and under in vivo conditions respectively. The results are both surprising and reasonable. Under laboratory conditions, if we have precise control over time and a reasonably accurate prior knowledge of the magnetic field direction, the Ramsey-like model will outperform the RPM model. However, when such information is unavailable, as under in vivo conditions, the RPM model stands out. The RPM model achieves greater practicality at the cost of reduced accuracy.",
    "pdf_url": "https://arxiv.org/pdf/2512.05974v1",
    "github_url": null,
    "published": "2025-11-25T11:30:28+00:00",
    "updated": "2025-11-25T11:30:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20180v1",
    "title": "Hibikino-Musashi@Home 2025 Team Description Paper",
    "authors": [
      "Kobayashi",
      "Isomoto",
      "Yamao"
    ],
    "summary": "This paper provides an overview of the techniques employed by Hibikino-Musashi@Home, which intends to participate in the domestic standard platform league. The team developed a dataset generator for training a robot vision system and an open-source development environment running on a Human Support Robot simulator. The large-language-model-powered task planner selects appropriate primitive skills to perform the task requested by the user. Moreover, the team has focused on research involving brain-inspired memory models for adaptation to individual home environments. This approach aims to provide intuitive and personalized assistance. Additionally, the team contributed to the reusability of the navigation system developed by Pumas in RoboCup2024. The team aimed to design a home service robot to assist humans in their homes and continuously attend competitions to evaluate and improve the developed system.",
    "pdf_url": "https://arxiv.org/pdf/2511.20180v1",
    "github_url": null,
    "published": "2025-11-25T11:02:34+00:00",
    "updated": "2025-11-25T11:02:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00080v1",
    "title": "Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels",
    "authors": [
      "Dehne",
      "Zach",
      "Stelldinger"
    ],
    "summary": "The MARWIN robot operates at the European XFEL to perform autonomous radiation monitoring in long, monotonous accelerator tunnels where conventional localization approaches struggle. Its current navigation concept combines lidar-based edge detection, wheel/lidar odometry with periodic QR-code referencing, and fuzzy control of wall distance, rotation, and longitudinal position. While robust in predefined sections, this design lacks flexibility for unknown geometries and obstacles. This paper explores deep visual stereo odometry (DVSO) with 3D-geometric constraints as a focused alternative. DVSO is purely vision-based, leveraging stereo disparity, optical flow, and self-supervised learning to jointly estimate depth and ego-motion without labeled data. For global consistency, DVSO can subsequently be fused with absolute references (e.g., landmarks) or other sensors. We provide a conceptual evaluation for accelerator tunnel environments, using the European XFEL as a case study. Expected benefits include reduced scale drift via stereo, low-cost sensing, and scalable data collection, while challenges remain in low-texture surfaces, lighting variability, computational load, and robustness under radiation. The paper defines a research agenda toward enabling MARWIN to navigate more autonomously in constrained, safety-critical infrastructures.",
    "pdf_url": "https://arxiv.org/pdf/2512.00080v1",
    "github_url": null,
    "published": "2025-11-25T09:22:22+00:00",
    "updated": "2025-11-25T09:22:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20100v1",
    "title": "QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation",
    "authors": [
      "Zhu",
      "Peng",
      "Guo"
    ],
    "summary": "Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.",
    "pdf_url": "https://arxiv.org/pdf/2511.20100v1",
    "github_url": null,
    "published": "2025-11-25T09:17:47+00:00",
    "updated": "2025-11-25T09:17:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20058v1",
    "title": "DeLightMono: Enhancing Self-Supervised Monocular Depth Estimation in Endoscopy by Decoupling Uneven Illumination",
    "authors": [
      "Ou",
      "Li",
      "Zhang"
    ],
    "summary": "Self-supervised monocular depth estimation serves as a key task in the development of endoscopic navigation systems. However, performance degradation persists due to uneven illumination inherent in endoscopic images, particularly in low-intensity regions. Existing low-light enhancement techniques fail to effectively guide the depth network. Furthermore, solutions from other fields, like autonomous driving, require well-lit images, making them unsuitable and increasing data collection burdens. To this end, we present DeLight-Mono - a novel self-supervised monocular depth estimation framework with illumination decoupling. Specifically, endoscopic images are represented by a designed illumination-reflectance-depth model, and are decomposed with auxiliary networks. Moreover, a self-supervised joint-optimizing framework with novel losses leveraging the decoupled components is proposed to mitigate the effects of uneven illumination on depth estimation. The effectiveness of the proposed methods was rigorously verified through extensive comparisons and an ablation study performed on two public datasets.",
    "pdf_url": "https://arxiv.org/pdf/2511.20058v1",
    "github_url": null,
    "published": "2025-11-25T08:29:03+00:00",
    "updated": "2025-11-25T08:29:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00076v1",
    "title": "Arcadia: Toward a Full-Lifecycle Framework for Embodied Lifelong Learning",
    "authors": [
      "Gao",
      "Li",
      "Lin"
    ],
    "summary": "We contend that embodied learning is fundamentally a lifecycle problem rather than a single-stage optimization. Systems that optimize only one link (data collection, simulation, learning, or deployment) rarely sustain improvement or generalize beyond narrow settings. We introduce Arcadia, a closed-loop framework that operationalizes embodied lifelong learning by tightly coupling four stages: (1) Self-evolving exploration and grounding for autonomous data acquisition in physical environments, (2) Generative scene reconstruction and augmentation for realistic and extensible scene creation, (3) a Shared embodied representation architecture that unifies navigation and manipulation within a single multimodal backbone, and (4) Sim-from-real evaluation and evolution that closes the feedback loop through simulation-based adaptation. This coupling is non-decomposable: removing any stage breaks the improvement loop and reverts to one-shot training. Arcadia delivers consistent gains on navigation and manipulation benchmarks and transfers robustly to physical robots, indicating that a tightly coupled lifecycle: continuous real-world data acquisition, generative simulation update, and shared-representation learning, supports lifelong improvement and end-to-end generalization. We release standardized interfaces enabling reproducible evaluation and cross-model comparison in reusable environments, positioning Arcadia as a scalable foundation for general-purpose embodied agents.",
    "pdf_url": "https://arxiv.org/pdf/2512.00076v1",
    "github_url": null,
    "published": "2025-11-25T07:26:00+00:00",
    "updated": "2025-11-25T07:26:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.20720v1",
    "title": "DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving",
    "authors": [
      "HU",
      "Huang",
      "Guan"
    ],
    "summary": "Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.",
    "pdf_url": "https://arxiv.org/pdf/2511.20720v1",
    "github_url": null,
    "published": "2025-11-25T07:00:26+00:00",
    "updated": "2025-11-25T07:00:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.19885v1",
    "title": "Complex Instruction Following with Diverse Style Policies in Football Games",
    "authors": [
      "Sun",
      "Shen",
      "Hu"
    ],
    "summary": "Despite advancements in language-controlled reinforcement learning (LC-RL) for basic domains and straightforward commands (e.g., object manipulation and navigation), effectively extending LC-RL to comprehend and execute high-level or abstract instructions in complex, multi-agent environments, such as football games, remains a significant challenge. To address this gap, we introduce Language-Controlled Diverse Style Policies (LCDSP), a novel LC-RL paradigm specifically designed for complex scenarios. LCDSP comprises two key components: a Diverse Style Training (DST) method and a Style Interpreter (SI). The DST method efficiently trains a single policy capable of exhibiting a wide range of diverse behaviors by modulating agent actions through style parameters (SP). The SI is designed to accurately and rapidly translate high-level language instructions into these corresponding SP. Through extensive experiments in a complex 5v5 football environment, we demonstrate that LCDSP effectively comprehends abstract tactical instructions and accurately executes the desired diverse behavioral styles, showcasing its potential for complex, real-world applications.",
    "pdf_url": "https://arxiv.org/pdf/2511.19885v1",
    "github_url": null,
    "published": "2025-11-25T03:45:34+00:00",
    "updated": "2025-11-25T03:45:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.19770v2",
    "title": "Multi-Hypotheses Ego-Tracking for Resilient Navigation",
    "authors": [
      "Karstensen",
      "Galeazzi"
    ],
    "summary": "Autonomous robots relying on radio frequency (RF)-based localization such as global navigation satellite system (GNSS), ultra-wide band (UWB), and 5G integrated sensing and communication (ISAC) are vulnerable to spoofing and sensor manipulation. This paper presents a resilient navigation architecture that combines multi-hypothesis estimation with a Poisson binomial windowed-count detector for anomaly identification and isolation. A state machine coordinates transitions between operation, diagnosis, and mitigation, enabling adaptive response to adversarial conditions. When attacks are detected, trajectory re-planning based on differential flatness allows information-gathering maneuvers minimizing performance loss. Case studies demonstrate effective detection of biased sensors, maintenance of state estimation, and recovery of nominal operation under persistent spoofing attacks",
    "pdf_url": "https://arxiv.org/pdf/2511.19770v2",
    "github_url": null,
    "published": "2025-11-24T22:54:59+00:00",
    "updated": "2025-11-26T07:55:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.19768v1",
    "title": "Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering",
    "authors": [
      "Frahm",
      "Patel",
      "Zhang"
    ],
    "summary": "Large vision-language models (VLMs) have improved embodied question answering (EQA) agents by providing strong semantic priors for open-vocabulary reasoning. However, when used directly for step-level exploration, VLMs often exhibit frontier oscillations, unstable back-and-forth movements caused by overconfidence and miscalibration, leading to inefficient navigation and degraded answer quality. We propose Prune-Then-Plan, a simple and effective framework that stabilizes exploration through step-level calibration. Instead of trusting raw VLM scores, our method prunes implausible frontier choices using a Holm-Bonferroni inspired pruning procedure and then delegates final decisions to a coverage-based planner. This separation converts overconfident predictions into conservative, interpretable actions by relying on human-level judgments to calibrate the step-level behavior of VLMs. Integrated into the 3D-Mem EQA framework, our approach achieves relative improvements of up to 49% and 33% in visually grounded SPL and LLM-Match metrics respectively over baselines. Overall, our method achieves better scene coverage under equal exploration budgets on both OpenEQA and EXPRESS-Bench datasets.",
    "pdf_url": "https://arxiv.org/pdf/2511.19768v1",
    "github_url": null,
    "published": "2025-11-24T22:50:50+00:00",
    "updated": "2025-11-24T22:50:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.19652v1",
    "title": "Navigating Gigapixel Pathology Images with Large Multimodal Models",
    "authors": [
      "Buckley",
      "Weihrauch",
      "Latham"
    ],
    "summary": "Despite being widely used to support clinical care, general-purpose large multimodal models (LMMs) have generally shown poor or inconclusive performance in medical image interpretation, particularly in pathology, where gigapixel images are used. However, prior studies have used either low-resolution thumbnails or random patches, which likely underestimated model performance. Here, we ask whether LMMs can be adapted to reason coherently and accurately in the evaluation of such images. In this study, we introduce Gigapixel Image Agent for Navigating Tissue (GIANT), the first framework that allows LMMs to iteratively navigate whole-slide images (WSIs) like a pathologist. Accompanying GIANT, we release MultiPathQA, a new benchmark, which comprises 934 WSI-level questions, encompassing five clinically-relevant tasks ranging from cancer diagnosis to open-ended reasoning. MultiPathQA also includes 128 questions, authored by two professional pathologists, requiring direct slide interpretation. Using MultiPathQA, we show that our simple agentic system substantially outperforms conventional patch- and thumbnail-based baselines, approaching or surpassing the performance of specialized models trained on millions of images. For example, on pathologist-authored questions, GPT-5 with GIANT achieves 62.5% accuracy, outperforming specialist pathology models such as TITAN (43.8%) and SlideChat (37.5%). Our findings reveal the strengths and limitations of current foundation models and ground future development of LMMs for expert reasoning in pathology.",
    "pdf_url": "https://arxiv.org/pdf/2511.19652v1",
    "github_url": null,
    "published": "2025-11-24T19:33:56+00:00",
    "updated": "2025-11-24T19:33:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.19628v1",
    "title": "Optimization and Regularization Under Arbitrary Objectives",
    "authors": [
      "Lakhani",
      "Pienaar"
    ],
    "summary": "This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode.",
    "pdf_url": "https://arxiv.org/pdf/2511.19628v1",
    "github_url": null,
    "published": "2025-11-24T19:03:43+00:00",
    "updated": "2025-11-24T19:03:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.19279v2",
    "title": "MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings",
    "authors": [
      "Rambaud",
      "Mascarenhas",
      "Lakretz"
    ],
    "summary": "A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.",
    "pdf_url": "https://arxiv.org/pdf/2511.19279v2",
    "github_url": null,
    "published": "2025-11-24T16:29:02+00:00",
    "updated": "2025-12-12T14:36:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.19551v1",
    "title": "SPARTA: $χ^2$-calibrated, risk-controlled exploration-exploitation for variational quantum algorithms",
    "authors": [
      "Zubarev"
    ],
    "summary": "Variational quantum algorithms face a fundamental trainability crisis: barren plateaus render optimization exponentially difficult as system size grows. While recent Lie algebraic theory precisely characterizes when and why these plateaus occur, no practical optimization method exists with finite-sample guarantees for navigating them. We present the sequential plateau-adaptive regime-testing algorithm (SPARTA), the first measurement-frugal scheduler that provides explicit, anytime-valid risk control for quantum optimization. Our approach integrates three components with rigorous statistical foundations: (i) a $χ^2$-calibrated sequential test that distinguishes barren plateaus from informative regions using likelihood-ratio supermartingales; (ii) a probabilistic trust-region exploration strategy with one-sided acceptance to prevent false improvements under shot noise; and (iii) a theoretically-optimal exploitation phase that achieves the best attainable convergence rate. We prove geometric bounds on plateau exit times, linear convergence in informative basins, and show how Lie-algebraic variance proxies enhance test power without compromising statistical calibration.",
    "pdf_url": "https://arxiv.org/pdf/2511.19551v1",
    "github_url": null,
    "published": "2025-11-24T13:54:01+00:00",
    "updated": "2025-11-24T13:54:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.19033v1",
    "title": "ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay",
    "authors": [
      "Zhang",
      "Ding",
      "Wu"
    ],
    "summary": "Embodied exploration is a target-driven process that requires embodied agents to possess fine-grained perception and knowledge-enhanced decision making. While recent attempts leverage MLLMs for exploration due to their strong perceptual and reasoning abilities, we find that MLLM-based embodied agents remain suboptimal in exploring new environments: (i) they rely on profound but stale pre-trained knowledge, (ii) training-based approaches such as imitation learning or reinforcement learning are expensive for long-horizon tasks with sparse outcome rewards, and (iii) frontier-based exploration yields a large, visually nuanced action space that is difficult for MLLMs to make reliable decisions. We address these challenges with ReEXplore, a training-free framework that performs retrospective experience replay to inject distilled, abstract experience at inference time, and hierarchical frontier selection to decompose frontier ranking into coarse-to-fine decisions. Our approach enables robust, traceable, and efficient exploration. Across multiple embodied exploration benchmarks, ReEXplore yields great improvements over strong MLLM baselines, up to 3x higher performance in both success rate and in navigation efficiency under open-source backbones.",
    "pdf_url": "https://arxiv.org/pdf/2511.19033v1",
    "github_url": null,
    "published": "2025-11-24T12:13:05+00:00",
    "updated": "2025-11-24T12:13:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18878v1",
    "title": "Accelerating Reinforcement Learning via Error-Related Human Brain Signals",
    "authors": [
      "Kim",
      "Shin",
      "Jang"
    ],
    "summary": "In this work, we investigate how implicit neural feed back can accelerate reinforcement learning in complex robotic manipulation settings. While prior electroencephalogram (EEG) guided reinforcement learning studies have primarily focused on navigation or low-dimensional locomotion tasks, we aim to understand whether such neural evaluative signals can improve policy learning in high-dimensional manipulation tasks involving obstacles and precise end-effector control. We integrate error related potentials decoded from offline-trained EEG classifiers into reward shaping and systematically evaluate the impact of human-feedback weighting. Experiments on a 7-DoF manipulator in an obstacle-rich reaching environment show that neural feedback accelerates reinforcement learning and, depending on the human-feedback weighting, can yield task success rates that at times exceed those of sparse-reward baselines. Moreover, when applying the best-performing feedback weighting across all sub jects, we observe consistent acceleration of reinforcement learning relative to the sparse-reward setting. Furthermore, leave-one subject-out evaluations confirm that the proposed framework remains robust despite the intrinsic inter-individual variability in EEG decodability. Our findings demonstrate that EEG-based reinforcement learning can scale beyond locomotion tasks and provide a viable pathway for human-aligned manipulation skill acquisition.",
    "pdf_url": "https://arxiv.org/pdf/2511.18878v1",
    "github_url": null,
    "published": "2025-11-24T08:33:47+00:00",
    "updated": "2025-11-24T08:33:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18868v1",
    "title": "KernelBand: Boosting LLM-based Kernel Optimization with a Hierarchical and Hardware-aware Multi-armed Bandit",
    "authors": [
      "Ran",
      "Xie",
      "Ji"
    ],
    "summary": "High quality kernels are critical for reducing training and inference costs of Large Language Models (LLMs), yet they traditionally require significant expertise in hardware architecture and software optimization. While recent advances in LLM-based code generation show promise for complex optimization, existing methods struggle with the vast optimization space due to insufficient hardware domain knowledge, failing to effectively balance exploration and exploitation. We present KernelBand, a novel framework that formulates kernel optimization as a hierarchical multi-armed bandit problem, enabling LLM agents to strategically navigate the optimization space by treating kernel selection and optimization strategy application as sequential decision-making processes. Our approach leverages hardware profiling information to identify promising optimization strategies and employs runtime behavior clustering to reduce exploration overhead across kernel candidates. Extensive experiments on TritonBench demonstrate that KernelBand significantly outperforms state-of-the-art methods, achieving superior performance with fewer tokens while exhibiting consistent improvement without saturation as computational resources increase.",
    "pdf_url": "https://arxiv.org/pdf/2511.18868v1",
    "github_url": null,
    "published": "2025-11-24T08:11:50+00:00",
    "updated": "2025-11-24T08:11:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18857v1",
    "title": "AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion",
    "authors": [
      "Luo",
      "Wang",
      "Cai"
    ],
    "summary": "Accurate proprioceptive odometry is fundamental for legged robot navigation in GPS-denied and visually degraded environments where conventional visual odometry systems fail. Current approaches face critical limitations: analytical filtering methods suffer from modeling uncertainties and cumulative drift, hybrid learning-filtering approaches remain constrained by their analytical components, while pure learning-based methods struggle with simulation-to-reality transfer and demand extensive real-world data collection. This paper introduces AutoOdom, a novel autoregressive proprioceptive odometry system that overcomes these challenges through an innovative two-stage training paradigm. Stage 1 employs large-scale simulation data to learn complex nonlinear dynamics and rapidly changing contact states inherent in legged locomotion, while Stage 2 introduces an autoregressive enhancement mechanism using limited real-world data to effectively bridge the sim-to-real gap. The key innovation lies in our autoregressive training approach, where the model learns from its own predictions to develop resilience against sensor noise and improve robustness in highly dynamic environments. Comprehensive experimental validation on the Booster T1 humanoid robot demonstrates that AutoOdom significantly outperforms state-of-the-art methods across all evaluation metrics, achieving 57.2% improvement in absolute trajectory error, 59.2% improvement in Umeyama-aligned error, and 36.2% improvement in relative pose error compared to the Legolas baseline. Extensive ablation studies provide critical insights into sensor modality selection and temporal modeling, revealing counterintuitive findings about IMU acceleration data and validating our systematic design choices for robust proprioceptive odometry in challenging locomotion scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2511.18857v1",
    "github_url": null,
    "published": "2025-11-24T07:56:12+00:00",
    "updated": "2025-11-24T07:56:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18849v1",
    "title": "Pre-Filtering Code Suggestions using Developer Behavioral Telemetry to Optimize LLM-Assisted Programming",
    "authors": [
      "Awad",
      "Ivanov",
      "Tikhonova"
    ],
    "summary": "Large Language Models (LLMs) are increasingly integrated into code editors to provide AI-powered code suggestions. Yet many of these suggestions are ignored, resulting in wasted computation, increased latency, and unnecessary interruptions. We introduce a lightweight pre-filtering model that predicts the likelihood of suggestion acceptance before invoking the LLM, using only real-time developer telemetry such as typing speed, file navigation, and editing activity. Deployed in a production-grade Visual Studio Code plugin over four months of naturalistic use, our approach nearly doubled acceptance rates (18.4% -> 34.2%) while suppressing 35% of low-value LLM calls. These findings demonstrate that behavioral signals alone can meaningfully improve both user experience and system efficiency in LLM-assisted programming, highlighting the value of timing-aware, privacy-preserving adaptation mechanisms. The filter operates solely on pre-invocation editor telemetry and never inspects code or prompts.",
    "pdf_url": "https://arxiv.org/pdf/2511.18849v1",
    "github_url": null,
    "published": "2025-11-24T07:42:07+00:00",
    "updated": "2025-11-24T07:42:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18845v1",
    "title": "UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model",
    "authors": [
      "Huang",
      "Tang",
      "Zhan"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires agents to autonomously navigate complex environments via visual images and natural language instruction--remains highly challenging. Recent research on enhancing language-guided navigation reasoning using pre-trained large language models (LLMs) has shown promising prospects. However, the reasoning of such methods is limited to the linguistic modality, lacking visual reasoning capabilities. Moreover, existing reasoning modules are optimized separately from navigation policies, leading to incompatibility and potential conflicts in optimization objectives. To tackle these challenges, we introduce UNeMo, a novel framework designed for the collaborative optimization of visual state reasoning and navigational decision-making. It introduces a Multimodal World Model (MWM) that takes visual features, language instructions, and navigational actions as inputs to jointly predict subsequent visual states, enabling cross-modal reasoning. Via a Hierarchical Prediction-Feedback (HPN) mechanism, MWM collaborates with navigation policies: the first layer generates actions using current vision-and-language features; MWM then infers post-action visual states to guide the second layer's fine-grained decisions. This forms a dynamic bidirectional promotion mechanism where MWM reasoning optimizes navigation policies, while policy decisions feedback to improve MWM's reasoning accuracy. Experiments on R2R and REVERIE datasets show UNeMo outperforms state-of-the-art methods by 2.1% and 0.7% in navigation accuracy for unseen scenes, validating its effectiveness.",
    "pdf_url": "https://arxiv.org/pdf/2511.18845v1",
    "github_url": null,
    "published": "2025-11-24T07:31:58+00:00",
    "updated": "2025-11-24T07:31:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18756v1",
    "title": "SP-VINS: A Hybrid Stereo Visual Inertial Navigation System based on Implicit Environmental Map",
    "authors": [
      "Du",
      "Zhang",
      "Duan"
    ],
    "summary": "Filter-based visual inertial navigation system (VINS) has attracted mobile-robot researchers for the good balance between accuracy and efficiency, but its limited mapping quality hampers long-term high-accuracy state estimation. To this end, we first propose a novel filter-based stereo VINS, differing from traditional simultaneous localization and mapping (SLAM) systems based on 3D map, which performs efficient loop closure constraints with implicit environmental map composed of keyframes and 2D keypoints. Secondly, we proposed a hybrid residual filter framework that combines landmark reprojection and ray constraints to construct a unified Jacobian matrix for measurement updates. Finally, considering the degraded environment, we incorporated the camera-IMU extrinsic parameters into visual description to achieve online calibration. Benchmark experiments demonstrate that the proposed SP-VINS achieves high computational efficiency while maintaining long-term high-accuracy localization performance, and is superior to existing state-of-the-art (SOTA) methods.",
    "pdf_url": "https://arxiv.org/pdf/2511.18756v1",
    "github_url": null,
    "published": "2025-11-24T04:32:19+00:00",
    "updated": "2025-11-24T04:32:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18745v1",
    "title": "On the role of fractional Brownian motion in models of chemotaxis and stochastic gradient ascent",
    "authors": [
      "Cornejo-Olea",
      "Buvinic",
      "Darbon"
    ],
    "summary": "Cell migration often exhibits long-range temporal correlations and anomalous diffusion, even in the absence of external guidance cues such as chemical gradients or topographical constraints. These observations raise a fundamental question: do such correlations simply reflect internal cellular processes, or do they enhance a cell's ability to navigate complex environments? In this work, we explore how temporally correlated noise (modeled using fractional Brownian motion) influences chemotactic search dynamics. Through computational experiments, we show that superdiffusive motion, when combined with gradient-driven migration, enables robust exploration of the chemoattractant landscape. Cells reliably reach the global maximum of the concentration field, even in the presence of spatial noise, secondary cues, or irregular signal geometry. We quantify this behavior by analyzing the distribution of first hitting times under varying degrees of temporal correlation. Notably, our results are consistent across diverse conditions, including flat and curved substrates, and scenarios involving both primary and self-generated chemotactic signals. Beyond biological implications, these findings also offer insight into the design of optimization and sampling algorithms that benefit from structured stochasticity.",
    "pdf_url": "https://arxiv.org/pdf/2511.18745v1",
    "github_url": null,
    "published": "2025-11-24T04:15:32+00:00",
    "updated": "2025-11-24T04:15:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18694v1",
    "title": "Stable Multi-Drone GNSS Tracking System for Marine Robots",
    "authors": [
      "Wen",
      "Meriaux",
      "Guzmán"
    ],
    "summary": "Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.",
    "pdf_url": "https://arxiv.org/pdf/2511.18694v1",
    "github_url": null,
    "published": "2025-11-24T02:28:31+00:00",
    "updated": "2025-11-24T02:28:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18672v1",
    "title": "Sphinx: Efficiently Serving Novel View Synthesis using Regression-Guided Selective Refinement",
    "authors": [
      "Xia",
      "Kundu",
      "Chowdhury"
    ],
    "summary": "Novel View Synthesis (NVS) is the task of generating new images of a scene from viewpoints that were not part of the original input. Diffusion-based NVS can generate high-quality, temporally consistent images, however, remains computationally prohibitive. Conversely, regression-based NVS offers suboptimal generation quality despite requiring significantly lower compute; leaving the design objective of a high-quality, inference-efficient NVS framework an open challenge. To close this critical gap, we present Sphinx, a training-free hybrid inference framework that achieves diffusion-level fidelity at a significantly lower compute. Sphinx proposes to use regression-based fast initialization to guide and reduce the denoising workload for the diffusion model. Additionally, it integrates selective refinement with adaptive noise scheduling, allowing more compute to uncertain regions and frames. This enables Sphinx to provide flexible navigation of the performance-quality trade-off, allowing adaptation to latency and fidelity requirements for dynamically changing inference scenarios. Our evaluation shows that Sphinx achieves an average 1.8x speedup over diffusion model inference with negligible perceptual degradation of less than 5%, establishing a new Pareto frontier between quality and latency in NVS serving.",
    "pdf_url": "https://arxiv.org/pdf/2511.18672v1",
    "github_url": null,
    "published": "2025-11-24T01:09:23+00:00",
    "updated": "2025-11-24T01:09:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18653v1",
    "title": "FHE-Agent: Automating CKKS Configuration for Practical Encrypted Inference via an LLM-Guided Agentic Framework",
    "authors": [
      "Xu",
      "Gong",
      "Ran"
    ],
    "summary": "Fully Homomorphic Encryption (FHE), particularly the CKKS scheme, is a promising enabler for privacy-preserving MLaaS, but its practical deployment faces a prohibitive barrier: it heavily relies on domain expertise. Configuring CKKS involves a tightly coupled space of ring dimensions, modulus chains, and packing layouts. Without deep cryptographic knowledge to navigate these interactions, practitioners are restricted to compilers that rely on fixed heuristics. These \"one-shot\" tools often emit rigid configurations that are either severely over-provisioned in latency or fail to find a feasible solution entirely for deeper networks.   We present FHE-Agent, an agentic framework that automates this expert reasoning process. By coupling a Large Language Model (LLM) controller with a deterministic tool suite, FHE-Agent decomposes the search into global parameter selection and layer-wise bottleneck repair. The agents operate within a multi-fidelity workflow, pruning invalid regimes using cheap static analysis and reserving expensive encrypted evaluations for the most promising candidates.   We instantiate FHE-Agent on the Orion compiler and evaluate it on standard benchmarks (MLP, LeNet, LoLa) and deeper architectures (AlexNet). FHE-Agent consistently achieves better precision and lower latency than naïve search strategies. Crucially, it automatically discovers feasible, 128-bit secure configurations for complex models where baseline heuristics and one-shot prompts fail to produce a valid setup.",
    "pdf_url": "https://arxiv.org/pdf/2511.18653v1",
    "github_url": null,
    "published": "2025-11-23T23:26:21+00:00",
    "updated": "2025-11-23T23:26:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18645v1",
    "title": "A Recommender System Based on Binary Matrix Representations for Cognitive Disorders",
    "authors": [
      "Kutil",
      "Zimmermann",
      "Borgelt"
    ],
    "summary": "Diagnosing cognitive (mental health) disorders is a delicate and complex task. Identifying the next most informative symptoms to assess, in order to distinguish between possible disorders, presents an additional challenge. This process requires comprehensive knowledge of diagnostic criteria and symptom overlap across disorders, making it difficult to navigate based on symptoms alone. This research aims to develop a recommender system for cognitive disorder diagnosis using binary matrix representations. The core algorithm utilizes a binary matrix of disorders and their symptom combinations. It filters through the rows and columns based on the patient's current symptoms to identify potential disorders and recommend the most informative next symptoms to examine. A prototype of the recommender system was implemented in Python. Using synthetic test and some real-life data, the system successfully identified plausible disorders from an initial symptom set and recommended further symptoms to refine the diagnosis. It also provided additional context on the symptom-disorder relationships. Although this is a prototype, the recommender system shows potential as a clinical support tool. A fully-developed application of this recommender system may assist mental health professionals in identifying relevant disorders more efficiently and guiding symptom-specific follow-up investigations to improve diagnostic accuracy.",
    "pdf_url": "https://arxiv.org/pdf/2511.18645v1",
    "github_url": null,
    "published": "2025-11-23T23:04:21+00:00",
    "updated": "2025-11-23T23:04:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18618v1",
    "title": "A Unified BERT-CNN-BiLSTM Framework for Simultaneous Headline Classification and Sentiment Analysis of Bangla News",
    "authors": [
      "Raquib",
      "Akash",
      "Ahmed"
    ],
    "summary": "In our daily lives, newspapers are an essential information source that impacts how the public talks about present-day issues. However, effectively navigating the vast amount of news content from different newspapers and online news portals can be challenging. Newspaper headlines with sentiment analysis tell us what the news is about (e.g., politics, sports) and how the news makes us feel (positive, negative, neutral). This helps us quickly understand the emotional tone of the news. This research presents a state-of-the-art approach to Bangla news headline classification combined with sentiment analysis applying Natural Language Processing (NLP) techniques, particularly the hybrid transfer learning model BERT-CNN-BiLSTM. We have explored a dataset called BAN-ABSA of 9014 news headlines, which is the first time that has been experimented with simultaneously in the headline and sentiment categorization in Bengali newspapers. Over this imbalanced dataset, we applied two experimental strategies: technique-1, where undersampling and oversampling are applied before splitting, and technique-2, where undersampling and oversampling are applied after splitting on the In technique-1 oversampling provided the strongest performance, both headline and sentiment, that is 78.57\\% and 73.43\\% respectively, while technique-2 delivered the highest result when trained directly on the original imbalanced dataset, both headline and sentiment, that is 81.37\\% and 64.46\\% respectively. The proposed model BERT-CNN-BiLSTM significantly outperforms all baseline models in classification tasks, and achieves new state-of-the-art results for Bangla news headline classification and sentiment analysis. These results demonstrate the importance of leveraging both the headline and sentiment datasets, and provide a strong baseline for Bangla text classification in low-resource.",
    "pdf_url": "https://arxiv.org/pdf/2511.18618v1",
    "github_url": null,
    "published": "2025-11-23T21:22:56+00:00",
    "updated": "2025-11-23T21:22:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18609v1",
    "title": "Universality in Collective Intelligence on the Rubik's Cube",
    "authors": [
      "Krakauer",
      "Kardeş",
      "Grochow"
    ],
    "summary": "Progress in understanding expert performance is limited by the scarcity of quantitative data on long-term knowledge acquisition and deployment. Here we use the Rubik's Cube as a cognitive model system existing at the intersection of puzzle solving, skill learning, expert knowledge, cultural transmission, and group theory. By studying competitive cube communities, we find evidence for universality in the collective learning of the Rubik's Cube in both sighted and blindfolded conditions: expert performance follows exponential progress curves whose parameters reflect the delayed acquisition of algorithms that shorten solution paths. Blindfold solves form a distinct problem class from sighted solves and are constrained not only by expert knowledge but also by the skill improvements required to overcome short-term memory bottlenecks, a constraint shared with blindfold chess. Cognitive artifacts such as the Rubik's Cube help solvers navigate an otherwise enormous mathematical state space. In doing so, they sustain collective intelligence by integrating communal knowledge stores with individual expertise and skill, illustrating how expertise can, in practice, continue to deepen over the course of a single lifetime.",
    "pdf_url": "https://arxiv.org/pdf/2511.18609v1",
    "github_url": null,
    "published": "2025-11-23T20:30:38+00:00",
    "updated": "2025-11-23T20:30:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.19504v1",
    "title": "Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma",
    "authors": [
      "Sahoo",
      "Chadha",
      "Jain"
    ],
    "summary": "Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.",
    "pdf_url": "https://arxiv.org/pdf/2511.19504v1",
    "github_url": null,
    "published": "2025-11-23T20:23:23+00:00",
    "updated": "2025-11-23T20:23:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.05128v1",
    "title": "GNSS Jammer Direction Finding in Dynamic Scenarios Using an Inertial-based Multi-Antenna System",
    "authors": [
      "Heublein",
      "Nowak",
      "Feigl"
    ],
    "summary": "Jamming devices disrupt signals from the global navigation satellite system (GNSS) and pose a significant threat by compromising the reliability of accurate positioning. Consequently, the detection and localization of these interference signals are essential to achieve situational awareness, mitigating their impact, and implementing effective countermeasures. In this paper, we utilize a two-times-two patch antenna system (i.e., the software defined radio device Ettus USRP X440) to predict the angle, elevation, and distance to the jamming source based on in-phase and quadrature (IQ) samples. We propose to use an inertial measurement unit (IMU) attached to the antenna system to predict the relative movement of the antenna in dynamic scenarios. We present a synthetic aperture system that enables coherent spatial imaging using platform motion to synthesize larger virtual apertures, offering superior angular resolution without mechanically rotating antennas. While classical angle-of-arrival (AoA) methods exhibit reduced accuracy in multipath environments due to signal reflections and scattering, leading to localization errors, we utilize a methodology that fuses IQ and Fast Fourier Transform (FFT)-computed spectrograms with 22 AoA features and the predicted relative movement to enhance GNSS jammer direction finding.",
    "pdf_url": "https://arxiv.org/pdf/2512.05128v1",
    "github_url": null,
    "published": "2025-11-23T20:12:36+00:00",
    "updated": "2025-11-23T20:12:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18589v1",
    "title": "Strategic Decision Framework for Enterprise LLM Adoption",
    "authors": [
      "Trusov",
      "Hwang",
      "Jamal"
    ],
    "summary": "Organizations are rapidly adopting Large Language Models (LLMs) to transform their operations, yet they lack clear guidance on key decisions for adoption and implementation. While LLMs offer powerful capabilities in content generation, assisted coding, and process automation, businesses face critical challenges in data security, LLM solution development approach, infrastructure requirements, and deployment strategies. Healthcare providers must protect patient data while leveraging LLMs for medical analysis, financial institutions need to balance automated customer service with regulatory compliance, and software companies seek to enhance development productivity while maintaining code security.   This article presents a systematic six-step decision framework for LLM adoption, helping organizations navigate from initial application selection to final deployment. Based on extensive interviews and analysis of successful and failed implementations, our framework provides practical guidance for business leaders to align technological capabilities with business objectives. Through key decision points and real-world examples from both B2B and B2C contexts, organizations can make informed decisions about LLM adoption while ensuring secure and efficient integration across various use cases, from customer service automation to content creation and advanced analytics.",
    "pdf_url": "https://arxiv.org/pdf/2511.18589v1",
    "github_url": null,
    "published": "2025-11-23T19:05:52+00:00",
    "updated": "2025-11-23T19:05:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18525v1",
    "title": "Splatblox: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation",
    "authors": [
      "Chopra",
      "Liang",
      "Seneviratne"
    ],
    "summary": "We present Splatblox, a real-time system for autonomous navigation in outdoor environments with dense vegetation, irregular obstacles, and complex terrain. Our method fuses segmented RGB images and LiDAR point clouds using Gaussian Splatting to construct a traversability-aware Euclidean Signed Distance Field (ESDF) that jointly encodes geometry and semantics. Updated online, this field enables semantic reasoning to distinguish traversable vegetation (e.g., tall grass) from rigid obstacles (e.g., trees), while LiDAR ensures 360-degree geometric coverage for extended planning horizons. We validate Splatblox on a quadruped robot and demonstrate transfer to a wheeled platform. In field trials across vegetation-rich scenarios, it outperforms state-of-the-art methods with over 50% higher success rate, 40% fewer freezing incidents, 5% shorter paths, and up to 13% faster time to goal, while supporting long-range missions up to 100 meters. Experiment videos and more details can be found on our project page: https://splatblox.github.io",
    "pdf_url": "https://arxiv.org/pdf/2511.18525v1",
    "github_url": null,
    "published": "2025-11-23T16:35:51+00:00",
    "updated": "2025-11-23T16:35:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18486v1",
    "title": "Expanding the Workspace of Electromagnetic Navigation Systems Using Dynamic Feedback for Single- and Multi-agent Control",
    "authors": [
      "Zughaibi",
      "Arx",
      "Derungs"
    ],
    "summary": "Electromagnetic navigation systems (eMNS) enable a number of magnetically guided surgical procedures. A challenge in magnetically manipulating surgical tools is that the effective workspace of an eMNS is often severely constrained by power and thermal limits. We show that system-level control design significantly expands this workspace by reducing the currents needed to achieve a desired motion. We identified five key system approaches that enable this expansion: (i) motion-centric torque/force objectives, (ii) energy-optimal current allocation, (iii) real-time pose estimation, (iv) dynamic feedback, and (v) high-bandwidth eMNS components. As a result, we stabilize a 3D inverted pendulum on an eight-coil OctoMag eMNS with significantly lower currents (0.1-0.2 A vs. 8-14 A), by replacing a field-centric field-alignment strategy with a motion-centric torque/force-based approach. We generalize to multi-agent control by simultaneously stabilizing two inverted pendulums within a shared workspace, exploiting magnetic-field nonlinearity and coil redundancy for independent actuation. A structured analysis compares the electromagnetic workspaces of both paradigms and examines current-allocation strategies that map motion objectives to coil currents. Cross-platform evaluation of the clinically oriented Navion eMNS further demonstrates substantial workspace expansion by maintaining stable balancing at distances up to 50 cm from the coils. The results demonstrate that feedback is a practical path to scalable, efficient, and clinically relevant magnetic manipulation.",
    "pdf_url": "https://arxiv.org/pdf/2511.18486v1",
    "github_url": null,
    "published": "2025-11-23T15:14:39+00:00",
    "updated": "2025-11-23T15:14:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18236v2",
    "title": "APULSE: A Scalable Hybrid Algorithm for the RCSPP on Large-Scale Dense Graphs",
    "authors": [
      "Soares",
      "Grilo"
    ],
    "summary": "The resource-constrained shortest path problem (RCSPP) is a fundamental NP-hard optimization challenge with broad applications, from network routing to autonomous navigation. This problem involves finding a path that minimizes a primary cost subject to a budget on a secondary resource. While various RCSPP solvers exist, they often face critical scalability limitations when applied to the large, dense graphs characteristic of complex, real-world scenarios, making them impractical for time-critical planning. This challenge is particularly acute in domains like mission planning for unmanned ground vehicles (UGVs), which demand solutions on large-scale terrain graphs. This paper introduces APULSE, a hybrid label-setting algorithm designed to efficiently solve the RCSPP on such challenging graphs. APULSE integrates a best-first search guided by an A* heuristic with aggressive, Pulse-style pruning mechanisms and a time-bucketing strategy for effective state-space reduction. A computational study, using a large-scale UGV planning scenario, benchmarks APULSE against state-of-the-art algorithms. The results demonstrate that APULSE consistently finds near-optimal solutions while being orders of magnitude faster and more robust, particularly on large problem instances where competing methods fail. This superior scalability establishes APULSE as an effective solution for RCSPP in complex, large-scale environments, enabling capabilities such as interactive decision support and dynamic replanning.",
    "pdf_url": "https://arxiv.org/pdf/2511.18236v2",
    "github_url": null,
    "published": "2025-11-23T00:49:33+00:00",
    "updated": "2025-11-30T02:51:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18183v2",
    "title": "Off-Road Navigation via Implicit Neural Representation of Terrain Traversability",
    "authors": [
      "Jia",
      "Li",
      "How"
    ],
    "summary": "Autonomous off-road navigation requires robots to estimate terrain traversability from onboard sensors and plan accordingly. Conventional approaches typically rely on sampling-based planners such as MPPI to generate short-term control actions that aim to minimize traversal time and risk measures derived from the traversability estimates. These planners can react quickly but optimize only over a short look-ahead window, limiting their ability to reason about the full path geometry, which is important for navigating in challenging off-road environments. Moreover, they lack the ability to adjust speed based on the terrain bumpiness, which is important for smooth navigation on challenging terrains. In this paper, we introduce TRAIL (Traversability with an Implicit Learned Representation), an off-road navigation framework that leverages an implicit neural representation to continuously parameterize terrain properties. This representation yields spatial gradients that enable integration with a novel gradient-based trajectory optimization method that adapts the path geometry and speed profile based on terrain traversability.",
    "pdf_url": "https://arxiv.org/pdf/2511.18183v2",
    "github_url": null,
    "published": "2025-11-22T20:39:31+00:00",
    "updated": "2025-12-17T05:05:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18182v1",
    "title": "The Workflow as Medium: A Framework for Navigating Human-AI Co-Creation",
    "authors": [
      "Ackerman"
    ],
    "summary": "This paper introduces the Creative Intelligence Loop (CIL), a novel socio-technical framework for responsible human-AI co-creation. Rooted in the 'Workflow as Medium' paradigm, the CIL proposes a disciplined structure for dynamic human-AI collaboration, guiding the strategic integration of diverse AI teammates who function as collaborators while the human remains the final arbiter for ethical alignment and creative integrity. The CIL was empirically demonstrated through the practice-led creation of two graphic novellas, investigating how AI could serve as an effective creative colleague within a subjective medium lacking objective metrics. The process required navigating multifaceted challenges including AI's 'jagged frontier' of capabilities, sycophancy, and attention-scarce feedback environments. This prompted iterative refinement of teaming practices, yielding emergent strategies: a multi-faceted critique system integrating adversarial AI roles to counter sycophancy, and prioritizing 'feedback-ready' concrete artifacts to elicit essential human critique. The resulting graphic novellas analyze distinct socio-technical governance failures: 'The Steward' examines benevolent AI paternalism in smart cities, illustrating how algorithmic hubris can erode freedom; 'Fork the Vote' probes democratic legitimacy by comparing centralized AI opacity with emergent collusion in federated networks. This work contributes a self-improving framework for responsible human-AI co-creation and two graphic novellas designed to foster AI literacy and dialogue through accessible narrative analysis of AI's societal implications.",
    "pdf_url": "https://arxiv.org/pdf/2511.18182v1",
    "github_url": null,
    "published": "2025-11-22T20:36:13+00:00",
    "updated": "2025-11-22T20:36:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18170v1",
    "title": "Time-aware Motion Planning in Dynamic Environments with Conformal Prediction",
    "authors": [
      "Liang",
      "Luo",
      "Wang"
    ],
    "summary": "Safe navigation in dynamic environments remains challenging due to uncertain obstacle behaviors and the lack of formal prediction guarantees. We propose two motion planning frameworks that leverage conformal prediction (CP): a global planner that integrates Safe Interval Path Planning (SIPP) for uncertainty-aware trajectory generation, and a local planner that performs online reactive planning. The global planner offers distribution-free safety guarantees for long-horizon navigation, while the local planner mitigates inaccuracies in obstacle trajectory predictions through adaptive CP, enabling robust and responsive motion in dynamic environments. To further enhance trajectory feasibility, we introduce an adaptive quantile mechanism in the CP-based uncertainty quantification. Instead of using a fixed confidence level, the quantile is automatically tuned to the optimal value that preserves trajectory feasibility, allowing the planner to adaptively tighten safety margins in regions with higher uncertainty. We validate the proposed framework through numerical experiments conducted in dynamic and cluttered environments. The project page is available at https://time-aware-planning.github.io",
    "pdf_url": "https://arxiv.org/pdf/2511.18170v1",
    "github_url": null,
    "published": "2025-11-22T19:51:10+00:00",
    "updated": "2025-11-22T19:51:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18151v1",
    "title": "AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems",
    "authors": [
      "Bhattacharjya",
      "Wu",
      "Oh"
    ],
    "summary": "Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution \"context stream\" for real-time awareness and a low-frequency, high-fidelity \"insight stream\" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.18151v1",
    "github_url": null,
    "published": "2025-11-22T18:42:04+00:00",
    "updated": "2025-11-22T18:42:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18112v1",
    "title": "EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation",
    "authors": [
      "Lin",
      "Liang",
      "Lin"
    ],
    "summary": "Recent progress in Vision-Language-Action (VLA) models has enabled embodied agents to interpret multimodal instructions and perform complex tasks. However, existing VLAs are mostly confined to short-horizon, table-top manipulation, lacking the memory and reasoning capability required for long-horizon mobile manipulation, where agents must coordinate navigation and manipulation under changing spatial contexts. In this work, we present EchoVLA, a memory-aware VLA model for long-horizon mobile manipulation. EchoVLA incorporates a synergistic declarative memory inspired by the human brain, consisting of a scene memory that maintains a collection of spatial-semantic maps and an episodic memory that stores task-level experiences with multimodal contextual features. During both training and inference, the two memories are individually stored, updated, and retrieved based on current observations, task history, and instructions, and their retrieved representations are fused via coarse- and fine-grained attention to guide mobile-arm diffusion policies. To support large-scale training and evaluation, we further introduce MoMani, an automated benchmark that generates expert-level long-horizon trajectories through multimodal large language model (MLLM)-guided planning and feedback-driven refinement, supplemented with real-robot demonstrations. Experiments in simulated and real-world settings show that EchoVLA improves long-horizon performance, reaching 0.52 SR on manipulation/navigation and 0.31 on mobile manipulation, exceeding $π_{0.5}$ by +0.08 and +0.11.",
    "pdf_url": "https://arxiv.org/pdf/2511.18112v1",
    "github_url": null,
    "published": "2025-11-22T16:30:55+00:00",
    "updated": "2025-11-22T16:30:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.18039v1",
    "title": "Curvature-Aware Safety Restoration In LLMs Fine-Tuning",
    "authors": [
      "Bach",
      "Nguyen-Tang",
      "Nguyen"
    ],
    "summary": "Fine-tuning Large Language Models (LLMs) for downstream tasks often compromises safety alignment, even when using parameter-efficient methods like LoRA. In this work, we uncover a notable property: fine-tuned models preserve the geometric structure of their loss landscapes concerning harmful content, regardless of the fine-tuning method employed. This suggests that safety behaviors are not erased but shifted to less influential regions of the parameter space. Building on this insight, we propose a curvature-aware alignment restoration method that leverages influence functions and second-order optimization to selectively increase loss on harmful inputs while preserving task performance. By navigating the shared geometry between base and fine-tuned models, our method discourages unsafe outputs while preserving task-relevant performance, avoiding full reversion and enabling precise, low-impact updates. Extensive evaluations across multiple model families and adversarial settings show that our approach efficiently reduces harmful responses while maintaining or even improving utility and few-shot learning performance.",
    "pdf_url": "https://arxiv.org/pdf/2511.18039v1",
    "github_url": null,
    "published": "2025-11-22T12:33:31+00:00",
    "updated": "2025-11-22T12:33:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17999v1",
    "title": "A Targeted Quadrature Framework for Simulating Large-Scale 3D Anisotropic Electromagnetic Measurements",
    "authors": [
      "Zimmerling",
      "Druskin",
      "Davydycheva"
    ],
    "summary": "We develop a new, efficient, and accurate method to simulate frequency-domain borehole electromagnetic (EM) measurements acquired in the presence of three-dimensional (3D) variations of the anisotropic subsurface conductivity. The method is based on solving the quasi-static Maxwell equations with a goal-oriented finite-volume discretization via block-quadrature reduced-order modeling. Discretization is performed with a Lebedev grid that enables accurate and conservative solutions in the presence of any form of anisotropic electrical conductivity. Likewise, the method makes use of a new effective-medium approximation to locally account for non-conformal boundaries and large contrasts in electrical conductivity, especially in the vicinity of EM sources and receivers. The finite-volume discretization yields a large symmetric linear system of equations, which is reduced to a set of smaller structured problems via block Lanczos recursion. The formulation also enables the efficient calculation of the adjoint solution, which is necessary for gradient-based inversion of the measurements to estimate the associated spatial distribution of electrical conductivity, i.e., to solve the inverse problem. Specific applications and verifications of the new numerical simulation algorithm are considered for the case of borehole ultra-deep azimuthal resistivity measurements (UDAR) typically used for subsurface well geosteering and navigation. We verify the efficiency, robustness, and scalability of this approach using synthetic UDAR measurements acquired in a 3D formation inspired by North-Sea geology. The numerical experiments successfully verify the applicability of our modeling approach to real-time UDAR processing frameworks.",
    "pdf_url": "https://arxiv.org/pdf/2511.17999v1",
    "github_url": null,
    "published": "2025-11-22T09:43:30+00:00",
    "updated": "2025-11-22T09:43:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17992v1",
    "title": "Unobservable Subspace Evolution and Alignment for Consistent Visual-Inertial Navigation",
    "authors": [
      "Tian",
      "He",
      "Hao"
    ],
    "summary": "The inconsistency issue in the Visual-Inertial Navigation System (VINS) is a long-standing and fundamental challenge. While existing studies primarily attribute the inconsistency to observability mismatch, these analyses are often based on simplified theoretical formulations that consider only prediction and SLAM correction. Such formulations fail to cover the non-standard estimation steps, such as MSCKF correction and delayed initialization, which are critical for practical VINS estimators. Furthermore, the lack of a comprehensive understanding of how inconsistency dynamically emerges across estimation steps has hindered the development of precise and efficient solutions. As a result, current approaches often face a trade-off between estimator accuracy, consistency, and implementation complexity. To address these limitations, this paper proposes a novel analysis framework termed Unobservable Subspace Evolution (USE), which systematically characterizes how the unobservable subspace evolves throughout the entire estimation pipeline by explicitly tracking changes in its evaluation points. This perspective sheds new light on how individual estimation steps contribute to inconsistency. Our analysis reveals that observability misalignment induced by certain steps is the antecedent of observability mismatch. Guided by this insight, we propose a simple yet effective solution paradigm, Unobservable Subspace Alignment (USA), which eliminates inconsistency by selectively intervening only in those estimation steps that induce misalignment. We design two USA methods: transformation-based and re-evaluation-based, both offering accurate and computationally lightweight solutions. Extensive simulations and real-world experiments validate the effectiveness of the proposed methods.",
    "pdf_url": "https://arxiv.org/pdf/2511.17992v1",
    "github_url": null,
    "published": "2025-11-22T09:19:08+00:00",
    "updated": "2025-11-22T09:19:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.19472v2",
    "title": "PrefixGPT: Prefix Adder Optimization by a Generative Pre-trained Transformer",
    "authors": [
      "Ding",
      "Ning",
      "Schlichtmann"
    ],
    "summary": "Prefix adders are widely used in compute-intensive applications for their high speed. However, designing optimized prefix adders is challenging due to strict design rules and an exponentially large design space. We introduce PrefixGPT, a generative pre-trained Transformer (GPT) that directly generates optimized prefix adders from scratch. Our approach represents an adder's topology as a two-dimensional coordinate sequence and applies a legality mask during generation, ensuring every design is valid by construction. PrefixGPT features a customized decoder-only Transformer architecture. The model is first pre-trained on a corpus of randomly synthesized valid prefix adders to learn design rules and then fine-tuned to navigate the design space for optimized design quality. Compared with existing works, PrefixGPT not only finds a new optimal design with a 7.7% improved area-delay product (ADP) but exhibits superior exploration quality, lowering the average ADP by up to 79.1%. This demonstrates the potential of GPT-style models to first master complex hardware design principles and then apply them for more efficient design optimization.",
    "pdf_url": "https://arxiv.org/pdf/2511.19472v2",
    "github_url": null,
    "published": "2025-11-22T06:43:43+00:00",
    "updated": "2025-11-26T02:21:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17939v1",
    "title": "Neural Graph Navigation for Intelligent Subgraph Matching",
    "authors": [
      "Ying",
      "Dai",
      "Li"
    ],
    "summary": "Subgraph matching, a cornerstone of relational pattern detection in domains ranging from biochemical systems to social network analysis, faces significant computational challenges due to the dramatically growing search space. Existing methods address this problem within a filtering-ordering-enumeration framework, in which the enumeration stage recursively matches the query graph against the candidate subgraphs of the data graph. However, the lack of awareness of subgraph structural patterns leads to a costly brute-force enumeration, thereby critically motivating the need for intelligent navigation in subgraph matching. To address this challenge, we propose Neural Graph Navigation (NeuGN), a neuro-heuristic framework that transforms brute-force enumeration into neural-guided search by integrating neural navigation mechanisms into the core enumeration process. By preserving heuristic-based completeness guarantees while incorporating neural intelligence, NeuGN significantly reduces the \\textit{First Match Steps} by up to 98.2\\% compared to state-of-the-art methods across six real-world datasets.",
    "pdf_url": "https://arxiv.org/pdf/2511.17939v1",
    "github_url": null,
    "published": "2025-11-22T06:40:46+00:00",
    "updated": "2025-11-22T06:40:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17889v1",
    "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots",
    "authors": [
      "Huang",
      "Li",
      "Yang"
    ],
    "summary": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.",
    "pdf_url": "https://arxiv.org/pdf/2511.17889v1",
    "github_url": "https://github.com/AIGeeksGroup/MobileVLA-R1",
    "published": "2025-11-22T02:34:10+00:00",
    "updated": "2025-11-22T02:34:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17833v2",
    "title": "Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures",
    "authors": [
      "Bai",
      "Ren"
    ],
    "summary": "Debugging is the dominant cost in modern hardware verification, where assertion failures are among the most frequent and expensive to resolve. While Large Language Models (LLMs) show promise, they often fail to capture the precise, reusable expertise that engineers apply, leading to inaccurate responses. We propose GROVE, a hierarchical knowledge management framework that learns and organizes reusable debugging expertise into an LLM-organized knowledge tree for solving assertion failures. GROVE distills debugging knowledge from prior cases and organizes it into a vertical tree of configurable depth, with each node encoding a concise knowledge item and explicit applicability conditions. During training, GROVE uses a parallel, gradient-free loop where an LLM proposes tree modifications as structured JSON edits by learning from the cases. At test time, a budget-aware iterative zoom is performed to navigate the tree, retrieving a small set of applicable knowledge items that guide a base LLM's hypothesis generation and fix proposals. Evaluated on a suite of assertion-failure cases, GROVE delivers consistent gains in pass@1 and pass@5, demonstrating the value of structured knowledge evolution.",
    "pdf_url": "https://arxiv.org/pdf/2511.17833v2",
    "github_url": null,
    "published": "2025-11-21T22:57:45+00:00",
    "updated": "2025-12-13T00:41:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17798v1",
    "title": "SM2ITH: Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control",
    "authors": [
      "D'Orazio",
      "Samavi",
      "Du"
    ],
    "summary": "Mobile manipulators are designed to perform complex sequences of navigation and manipulation tasks in human-centered environments. While recent optimization-based methods such as Hierarchical Task Model Predictive Control (HTMPC) enable efficient multitask execution with strict task priorities, they have so far been applied mainly to static or structured scenarios. Extending these approaches to dynamic human-centered environments requires predictive models that capture how humans react to the actions of the robot. This work introduces Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control (SM$^2$ITH), a unified framework that combines HTMPC with interactive human motion prediction through bilevel optimization that jointly accounts for robot and human dynamics. The framework is validated on two different mobile manipulators, the Stretch 3 and the Ridgeback-UR10, across three experimental settings: (i) delivery tasks with different navigation and manipulation priorities, (ii) sequential pick-and-place tasks with different human motion prediction models, and (iii) interactions involving adversarial human behavior. Our results highlight how interactive prediction enables safe and efficient coordination, outperforming baselines that rely on weighted objectives or open-loop human models.",
    "pdf_url": "https://arxiv.org/pdf/2511.17798v1",
    "github_url": null,
    "published": "2025-11-21T21:42:59+00:00",
    "updated": "2025-11-21T21:42:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17781v1",
    "title": "SAFE-SMART: Safety Analysis and Formal Evaluation using STL Metrics for Autonomous RoboTs",
    "authors": [
      "Sakano",
      "An",
      "Manocha"
    ],
    "summary": "We present a novel, regulator-driven approach for post hoc safety evaluation of learning-based, black-box autonomous mobile robots, ensuring ongoing compliance with evolving, human-defined safety rules. In our iterative workflow, human safety requirements are translated by regulators into Signal Temporal Logic (STL) specifications. Rollout traces from the black-box model are externally verified for compliance, yielding quantitative safety metrics, Total Robustness Value (TRV) and Largest Robustness Value (LRV), which measure average and worst-case specification adherence. These metrics inform targeted retraining and iterative improvement by model designers. We apply our method across two different applications: a virtual driving scenario and an autonomous mobile robot navigating a complex environment, and observe statistically significant improvements across both scenarios. In the virtual driving scenario, we see a 177% increase in traces adhering to the simulation speed limit, a 1138% increase in traces minimizing off-road driving, and a 16% increase in traces successfully reaching the goal within the time limit. In the autonomous navigation scenario, there is a 300% increase in traces avoiding sharp turns, a 200% increase in traces reaching the goal within the time limit, and a 49% increase in traces minimizing time spent near obstacles. Finally, we validate our approach on a TurtleBot3 robot in the real world, and demonstrate improved obstacle navigation with safety buffers.",
    "pdf_url": "https://arxiv.org/pdf/2511.17781v1",
    "github_url": null,
    "published": "2025-11-21T20:58:27+00:00",
    "updated": "2025-11-21T20:58:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17765v1",
    "title": "LEARN: Learning End-to-End Aerial Resource-Constrained Multi-Robot Navigation",
    "authors": [
      "Chiu",
      "Huang",
      "Ge"
    ],
    "summary": "Nano-UAV teams offer great agility yet face severe navigation challenges due to constrained onboard sensing, communication, and computation. Existing approaches rely on high-resolution vision or compute-intensive planners, rendering them infeasible for these platforms. We introduce LEARN, a lightweight, two-stage safety-guided reinforcement learning (RL) framework for multi-UAV navigation in cluttered spaces. Our system combines low-resolution Time-of-Flight (ToF) sensors and a simple motion planner with a compact, attention-based RL policy. In simulation, LEARN outperforms two state-of-the-art planners by $10\\%$ while using substantially fewer resources. We demonstrate LEARN's viability on six Crazyflie quadrotors, achieving fully onboard flight in diverse indoor and outdoor environments at speeds up to $2.0 m/s$ and traversing $0.2 m$ gaps.",
    "pdf_url": "https://arxiv.org/pdf/2511.17765v1",
    "github_url": null,
    "published": "2025-11-21T20:29:03+00:00",
    "updated": "2025-11-21T20:29:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17720v1",
    "title": "Vision-Guided Optic Flow Navigation for Small Lunar Missions",
    "authors": [
      "Cowan",
      "Fanti",
      "Williams"
    ],
    "summary": "Private lunar missions are faced with the challenge of robust autonomous navigation while operating under stringent constraints on mass, power, and computational resources. This work proposes a motion-field inversion framework that uses optical flow and rangefinder-based depth estimation as a lightweight CPU-based solution for egomotion estimation during lunar descent. We extend classical optical flow formulations by integrating them with depth modeling strategies tailored to the geometry for lunar/planetary approach, descent, and landing, specifically, planar and spherical terrain approximations parameterized by a laser rangefinder. Motion field inversion is performed through a least-squares framework, using sparse optical flow features extracted via the pyramidal Lucas-Kanade algorithm. We verify our approach using synthetically generated lunar images over the challenging terrain of the lunar south pole, using CPU budgets compatible with small lunar landers. The results demonstrate accurate velocity estimation from approach to landing, with sub-10% error for complex terrain and on the order of 1% for more typical terrain, as well as performances suitable for real-time applications. This framework shows promise for enabling robust, lightweight on-board navigation for small lunar missions.",
    "pdf_url": "https://arxiv.org/pdf/2511.17720v1",
    "github_url": null,
    "published": "2025-11-21T19:16:37+00:00",
    "updated": "2025-11-21T19:16:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17497v1",
    "title": "HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation",
    "authors": [
      "Tao",
      "Ong",
      "Cladera"
    ],
    "summary": "We demonstrate real-time high-altitude aerial metric-semantic mapping and exploration using a monocular camera paired with a global positioning system (GPS) and an inertial measurement unit (IMU). Our system, named HALO, addresses two key challenges: (i) real-time dense 3D reconstruction using vision at large distances, and (ii) mapping and exploration of large-scale outdoor environments with accurate scene geometry and semantics. We demonstrate that HALO can plan informative paths that exploit this information to complete missions with multiple tasks specified in natural language. In simulation-based evaluation across large-scale environments of size up to 78,000 sq. m., HALO consistently completes tasks with less exploration time and achieves up to 68% higher competitive ratio in terms of the distance traveled compared to the state-of-the-art semantic exploration baseline. We use real-world experiments on a custom quadrotor platform to demonstrate that (i) all modules can run onboard the robot, and that (ii) in diverse environments HALO can support effective autonomous execution of missions covering up to 24,600 sq. m. area at an altitude of 40 m. Experiment videos and more details can be found on our project page: https://tyuezhan.github.io/halo/.",
    "pdf_url": "https://arxiv.org/pdf/2511.17497v1",
    "github_url": null,
    "published": "2025-11-21T18:55:02+00:00",
    "updated": "2025-11-21T18:55:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17429v1",
    "title": "Semantic and Semiotic Interplays in Text-to-Audio AI: Exploring Cognitive Dynamics and Musical Interactions",
    "authors": [
      "Coelho"
    ],
    "summary": "This paper investigates the emerging text-to-audio paradigm in artificial intelligence (AI), examining its transformative implications for musical creation, interpretation, and cognition. I explore the complex semantic and semiotic interplays that occur when descriptive natural language prompts are translated into nuanced sound objects across the text-to-audio modality. Drawing from structuralist and post-structuralist perspectives, as well as cognitive theories of schema dynamics and metacognition, the paper explores how these AI systems reconfigure musical signification processes and navigate established cognitive frameworks. The research analyzes some of the cognitive dynamics at play in AI-mediated musicking, including processes of schema assimilation and accommodation, metacognitive reflection, and constructive perception. The paper argues that text-to-audio AI models function as quasi-objects of musical signification, simultaneously stabilizing and destabilizing conventional forms while fostering new modes of listening and aesthetic reflexivity.Using Udio as a primary case study, this study explores how these models navigate the liminal spaces between linguistic prompts and sonic outputs. This process not only generates novel musical expressions but also prompts listeners to engage in forms of critical and \"structurally-aware listening.\", encouraging a deeper understanding of music's structures, semiotic nuances, and the socio-cultural contexts that shape our musical cognition. The paper concludes by reflecting on the potential of text-to-audio AI models to serve as epistemic tools and quasi-objects, facilitating a significant shift in musical interactions and inviting users to develop a more nuanced comprehension of the cognitive and cultural foundations of music.",
    "pdf_url": "https://arxiv.org/pdf/2511.17429v1",
    "github_url": null,
    "published": "2025-11-21T17:24:22+00:00",
    "updated": "2025-11-21T17:24:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17404v1",
    "title": "The Artist is Present: Traces of Artists Resigind and Spawning in Text-to-Audio AI",
    "authors": [
      "Coelho"
    ],
    "summary": "Text-to-audio (TTA) systems are rapidly transforming music creation and distribution, with platforms like Udio and Suno generating thousands of tracks daily and integrating into mainstream music platforms and ecosystems. These systems, trained on vast and largely undisclosed datasets, are fundamentally reshaping how music is produced, reproduced and consumed. This paper presents empirical evidence that artist-conditioned regions can be systematically microlocated through metatag-based prompt design, effectively enabling the spawning of artist-like content through strategic prompt engineering. Through systematic exploration of metatag-based prompt engineering techniques this research reveals how users can access the distinctive sonic signatures of specific artists, evidencing their inclusion in training datasets. Using descriptor constellations drawn from public music taxonomies, the paper demonstrates reproducible proximity to artists such as Bon Iver, Philip Glass, Panda Bear and William Basinski. The results indicate stable text-audio correspondences consistent with artist-specific training signals, enabling precise traversal of stylistic microlocations without explicitly naming artists. This capacity to summon artist-specific outputs shows that artists' creative works fuction as foundational material from which these systems generate new content, often without explicit consent or attribuition. Conceptually, the work clarifies how textual descriptors act as navigational cues in high-dimensional representation spaces; methodologically, it provides a replicable protocol for auditing stylistic inducibility. The findings raise immediate queestions for governance-attribution, consent and disclosure standards-and for creative practice, where induced stylistic proximity complicates boundaries between ownership, reproduction, imitation, creative agency and the ethics of algorithmic creation.",
    "pdf_url": "https://arxiv.org/pdf/2511.17404v1",
    "github_url": null,
    "published": "2025-11-21T17:05:50+00:00",
    "updated": "2025-11-21T17:05:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17401v1",
    "title": "Feasibility of Embodied Dynamics Based Bayesian Learning for Continuous Pursuit Motion Control of Assistive Mobile Robots in the Built Environment",
    "authors": [
      "Zhou",
      "Menassa",
      "Kamat"
    ],
    "summary": "Non-invasive electroencephalography (EEG)-based brain-computer interfaces (BCIs) offer an intuitive means for individuals with severe motor impairments to independently operate assistive robotic wheelchairs and navigate built environments. Despite considerable progress in BCI research, most current motion control systems are limited to discrete commands, rather than supporting continuous pursuit, where users can freely adjust speed and direction in real time. Such natural mobility control is, however, essential for wheelchair users to navigate complex public spaces, such as transit stations, airports, hospitals, and indoor corridors, to interact socially with the dynamic populations with agility, and to move flexibly and comfortably as autonomous driving is refined to allow movement at will. In this study, we address the gap of continuous pursuit motion control in BCIs by proposing and validating a brain-inspired Bayesian inference framework, where embodied dynamics in acceleration-based motor representations are decoded. This approach contrasts with conventional kinematics-level decoding and deep learning-based methods. Using a public dataset with sixteen hours of EEG from four subjects performing motor imagery-based target-following, we demonstrate that our method, utilizing Automatic Relevance Determination for feature selection and continual online learning, reduces the normalized mean squared error between predicted and true velocities by 72% compared to autoregressive and EEGNet-based methods in a session-accumulative transfer learning setting. Theoretically, these findings empirically support embodied cognition theory and reveal the brain's intrinsic motor control dynamics in an embodied and predictive nature. Practically, grounding EEG decoding in the same dynamical principles that govern biological motion offers a promising path toward more stable and intuitive BCI control.",
    "pdf_url": "https://arxiv.org/pdf/2511.17401v1",
    "github_url": null,
    "published": "2025-11-21T17:00:48+00:00",
    "updated": "2025-11-21T17:00:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17384v1",
    "title": "IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation",
    "authors": [
      "Li",
      "Li",
      "Dao"
    ],
    "summary": "While Visual Large Language Models (VLLMs) show great promise as embodied agents, they continue to face substantial challenges in spatial reasoning. Existing embodied benchmarks largely focus on passive, static household environments and evaluate only isolated capabilities, failing to capture holistic performance in dynamic, real-world complexity. To fill this gap, we present IndustryNav, the first dynamic industrial navigation benchmark for active spatial reasoning. IndustryNav leverages 12 manually created, high-fidelity Unity warehouse scenarios featuring dynamic objects and human movement. Our evaluation employs a PointGoal navigation pipeline that effectively combines egocentric vision with global odometry to assess holistic local-global planning. Crucially, we introduce the \"collision rate\" and \"warning rate\" metrics to measure safety-oriented behaviors and distance estimation. A comprehensive study of nine state-of-the-art VLLMs (including models such as GPT-5-mini, Claude-4.5, and Gemini-2.5) reveals that closed-source models maintain a consistent advantage; however, all agents exhibit notable deficiencies in robust path planning, collision avoidance and active exploration. This highlights a critical need for embodied research to move beyond passive perception and toward tasks that demand stable planning, active exploration, and safe behavior in dynamic, real-world environment.",
    "pdf_url": "https://arxiv.org/pdf/2511.17384v1",
    "github_url": null,
    "published": "2025-11-21T16:48:49+00:00",
    "updated": "2025-11-21T16:48:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17269v1",
    "title": "Range-Edit: Semantic Mask Guided Outdoor LiDAR Scene Editing",
    "authors": [
      "Uppur",
      "Kumar",
      "Kumar"
    ],
    "summary": "Training autonomous driving and navigation systems requires large and diverse point cloud datasets that capture complex edge case scenarios from various dynamic urban settings. Acquiring such diverse scenarios from real-world point cloud data, especially for critical edge cases, is challenging, which restricts system generalization and robustness. Current methods rely on simulating point cloud data within handcrafted 3D virtual environments, which is time-consuming, computationally expensive, and often fails to fully capture the complexity of real-world scenes. To address some of these issues, this research proposes a novel approach that addresses the problem discussed by editing real-world LiDAR scans using semantic mask-based guidance to generate novel synthetic LiDAR point clouds. We incorporate range image projection and semantic mask conditioning to achieve diffusion-based generation. Point clouds are transformed to 2D range view images, which are used as an intermediate representation to enable semantic editing using convex hull-based semantic masks. These masks guide the generation process by providing information on the dimensions, orientations, and locations of objects in the real environment, ensuring geometric consistency and realism. This approach demonstrates high-quality LiDAR point cloud generation, capable of producing complex edge cases and dynamic scenes, as validated on the KITTI-360 dataset. This offers a cost-effective and scalable solution for generating diverse LiDAR data, a step toward improving the robustness of autonomous driving systems.",
    "pdf_url": "https://arxiv.org/pdf/2511.17269v1",
    "github_url": null,
    "published": "2025-11-21T14:16:27+00:00",
    "updated": "2025-11-21T14:16:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17687v1",
    "title": "Boosting Brain-inspired Path Integration Efficiency via Learning-based Replication of Continuous Attractor Neurodynamics",
    "authors": [
      "Ge",
      "He",
      "Mo"
    ],
    "summary": "The brain's Path Integration (PI) mechanism offers substantial guidance and inspiration for Brain-Inspired Navigation (BIN). However, the PI capability constructed by the Continuous Attractor Neural Networks (CANNs) in most existing BIN studies exhibits significant computational redundancy, and its operational efficiency needs to be improved; otherwise, it will not be conducive to the practicality of BIN technology. To address this, this paper proposes an efficient PI approach using representation learning models to replicate CANN neurodynamic patterns. This method successfully replicates the neurodynamic patterns of CANN-modeled Head Direction Cells (HDCs) and Grid Cells (GCs) using lightweight Artificial Neural Networks (ANNs). These ANN-reconstructed HDC and GC models are then integrated to achieve brain-inspired PI for Dead Reckoning (DR). Benchmark tests in various environments, compared with the well-known NeuroSLAM system, demonstrate that this work not only accurately replicates the neurodynamic patterns of navigation cells but also matches NeuroSLAM in positioning accuracy. Moreover, efficiency improvements of approximately 17.5% on the general-purpose device and 40~50% on the edge device were observed, compared with NeuroSLAM. This work offers a novel implementation strategy to enhance the practicality of BIN technology and holds potential for further extension.",
    "pdf_url": "https://arxiv.org/pdf/2511.17687v1",
    "github_url": null,
    "published": "2025-11-21T13:13:45+00:00",
    "updated": "2025-11-21T13:13:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17225v1",
    "title": "TP-MDDN: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making",
    "authors": [
      "Li",
      "Huang",
      "He"
    ],
    "summary": "In daily life, people often move through spaces to find objects that meet their needs, posing a key challenge in embodied AI. Traditional Demand-Driven Navigation (DDN) handles one need at a time but does not reflect the complexity of real-world tasks involving multiple needs and personal choices. To bridge this gap, we introduce Task-Preferenced Multi-Demand-Driven Navigation (TP-MDDN), a new benchmark for long-horizon navigation involving multiple sub-demands with explicit task preferences. To solve TP-MDDN, we propose AWMSystem, an autonomous decision-making system composed of three key modules: BreakLLM (instruction decomposition), LocateLLM (goal selection), and StatusMLLM (task monitoring). For spatial memory, we design MASMap, which combines 3D point cloud accumulation with 2D semantic mapping for accurate and efficient environmental understanding. Our Dual-Tempo action generation framework integrates zero-shot planning with policy-based fine control, and is further supported by an Adaptive Error Corrector that handles failure cases in real time. Experiments demonstrate that our approach outperforms state-of-the-art baselines in both perception accuracy and navigation robustness.",
    "pdf_url": "https://arxiv.org/pdf/2511.17225v1",
    "github_url": null,
    "published": "2025-11-21T13:12:13+00:00",
    "updated": "2025-11-21T13:12:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17186v2",
    "title": "Distributed Switching Model Predictive Control Meets Koopman Operator for Dynamic Obstacle Avoidance",
    "authors": [
      "Azarbahram",
      "Huanca",
      "Incremona"
    ],
    "summary": "This paper introduces a Koopman-enhanced distributed switched model predictive control (SMPC) framework for safe and scalable navigation of quadrotor unmanned aerial vehicles (UAVs) in dynamic environments with moving obstacles. The proposed method integrates switched motion modes and data-driven prediction to enable real-time, collision-free coordination. A localized Koopman operator approximates nonlinear obstacle dynamics as linear models based on online measurements, enabling accurate trajectory forecasting. These predictions are embedded into a distributed SMPC structure, where each UAV makes autonomous decisions using local and cluster-based information. This computationally efficient architecture is particularly promising for applications in surface transportation, including coordinated vehicle flows, shared infrastructure with pedestrians or cyclists, and urban UAV traffic. Simulation results demonstrate reliable formation control and real-time obstacle avoidance, highlighting the frameworks broad relevance for intelligent and cooperative mobility systems.",
    "pdf_url": "https://arxiv.org/pdf/2511.17186v2",
    "github_url": null,
    "published": "2025-11-21T12:06:58+00:00",
    "updated": "2025-11-28T07:59:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17183v1",
    "title": "Navigating in the Dark: A Multimodal Framework and Dataset for Nighttime Traffic Sign Recognition",
    "authors": [
      "Mishra",
      "Agarwal",
      "Lone"
    ],
    "summary": "Traffic signboards are vital for road safety and intelligent transportation systems, enabling navigation and autonomous driving. Yet, recognizing traffic signs at night remains challenging due to visual noise and scarcity of public nighttime datasets. Despite advances in vision architectures, existing methods struggle with robustness under low illumination and fail to leverage complementary mutlimodal cues effectively. To overcome these limitations, firstly, we introduce INTSD, a large-scale dataset comprising street-level night-time images of traffic signboards collected across diverse regions of India. The dataset spans 41 traffic signboard classes captured under varying lighting and weather conditions, providing a comprehensive benchmark for both detection and classification tasks. To benchmark INTSD for night-time sign recognition, we conduct extensive evaluations using state-of-the-art detection and classification models. Secondly, we propose LENS-Net, which integrates an adaptive image enhancement detector for joint illumination correction and sign localization, followed by a structured multimodal CLIP-GCNN classifier that leverages cross-modal attention and graph-based reasoning for robust and semantically consistent recognition. Our method surpasses existing frameworks, with ablation studies confirming the effectiveness of its key components. The dataset and code for LENS-Net is publicly available for research.",
    "pdf_url": "https://arxiv.org/pdf/2511.17183v1",
    "github_url": null,
    "published": "2025-11-21T12:04:53+00:00",
    "updated": "2025-11-21T12:04:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17097v1",
    "title": "Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation",
    "authors": [
      "Wang",
      "Wang",
      "Lian"
    ],
    "summary": "Vision-Language Navigation requires agents to act coherently over long horizons by understanding not only local visual context but also how far they have advanced within a multi-step instruction. However, recent Vision-Language-Action models focus on direct action prediction and earlier progress methods predict numeric achievements; both overlook the monotonic co-progression property of the observation and instruction sequences. Building on this insight, Progress-Think introduces semantic progress reasoning, predicting instruction-style progress from visual observations to enable more accurate navigation. To achieve this without expensive annotations, we propose a three-stage framework. In the initial stage, Self-Aligned Progress Pretraining bootstraps a reasoning module via a novel differentiable alignment between visual history and instruction prefixes. Then, Progress-Guided Policy Pretraining injects learned progress states into the navigation context, guiding the policy toward consistent actions. Finally, Progress-Policy Co-Finetuning jointly optimizes both modules with tailored progress-aware reinforcement objectives. Experiments on R2R-CE and RxR-CE show state-of-the-art success and efficiency, demonstrating that semantic progress yields a more consistent representation of navigation advancement.",
    "pdf_url": "https://arxiv.org/pdf/2511.17097v1",
    "github_url": null,
    "published": "2025-11-21T09:52:07+00:00",
    "updated": "2025-11-21T09:52:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17066v1",
    "title": "Distributed Cubature Kalman Filter based on MEEF with Adaptive Cauchy Kernel for State Estimation",
    "authors": [
      "Nguyen",
      "Zhao",
      "Hu"
    ],
    "summary": "Nowadays, with the development of multi-sensor networks, the distributed cubature Kalman filter is one of the well-known existing schemes for state estimation, for which the influence of the non-Gaussian noise, abnormal data, and communication burden are urgent challenges. In this paper, a distributed cubature Kalman filter based on adaptive minimum error entropy with fiducial points (AMEEF) criterion (AMEEF-DCKF) is proposed to overcome the above limitations. Specifically, firstly, in order to solve the influence of various types of non-Gaussian noise and abnormal data, the AMEEF optimization criterion is designed, in which the kernels used are Cauchy kernels with adaptive bandwidth. At the same time, the designed optimization criterion has enhanced the numerical stability and optimized the kernel bandwidth value. Next, in order to address the communication burden problem in multi-sensor networks, where a leader and a follower are distinguished, a distributed algorithm is constructed to achieve an average consensus among these sensors, called leader-follower average consensus (LFAC). Additionally, the convergence proof of the average consensus algorithm and the computational complexity analysis of the AMEEF-DCKF algorithm are also presented. Finally, through a 10-node sensor network, the effectiveness of the proposed algorithm is demonstrated in estimating the state of the power system and navigating land vehicles in complex environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.17066v1",
    "github_url": null,
    "published": "2025-11-21T09:18:05+00:00",
    "updated": "2025-11-21T09:18:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17052v1",
    "title": "PathAgent: Toward Interpretable Analysis of Whole-slide Pathology Images via Large Language Model-based Agentic Reasoning",
    "authors": [
      "Chen",
      "Cai",
      "Wang"
    ],
    "summary": "Analyzing whole-slide images (WSIs) requires an iterative, evidence-driven reasoning process that parallels how pathologists dynamically zoom, refocus, and self-correct while collecting the evidence. However, existing computational pipelines often lack this explicit reasoning trajectory, resulting in inherently opaque and unjustifiable predictions. To bridge this gap, we present PathAgent, a training-free, large language model (LLM)-based agent framework that emulates the reflective, stepwise analytical approach of human experts. PathAgent can autonomously explore WSI, iteratively and precisely locating significant micro-regions using the Navigator module, extracting morphology visual cues using the Perceptor, and integrating these findings into the continuously evolving natural language trajectories in the Executor. The entire sequence of observations and decisions forms an explicit chain-of-thought, yielding fully interpretable predictions. Evaluated across five challenging datasets, PathAgent exhibits strong zero-shot generalization, surpassing task-specific baselines in both open-ended and constrained visual question-answering tasks. Moreover, a collaborative evaluation with human pathologists confirms PathAgent's promise as a transparent and clinically grounded diagnostic assistant.",
    "pdf_url": "https://arxiv.org/pdf/2511.17052v1",
    "github_url": null,
    "published": "2025-11-21T08:50:14+00:00",
    "updated": "2025-11-21T08:50:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17013v1",
    "title": "MfNeuPAN: Proactive End-to-End Navigation in Dynamic Environments via Direct Multi-Frame Point Constraints",
    "authors": [
      "Ying",
      "Ye",
      "Luo"
    ],
    "summary": "Obstacle avoidance in complex and dynamic environments is a critical challenge for real-time robot navigation. Model-based and learning-based methods often fail in highly dynamic scenarios because traditional methods assume a static environment and cannot adapt to real-time changes, while learning-based methods rely on single-frame observations for motion constraint estimation, limiting their adaptability. To overcome these limitations, this paper proposes a novel framework that leverages multi-frame point constraints, including current and future frames predicted by a dedicated module, to enable proactive end-to-end navigation. By incorporating a prediction module that forecasts the future path of moving obstacles based on multi-frame observations, our method allows the robot to proactively anticipate and avoid potential dangers. This proactive planning capability significantly enhances navigation robustness and efficiency in unknown dynamic environments. Simulations and real-world experiments validate the effectiveness of our approach.",
    "pdf_url": "https://arxiv.org/pdf/2511.17013v1",
    "github_url": null,
    "published": "2025-11-21T07:30:24+00:00",
    "updated": "2025-11-21T07:30:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17005v1",
    "title": "FLUID: Training-Free Face De-identification via Latent Identity Substitution",
    "authors": [
      "Park",
      "Muhammad",
      "Lee"
    ],
    "summary": "We present FLUID (Face de-identification in the Latent space via Utility-preserving Identity Displacement), a training-free framework that directly substitutes identity in the latent space of pretrained diffusion models. Inspired by substitution mechanisms in chemistry, we reinterpret identity editing as semantic displacement in the latent h-space of a pretrained unconditional diffusion model. Our framework discovers identity-editing directions through optimization guided by novel reagent losses, which supervise for attribute preservation and identity suppression. We further propose both linear and geodesic (tangent-based) editing schemes to effectively navigate the latent manifold. Experimental results on CelebA-HQ and FFHQ demonstrate that FLUID achieves a superior trade-off between identity suppression and attribute preservation, outperforming state-of-the-art de-identification methods in both qualitative and quantitative metrics.",
    "pdf_url": "https://arxiv.org/pdf/2511.17005v1",
    "github_url": null,
    "published": "2025-11-21T07:18:37+00:00",
    "updated": "2025-11-21T07:18:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17669v1",
    "title": "Empa: An AI-Powered Virtual Mentor for Developing Global Collaboration Skills in HPC Education",
    "authors": [
      "Ashish",
      "Jaiswal",
      "Vhaduri"
    ],
    "summary": "High-performance computing (HPC) and parallel computing increasingly rely on global collaboration among diverse teams, yet traditional computing curricula inadequately prepare students for cross-cultural teamwork essential in modern computational research environments. This paper presents Empa, an AI-powered virtual mentor that integrates intercultural collaboration training into undergraduate computing education. Built using large language models and deployed through a progressive web application, Empa guides students through structured activities covering cultural dimensions, communication styles, and conflict resolution that are critical for effective multicultural teamwork. Our system addresses the growing need for culturally competent HPC professionals by helping computing students develop skills to collaborate effectively in international research teams, contribute to global computational projects, and navigate the cultural complexities inherent in distributed computing environments. Pilot preparation for deployment in computing courses demonstrates the feasibility of AI-mediated intercultural training and provides insights into scalable approaches for developing intercultural collaboration skills essential for HPC workforce development.",
    "pdf_url": "https://arxiv.org/pdf/2511.17669v1",
    "github_url": null,
    "published": "2025-11-21T03:52:22+00:00",
    "updated": "2025-11-21T03:52:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17664v1",
    "title": "CubeletWorld: A New Abstraction for Scalable 3D Modeling",
    "authors": [
      "Samad",
      "Nguyen",
      "Berg"
    ],
    "summary": "Modern cities produce vast streams of heterogeneous data, from infrastructure maps to mobility logs and satellite imagery. However, integrating these sources into coherent spatial models for planning and prediction remains a major challenge. Existing agent-centric methods often rely on direct environmental sensing, limiting scalability and raising privacy concerns. This paper introduces CubeletWorld, a novel framework for representing and analyzing urban environments through a discretized 3D grid of spatial units called cubelets. This abstraction enables privacy-preserving modeling by embedding diverse data signals, such as infrastructure, movement, or environmental indicators, into localized cubelet states. CubeletWorld supports downstream tasks such as planning, navigation, and occupancy prediction without requiring agent-driven sensing. To evaluate this paradigm, we propose the CubeletWorld State Prediction task, which involves predicting the cubelet state using a realistic dataset containing various urban elements like streets and buildings through this discretized representation. We explore a range of modified core models suitable for our setting and analyze challenges posed by increasing spatial granularity, specifically the issue of sparsity in representation and scalability of baselines. In contrast to existing 3D occupancy prediction models, our cubelet-centric approach focuses on inferring state at the spatial unit level, enabling greater generalizability across regions and improved privacy compliance. Our results demonstrate that CubeletWorld offers a flexible and extensible framework for learning from complex urban data, and it opens up new possibilities for scalable simulation and decision support in domains such as socio-demographic modeling, environmental monitoring, and emergency response. The code and datasets can be downloaded from here.",
    "pdf_url": "https://arxiv.org/pdf/2511.17664v1",
    "github_url": null,
    "published": "2025-11-21T00:19:02+00:00",
    "updated": "2025-11-21T00:19:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16825v1",
    "title": "WorldGen: From Text to Traversable and Interactive 3D Worlds",
    "authors": [
      "Wang",
      "Jung",
      "Monnier"
    ],
    "summary": "We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning, procedural generation, diffusion-based 3D generation, and object-aware scene decomposition, WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.16825v1",
    "github_url": null,
    "published": "2025-11-20T22:13:18+00:00",
    "updated": "2025-11-20T22:13:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16805v1",
    "title": "Scene Awareness While Using Multiple Navigation Aids in AR Search",
    "authors": [
      "Kumaran",
      "Kim",
      "Machniak"
    ],
    "summary": "Augmented reality (AR) allows virtual information to be presented in the real world, providing support for numerous tasks including search and navigation. Allowing users access to multiple navigation aids may help leverage the benefits of different navigational guidance methods, but may also have negative perceptual and cognitive impacts. In this study, users performed searches for virtual gems within a large-scale augmented environment while choosing to deploy two different navigation aids either independently or simultaneously: world-locked arrows and an on-screen radar. After completing the search, participants were asked to recall objects that may or may not have been present in the scene. The use of navigation aids impacted object recall, with impaired recall of objects in the environment when an aid was switched on. The results point at possible impact factors of object awareness in mobile AR and underscore the potential for adaptable interfaces to support users navigating the physical world.",
    "pdf_url": "https://arxiv.org/pdf/2511.16805v1",
    "github_url": null,
    "published": "2025-11-20T21:13:06+00:00",
    "updated": "2025-11-20T21:13:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16724v1",
    "title": "Navigating the Quantum Resource Landscape of Entropy Vector Space Using Machine Learning and Optimization",
    "authors": [
      "Khumalo",
      "Mehta",
      "Munizzi"
    ],
    "summary": "We present a machine learning framework to study the dynamics of entropy vectors and quantum resources, including entanglement and magic, focusing on violations of entropy inequalities. Using a reinforcement learning agent formulated as a Markov decision process, we identify quantum circuits that optimally navigate the entropy vector space to generate violations of Ingleton's inequality. We complement this approach with a classical optimization algorithm to produce arbitrary numbers of Ingleton-violating states, with tunable degrees of violation, and empirically determine the maximal attainable violation for Ingleton's inequality. Our analysis reveals characteristic patterns of quantum resources that accompany Ingleton violation. A comprehensive statistical analysis shows that Ingleton-violating states occupy sharply-defined, isolated regions of the Hilbert space, and are extremely rare. Together, these results establish a unified computational toolkit for studying entropy vector dynamics, tracking quantum resource evolution, and engineering circuits with controlled information-theoretic features.",
    "pdf_url": "https://arxiv.org/pdf/2511.16724v1",
    "github_url": null,
    "published": "2025-11-20T19:00:00+00:00",
    "updated": "2025-11-20T19:00:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17656v1",
    "title": "Multi-Agent Coordination in Autonomous Vehicle Routing: A Simulation-Based Study of Communication, Memory, and Routing Loops",
    "authors": [
      "Saifullah",
      "Palmer"
    ],
    "summary": "Multi-agent coordination is critical for next-generation autonomous vehicle (AV) systems, yet naive implementations of communication-based rerouting can lead to catastrophic performance degradation. This study investigates a fundamental problem in decentralized multi-agent navigation: routing loops, where vehicles without persistent obstacle memory become trapped in cycles of inefficient path recalculation. Through systematic simulation experiments involving 72 unique configurations across varying vehicle densities (15, 35, 55 vehicles) and obstacle frequencies (6, 20 obstacles), we demonstrate that memory-less reactive rerouting increases average travel time by up to 682% compared to baseline conditions. To address this, we introduce Object Memory Management (OMM), a lightweight mechanism enabling agents to retain and share knowledge of previously encountered obstacles. OMM operates by maintaining a distributed blacklist of blocked nodes, which each agent consults during Dijkstra-based path recalculation, effectively preventing redundant routing attempts. Our results show that OMM-enabled coordination reduces average travel time by 75.7% and wait time by 88% compared to memory-less systems, while requiring only 1.67 route recalculations per vehicle versus 9.83 in memory-less scenarios. This work provides empirical evidence that persistent, shared memory is not merely beneficial but essential for robust multi-agent coordination in dynamic environments. The findings have implications beyond autonomous vehicles, informing the design of decentralized systems in robotics, network routing, and distributed AI. We provide a comprehensive experimental analysis, including detailed scenario breakdowns, scalability assessments, and visual documentation of the routing loop phenomenon, demonstrating OMM's critical role in preventing detrimental feedback cycles in cooperative multi-agent systems.",
    "pdf_url": "https://arxiv.org/pdf/2511.17656v1",
    "github_url": null,
    "published": "2025-11-20T17:42:49+00:00",
    "updated": "2025-11-20T17:42:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16567v2",
    "title": "POMA-3D: The Point Map Way to 3D Scene Understanding",
    "authors": [
      "Mao",
      "Luo",
      "Huang"
    ],
    "summary": "In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/",
    "pdf_url": "https://arxiv.org/pdf/2511.16567v2",
    "github_url": null,
    "published": "2025-11-20T17:22:51+00:00",
    "updated": "2025-11-21T11:07:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16457v2",
    "title": "Distinguishing thermal versus quantum annealing using probability-flux signatures across interaction networks",
    "authors": [
      "Horiike",
      "Kawaguchi"
    ],
    "summary": "Simulated annealing provides a heuristic solution to combinatorial optimization problems. The cost function of a problem is mapped onto the energy function of a physical many-body system, and, by using thermal or quantum fluctuations, the system explores the state space to find the ground state, which corresponds to the optimal solution of the problem. Studies have highlighted both the similarities and differences between thermal and quantum fluctuations. Nevertheless, fundamental understanding of thermal and quantum annealing remains incomplete, making it unclear how quantum annealing outperforms thermal annealing in which problem instances. Here, we investigate the many-body dynamics of thermal and quantum annealing by examining all possible interaction networks of $\\pm J$ Ising spin systems up to seven spins. Our comprehensive investigation reveals that differences between thermal and quantum annealing emerge for particular interaction networks, indicating that the structure of the energy landscape distinguishes the two dynamics. We identify the microscopic origin of these differences through probability fluxes in state space, finding that the two dynamics are broadly similar, but that quantum tunnelling produces qualitative differences. Our results provide insight into how thermal and quantum fluctuations navigate a system toward the ground state in simulated annealing, and are experimentally verifiable in atomic, molecular, and optical systems. Furthermore, these insights may improve mappings of optimization problems to Ising spin systems, yielding more accurate solutions in faster simulated annealing and thus benefiting real-world applications in industry. Our comprehensive survey of interaction networks and visualization of probability flux can help to understand, predict, and control quantum advantage in quantum annealing.",
    "pdf_url": "https://arxiv.org/pdf/2511.16457v2",
    "github_url": null,
    "published": "2025-11-20T15:23:53+00:00",
    "updated": "2025-12-04T14:54:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16349v1",
    "title": "CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering",
    "authors": [
      "Vanherck",
      "Moonen",
      "Zoomers"
    ],
    "summary": "Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content. Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure. This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud. By rendering synthetic views from this cloud, 2D-3D correspondences are established between live frames and the point cloud. A neural rendering technique narrows the domain gap between synthetic and real images, reducing occlusion and background artifacts to improve feature matching. The result is drift-free camera tracking with correct metric scale in the global LiDAR coordinate system. Two real-time variants are presented: Online Render and Match, and Prebuild and Localize. We demonstrate improved results on the ScanNet++ dataset and outperform existing SLAM pipelines.",
    "pdf_url": "https://arxiv.org/pdf/2511.16349v1",
    "github_url": null,
    "published": "2025-11-20T13:34:34+00:00",
    "updated": "2025-11-20T13:34:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16282v2",
    "title": "Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM",
    "authors": [
      "Dinya",
      "Halász",
      "Lőrincz"
    ],
    "summary": "We present a fast, spatio-temporal scene understanding framework based on Visual Geometry Grounded Transformer (VGGT). The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation. To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT's high memory demands. We exploit the VGGT tracking head to aggregate 2D semantic instance masks into 3D objects. To allow for temporal consistency and richer contextual reasoning the system stores timestamps and instance-level identities, thereby enabling the detection of changes in the environment. We evaluate the approach on well-known benchmarks and custom datasets specifically designed for assistive navigation scenarios. The results demonstrate the applicability of the framework to real-world scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2511.16282v2",
    "github_url": null,
    "published": "2025-11-20T12:03:28+00:00",
    "updated": "2025-11-27T00:08:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16140v1",
    "title": "Real-Time 3D Object Detection with Inference-Aligned Learning",
    "authors": [
      "Zhao",
      "Zheng",
      "Xia"
    ],
    "summary": "Real-time 3D object detection from point clouds is essential for dynamic scene understanding in applications such as augmented reality, robotics and navigation. We introduce a novel Spatial-prioritized and Rank-aware 3D object detection (SR3D) framework for indoor point clouds, to bridge the gap between how detectors are trained and how they are evaluated. This gap stems from the lack of spatial reliability and ranking awareness during training, which conflicts with the ranking-based prediction selection used as inference. Such a training-inference gap hampers the model's ability to learn representations aligned with inference-time behavior. To address the limitation, SR3D consists of two components tailored to the spatial nature of point clouds during training: a novel spatial-prioritized optimal transport assignment that dynamically emphasizes well-located and spatially reliable samples, and a rank-aware adaptive self-distillation scheme that adaptively injects ranking perception via a self-distillation paradigm. Extensive experiments on ScanNet V2 and SUN RGB-D show that SR3D effectively bridges the training-inference gap and significantly outperforms prior methods in accuracy while maintaining real-time speed.",
    "pdf_url": "https://arxiv.org/pdf/2511.16140v1",
    "github_url": null,
    "published": "2025-11-20T08:27:00+00:00",
    "updated": "2025-11-20T08:27:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16132v1",
    "title": "An Interpretability-Guided Framework for Responsible Synthetic Data Generation in Emotional Text",
    "authors": [
      "Martinez",
      "Miñoza",
      "Ibañez"
    ],
    "summary": "Emotion recognition from social media is critical for understanding public sentiment, but accessing training data has become prohibitively expensive due to escalating API costs and platform restrictions. We introduce an interpretability-guided framework where Shapley Additive Explanations (SHAP) provide principled guidance for LLM-based synthetic data generation. With sufficient seed data, SHAP-guided approach matches real data performance, significantly outperforms naïve generation, and substantially improves classification for underrepresented emotion classes. However, our linguistic analysis reveals that synthetic text exhibits reduced vocabulary richness and fewer personal or temporally complex expressions than authentic posts. This work provides both a practical framework for responsible synthetic data generation and a critical perspective on its limitations, underscoring that the future of trustworthy AI depends on navigating the trade-offs between synthetic utility and real-world authenticity.",
    "pdf_url": "https://arxiv.org/pdf/2511.16132v1",
    "github_url": null,
    "published": "2025-11-20T08:07:05+00:00",
    "updated": "2025-11-20T08:07:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16108v1",
    "title": "SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent",
    "authors": [
      "Cao",
      "Li",
      "Zhao"
    ],
    "summary": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. It provides efficient asynchronous dispatching, lightweight tool integration, and flexible backend interoperability, enabling seamless use with existing RL frameworks such as SkyRL-train, VeRL, and Tinker.   Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Together, these optimizations enable SA-SWE-32B to reach 39.4% Pass@1 on SWE-Bench Verified with more than 2x cost reduction compared to prior models reaching similar performance. Despite being trained solely on SWE tasks, SA-SWE-32B generalizes effectively to other agentic tasks, including Terminal-Bench, BrowseComp-Plus, and WebArena. We further demonstrate SkyRL-Agent's extensibility through case studies on deep research, computer use, and memory agents, each trained using a different training backend.",
    "pdf_url": "https://arxiv.org/pdf/2511.16108v1",
    "github_url": null,
    "published": "2025-11-20T07:05:19+00:00",
    "updated": "2025-11-20T07:05:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16048v1",
    "title": "Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud",
    "authors": [
      "Zhang",
      "Huang",
      "Xu"
    ],
    "summary": "While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately \"lo-fi\" approach. We present the \"Semantic Glitch,\" a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a \"physical glitch\" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a \"narrative mind\" that complements the \"weak,\" historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling \"plan to execution\" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2511.16048v1",
    "github_url": null,
    "published": "2025-11-20T05:10:13+00:00",
    "updated": "2025-11-20T05:10:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16028v1",
    "title": "Can Online GenAI Discussion Serve as Bellwether for Labor Market Shifts?",
    "authors": [
      "Cao",
      "Hua",
      "Wang"
    ],
    "summary": "The rapid advancement of Large Language Models (LLMs) has generated considerable speculation regarding their transformative potential for labor markets. However, existing approaches to measuring AI exposure in the workforce predominantly rely on concurrent market conditions, offering limited predictive capacity for anticipating future disruptions. This paper presents a predictive study examining whether online discussions about LLMs can function as early indicators of labor market shifts. We employ four distinct analytical approaches to identify the domains and timeframes in which public discourse serves as a leading signal for employment changes, thereby demonstrating its predictive validity for labor market dynamics. Drawing on a comprehensive dataset that integrates the REALM corpus of LLM discussions, LinkedIn job postings, Indeed employment indices, and over 4 million LinkedIn user profiles, we analyze the relationship between discussion intensity across news media and Reddit forums and subsequent variations in job posting volumes, occupational net change ratios, job tenure patterns, unemployment duration, and transitions to GenAI-related roles across thirteen occupational categories. Our findings reveal that discussion intensity predicts employment changes 1-7 months in advance across multiple indicators, including job postings, net hiring rates, tenure patterns, and unemployment duration. These findings suggest that monitoring online discourse can provide actionable intelligence for workers making reskilling decisions and organizations anticipating skill requirements, offering a real-time complement to traditional labor statistics in navigating technological disruption.",
    "pdf_url": "https://arxiv.org/pdf/2511.16028v1",
    "github_url": null,
    "published": "2025-11-20T04:18:25+00:00",
    "updated": "2025-11-20T04:18:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.16005v1",
    "title": "InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution",
    "authors": [
      "Dong",
      "Wang",
      "Zhang"
    ],
    "summary": "Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \\texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.",
    "pdf_url": "https://arxiv.org/pdf/2511.16005v1",
    "github_url": null,
    "published": "2025-11-20T03:05:26+00:00",
    "updated": "2025-11-20T03:05:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15873v1",
    "title": "Parametric Disjunctive Cuts for Sequences of Mixed Integer Linear Optimization Problems",
    "authors": [
      "Kelley",
      "Kazachkov",
      "Ralphs"
    ],
    "summary": "Many applications require solving sequences of related mixed-integer linear programs. We introduce a class of parametric disjunctive inequalities (PDIs), obtained by reusing the disjunctive proofs of optimality from prior solves to construct cuts valid for perturbed instances. We describe several methods of generating such cuts that navigate the tradeoff between computational expense and strength. We provide sufficient conditions under which PDIs support the disjunctive hull and a tightening step that guarantees support when needed. On perturbed instances from MIPLIB 2017, augmenting branch-and-cut with PDIs substantially improves performance, reducing total solve times on the majority of challenging cases.",
    "pdf_url": "https://arxiv.org/pdf/2511.15873v1",
    "github_url": null,
    "published": "2025-11-19T21:00:49+00:00",
    "updated": "2025-11-19T21:00:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15768v1",
    "title": "Navigating the Ethical and Societal Impacts of Generative AI in Higher Computing Education",
    "authors": [
      "Mak",
      "Nakatumba-Nabende",
      "Clear"
    ],
    "summary": "Generative AI (GenAI) presents societal and ethical challenges related to equity, academic integrity, bias, and data provenance. In this paper, we outline the goals, methodology and deliverables of their collaborative research, considering the ethical and societal impacts of GenAI in higher computing education. A systematic literature review that addresses a wide set of issues and topics covering the rapidly emerging technology of GenAI from the perspective of its ethical and societal impacts is presented. This paper then presents an evaluation of a broad international review of a set of university adoption, guidelines, and policies related to the use of GenAI and the implications for computing education. The Ethical and Societal Impacts-Framework (ESI-Framework), derived from the literature and policy review and evaluation, outlines the ethical and societal impacts of GenAI in computing education. This work synthesizes existing research and considers the implications for computing higher education. Educators, computing professionals and policy makers facing dilemmas related to the integration of GenAI in their respective contexts may use this framework to guide decision-making in the age of GenAI.",
    "pdf_url": "https://arxiv.org/pdf/2511.15768v1",
    "github_url": null,
    "published": "2025-11-19T17:38:28+00:00",
    "updated": "2025-11-19T17:38:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15642v2",
    "title": "Navigating Quantum Missteps in Agent-Based Modeling: A Schelling Model Case Study",
    "authors": [
      "Barati",
      "Croitoru",
      "Gore"
    ],
    "summary": "Quantum computing promises transformative advances, but remains constrained by recurring misconceptions and methodological pitfalls. This paper demonstrates a fundamental incompatibility between traditional agent-based modeling (ABM) implementations and quantum optimization frameworks like Quadratic Unconstrained Binary Optimization (QUBO). Using Schelling's segregation model as a case study, we show that the standard practice of directly translating ABM state observations into QUBO formulations not only fails to deliver quantum advantage, but actively undermines computational efficiency. The fundamental issue is architectural. Traditional ABM implementations entail observing the state of the system at each iteration, systematically destroying the quantum superposition required for computational advantage. Through analysis of Schelling's segregation dynamics on lollipop networks, we demonstrate how abandoning the QUBO reduction paradigm and instead reconceptualizing the research question, from \"simulate agent dynamics iteratively until convergence\" to \"compute minimum of agent moves required for global satisfaction\", enables a faster classical solution. This structural reconceptualization yields an algorithm that exploits network symmetries obscured in traditional ABM simulations and QUBO formulations. It establishes a new lower bound which quantum approaches must outperform to achieve advantage. Our work emphasizes that progress in quantum agent-based modeling does not require forcing classical ABM implementations into quantum frameworks. Instead, it should focus on clarifying when quantum advantage is structurally possible, developing best-in-class classical baselines through problem analysis, and fundamentally reformulating research questions rather than preserving classical iterative state change observation paradigms.",
    "pdf_url": "https://arxiv.org/pdf/2511.15642v2",
    "github_url": null,
    "published": "2025-11-19T17:24:43+00:00",
    "updated": "2025-11-26T01:17:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15614v1",
    "title": "Optimus-Q: Utilizing Federated Learning in Adaptive Robots for Intelligent Nuclear Power Plant Operations through Quantum Cryptography",
    "authors": [
      "Puppala",
      "Hossain",
      "Alam"
    ],
    "summary": "The integration of advanced robotics in nuclear power plants (NPPs) presents a transformative opportunity to enhance safety, efficiency, and environmental monitoring in high-stakes environments. Our paper introduces the Optimus-Q robot, a sophisticated system designed to autonomously monitor air quality and detect contamination while leveraging adaptive learning techniques and secure quantum communication. Equipped with advanced infrared sensors, the Optimus-Q robot continuously streams real-time environmental data to predict hazardous gas emissions, including carbon dioxide (CO$_2$), carbon monoxide (CO), and methane (CH$_4$). Utilizing a federated learning approach, the robot collaborates with other systems across various NPPs to improve its predictive capabilities without compromising data privacy. Additionally, the implementation of Quantum Key Distribution (QKD) ensures secure data transmission, safeguarding sensitive operational information. Our methodology combines systematic navigation patterns with machine learning algorithms to facilitate efficient coverage of designated areas, thereby optimizing contamination monitoring processes. Through simulations and real-world experiments, we demonstrate the effectiveness of the Optimus-Q robot in enhancing operational safety and responsiveness in nuclear facilities. This research underscores the potential of integrating robotics, machine learning, and quantum technologies to revolutionize monitoring systems in hazardous environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.15614v1",
    "github_url": null,
    "published": "2025-11-19T17:01:24+00:00",
    "updated": "2025-11-19T17:01:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15597v1",
    "title": "Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition",
    "authors": [
      "Wang",
      "Zhao",
      "Tao"
    ],
    "summary": "LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving. However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting. To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism. The proposed sampling strategy estimates the learning difficulty of each sample via its loss value and selects samples for replay according to their estimated difficulty. Harder samples, which tend to encode more discriminative information, are sampled with higher probability while maintaining distributional coverage across the dataset. In addition, the rehearsal enhancement mechanism encourages memory samples to be further refined during new-task training by slightly reducing their loss relative to previous tasks, thereby reinforcing long-term knowledge retention. Extensive experiments across multiple benchmarks demonstrate that KDF+ consistently outperforms existing continual learning methods and can be seamlessly integrated into state-of-the-art continual learning for LiDAR place recognition frameworks to yield significant and stable performance gains. The code will be available at https://github.com/repo/KDF-plus.",
    "pdf_url": "https://arxiv.org/pdf/2511.15597v1",
    "github_url": "https://github.com/repo/KDF-plus",
    "published": "2025-11-19T16:41:30+00:00",
    "updated": "2025-11-19T16:41:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15567v1",
    "title": "Computer-Use Agents as Judges for Generative User Interface",
    "authors": [
      "Lin",
      "Hu",
      "Li"
    ],
    "summary": "Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.",
    "pdf_url": "https://arxiv.org/pdf/2511.15567v1",
    "github_url": "https://github.com/showlab/AUI",
    "published": "2025-11-19T16:00:02+00:00",
    "updated": "2025-11-19T16:00:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15536v1",
    "title": "The CAPIRE Curriculum Graph: Structural Feature Engineering for Curriculum-Constrained Student Modelling in Higher Education",
    "authors": [
      "Paz"
    ],
    "summary": "Curricula in long-cycle programmes are usually recorded in institutional databases as linear lists of courses, yet in practice they operate as directed graphs of prerequisite relationships that constrain student progression through complex dependencies. This paper introduces the CAPIRE Curriculum Graph, a structural feature engineering layer embedded within the CAPIRE framework for student attrition prediction in Civil Engineering at Universidad Nacional de Tucuman, Argentina. We formalise the curriculum as a directed acyclic graph, compute course-level centrality metrics to identify bottleneck and backbone courses, and derive nine structural features at the student-semester level that capture how students navigate the prerequisite network over time. These features include backbone completion rate, bottleneck approval ratio, blocked credits due to incomplete prerequisites, and graph distance to graduation. We compare three model configurations - baseline CAPIRE, CAPIRE plus macro-context variables, and CAPIRE plus macro plus structural features - using Random Forest classifiers on 1,343 students across seven cohorts (2015-2021). While macro-context socioeconomic indicators fail to improve upon the baseline, structural curriculum features yield consistent gains in performance, with the best configuration achieving overall Accuracy of 86.66% and F1-score of 88.08% and improving Balanced Accuracy by 0.87 percentage points over a strong baseline. Ablation analysis further shows that all structural features contribute in a synergistic fashion rather than through a single dominant metric. By making curriculum structure an explicit object in the feature layer, this work extends CAPIRE from a multilevel leakage-aware framework to a curriculum-constrained prediction system that bridges network science, educational data mining, and institutional research.",
    "pdf_url": "https://arxiv.org/pdf/2511.15536v1",
    "github_url": null,
    "published": "2025-11-19T15:33:00+00:00",
    "updated": "2025-11-19T15:33:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15513v1",
    "title": "Discovering Optimal Natural Gaits of Dissipative Systems via Virtual Energy Injection",
    "authors": [
      "Griesbauer",
      "Calzolari",
      "Raff"
    ],
    "summary": "Legged robots offer several advantages when navigating unstructured environments, but they often fall short of the efficiency achieved by wheeled robots. One promising strategy to improve their energy economy is to leverage their natural (unactuated) dynamics using elastic elements. This work explores that concept by designing energy-optimal control inputs through a unified, multi-stage framework. It starts with a novel energy injection technique to identify passive motion patterns by harnessing the system's natural dynamics. This enables the discovery of passive solutions even in systems with energy dissipation caused by factors such as friction or plastic collisions. Building on these passive solutions, we then employ a continuation approach to derive energy-optimal control inputs for the fully actuated, dissipative robotic system. The method is tested on simulated models to demonstrate its applicability in both single- and multi-legged robotic systems. This analysis provides valuable insights into the design and operation of elastic legged robots, offering pathways to improve their efficiency and adaptability by exploiting the natural system dynamics.",
    "pdf_url": "https://arxiv.org/pdf/2511.15513v1",
    "github_url": null,
    "published": "2025-11-19T15:06:30+00:00",
    "updated": "2025-11-19T15:06:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15429v1",
    "title": "WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes",
    "authors": [
      "Graviers",
      "Ammar",
      "Guettier"
    ],
    "summary": "We introduce WarNav, a novel real-world dataset constructed from images of the open-source DATTALION repository, specifically tailored to enable the development and benchmarking of semantic segmentation models for autonomous ground vehicle navigation in unstructured, conflict-affected environments. This dataset addresses a critical gap between conventional urban driving resources and the unique operational scenarios encountered by unmanned systems in hazardous and damaged war-zones. We detail the methodological challenges encountered, ranging from data heterogeneity to ethical considerations, providing guidance for future efforts that target extreme operational contexts. To establish performance references, we report baseline results on WarNav using several state-of-the-art semantic segmentation models trained on structured urban scenes. We further analyse the impact of training data environments and propose a first step towards effective navigability in challenging environments with the constraint of having no annotation of the targeted images. Our goal is to foster impactful research that enhances the robustness and safety of autonomous vehicles in high-risk scenarios while being frugal in annotated data.",
    "pdf_url": "https://arxiv.org/pdf/2511.15429v1",
    "github_url": null,
    "published": "2025-11-19T13:32:26+00:00",
    "updated": "2025-11-19T13:32:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15343v1",
    "title": "Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection",
    "authors": [
      "Loukovitis",
      "Karampinis",
      "Voulodimos"
    ],
    "summary": "Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.",
    "pdf_url": "https://arxiv.org/pdf/2511.15343v1",
    "github_url": null,
    "published": "2025-11-19T11:03:47+00:00",
    "updated": "2025-11-19T11:03:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15239v1",
    "title": "Symmetry-Breaking in Multi-Agent Navigation: Winding Number-Aware MPC with a Learned Topological Strategy",
    "authors": [
      "Nakao",
      "Kasaura",
      "Kozuno"
    ],
    "summary": "We address the fundamental challenge of resolving symmetry-induced deadlocks in distributed multi-agent navigation by proposing a new hierarchical navigation method. When multiple agents interact, it is inherently difficult for them to autonomously break the symmetry of deciding how to pass each other. To tackle this problem, we introduce an approach that quantifies cooperative symmetry-breaking strategies using a topological invariant called the winding number, and learns the strategies themselves through reinforcement learning. Our method features a hierarchical policy consisting of a learning-based Planner, which plans topological cooperative strategies, and a model-based Controller, which executes them. Through reinforcement learning, the Planner learns to produce two types of parameters for the Controller: one is the topological cooperative strategy represented by winding numbers, and the other is a set of dynamic weights that determine which agent interaction to prioritize in dense scenarios where multiple agents cross simultaneously. The Controller then generates collision-free and efficient motions based on the strategy and weights provided by the Planner. This hierarchical structure combines the flexible decision-making ability of learning-based methods with the reliability of model-based approaches. Simulation and real-world robot experiments demonstrate that our method outperforms existing baselines, particularly in dense environments, by efficiently avoiding collisions and deadlocks while achieving superior navigation performance. The code for the experiments is available at https://github.com/omron-sinicx/WNumMPC.",
    "pdf_url": "https://arxiv.org/pdf/2511.15239v1",
    "github_url": "https://github.com/omron-sinicx/WNumMPC",
    "published": "2025-11-19T08:47:03+00:00",
    "updated": "2025-11-19T08:47:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.19451v1",
    "title": "Strong Duality and Dual Ascent Approach to Continuous-Time Chance-Constrained Stochastic Optimal Control",
    "authors": [
      "Patil",
      "Duarte",
      "Bisetti"
    ],
    "summary": "The paper addresses a continuous-time continuous-space chance-constrained stochastic optimal control (SOC) problem where the probability of failure to satisfy given state constraints is explicitly bounded. We leverage the notion of exit time from continuous-time stochastic calculus to formulate a chance-constrained SOC problem. Without any conservative approximation, the chance constraint is transformed into an expectation of an indicator function which can be incorporated into the cost function by considering a dual formulation. We then express the dual function in terms of the solution to a Hamilton-Jacobi-Bellman partial differential equation parameterized by the dual variable. Under a certain assumption on the system dynamics and cost function, it is shown that a strong duality holds between the primal chance-constrained problem and its dual. The Path integral approach is utilized to numerically solve the dual problem via gradient ascent using open-loop samples of system trajectories. We present simulation studies on chance-constrained motion planning for spatial navigation of mobile robots and the solution of the path integral approach is compared with that of the finite difference method.",
    "pdf_url": "https://arxiv.org/pdf/2511.19451v1",
    "github_url": null,
    "published": "2025-11-19T08:09:36+00:00",
    "updated": "2025-11-19T08:09:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15153v1",
    "title": "SceneEdited: A City-Scale Benchmark for 3D HD Map Updating via Image-Guided Change Detection",
    "authors": [
      "Lin",
      "Chin",
      "Garg"
    ],
    "summary": "Accurate, up-to-date High-Definition (HD) maps are critical for urban planning, infrastructure monitoring, and autonomous navigation. However, these maps quickly become outdated as environments evolve, creating a need for robust methods that not only detect changes but also incorporate them into updated 3D representations. While change detection techniques have advanced significantly, there remains a clear gap between detecting changes and actually updating 3D maps, particularly when relying on 2D image-based change detection. To address this gap, we introduce SceneEdited, the first city-scale dataset explicitly designed to support research on HD map maintenance through 3D point cloud updating. SceneEdited contains over 800 up-to-date scenes covering 73 km of driving and approximate 3 $\\text{km}^2$ of urban area, with more than 23,000 synthesized object changes created both manually and automatically across 2000+ out-of-date versions, simulating realistic urban modifications such as missing roadside infrastructure, buildings, overpasses, and utility poles. Each scene includes calibrated RGB images, LiDAR scans, and detailed change masks for training and evaluation. We also provide baseline methods using a foundational image-based structure-from-motion pipeline for updating outdated scenes, as well as a comprehensive toolkit supporting scalability, trackability, and portability for future dataset expansion and unification of out-of-date object annotations. Both the dataset and the toolkit are publicly available at https://github.com/ChadLin9596/ScenePoint-ETK, establising a standardized benchmark for 3D map updating research.",
    "pdf_url": "https://arxiv.org/pdf/2511.15153v1",
    "github_url": "https://github.com/ChadLin9596/ScenePoint-ETK",
    "published": "2025-11-19T06:10:55+00:00",
    "updated": "2025-11-19T06:10:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.15743v1",
    "title": "Connecting the Dots: A Machine Learning Ready Dataset for Ionospheric Forecasting Models",
    "authors": [
      "Wolniewicz",
      "Kelebek",
      "Mestici"
    ],
    "summary": "Operational forecasting of the ionosphere remains a critical space weather challenge due to sparse observations, complex coupling across geospatial layers, and a growing need for timely, accurate predictions that support Global Navigation Satellite System (GNSS), communications, aviation safety, as well as satellite operations. As part of the 2025 NASA Heliolab, we present a curated, open-access dataset that integrates diverse ionospheric and heliospheric measurements into a coherent, machine learning-ready structure, designed specifically to support next-generation forecasting models and address gaps in current operational frameworks. Our workflow integrates a large selection of data sources comprising Solar Dynamic Observatory data, solar irradiance indices (F10.7), solar wind parameters (velocity and interplanetary magnetic field), geomagnetic activity indices (Kp, AE, SYM-H), and NASA JPL's Global Ionospheric Maps of Total Electron Content (GIM-TEC). We also implement geospatially sparse data such as the TEC derived from the World-Wide GNSS Receiver Network and crowdsourced Android smartphone measurements. This novel heterogeneous dataset is temporally and spatially aligned into a single, modular data structure that supports both physical and data-driven modeling. Leveraging this dataset, we train and benchmark several spatiotemporal machine learning architectures for forecasting vertical TEC under both quiet and geomagnetically active conditions. This work presents an extensive dataset and modeling pipeline that enables exploration of not only ionospheric dynamics but also broader Sun-Earth interactions, supporting both scientific inquiry and operational forecasting efforts.",
    "pdf_url": "https://arxiv.org/pdf/2511.15743v1",
    "github_url": null,
    "published": "2025-11-18T20:13:25+00:00",
    "updated": "2025-11-18T20:13:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14755v1",
    "title": "Robust Verification of Controllers under State Uncertainty via Hamilton-Jacobi Reachability Analysis",
    "authors": [
      "Lin",
      "Pinto",
      "Bansal"
    ],
    "summary": "As perception-based controllers for autonomous systems become increasingly popular in the real world, it is important that we can formally verify their safety and performance despite perceptual uncertainty. Unfortunately, the verification of such systems remains challenging, largely due to the complexity of the controllers, which are often nonlinear, nonconvex, learning-based, and/or black-box. Prior works propose verification algorithms that are based on approximate reachability methods, but they often restrict the class of controllers and systems that can be handled or result in overly conservative analyses. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for general nonlinear systems that can compute optimal reachable sets under worst-case system uncertainties; however, its application to perception-based systems is currently underexplored. In this work, we propose RoVer-CoRe, a framework for the Robust Verification of Controllers via HJ Reachability. To the best of our knowledge, RoVer-CoRe is the first HJ reachability-based framework for the verification of perception-based systems under perceptual uncertainty. Our key insight is to concatenate the system controller, observation function, and the state estimation modules to obtain an equivalent closed-loop system that is readily compatible with existing reachability frameworks. Within RoVer-CoRe, we propose novel methods for formal safety verification and robust controller design. We demonstrate the efficacy of the framework in case studies involving aircraft taxiing and NN-based rover navigation. Code is available at the link in the footnote.",
    "pdf_url": "https://arxiv.org/pdf/2511.14755v1",
    "github_url": null,
    "published": "2025-11-18T18:55:20+00:00",
    "updated": "2025-11-18T18:55:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14711v1",
    "title": "Why Do We Code? A Theory on Motivations and Challenges in Software Engineering from Education to Practice",
    "authors": [
      "Chang",
      "Guizani",
      "Johnson"
    ],
    "summary": "Motivations and challenges jointly shape how individuals enter, persist, and evolve within software engineering (SE), yet their interplay remains underexplored across the transition from education to professional practice. We conducted 15 semi-structured interviews and employed the Gioia Methodology, an adapted grounded theory methodology from organizational behavior, to inductively derive taxonomies of motivations and challenges, and build the Exposure-Pursuit-Evaluation (EPE) Process Model. Our findings reveal that impactful early exposure triggers intrinsic motivations, while non-impactful exposure requires an extrinsic push (e.g., career/ personal goals, external validation). We identify curiosity and avoiding alternatives as a distinct educational drivers, and barriers to belonging as the only challenge persisting across education and career. Our findings show that career progression challenges (e.g., navigating the corporate world) constrain extrinsic fulfillment while technical training challenges, barriers to belonging and threats to motivation constrain intrinsic fulfillment. The theory shows how unmet motivations and recurring challenges influence persistence, career shifts, or departure from the field. Our results provide a grounded model for designing interventions that strengthen intrinsic fulfillment and reduce systemic barriers in SE education and practice.",
    "pdf_url": "https://arxiv.org/pdf/2511.14711v1",
    "github_url": null,
    "published": "2025-11-18T17:54:36+00:00",
    "updated": "2025-11-18T17:54:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14625v1",
    "title": "Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains",
    "authors": [
      "Ben",
      "Xu",
      "Li"
    ],
    "summary": "Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization.",
    "pdf_url": "https://arxiv.org/pdf/2511.14625v1",
    "github_url": null,
    "published": "2025-11-18T16:16:31+00:00",
    "updated": "2025-11-18T16:16:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17620v1",
    "title": "Enabling Blind and Visually Impaired Individuals to Pursue Careers in Science",
    "authors": [
      "Petitdemange",
      "Nashed"
    ],
    "summary": "Blind and Visually Impaired (BVI) Individuals face significant challenges in science due to the discipline's reliance on visual elements such as graphs, diagrams, and laboratory work. Traditional learning materials, such as Braille and large-print textbooks, are often scarce or delayed, while practical experiments are rarely adapted for accessibility. Additionally, mainstream educators lack the training to effectively support BVI students, and Teachers for the Visually Impaired (TVIs) often lack scientific expertise. As a result, BVI individuals remain underrepresented in scientific jobs, reinforcing a cycle of exclusion. However, technological advancements and inclusive initiatives are opening new opportunities. Outreach programs aim to make science engaging and accessible for BVI individuals through multi-sensory learning experiences. Hands-on involvement in these activities fosters confidence and interest in scientific careers. Beyond sparking interest, equipping BVI students with the right tools and skills is crucial for their academic success. Early exposure to assistive technologies enables BVI students to navigate scientific studies independently. Artificial Intelligence (AI) tools further enhance accessibility by converting visual data into descriptive text and providing interactive assistance. Several learning sessions demonstrated the effectiveness of these interventions, with participants successfully integrating into university-level science programs. Educating BVI and their teachers on these tools and good pratices is the aim of our project AccesSciencesDV. Research careers offer promising opportunities for BVI, especially in computational fields. By leveraging coding, data analysis, and AI-driven tools, BVI researchers can conduct high-level scientific work without relying on direct visual observations. The presence of BVI scientists enriches research environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.17620v1",
    "github_url": null,
    "published": "2025-11-18T15:45:58+00:00",
    "updated": "2025-11-18T15:45:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14499v1",
    "title": "Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM",
    "authors": [
      "Qin",
      "Wang",
      "Zheng"
    ],
    "summary": "The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor configurations.Related works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention maps.This approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky objects.By focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities.",
    "pdf_url": "https://arxiv.org/pdf/2511.14499v1",
    "github_url": null,
    "published": "2025-11-18T13:46:18+00:00",
    "updated": "2025-11-18T13:46:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14471v1",
    "title": "Dynamic Carbon Intensity Indicator (CII) Management in Stochastic Tramp Shipping Market",
    "authors": [
      "Cheng",
      "Cheng",
      "Bai"
    ],
    "summary": "In the maritime sector, tramp shipping companies manage fleets to maximize profit while navigating market uncertainties. The International Maritime Organization (IMO) recently introduced the Carbon Intensity Indicator (CII) to reduce greenhouse gas emissions, further complicating deployment decisions. This paper introduces a novel two-stage stochastic programming model for long-term fleet deployment under market uncertainty and CII regulation. It is the first to integrate key operational uncertainties such as fuel prices, freight rates, and cargo demand into a unified tactical planning framework under CII regulation, simultaneously optimizing routing, cargo allocation, and speed. Furthermore, we develop an novel efficient heuristic algorithm that reliably converges to solutions within a 5\\% optimality gap, enabling practical decision-support under uncertainty. Numerical analysis highlights two key findings based on our model: (1) It uncovers the ``CII paradox,'' a critical counterintuitive phenomenon where the present Supply-based CII regulation may increase total emissions significantly and drastically reduce profits. This challenges the conventional wisdom that stricter carbon-intensity rules invariably reduce emissions. (2) It demonstrates the advantage of stochastic modeling, showing that accounting for future uncertainties significantly narrows the revenue gap with perfect-foresight solutions, thereby offering superior economic performance over deterministic approaches. Collectively, these results deepen the understanding of environmental regulation's operational impacts and pave the way for more effective and sustainable fleet management strategies.",
    "pdf_url": "https://arxiv.org/pdf/2511.14471v1",
    "github_url": null,
    "published": "2025-11-18T13:11:33+00:00",
    "updated": "2025-11-18T13:11:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14461v1",
    "title": "Effective Diversification of Multi-Carousel Book Recommendation",
    "authors": [
      "Wilten",
      "Wenniger",
      "Hommersom"
    ],
    "summary": "Using multiple carousels, lists that wrap around and can be scrolled, is the basis for offering content in most contemporary movie streaming platforms. Carousels allow for highlighting different aspects of users' taste, that fall in categories such as genres and authors. However, while carousels offer structure and greater ease of navigation, they alone do not increase diversity in recommendations, while this is essential to keep users engaged. In this work we propose several approaches to effectively increase item diversity within the domain of book recommendations, on top of a collaborative filtering algorithm. These approaches are intended to improve book recommendations in the web catalogs of public libraries. Furthermore, we introduce metrics to evaluate the resulting strategies, and show that the proposed system finds a suitable balance between accuracy and beyond-accuracy aspects.",
    "pdf_url": "https://arxiv.org/pdf/2511.14461v1",
    "github_url": null,
    "published": "2025-11-18T13:03:16+00:00",
    "updated": "2025-11-18T13:03:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14458v1",
    "title": "Advancing Minimally Invasive Precision Surgery in Open Cavities with Robotic Flexible Endoscopy",
    "authors": [
      "Mattille",
      "Mesot",
      "Weisskopf"
    ],
    "summary": "Flexible robots hold great promise for enhancing minimally invasive surgery (MIS) by providing superior dexterity, precise control, and safe tissue interaction. Yet, translating these advantages into endoscopic interventions within open cavities remains challenging. The lack of anatomical constraints and the inherent flexibility of such devices complicate their control, while the limited field of view of endoscopes restricts situational awareness. We present a robotic platform designed to overcome these challenges and demonstrate its potential in fetoscopic laser coagulation, a complex MIS procedure typically performed only by highly experienced surgeons. Our system combines a magnetically actuated flexible endoscope with teleoperated and semi-autonomous navigation capabilities for performing targeted laser ablations. To enhance surgical awareness, the platform reconstructs real-time mosaics of the endoscopic scene, providing an extended and continuous visual context. The ability of this system to address the key limitations of MIS in open spaces is validated in vivo in an ovine model.",
    "pdf_url": "https://arxiv.org/pdf/2511.14458v1",
    "github_url": null,
    "published": "2025-11-18T13:01:05+00:00",
    "updated": "2025-11-18T13:01:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.19448v1",
    "title": "PuzzlePoles: Cylindrical Fiducial Markers Based on the PuzzleBoard Pattern",
    "authors": [
      "Zach",
      "Stelldinger"
    ],
    "summary": "Reliable perception of the environment is a key enabler for autonomous systems, where calibration and localization tasks often rely on robust visual markers. We introduce the PuzzlePole, a new type of fiducial markers derived from the recently proposed PuzzleBoard calibration pattern. The PuzzlePole is a cylindrical marker, enabling reliable recognition and pose estimation from 360° viewing direction. By leveraging the unique combinatorial structure of the PuzzleBoard pattern, PuzzlePoles provide a high accuracy in localization and orientation while being robust to occlusions. The design offers flexibility for deployment in diverse autonomous systems scenarios, ranging from robot navigation and SLAM to tangible interfaces.",
    "pdf_url": "https://arxiv.org/pdf/2511.19448v1",
    "github_url": null,
    "published": "2025-11-18T12:30:15+00:00",
    "updated": "2025-11-18T12:30:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14363v1",
    "title": "Quantum Biology, Quantum Simulation and Quantum Coherent Devices",
    "authors": [
      "Chen",
      "Dong",
      "Yang"
    ],
    "summary": "Many living organisms can exploit quantum mechanical effects to gain distinct biological advantages. In plants, photosynthesis uses quantum coherence to achieve near 100% efficiency in energy transfer. With advances in experimental techniques, two-dimensional electronic spectroscopy can reveal dynamic processes such as coherence and coupling within a system, and it plays an important role in studying energy transfer in photosynthesis. On the theory side, methods such as the generalized Bloch-Redfield theory and the hierarchical equations of motion are used to model photosynthetic systems. Quantum simulation, as a high-efficiency and low-complexity approach, has also made progress across various platforms in the study of photosynthesis. In recent years, a series of studies has introduced quantum coherence into artificial systems to enhance energy transfer efficiency, laying the groundwork for the design of coherent devices with efficient energy transport. Birds can use the weak geomagnetic field and spin-dependent chemical reactions to detect direction. Theoretical frameworks for animal navigation include magnetite-based mechanisms, magnetoreceptor genes, and the radical-pair mechanism. Quantum simulations of navigation have also advanced on multiple platforms. Inspired by animal navigation, diverse quantum effects have been applied to improve sensing and to support navigation tasks. This paper presents a comprehensive review of progress on quantum coherence in photosynthesis and avian navigation, along with related theoretical methods, quantum simulation approaches, and research on quantum coherent devices.",
    "pdf_url": "https://arxiv.org/pdf/2511.14363v1",
    "github_url": null,
    "published": "2025-11-18T11:10:31+00:00",
    "updated": "2025-11-18T11:10:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14349v1",
    "title": "ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries",
    "authors": [
      "Pu",
      "Wang",
      "Ge"
    ],
    "summary": "The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated a bilingual English-Chinese chapter dataset via a structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design a new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes a new state-of-the-art by a significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2.",
    "pdf_url": "https://arxiv.org/pdf/2511.14349v1",
    "github_url": null,
    "published": "2025-11-18T10:53:14+00:00",
    "updated": "2025-11-18T10:53:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14341v1",
    "title": "Going Places: Place Recognition in Artificial and Natural Systems",
    "authors": [
      "Milford",
      "Fischer"
    ],
    "summary": "Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies.",
    "pdf_url": "https://arxiv.org/pdf/2511.14341v1",
    "github_url": null,
    "published": "2025-11-18T10:49:14+00:00",
    "updated": "2025-11-18T10:49:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14335v2",
    "title": "Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors",
    "authors": [
      "Danial",
      "Asher",
      "Klein"
    ],
    "summary": "Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.14335v2",
    "github_url": null,
    "published": "2025-11-18T10:42:36+00:00",
    "updated": "2025-11-23T20:36:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14265v1",
    "title": "Unified Multimodal Vessel Trajectory Prediction with Explainable Navigation Intention",
    "authors": [
      "Zhang",
      "Li",
      "Liu"
    ],
    "summary": "Vessel trajectory prediction is fundamental to intelligent maritime systems. Within this domain, short-term prediction of rapid behavioral changes in complex maritime environments has established multimodal trajectory prediction (MTP) as a promising research area. However, existing vessel MTP methods suffer from limited scenario applicability and insufficient explainability. To address these challenges, we propose a unified MTP framework incorporating explainable navigation intentions, which we classify into sustained and transient categories. Our method constructs sustained intention trees from historical trajectories and models dynamic transient intentions using a Conditional Variational Autoencoder (CVAE), while using a non-local attention mechanism to maintain global scenario consistency. Experiments on real Automatic Identification System (AIS) datasets demonstrates our method's broad applicability across diverse scenarios, achieving significant improvements in both ADE and FDE. Furthermore, our method improves explainability by explicitly revealing the navigational intentions underlying each predicted trajectory.",
    "pdf_url": "https://arxiv.org/pdf/2511.14265v1",
    "github_url": null,
    "published": "2025-11-18T08:56:30+00:00",
    "updated": "2025-11-18T08:56:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17609v1",
    "title": "3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF",
    "authors": [
      "Ma",
      "Fatima",
      "Chriv"
    ],
    "summary": "Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.",
    "pdf_url": "https://arxiv.org/pdf/2511.17609v1",
    "github_url": null,
    "published": "2025-11-18T08:15:14+00:00",
    "updated": "2025-11-18T08:15:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14205v1",
    "title": "FreeMusco: Motion-Free Learning of Latent Control for Morphology-Adaptive Locomotion in Musculoskeletal Characters",
    "authors": [
      "Kim",
      "Lee"
    ],
    "summary": "We propose FreeMusco, a motion-free framework that jointly learns latent representations and control policies for musculoskeletal characters. By leveraging the musculoskeletal model as a strong prior, our method enables energy-aware and morphology-adaptive locomotion to emerge without motion data. The framework generalizes across human, non-human, and synthetic morphologies, where distinct energy-efficient strategies naturally appear--for example, quadrupedal gaits in Chimanoid versus bipedal gaits in Humanoid. The latent space and corresponding control policy are constructed from scratch, without demonstration, and enable downstream tasks such as goal navigation and path following--representing, to our knowledge, the first motion-free method to provide such capabilities. FreeMusco learns diverse and physically plausible locomotion behaviors through model-based reinforcement learning, guided by the locomotion objective that combines control, balancing, and biomechanical terms. To better capture the periodic structure of natural gait, we introduce the temporally averaged loss formulation, which compares simulated and target states over a time window rather than on a per-frame basis. We further encourage behavioral diversity by randomizing target poses and energy levels during training, enabling locomotion to be flexibly modulated in both form and intensity at runtime. Together, these results demonstrate that versatile and adaptive locomotion control can emerge without motion capture, offering a new direction for simulating movement in characters where data collection is impractical or impossible.",
    "pdf_url": "https://arxiv.org/pdf/2511.14205v1",
    "github_url": null,
    "published": "2025-11-18T07:23:46+00:00",
    "updated": "2025-11-18T07:23:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14161v2",
    "title": "RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action",
    "authors": [
      "Sun",
      "Zhang",
      "Pang"
    ],
    "summary": "Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an \"Action (Object, Container)\" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.",
    "pdf_url": "https://arxiv.org/pdf/2511.14161v2",
    "github_url": null,
    "published": "2025-11-18T05:54:05+00:00",
    "updated": "2025-11-19T04:44:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00049v1",
    "title": "Socially aware navigation for mobile robots: a survey on deep reinforcement learning approaches",
    "authors": [
      "Kabir",
      "Mysorewala"
    ],
    "summary": "Socially aware navigation is a fast-evolving research area in robotics that enables robots to move within human environments while adhering to the implicit human social norms. The advent of Deep Reinforcement Learning (DRL) has accelerated the development of navigation policies that enable robots to incorporate these social conventions while effectively reaching their objectives. This survey offers a comprehensive overview of DRL-based approaches to socially aware navigation, highlighting key aspects such as proxemics, human comfort, naturalness, trajectory and intention prediction, which enhance robot interaction in human environments. This work critically analyzes the integration of value-based, policy-based, and actor-critic reinforcement learning algorithms alongside neural network architectures, such as feedforward, recurrent, convolutional, graph, and transformer networks, for enhancing agent learning and representation in socially aware navigation. Furthermore, we examine crucial evaluation mechanisms, including metrics, benchmark datasets, simulation environments, and the persistent challenges of sim-to-real transfer. Our comparative analysis of the literature reveals that while DRL significantly improves safety, and human acceptance over traditional approaches, the field still faces setback due to non-uniform evaluation mechanisms, absence of standardized social metrics, computational burdens that limit scalability, and difficulty in transferring simulation to real robotic hardware applications. We assert that future progress will depend on hybrid approaches that leverage the strengths of multiple approaches and producing benchmarks that balance technical efficiency with human-centered evaluation.",
    "pdf_url": "https://arxiv.org/pdf/2512.00049v1",
    "github_url": null,
    "published": "2025-11-18T05:33:28+00:00",
    "updated": "2025-11-18T05:33:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14149v1",
    "title": "iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion",
    "authors": [
      "Wang",
      "Zhao",
      "Xu"
    ],
    "summary": "Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model. However, existing approaches typically rely on an iterative \\textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose. This multi-round process incurs significant computational overhead, hindering real-time performance in robotics. In this paper, we propose iGaussian, a two-stage feed-forward framework that achieves real-time camera pose estimation through direct 3D Gaussian inversion. Our method first regresses a coarse 6DoF pose using a Gaussian Scene Prior-based Pose Regression Network with spatial uniform sampling and guided attention mechanisms, then refines it through feature matching and multi-model fusion. The key contribution lies in our cross-correlation module that aligns image embeddings with 3D Gaussian attributes without differentiable rendering, coupled with a Weighted Multiview Predictor that fuses features from Multiple strategically sampled viewpoints. Experimental results on the NeRF Synthetic, Mip-NeRF 360, and T\\&T+DB datasets demonstrate a significant performance improvement over previous methods, reducing median rotation errors to 0.2° while achieving 2.87 FPS tracking on mobile robots, which is an impressive 10 times speedup compared to optimization-based approaches. Code: https://github.com/pythongod-exe/iGaussian",
    "pdf_url": "https://arxiv.org/pdf/2511.14149v1",
    "github_url": "https://github.com/pythongod-exe/iGaussian",
    "published": "2025-11-18T05:22:22+00:00",
    "updated": "2025-11-18T05:22:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14131v1",
    "title": "Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation",
    "authors": [
      "Zhong",
      "Zhang",
      "Zhang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.",
    "pdf_url": "https://arxiv.org/pdf/2511.14131v1",
    "github_url": null,
    "published": "2025-11-18T04:32:00+00:00",
    "updated": "2025-11-18T04:32:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14107v1",
    "title": "RTS-Mono: A Real-Time Self-Supervised Monocular Depth Estimation Method for Real-World Deployment",
    "authors": [
      "Cheng",
      "Liu",
      "Lei"
    ],
    "summary": "Depth information is crucial for autonomous driving and intelligent robot navigation. The simplicity and flexibility of self-supervised monocular depth estimation are conducive to its role in these fields. However, most existing monocular depth estimation models consume many computing resources. Although some methods have reduced the model's size and improved computing efficiency, the performance deteriorates, seriously hindering the real-world deployment of self-supervised monocular depth estimation models in the real world. To address this problem, we proposed a real-time self-supervised monocular depth estimation method and implemented it in the real world. It is called RTS-Mono, which is a lightweight and efficient encoder-decoder architecture. The encoder is based on Lite-Encoder, and the decoder is designed with a multi-scale sparse fusion framework to minimize redundancy, ensure performance, and improve inference speed. RTS-Mono achieved state-of-the-art (SoTA) performance in high and low resolutions with extremely low parameter counts (3 M) in experiments based on the KITTI dataset. Compared with lightweight methods, RTS-Mono improved Abs Rel and Sq Rel by 5.6% and 9.8% at low resolution and improved Sq Rel and RMSE by 6.1% and 1.9% at high resolution. In real-world deployment experiments, RTS-Mono has extremely high accuracy and can perform real-time inference on Nvidia Jetson Orin at a speed of 49 FPS. Source code is available at https://github.com/ZYCheng777/RTS-Mono.",
    "pdf_url": "https://arxiv.org/pdf/2511.14107v1",
    "github_url": "https://github.com/ZYCheng777/RTS-Mono",
    "published": "2025-11-18T03:47:04+00:00",
    "updated": "2025-11-18T03:47:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14096v1",
    "title": "NeuroPath: Neurobiology-Inspired Path Tracking and Reflection for Semantically Coherent Retrieval",
    "authors": [
      "Li",
      "Wang",
      "Huang"
    ],
    "summary": "Retrieval-augmented generation (RAG) greatly enhances large language models (LLMs) performance in knowledge-intensive tasks. However, naive RAG methods struggle with multi-hop question answering due to their limited capacity to capture complex dependencies across documents. Recent studies employ graph-based RAG to capture document connections. However, these approaches often result in a loss of semantic coherence and introduce irrelevant noise during node matching and subgraph construction. To address these limitations, we propose NeuroPath, an LLM-driven semantic path tracking RAG framework inspired by the path navigational planning of place cells in neurobiology. It consists of two steps: Dynamic Path Tracking and Post-retrieval Completion. Dynamic Path Tracking performs goal-directed semantic path tracking and pruning over the constructed knowledge graph (KG), improving noise reduction and semantic coherence. Post-retrieval Completion further reinforces these benefits by conducting second-stage retrieval using intermediate reasoning and the original query to refine the query goal and complete missing information in the reasoning path. NeuroPath surpasses current state-of-the-art baselines on three multi-hop QA datasets, achieving average improvements of 16.3% on recall@2 and 13.5% on recall@5 over advanced graph-based RAG methods. Moreover, compared to existing iter-based RAG methods, NeuroPath achieves higher accuracy and reduces token consumption by 22.8%. Finally, we demonstrate the robustness of NeuroPath across four smaller LLMs (Llama3.1, GLM4, Mistral0.3, and Gemma3), and further validate its scalability across tasks of varying complexity. Code is available at https://github.com/KennyCaty/NeuroPath.",
    "pdf_url": "https://arxiv.org/pdf/2511.14096v1",
    "github_url": "https://github.com/KennyCaty/NeuroPath",
    "published": "2025-11-18T03:28:23+00:00",
    "updated": "2025-11-18T03:28:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14037v1",
    "title": "BIM-Discrepancy-Driven Active Sensing for Risk-Aware UAV-UGV Navigation",
    "authors": [
      "Mojtahedi",
      "Akhavian"
    ],
    "summary": "This paper presents a BIM-discrepancy-driven active sensing framework for cooperative navigation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) in dynamic construction environments. Traditional navigation approaches rely on static Building Information Modeling (BIM) priors or limited onboard perception. In contrast, our framework continuously fuses real-time LiDAR data from aerial and ground robots with BIM priors to maintain an evolving 2D occupancy map. We quantify navigation safety through a unified corridor-risk metric integrating occupancy uncertainty, BIM-map discrepancy, and clearance. When risk exceeds safety thresholds, the UAV autonomously re-scans affected regions to reduce uncertainty and enable safe replanning. Validation in PX4-Gazebo simulation with Robotec GPU LiDAR demonstrates that risk-triggered re-scanning reduces mean corridor risk by 58% and map entropy by 43% compared to static BIM navigation, while maintaining clearance margins above 0.4 m. Compared to frontier-based exploration, our approach achieves similar uncertainty reduction in half the mission time. These results demonstrate that integrating BIM priors with risk-adaptive aerial sensing enables scalable, uncertainty-aware autonomy for construction robotics.",
    "pdf_url": "https://arxiv.org/pdf/2511.14037v1",
    "github_url": null,
    "published": "2025-11-18T01:43:04+00:00",
    "updated": "2025-11-18T01:43:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.14024v1",
    "title": "FACA: Fair and Agile Multi-Robot Collision Avoidance in Constrained Environments with Dynamic Priorities",
    "authors": [
      "Singh",
      "Chandra"
    ],
    "summary": "Multi-robot systems are increasingly being used for critical applications such as rescuing injured people, delivering food and medicines, and monitoring key areas. These applications usually involve navigating at high speeds through constrained spaces such as small gaps. Navigating such constrained spaces becomes particularly challenging when the space is crowded with multiple heterogeneous agents all of which have urgent priorities. What makes the problem even harder is that during an active response situation, roles and priorities can quickly change on a dime without informing the other agents. In order to complete missions in such environments, robots must not only be safe, but also agile, able to dodge and change course at a moment's notice. In this paper, we propose FACA, a fair and agile collision avoidance approach where robots coordinate their tasks by talking to each other via natural language (just as people do). In FACA, robots balance safety with agility via a novel artificial potential field algorithm that creates an automatic ``roundabout'' effect whenever a conflict arises. Our experiments show that FACA achieves a improvement in efficiency, completing missions more than 3.5X faster than baselines with a time reduction of over 70% while maintaining robust safety margins.",
    "pdf_url": "https://arxiv.org/pdf/2511.14024v1",
    "github_url": null,
    "published": "2025-11-18T01:04:04+00:00",
    "updated": "2025-11-18T01:04:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13996v1",
    "title": "Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education",
    "authors": [
      "Xu",
      "Martin"
    ],
    "summary": "ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional \"independent thinking-manual coding-iterative debugging\" to \"AI-assisted ideation-interactive programming-collaborative optimization.\" Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.",
    "pdf_url": "https://arxiv.org/pdf/2511.13996v1",
    "github_url": null,
    "published": "2025-11-17T23:54:08+00:00",
    "updated": "2025-11-17T23:54:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13985v1",
    "title": "LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry",
    "authors": [
      "Quenzel",
      "Behnke"
    ],
    "summary": "Autonomous robotic systems heavily rely on environment knowledge to safely navigate. For search & rescue, a flying robot requires robust real-time perception, enabled by complementary sensors. IMU data constrains acceleration and rotation, whereas LiDAR measures accurate distances around the robot. Building upon the LiDAR odometry MARS, our LiDAR-inertial odometry (LIO) jointly aligns multi-resolution surfel maps with a Gaussian mixture model (GMM) using a continuous-time B-spline trajectory. Our new scan window uses non-uniform temporal knot placement to ensure continuity over the whole trajectory without additional scan delay. Moreover, we accelerate essential covariance and GMM computations with Kronecker sums and products by a factor of 3.3. An unscented transform de-skews surfels, while a splitting into intra-scan segments facilitates motion compensation during spline optimization. Complementary soft constraints on relative poses and preintegrated IMU pseudo-measurements further improve robustness and accuracy. Extensive evaluation showcases the state-of-the-art quality of our LIO-MARS w.r.t. recent LIO systems on various handheld, ground and aerial vehicle-based datasets.",
    "pdf_url": "https://arxiv.org/pdf/2511.13985v1",
    "github_url": null,
    "published": "2025-11-17T23:35:07+00:00",
    "updated": "2025-11-17T23:35:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13981v1",
    "title": "Data Whitening Improves Sparse Autoencoder Learning",
    "authors": [
      "Saraswatula",
      "Klindt"
    ],
    "summary": "Sparse autoencoders (SAEs) have emerged as a promising approach for learning interpretable features from neural network activations. However, the optimization landscape for SAE training can be challenging due to correlations in the input data. We demonstrate that applying PCA Whitening to input activations -- a standard preprocessing technique in classical sparse coding -- improves SAE performance across multiple metrics. Through theoretical analysis and simulation, we show that whitening transforms the optimization landscape, making it more convex and easier to navigate. We evaluate both ReLU and Top-K SAEs across diverse model architectures, widths, and sparsity regimes. Empirical evaluation on SAEBench, a comprehensive benchmark for sparse autoencoders, reveals that whitening consistently improves interpretability metrics, including sparse probing accuracy and feature disentanglement, despite minor drops in reconstruction quality. Our results challenge the assumption that interpretability aligns with an optimal sparsity--fidelity trade-off and suggest that whitening should be considered as a default preprocessing step for SAE training, particularly when interpretability is prioritized over perfect reconstruction.",
    "pdf_url": "https://arxiv.org/pdf/2511.13981v1",
    "github_url": null,
    "published": "2025-11-17T23:20:58+00:00",
    "updated": "2025-11-17T23:20:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13524v1",
    "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
    "authors": [
      "Peng",
      "Pan",
      "He"
    ],
    "summary": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
    "pdf_url": "https://arxiv.org/pdf/2511.13524v1",
    "github_url": null,
    "published": "2025-11-17T15:58:46+00:00",
    "updated": "2025-11-17T15:58:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13489v1",
    "title": "PolicyBot - Reliable Question Answering over Policy Documents",
    "authors": [
      "Nagarajan",
      "Kumar",
      "Santhiappan"
    ],
    "summary": "All citizens of a country are affected by the laws and policies introduced by their government. These laws and policies serve essential functions for citizens. Such as granting them certain rights or imposing specific obligations. However, these documents are often lengthy, complex, and difficult to navigate, making it challenging for citizens to locate and understand relevant information. This work presents PolicyBot, a retrieval-augmented generation (RAG) system designed to answer user queries over policy documents with a focus on transparency and reproducibility. The system combines domain-specific semantic chunking, multilingual dense embeddings, multi-stage retrieval with reranking, and source-aware generation to provide responses grounded in the original documents. We implemented citation tracing to reduce hallucinations and improve user trust, and evaluated alternative retrieval and generation configurations to identify effective design choices. The end-to-end pipeline is built entirely with open-source tools, enabling easy adaptation to other domains requiring document-grounded question answering. This work highlights design considerations, practical challenges, and lessons learned in deploying trustworthy RAG systems for governance-related contexts.",
    "pdf_url": "https://arxiv.org/pdf/2511.13489v1",
    "github_url": null,
    "published": "2025-11-17T15:26:10+00:00",
    "updated": "2025-11-17T15:26:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13394v1",
    "title": "Fast and Robust Simulation-Based Inference With Optimization Monte Carlo",
    "authors": [
      "Gkolemis",
      "Diou",
      "Gutmann"
    ],
    "summary": "Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.",
    "pdf_url": "https://arxiv.org/pdf/2511.13394v1",
    "github_url": null,
    "published": "2025-11-17T14:07:36+00:00",
    "updated": "2025-11-17T14:07:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13371v1",
    "title": "Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning",
    "authors": [
      "Baumgartner",
      "Spens",
      "Burgess"
    ],
    "summary": "How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.",
    "pdf_url": "https://arxiv.org/pdf/2511.13371v1",
    "github_url": null,
    "published": "2025-11-17T13:46:19+00:00",
    "updated": "2025-11-17T13:46:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13334v1",
    "title": "Basis Immunity: Isotropy as a Regularizer for Uncertainty",
    "authors": [
      "Segonne"
    ],
    "summary": "Diversification is a cornerstone of robust portfolio construction, yet its application remains fraught with challenges due to model uncertainty and estimation errors. Practitioners often rely on sophisticated, proprietary heuristics to navigate these issues. Among recent advancements, Agnostic Risk Parity introduces eigenrisk parity (ERP), an innovative approach that leverages isotropy to evenly allocate risk across eigenmodes, enhancing portfolio stability.   In this paper, we review and extend the isotropy-enforced philosophy of ERP proposing a versatile framework that integrates mean-variance optimization with an isotropy constraint acting as a geometric regularizer against signal uncertainty. The resulting allocations decompose naturally into canonical portfolios, smoothly interpolating between full isotropy (closed-form isotropic-mean allocation) and pure mean-variance through a tunable isotropy penalty.   Beyond methodology, we revisit fundamental concepts and clarify foundational links between isotropy, canonical portfolios, principal portfolios, primal versus dual representations, and intrinsic basis-invariant metrics for returns, risk, and isotropy. Applied to sector trend-following, the isotropy constraint systematically induces negative average-signal exposure -- a structural, parameter-robust crash hedge.   This work offers both a practical, theoretically grounded tool for resilient allocation under signal uncertainty and a pedagogical synthesis of modern portfolio concepts.",
    "pdf_url": "https://arxiv.org/pdf/2511.13334v1",
    "github_url": null,
    "published": "2025-11-17T13:06:43+00:00",
    "updated": "2025-11-17T13:06:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13278v2",
    "title": "SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting",
    "authors": [
      "Li",
      "Wang",
      "Gan"
    ],
    "summary": "Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/",
    "pdf_url": "https://arxiv.org/pdf/2511.13278v2",
    "github_url": null,
    "published": "2025-11-17T11:50:52+00:00",
    "updated": "2025-11-21T09:21:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13269v1",
    "title": "Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation",
    "authors": [
      "Zhang",
      "Zhang",
      "Li"
    ],
    "summary": "Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.",
    "pdf_url": "https://arxiv.org/pdf/2511.13269v1",
    "github_url": "https://github.com/linglingxiansen/SpatialSKy",
    "published": "2025-11-17T11:39:20+00:00",
    "updated": "2025-11-17T11:39:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13259v1",
    "title": "GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models",
    "authors": [
      "Zheng",
      "Ying",
      "Duan"
    ],
    "summary": "Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \\textit{etc}. To bridge this gap, we introduce \\textbf{GeoX-Bench}, a comprehensive \\underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \\underline{cross}-view \\underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \\textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.",
    "pdf_url": "https://arxiv.org/pdf/2511.13259v1",
    "github_url": "https://github.com/IntMeGroup/GeoX-Bench",
    "published": "2025-11-17T11:19:07+00:00",
    "updated": "2025-11-17T11:19:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13216v1",
    "title": "GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry",
    "authors": [
      "Noh",
      "Jung",
      "Kim"
    ],
    "summary": "Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO",
    "pdf_url": "https://arxiv.org/pdf/2511.13216v1",
    "github_url": null,
    "published": "2025-11-17T10:29:31+00:00",
    "updated": "2025-11-17T10:29:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13207v1",
    "title": "PIGEON: VLM-Driven Object Navigation via Points of Interest Selection",
    "authors": [
      "Peng",
      "Zhang",
      "Chi"
    ],
    "summary": "Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.",
    "pdf_url": "https://arxiv.org/pdf/2511.13207v1",
    "github_url": null,
    "published": "2025-11-17T10:19:13+00:00",
    "updated": "2025-11-17T10:19:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13188v1",
    "title": "Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control",
    "authors": [
      "Ali",
      "Koutsoftas",
      "Zhang"
    ],
    "summary": "This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.13188v1",
    "github_url": null,
    "published": "2025-11-17T09:51:32+00:00",
    "updated": "2025-11-17T09:51:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13181v1",
    "title": "Probabilistic dynamics of small groups in crowd flows",
    "authors": [
      "Laan",
      "Corbetta"
    ],
    "summary": "Pedestrians in crowds frequently move as part of small groups, constituting up to 70% of individuals. Dyads (groups of two) are the most frequent. Understanding quantitatively the dynamics of dyads walking in crowds is therefore an essential building block towards a fundamental comprehension of crowd behavior as a whole, and mandatory for accurate crowd dynamics models. Unavoidably, due to the non-deterministic behavior of pedestrians, characterizations of the dynamics must be probabilistic. In this work, we analyze the dynamics of over 6M dyads: a statistical ensemble of unprecedented resolution within a multi-year real-life pedestrian trajectory measurement campaign (21M trajectories, from Eindhoven Station, NL). We provide phenomenological models for dyad behavior in dependence of the surrounding crowds state. We present a thorough collection of fundamental diagrams that probabilistically relate both dyad velocity and formation to the state of the surrounding crowd (density, relative velocity). Depending on the surrounding crowd, dyads adjust interpersonal distance and may shift in formation, possibly turning from abreast states (which favors social interaction) to in-line (which favors navigationing dense crowds). To quantitatively investigate formation changes, we introduce a scalar indicator, which we dub Orientation Log-Odds (OLO), that quantifies the relative log-likelihood of abreast versus in-file formations. Conceptually, the OLO quantifies energy difference of the abreast vs. in-file configuration under a Boltzmann-like assumption. We model how OLO depends on the crowd state, showcasing that its derivative is a product of two velocity-density fundamental diagrams. Together, these results provide a statistically robust, data-driven description of dyad configuration dynamics in real-world crowds, establishing a foundation towards new predictive, group-aware crowd models.",
    "pdf_url": "https://arxiv.org/pdf/2511.13181v1",
    "github_url": null,
    "published": "2025-11-17T09:42:35+00:00",
    "updated": "2025-11-17T09:42:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13132v1",
    "title": "Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack",
    "authors": [
      "Li",
      "Tang",
      "Huang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.",
    "pdf_url": "https://arxiv.org/pdf/2511.13132v1",
    "github_url": null,
    "published": "2025-11-17T08:39:29+00:00",
    "updated": "2025-11-17T08:39:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13096v1",
    "title": "ResAlignNet: A Data-Driven Approach for INS/DVL Alignment",
    "authors": [
      "Damari",
      "Klein"
    ],
    "summary": "Autonomous underwater vehicles rely on precise navigation systems that combine the inertial navigation system and the Doppler velocity log for successful missions in challenging environments where satellite navigation is unavailable. The effectiveness of this integration critically depends on accurate alignment between the sensor reference frames. Standard model-based alignment methods between these sensor systems suffer from lengthy convergence times, dependence on prescribed motion patterns, and reliance on external aiding sensors, significantly limiting operational flexibility. To address these limitations, this paper presents ResAlignNet, a data-driven approach using the 1D ResNet-18 architecture that transforms the alignment problem into deep neural network optimization, operating as an in-situ solution that requires only sensors on board without external positioning aids or complex vehicle maneuvers, while achieving rapid convergence in seconds. Additionally, the approach demonstrates the learning capabilities of Sim2Real transfer, enabling training in synthetic data while deploying in operational sensor measurements. Experimental validation using the Snapir autonomous underwater vehicle demonstrates that ResAlignNet achieves alignment accuracy within 0.8° using only 25 seconds of data collection, representing a 65\\% reduction in convergence time compared to standard velocity-based methods. The trajectory-independent solution eliminates motion pattern requirements and enables immediate vehicle deployment without lengthy pre-mission procedures, advancing underwater navigation capabilities through robust sensor-agnostic alignment that scales across different operational scenarios and sensor specifications.",
    "pdf_url": "https://arxiv.org/pdf/2511.13096v1",
    "github_url": null,
    "published": "2025-11-17T07:50:44+00:00",
    "updated": "2025-11-17T07:50:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13071v1",
    "title": "Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers",
    "authors": [
      "Levin",
      "Klein"
    ],
    "summary": "Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.",
    "pdf_url": "https://arxiv.org/pdf/2511.13071v1",
    "github_url": null,
    "published": "2025-11-17T07:15:24+00:00",
    "updated": "2025-11-17T07:15:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13048v1",
    "title": "Unidirectional-Road-Network-Based Global Path Planning for Cleaning Robots in Semi-Structured Environments",
    "authors": [
      "Li",
      "Cheng"
    ],
    "summary": "Practical global path planning is critical for commercializing cleaning robots working in semi-structured environments. In the literature, global path planning methods for free space usually focus on path length and neglect the traffic rule constraints of the environments, which leads to high-frequency re-planning and increases collision risks. In contrast, those for structured environments are developed mainly by strictly complying with the road network representing the traffic rule constraints, which may result in an overlong path that hinders the overall navigation efficiency. This article proposes a general and systematic approach to improve global path planning performance in semi-structured environments. A unidirectional road network is built to represent the traffic constraints in semi-structured environments and a hybrid strategy is proposed to achieve a guaranteed planning result.Cutting across the road at the starting and the goal points are allowed to achieve a shorter path. Especially, a two-layer potential map is proposed to achieve a guaranteed performance when the starting and the goal points are in complex intersections. Comparative experiments are carried out to validate the effectiveness of the proposed method. Quantitative experimental results show that, compared with the state-of-art, the proposed method guarantees a much better balance between path length and the consistency with the road network.",
    "pdf_url": "https://arxiv.org/pdf/2511.13048v1",
    "github_url": null,
    "published": "2025-11-17T06:52:41+00:00",
    "updated": "2025-11-17T06:52:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13047v1",
    "title": "DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation",
    "authors": [
      "Gong",
      "Lu",
      "Gao"
    ],
    "summary": "Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.",
    "pdf_url": "https://arxiv.org/pdf/2511.13047v1",
    "github_url": "https://github.com/gongyan1/DiffPixelFormer",
    "published": "2025-11-17T06:51:07+00:00",
    "updated": "2025-11-17T06:51:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13042v1",
    "title": "APP: A* Post-Processing Algorithm for Robots with Bidirectional Shortcut and Path Perturbation",
    "authors": [
      "Li",
      "Cheng"
    ],
    "summary": "Paths generated by A* and other graph-search-based planners are widely used in the robotic field. Due to the restricted node-expansion directions, the resulting paths are usually not the shortest. Besides, unnecessary heading changes, or zig-zag patterns, exist even when no obstacle is nearby, which is inconsistent with the human intuition that the path segments should be straight in wide-open space due to the absence of obstacles. This article puts forward a general and systematic post-processing algorithm for A* and other graph-search-based planners. The A* post-processing algorithm, called APP, is developed based on the costmap, which is widely used in commercial service robots. First, a bidirectional vertices reduction algorithm is proposed to tackle the asymm- etry of the path and the environments. During the forward and backward vertices reduction, a thorough shortcut strategy is put forward to improve the path-shortening performance and avoid unnecessary heading changes. Second, an iterative path perturbation algorithm is adopted to locally reduce the number of unnecessary heading changes and improve the path smooth- ness. Comparative experiments are then carried out to validate the superiority of the proposed method. Quantitative performance indexes show that APP outperforms the existing methods in planning time, path length as well as the number of unnecessary heading changes. Finally, field navigation experiments are carried out to verify the practicability of APP.",
    "pdf_url": "https://arxiv.org/pdf/2511.13042v1",
    "github_url": null,
    "published": "2025-11-17T06:42:37+00:00",
    "updated": "2025-11-17T06:42:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13006v1",
    "title": "Cooperative ISAC for LAE: Joint Trajectory Planning, Power allocation, and Dynamic Time Division",
    "authors": [
      "Li",
      "Ren",
      "Pan"
    ],
    "summary": "To enhance the performance of aerial-ground networks, this paper proposes an integrated sensing and communication (ISAC) framework for multi-UAV systems. In our model, ground base stations (BSs) cooperatively serve multiple unmanned aerial vehicles (UAVs), and employ a time-division strategy in which beam scanning for sensing comes before data communication in each time slot. To maximize the sum communication rate while satisfying the total sensing mutual information (MI) requirement, we jointly optimize the UAV trajectories, communication and sensing power allocation, and the dynamic time-division ratio. The resulting non-convex optimization problem is efficiently solved using an alternating optimization (AO) framework. Simulation results demonstrate that our proposed joint design significantly outperforms benchmark schemes with static or partially optimized resources. The findings also reveal the critical importance of dynamic trajectory and resource management for effectively navigating the sensing-communication trade-off, especially under stringent power or sensing constraints.",
    "pdf_url": "https://arxiv.org/pdf/2511.13006v1",
    "github_url": null,
    "published": "2025-11-17T06:00:17+00:00",
    "updated": "2025-11-17T06:00:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12997v1",
    "title": "WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance",
    "authors": [
      "Liu",
      "Geng",
      "Li"
    ],
    "summary": "Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.",
    "pdf_url": "https://arxiv.org/pdf/2511.12997v1",
    "github_url": null,
    "published": "2025-11-17T05:38:50+00:00",
    "updated": "2025-11-17T05:38:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12984v1",
    "title": "CUTE-Planner: Confidence-aware Uneven Terrain Exploration Planner",
    "authors": [
      "Park",
      "Cho",
      "Kim"
    ],
    "summary": "Planetary exploration robots must navigate uneven terrain while building reliable maps for space missions. However, most existing methods incorporate traversability constraints but may not handle high uncertainty in elevation estimates near complex features like craters, do not consider exploration strategies for uncertainty reduction, and typically fail to address how elevation uncertainty affects navigation safety and map quality. To address the problems, we propose a framework integrating safe path generation, adaptive confidence updates, and confidence-aware exploration strategies. Using Kalman-based elevation estimation, our approach generates terrain traversability and confidence scores, then incorporates them into Graph-Based exploration Planner (GBP) to prioritize exploration of traversable low-confidence regions. We evaluate our framework through simulated lunar experiments using a novel low-confidence region ratio metric, achieving 69% uncertainty reduction compared to baseline GBP. In terms of mission success rate, our method achieves 100% while baseline GBP achieves 0%, demonstrating improvements in exploration safety and map reliability.",
    "pdf_url": "https://arxiv.org/pdf/2511.12984v1",
    "github_url": null,
    "published": "2025-11-17T05:14:05+00:00",
    "updated": "2025-11-17T05:14:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12972v1",
    "title": "SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models",
    "authors": [
      "Narasimhan",
      "Lisondra",
      "Wang"
    ],
    "summary": "The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.",
    "pdf_url": "https://arxiv.org/pdf/2511.12972v1",
    "github_url": null,
    "published": "2025-11-17T04:49:28+00:00",
    "updated": "2025-11-17T04:49:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12910v1",
    "title": "TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints",
    "authors": [
      "Li",
      "Huang",
      "Chen"
    ],
    "summary": "Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone programming (SOCP). Subsequently, comparative experiments are conducted to validate the superiority of the proposed method. Quantitative performance indexes show that TOPP-DWR achieves TOPP while adhering to all constraints. Finally, field autonomous navigation experiments are carried out to validate the practicability of TOPP-DWR in real-world applications.",
    "pdf_url": "https://arxiv.org/pdf/2511.12910v1",
    "github_url": null,
    "published": "2025-11-17T02:58:53+00:00",
    "updated": "2025-11-17T02:58:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12853v1",
    "title": "BrainNormalizer: Anatomy-Informed Pseudo-Healthy Brain Reconstruction from Tumor MRI via Edge-Guided ControlNet",
    "authors": [
      "Kwak",
      "Lee",
      "Wang"
    ],
    "summary": "Brain tumors are among the most clinically significant neurological diseases and remain a major cause of morbidity and mortality due to their aggressive growth and structural heterogeneity. As tumors expand, they induce substantial anatomical deformation that disrupts both local tissue organization and global brain architecture, complicating diagnosis, treatment planning, and surgical navigation. Yet a subject-specific reference of how the brain would appear without tumor-induced changes is fundamentally unobtainable in clinical practice. We present BrainNormalizer, an anatomy-informed diffusion framework that reconstructs pseudo-healthy MRIs directly from tumorous scans by conditioning the generative process on boundary cues extracted from the subject's own anatomy. This boundary-guided conditioning enables anatomically plausible pseudo-healthy reconstruction without requiring paired non-tumorous and tumorous scans. BrainNormalizer employs a two-stage training strategy. The pretrained diffusion model is first adapted through inpainting-based fine-tuning on tumorous and non-tumorous scans. Next, an edge-map-guided ControlNet branch is trained to inject fine-grained anatomical contours into the frozen decoder while preserving learned priors. During inference, a deliberate misalignment strategy pairs tumorous inputs with non-tumorous prompts and mirrored contralateral edge maps, leveraging hemispheric correspondence to guide reconstruction. On the BraTS2020 dataset, BrainNormalizer achieves strong quantitative performance and qualitatively produces anatomically plausible reconstructions in tumor-affected regions while retaining overall structural coherence. BrainNormalizer provides clinically reliable anatomical references for treatment planning and supports new research directions in counterfactual modeling and tumor-induced deformation analysis.",
    "pdf_url": "https://arxiv.org/pdf/2511.12853v1",
    "github_url": null,
    "published": "2025-11-17T00:48:30+00:00",
    "updated": "2025-11-17T00:48:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12848v1",
    "title": "Structured Imitation Learning of Interactive Policies through Inverse Games",
    "authors": [
      "Sun",
      "Murphey"
    ],
    "summary": "Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.",
    "pdf_url": "https://arxiv.org/pdf/2511.12848v1",
    "github_url": null,
    "published": "2025-11-17T00:42:45+00:00",
    "updated": "2025-11-17T00:42:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12798v1",
    "title": "Regime shifts and transformations in social-ecological systems: Advancing critical frontiers for safe and just futures",
    "authors": [
      "Rocha",
      "Schil",
      "Lindkvist"
    ],
    "summary": "Current research challenges in sustainability science require us to consider nonlinear changes e.g. shifts that do not happen gradually but can be sudden and difficult to predict. Central questions are therefore how we can prevent harmful shifts, promote desirable ones, and better anticipate both. The regime shifts and transformations literature is well-equipped to address these questions. Yet, even though both research streams stem from the same intellectual roots, they have developed along different paths, with limited exchange between the two, missing opportunities for cross- fertilisation. We here review the definitions and history of both research streams to disentangle common grounds and differences. We propose avenues for future research and highlight how stronger integration of both research streams could support the development of more powerful approaches to help us navigate toward safe and just futures.",
    "pdf_url": "https://arxiv.org/pdf/2511.12798v1",
    "github_url": null,
    "published": "2025-11-16T21:57:09+00:00",
    "updated": "2025-11-16T21:57:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12778v1",
    "title": "DR. Nav: Semantic-Geometric Representations for Proactive Dead-End Recovery and Navigation",
    "authors": [
      "Rajagopal",
      "Mudiyanselage",
      "Seneviratne"
    ],
    "summary": "We present DR. Nav (Dead-End Recovery-aware Navigation), a novel approach to autonomous navigation in scenarios where dead-end detection and recovery are critical, particularly in unstructured environments where robots must handle corners, vegetation occlusions, and blocked junctions. DR. Nav introduces a proactive strategy for navigation in unmapped environments without prior assumptions. Our method unifies dead-end prediction and recovery by generating a single, continuous, real-time semantic cost map. Specifically, DR. Nav leverages cross-modal RGB-LiDAR fusion with attention-based filtering to estimate per-cell dead-end likelihoods and recovery points, which are continuously updated through Bayesian inference to enhance robustness. Unlike prior mapping methods that only encode traversability, DR. Nav explicitly incorporates recovery-aware risk into the navigation cost map, enabling robots to anticipate unsafe regions and plan safer alternative trajectories. We evaluate DR. Nav across multiple dense indoor and outdoor scenarios and demonstrate an increase of 83.33% in accuracy in detection, a 52.4% reduction in time-to-goal (path efficiency), compared to state-of-the-art planners such as DWA, MPPI, and Nav2 DWB. Furthermore, the dead-end classifier functions",
    "pdf_url": "https://arxiv.org/pdf/2511.12778v1",
    "github_url": null,
    "published": "2025-11-16T21:04:08+00:00",
    "updated": "2025-11-16T21:04:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12751v1",
    "title": "Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving",
    "authors": [
      "Anvar",
      "Chen",
      "Wang"
    ],
    "summary": "Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.",
    "pdf_url": "https://arxiv.org/pdf/2511.12751v1",
    "github_url": null,
    "published": "2025-11-16T19:31:42+00:00",
    "updated": "2025-11-16T19:31:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17588v1",
    "title": "Synthesis of mass-spring networks from high-level code descriptions",
    "authors": [
      "Omidvar",
      "Serra-Garcia"
    ],
    "summary": "Structural nonlinearity can be harnessed to program complex functionalities in robotic devices. However, it remains a challenge to design nonlinear systems that will accomplish a specific, desired task. The responses that we typically describe as intelligent -- such a robot navigating a maze -- require a large number of degrees of freedom and cannot be captured by traditional optimization objective functions. In this work, we explore a code-based synthesis approach to design mass-spring systems with embodied intelligence. The approach starts from a source code, written in a \\emph{mechanical description language}, that details the system boundary, sensor and actuator locations, and desired behavior. A synthesizer software then automatically generates a mass-spring network that performs the described function from the source code description. We exemplify this methodology by designing mass-spring systems realizing a maze-navigating robot and a programmable lock. Remarkably, mechanical description languages can be combined with large-language models, to translate a natural-language description of a task into a functional device.",
    "pdf_url": "https://arxiv.org/pdf/2511.17588v1",
    "github_url": null,
    "published": "2025-11-16T17:57:42+00:00",
    "updated": "2025-11-16T17:57:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12676v1",
    "title": "BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections",
    "authors": [
      "Varghese",
      "Gao",
      "Rahman"
    ],
    "summary": "Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.   We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.   Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.",
    "pdf_url": "https://arxiv.org/pdf/2511.12676v1",
    "github_url": null,
    "published": "2025-11-16T16:30:38+00:00",
    "updated": "2025-11-16T16:30:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12590v2",
    "title": "Fine-Grained Representation for Lane Topology Reasoning",
    "authors": [
      "Xu",
      "Li",
      "Yang"
    ],
    "summary": "Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control decisions. Existing methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane queries. However, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology prediction. In this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG). It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR). Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query modeling. RFD constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each lane. RBTR models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching ambiguity. By integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology predictions. Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0 on subsetA and 45.4 on subsetB.",
    "pdf_url": "https://arxiv.org/pdf/2511.12590v2",
    "github_url": null,
    "published": "2025-11-16T13:24:30+00:00",
    "updated": "2025-11-18T16:06:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12533v1",
    "title": "Designing-with More-than-Human Through Human Augmentation",
    "authors": [
      "Hu",
      "Huang"
    ],
    "summary": "The recent more-than-human turn in design calls for \"designing-with\" other species and ecologies beyond humans. Yet-as Thomas Nagel's famous \"What is it like to be a bat?\" thought experiment highlights-human experience is constrained by our own sensorium and an irreducible gap in phenomenal access to nonhuman lifeworlds. This paper proposes More-than-Human through Human Augmentation (MtHtHA, denoted \">HtH+\") as a design approach that repurposes human augmentation technologies-typically aimed at enhancing human capabilities-away from human optimization and exceptionalism but toward eco-phenomenological awareness. Grounded in somaesthetic design and eco-somatics, MtHtHA entails creating temporary, embodied experiences that modulate the human Umwelt to re-sensitize us to pluriversal more-than-human perceptions. We articulate seven design principles and report five design cases-EchoVision (bat-like echolocation), FeltSight (star-nosed-mole tactile navigation), FungiSync (fungal network attunement), TentacUs (octopus-like distributed agency), and City of Sparkles (urban data from AI's perspective). We demonstrate that such experiential \"designing-with\" can cultivate ecological awareness, empathy and obligations of care across species boundaries.",
    "pdf_url": "https://arxiv.org/pdf/2511.12533v1",
    "github_url": null,
    "published": "2025-11-16T10:01:35+00:00",
    "updated": "2025-11-16T10:01:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12526v2",
    "title": "Botany Meets Robotics in Alpine Scree Monitoring",
    "authors": [
      "Benedittis",
      "Lorenzo",
      "Angelini"
    ],
    "summary": "According to the European Union's Habitat Directive, habitat monitoring plays a critical role in response to the escalating problems posed by biodiversity loss and environmental degradation. Scree habitats, hosting unique and often endangered species, face severe threats from climate change due to their high-altitude nature. Traditionally, their monitoring has required highly skilled scientists to conduct extensive fieldwork in remote, potentially hazardous locations, making the process resource-intensive and time-consuming. This paper presents a novel approach for scree habitat monitoring using a legged robot to assist botanists in data collection and species identification. Specifically, we deployed the ANYmal C robot in the Italian Alpine bio-region in two field campaigns spanning two years and leveraged deep learning to detect and classify key plant species of interest. Our results demonstrate that agile legged robots can navigate challenging terrains and increase the frequency and efficiency of scree monitoring. When paired with traditional phytosociological surveys performed by botanists, this robotics-assisted protocol not only streamlines field operations but also enhances data acquisition, storage, and usage. The outcomes of this research contribute to the evolving landscape of robotics in environmental science, paving the way for a more comprehensive and sustainable approach to habitat monitoring and preservation.",
    "pdf_url": "https://arxiv.org/pdf/2511.12526v2",
    "github_url": null,
    "published": "2025-11-16T09:43:29+00:00",
    "updated": "2025-12-09T18:06:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12500v1",
    "title": "Iris: First-Class Multi-GPU Programming Experience in Triton",
    "authors": [
      "Awad",
      "Osama",
      "Potter"
    ],
    "summary": "Multi-GPU programming traditionally requires developers to navigate complex trade-offs between performance and programmability. High-performance implementations typically rely on low-level HIP/CUDA communication libraries that demand substantial engineering effort for even basic overlap patterns, while simpler abstractions often sacrifice performance. We present Iris, a multi-GPU communication library implemented entirely in Python and Triton that eliminates this trade-off. Iris provides tile-based symmetric memory abstractions that naturally align with Triton's programming model, enabling developers to write single-source kernels that seamlessly interleave computation and communication. We demonstrate a taxonomy of compute-communication overlap patterns--from bulk-synchronous to fine-grained workgroup specialization--that can be implemented with minimal code changes in Iris, often requiring just a few additional lines within the same Triton kernel. Our evaluation shows that Iris achieves near-optimal bandwidth utilization in microbenchmarks and delivers up to 1.79x speedup over PyTorch and RCCL for GEMM+All-Scatter workloads, demonstrating that high-level implementations can match or exceed heavily-optimized libraries while dramatically simplifying multi-GPU programming.",
    "pdf_url": "https://arxiv.org/pdf/2511.12500v1",
    "github_url": null,
    "published": "2025-11-16T08:24:45+00:00",
    "updated": "2025-11-16T08:24:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12439v1",
    "title": "Multi-agent Self-triage System with Medical Flowcharts",
    "authors": [
      "Liu",
      "Yu",
      "Jin"
    ],
    "summary": "Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.",
    "pdf_url": "https://arxiv.org/pdf/2511.12439v1",
    "github_url": null,
    "published": "2025-11-16T03:48:22+00:00",
    "updated": "2025-11-16T03:48:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12436v1",
    "title": "RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation",
    "authors": [
      "Hao",
      "Tang",
      "Zhang"
    ],
    "summary": "Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.",
    "pdf_url": "https://arxiv.org/pdf/2511.12436v1",
    "github_url": null,
    "published": "2025-11-16T03:35:50+00:00",
    "updated": "2025-11-16T03:35:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12430v1",
    "title": "Integration of Navigation and Remote Sensing in LEO Satellite Constellations",
    "authors": [
      "Wang",
      "Chen",
      "Qi"
    ],
    "summary": "Low earth orbit (LEO) satellite constellations are becoming a cornerstone of next-generation satellite networks, enabling worldwide high-precision navigation and high-quality remote sensing. This paper proposes a novel dual-function LEO satellite constellation frame structure that effectively integrating navigation and remote sensing. Then, the Cramer-Rao bound (CRB)-based positioning, velocity measurement, and timing (PVT) error and the signal-to-ambiguity-interference-noise ratio (SAINR) are derived as performance metrics for navigation and remote sensing, respectively. Based on it, a joint beamforming design is proposed by minimizing the average weighted PVT error for navigation user equipments (UEs) while ensuring SAINR requirement for remote sensing. Simulation results validate the proposed multi-satellite cooperative beamforming design, demonstrating its effectiveness as an integrated solution for next-generation multi-function LEO satellite constellations.",
    "pdf_url": "https://arxiv.org/pdf/2511.12430v1",
    "github_url": null,
    "published": "2025-11-16T03:15:44+00:00",
    "updated": "2025-11-16T03:15:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13782v1",
    "title": "Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models",
    "authors": [
      "Lian",
      "Yang",
      "Zhu"
    ],
    "summary": "Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances",
    "pdf_url": "https://arxiv.org/pdf/2511.13782v1",
    "github_url": null,
    "published": "2025-11-16T03:09:55+00:00",
    "updated": "2025-11-16T03:09:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.13781v1",
    "title": "Human-Centered Threat Modeling in Practice: Lessons, Challenges, and Paths Forward",
    "authors": [
      "Usman",
      "Zou",
      "Zappala"
    ],
    "summary": "Human-centered threat modeling (HCTM) is an emerging area within security and privacy research that focuses on how people define and navigate threats in various social, cultural, and technological contexts. While researchers increasingly approach threat modeling from a human-centered perspective, little is known about how they prepare for and engage with HCTM in practice. In this work, we conduct 23 semi-structured interviews with researchers to examine the state of HCTM, including how researchers design studies, elicit threats, and navigate values, constraints, and long-term goals. We find that HCTM is not a prescriptive process but a set of evolving practices shaped by relationships with participants, disciplinary backgrounds, and institutional structures. Researchers approach threat modeling through sustained groundwork and participant-centered inquiry, guided by values such as care, justice, and autonomy. They also face challenges including emotional strain, ethical dilemmas, and structural barriers that complicate efforts to translate findings into real-world impact. We conclude by identifying opportunities to advance HCTM through shared infrastructure, broader recognition of diverse contributions, and stronger mechanisms for translating findings into policy, design, and societal change.",
    "pdf_url": "https://arxiv.org/pdf/2511.13781v1",
    "github_url": null,
    "published": "2025-11-16T03:03:41+00:00",
    "updated": "2025-11-16T03:03:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12423v1",
    "title": "GRAPHTEXTACK: A Realistic Black-Box Node Injection Attack on LLM-Enhanced GNNs",
    "authors": [
      "Ma",
      "Trivedi",
      "Koutra"
    ],
    "summary": "Text-attributed graphs (TAGs), which combine structural and textual node information, are ubiquitous across many domains. Recent work integrates Large Language Models (LLMs) with Graph Neural Networks (GNNs) to jointly model semantics and structure, resulting in more general and expressive models that achieve state-of-the-art performance on TAG benchmarks. However, this integration introduces dual vulnerabilities: GNNs are sensitive to structural perturbations, while LLM-derived features are vulnerable to prompt injection and adversarial phrasing. While existing adversarial attacks largely perturb structure or text independently, we find that uni-modal attacks cause only modest degradation in LLM-enhanced GNNs. Moreover, many existing attacks assume unrealistic capabilities, such as white-box access or direct modification of graph data. To address these gaps, we propose GRAPHTEXTACK, the first black-box, multi-modal{, poisoning} node injection attack for LLM-enhanced GNNs. GRAPHTEXTACK injects nodes with carefully crafted structure and semantics to degrade model performance, operating under a realistic threat model without relying on model internals or surrogate models. To navigate the combinatorial, non-differentiable search space of connectivity and feature assignments, GRAPHTEXTACK introduces a novel evolutionary optimization framework with a multi-objective fitness function that balances local prediction disruption and global graph influence. Extensive experiments on five datasets and two state-of-the-art LLM-enhanced GNN models show that GRAPHTEXTACK significantly outperforms 12 strong baselines.",
    "pdf_url": "https://arxiv.org/pdf/2511.12423v1",
    "github_url": null,
    "published": "2025-11-16T02:42:48+00:00",
    "updated": "2025-11-16T02:42:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12359v1",
    "title": "More Than Irrational: Modeling Belief-Biased Agents",
    "authors": [
      "Zhu",
      "Katt",
      "Kaski"
    ],
    "summary": "Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.",
    "pdf_url": "https://arxiv.org/pdf/2511.12359v1",
    "github_url": null,
    "published": "2025-11-15T21:14:37+00:00",
    "updated": "2025-11-15T21:14:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.17581v1",
    "title": "EgoCogNav: Cognition-aware Human Egocentric Navigation",
    "authors": [
      "Qiu",
      "Liu",
      "Niu"
    ],
    "summary": "Modeling the cognitive and experiential factors of human navigation is central to deepening our understanding of human-environment interaction and to enabling safe social navigation and effective assistive wayfinding. Most existing methods focus on forecasting motions in fully observed scenes and often neglect human factors that capture how people feel and respond to space. To address this gap, We propose EgoCogNav, a multimodal egocentric navigation framework that predicts perceived path uncertainty as a latent state and jointly forecasts trajectories and head motion by fusing scene features with sensory cues. To facilitate research in the field, we introduce the Cognition-aware Egocentric Navigation (CEN) dataset consisting 6 hours of real-world egocentric recordings capturing diverse navigation behaviors in real-world scenarios. Experiments show that EgoCogNav learns the perceived uncertainty that highly correlates with human-like behaviors such as scanning, hesitation, and backtracking while generalizing to unseen environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.17581v1",
    "github_url": null,
    "published": "2025-11-15T15:59:36+00:00",
    "updated": "2025-11-15T15:59:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12260v1",
    "title": "Reinforcement Learning for Chemical Ordering in Alloy Nanoparticles",
    "authors": [
      "Elsborg",
      "Bhowmik"
    ],
    "summary": "We approach the search for optimal element ordering in bimetallic alloy nanoparticles (NPs) as a reinforcement learning (RL) problem, and have built an RL agent that learns to perform such global optimisation using the geometric graph representation of the NPs. To demonstrate the effectiveness, we train an RL agent to perform composition-conserving atomic swap actions on the icosahedral nanoparticle structure. Trained once on randomised $Ag_{X}Au_{309-X}$ compositions and orderings, the agent discovers previously established ground state structure. We show that this optimization is robust to differently ordered initialisations of the same NP compositions. We also demonstrate that a trained policy can extrapolate effectively to NPs of unseen size. However, the efficacy is limited when multiple alloying elements are involved. Our results demonstrate that RL with pre-trained equivariant graph encodings can navigate combinatorial ordering spaces at the nanoparticle scale, and offer a transferable optimisation strategy with the potential to generalise across composition and reduce repeated individual search cost.",
    "pdf_url": "https://arxiv.org/pdf/2511.12260v1",
    "github_url": null,
    "published": "2025-11-15T15:33:20+00:00",
    "updated": "2025-11-15T15:33:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12232v2",
    "title": "SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation",
    "authors": [
      "Zhang",
      "Xiao",
      "Hao"
    ],
    "summary": "Social navigation in densely populated dynamic environments poses a significant challenge for autonomous mobile robots, requiring advanced strategies for safe interaction. Existing reinforcement learning (RL)-based methods require over 2000+ hours of extensive training and often struggle to generalize to unfamiliar environments without additional fine-tuning, limiting their practical application in real-world scenarios. To address these limitations, we propose SocialNav-Map, a novel zero-shot social navigation framework that combines dynamic human trajectory prediction with occupancy mapping, enabling safe and efficient navigation without the need for environment-specific training. Specifically, SocialNav-Map first transforms the task goal position into the constructed map coordinate system. Subsequently, it creates a dynamic occupancy map that incorporates predicted human movements as dynamic obstacles. The framework employs two complementary methods for human trajectory prediction: history prediction and orientation prediction. By integrating these predicted trajectories into the occupancy map, the robot can proactively avoid potential collisions with humans while efficiently navigating to its destination. Extensive experiments on the Social-HM3D and Social-MP3D datasets demonstrate that SocialNav-Map significantly outperforms state-of-the-art (SOTA) RL-based methods, which require 2,396 GPU hours of training. Notably, it reduces human collision rates by over 10% without necessitating any training in novel environments. By eliminating the need for environment-specific training, SocialNav-Map achieves superior navigation performance, paving the way for the deployment of social navigation systems in real-world environments characterized by diverse human behaviors. The code is available at: https://github.com/linglingxiansen/SocialNav-Map.",
    "pdf_url": "https://arxiv.org/pdf/2511.12232v2",
    "github_url": "https://github.com/linglingxiansen/SocialNav-Map",
    "published": "2025-11-15T14:20:05+00:00",
    "updated": "2025-11-18T02:52:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12058v1",
    "title": "Relativistic framework for high-precision GNSS-based navigation in cislunar space",
    "authors": [
      "Turyshev",
      "Bar-Sever",
      "Bertiger"
    ],
    "summary": "We present a relativistic modeling framework for GNSS-based navigation in Geocentric and Barycentric Celestial Reference Systems, i.e., GCRS and BCRS, respectively. Using the IAU-adopted GCRS and BCRS conventions, we derive closed-form transformations for position, velocity, and acceleration that are required to achieve fractional-frequency transfer precision of 10^{-16} and sub-cm positional accuracy. Numerical simulations with GipsyX validate cm-level orbit determination and timing stability within tens of picoseconds across GCRS and BCRS and demonstrate mm-level round-trip GCRS <--> BCRS frame closure over 24 h. These results underscore the necessity of relativistic corrections for high-precision GNSS throughout the Earth-Moon system and demonstrate that GNSS signals can support a decimeter-level orbit accuracy and sub-nanosecond-scale timing synchronization in cislunar space, establishing a robust framework for future operations. We introduce the Lunicentric Celestial Reference System (LCRS) and its associated time scale to extend GNSS-based navigation into cislunar space. We present a minimal cislunar case study (NRHO-like geometry) that applies the improved position/velocity/acceleration transformations and the light-time model.",
    "pdf_url": "https://arxiv.org/pdf/2511.12058v1",
    "github_url": null,
    "published": "2025-11-15T06:51:26+00:00",
    "updated": "2025-11-15T06:51:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12022v1",
    "title": "SBAMP: Sampling Based Adaptive Motion Planning",
    "authors": [
      "Pham",
      "Puri",
      "Raorane"
    ],
    "summary": "Autonomous robotic systems must navigate complex, dynamic environments in real time, often facing unpredictable obstacles and rapidly changing conditions. Traditional sampling-based methods, such as RRT*, excel at generating collision-free paths but struggle to adapt to sudden changes without extensive replanning. Conversely, learning-based dynamical systems, such as the Stable Estimator of Dynamical Systems (SEDS), offer smooth, adaptive trajectory tracking but typically rely on pre-collected demonstration data, limiting their generalization to novel scenarios. This paper introduces Sampling-Based Adaptive Motion Planning (SBAMP), a novel framework that overcomes these limitations by integrating RRT* for global path planning with a SEDS-based local controller for continuous, adaptive trajectory adjustment. Our approach requires no pre-trained datasets and ensures smooth transitions between planned waypoints, maintaining stability through Lyapunov-based guarantees. We validate SBAMP in both simulated environments and real hardware using the RoboRacer platform, demonstrating superior performance in dynamic obstacle scenarios, rapid recovery from perturbations, and robust handling of sharp turns. Experimental results highlight SBAMP's ability to adapt in real time without sacrificing global path optimality, providing a scalable solution for dynamic, unstructured environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.12022v1",
    "github_url": null,
    "published": "2025-11-15T04:16:20+00:00",
    "updated": "2025-11-15T04:16:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.12004v2",
    "title": "ComLQ: Benchmarking Complex Logical Queries in Information Retrieval",
    "authors": [
      "Xu",
      "Yin",
      "Zhang"
    ],
    "summary": "Information retrieval (IR) systems play a critical role in navigating information overload across various applications. Existing IR benchmarks primarily focus on simple queries that are semantically analogous to single- and multi-hop relations, overlooking \\emph{complex logical queries} involving first-order logic operations such as conjunction ($\\land$), disjunction ($\\lor$), and negation ($\\lnot$). Thus, these benchmarks can not be used to sufficiently evaluate the performance of IR models on complex queries in real-world scenarios. To address this problem, we propose a novel method leveraging large language models (LLMs) to construct a new IR dataset \\textbf{ComLQ} for \\textbf{Com}plex \\textbf{L}ogical \\textbf{Q}ueries, which comprises 2,909 queries and 11,251 candidate passages. A key challenge in constructing the dataset lies in capturing the underlying logical structures within unstructured text. Therefore, by designing the subgraph-guided prompt with the subgraph indicator, an LLM (such as GPT-4o) is guided to generate queries with specific logical structures based on selected passages. All query-passage pairs in ComLQ are ensured \\emph{structure conformity} and \\emph{evidence distribution} through expert annotation. To better evaluate whether retrievers can handle queries with negation, we further propose a new evaluation metric, \\textbf{Log-Scaled Negation Consistency} (\\textbf{LSNC@$K$}). As a supplement to standard relevance-based metrics (such as nDCG and mAP), LSNC@$K$ measures whether top-$K$ retrieved passages violate negation conditions in queries. Our experimental results under zero-shot settings demonstrate existing retrieval models' limited performance on complex logical queries, especially on queries with negation, exposing their inferior capabilities of modeling exclusion.",
    "pdf_url": "https://arxiv.org/pdf/2511.12004v2",
    "github_url": null,
    "published": "2025-11-15T02:58:21+00:00",
    "updated": "2025-11-23T06:31:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11992v1",
    "title": "Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams",
    "authors": [
      "Du",
      "Nguyen",
      "Thudumu"
    ],
    "summary": "Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.",
    "pdf_url": "https://arxiv.org/pdf/2511.11992v1",
    "github_url": null,
    "published": "2025-11-15T02:11:31+00:00",
    "updated": "2025-11-15T02:11:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11845v1",
    "title": "Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture",
    "authors": [
      "Jayarathne",
      "Rathnayaka",
      "Peiris"
    ],
    "summary": "Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.",
    "pdf_url": "https://arxiv.org/pdf/2511.11845v1",
    "github_url": null,
    "published": "2025-11-14T20:07:56+00:00",
    "updated": "2025-11-14T20:07:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11557v1",
    "title": "Drone Swarm Energy Management",
    "authors": [
      "Zgurovsky",
      "Kasyanov",
      "Paliichuk"
    ],
    "summary": "This note presents an analytical framework for decision-making in drone swarm systems operating under uncertainty, based on the integration of Partially Observable Markov Decision Processes (POMDP) with Deep Deterministic Policy Gradient (DDPG) reinforcement learning. The proposed approach enables adaptive control and cooperative behavior of unmanned aerial vehicles (UAVs) within a cognitive AI platform, where each agent learns optimal energy management and navigation policies from dynamic environmental states. We extend the standard DDPG architecture with a belief-state representation derived from Bayesian filtering, allowing for robust decision-making in partially observable environments. In this paper, for the Gaussian case, we numerically compare the performance of policies derived from DDPG to optimal policies for discretized versions of the original continuous problem. Simulation results demonstrate that the POMDP-DDPG-based swarm control model significantly improves mission success rates and energy efficiency compared to baseline methods. The developed framework supports distributed learning and decision coordination across multiple agents, providing a foundation for scalable cognitive swarm autonomy. The outcomes of this research contribute to the advancement of energy-aware control algorithms for intelligent multi-agent systems and can be applied in security, environmental monitoring, and infrastructure inspection scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2511.11557v1",
    "github_url": null,
    "published": "2025-11-14T18:47:30+00:00",
    "updated": "2025-11-14T18:47:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11552v1",
    "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding",
    "authors": [
      "Zhu",
      "Meng",
      "Chen"
    ],
    "summary": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.",
    "pdf_url": "https://arxiv.org/pdf/2511.11552v1",
    "github_url": null,
    "published": "2025-11-14T18:42:18+00:00",
    "updated": "2025-11-14T18:42:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11529v1",
    "title": "Terrain Costmap Generation via Scaled Preference Conditioning",
    "authors": [
      "Mao",
      "Warnell",
      "Stone"
    ],
    "summary": "Successful autonomous robot navigation in off-road domains requires the ability to generate high-quality terrain costmaps that are able to both generalize well over a wide variety of terrains and rapidly adapt relative costs at test time to meet mission-specific needs. Existing approaches for costmap generation allow for either rapid test-time adaptation of relative costs (e.g., semantic segmentation methods) or generalization to new terrain types (e.g., representation learning methods), but not both. In this work, we present scaled preference conditioned all-terrain costmap generation (SPACER), a novel approach for generating terrain costmaps that leverages synthetic data during training in order to generalize well to new terrains, and allows for rapid test-time adaptation of relative costs by conditioning on a user-specified scaled preference context. Using large-scale aerial maps, we provide empirical evidence that SPACER outperforms other approaches at generating costmaps for terrain navigation, with the lowest measured regret across varied preferences in five of seven environments for global path planning.",
    "pdf_url": "https://arxiv.org/pdf/2511.11529v1",
    "github_url": null,
    "published": "2025-11-14T18:04:20+00:00",
    "updated": "2025-11-14T18:04:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11515v1",
    "title": "PEtab-GUI: A graphical user interface to create, edit and inspect PEtab parameter estimation problems",
    "authors": [
      "Jost",
      "Bergmann",
      "Weindl"
    ],
    "summary": "Motivation: Parameter estimation is a cornerstone of data-driven modeling in systems biology. Yet, constructing such problems in a reproducible and accessible manner remains challenging. The PEtab format has established itself as a powerful community standard to encode parameter estimation problems, promoting interoperability and reusability. However, its reliance on multiple interlinked files - often edited manually - can introduce inconsistencies, and new users often struggle to navigate them. Here, we present PEtab-GUI, an open-source Python application designed to streamline the creation, editing, and validation of PEtab problems through an intuitive graphical user interface. PEtab-GUI integrates all PEtab components, including SBML models and tabular files, into a single environment with live error-checking and customizable defaults. Interactive visualization and simulation capabilities enable users to inspect the relationship between the model and the data. PEtab-GUI lowers the barrier to entry for specifying standardized parameter estimation problems, making dynamic modeling more accessible, especially in educational and interdisciplinary settings.   Availability and Implementation: PEtab-GUI is implemented in Python, open-source under a 3-Clause BSD license. The code, designed to be modular and extensible, is hosted on https://github.com/PEtab-dev/PEtab-GUI and can be installed from PyPI.   Key words: Parameter Estimation, Python, Graphical User Interface, Systems Biology",
    "pdf_url": "https://arxiv.org/pdf/2511.11515v1",
    "github_url": "https://github.com/PEtab-dev/PEtab-GUI",
    "published": "2025-11-14T17:39:27+00:00",
    "updated": "2025-11-14T17:39:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11402v1",
    "title": "Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning",
    "authors": [
      "Jain",
      "Rodriguez-Fernandez",
      "Linares"
    ],
    "summary": "Autonomous spacecraft control for mission phases such as launch, ascent, stage separation, and orbit insertion remains a critical challenge due to the need for adaptive policies that generalize across dynamically distinct regimes. While reinforcement learning (RL) has shown promise in individual astrodynamics tasks, existing approaches often require separate policies for distinct mission phases, limiting adaptability and increasing operational complexity. This work introduces a transformer-based RL framework that unifies multi-phase trajectory optimization through a single policy architecture, leveraging the transformer's inherent capacity to model extended temporal contexts. Building on proximal policy optimization (PPO), our framework replaces conventional recurrent networks with a transformer encoder-decoder structure, enabling the agent to maintain coherent memory across mission phases spanning seconds to minutes during critical operations. By integrating a Gated Transformer-XL (GTrXL) architecture, the framework eliminates manual phase transitions while maintaining stability in control decisions. We validate our approach progressively: first demonstrating near-optimal performance on single-phase benchmarks (double integrator and Van der Pol oscillator), then extending to multiphase waypoint navigation variants, and finally tackling a complex multiphase rocket ascent problem that includes atmospheric flight, stage separation, and vacuum operations. Results demonstrate that the transformer-based framework not only matches analytical solutions in simple cases but also effectively learns coherent control policies across dynamically distinct regimes, establishing a foundation for scalable autonomous mission planning that reduces reliance on phase-specific controllers while maintaining compatibility with safety-critical verification protocols.",
    "pdf_url": "https://arxiv.org/pdf/2511.11402v1",
    "github_url": null,
    "published": "2025-11-14T15:29:46+00:00",
    "updated": "2025-11-14T15:29:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11323v1",
    "title": "RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms",
    "authors": [
      "Kou",
      "Gu",
      "Zhou"
    ],
    "summary": "Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.",
    "pdf_url": "https://arxiv.org/pdf/2511.11323v1",
    "github_url": null,
    "published": "2025-11-14T13:59:40+00:00",
    "updated": "2025-11-14T13:59:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11310v1",
    "title": "Simulating an Autonomous System in CARLA using ROS 2",
    "authors": [
      "Abdo",
      "Shibu",
      "Saeed"
    ],
    "summary": "Autonomous racing offers a rigorous setting to stress test perception, planning, and control under high speed and uncertainty. This paper proposes an approach to design and evaluate a software stack for an autonomous race car in CARLA: Car Learning to Act simulator, targeting competitive driving performance in the Formula Student UK Driverless (FS-AI) 2025 competition. By utilizing a 360° light detection and ranging (LiDAR), stereo camera, global navigation satellite system (GNSS), and inertial measurement unit (IMU) sensor via ROS 2 (Robot Operating System), the system reliably detects the cones marking the track boundaries at distances of up to 35 m. Optimized trajectories are computed considering vehicle dynamics and simulated environmental factors such as visibility and lighting to navigate the track efficiently. The complete autonomous stack is implemented in ROS 2 and validated extensively in CARLA on a dedicated vehicle (ADS-DV) before being ported to the actual hardware, which includes the Jetson AGX Orin 64GB, ZED2i Stereo Camera, Robosense Helios 16P LiDAR, and CHCNAV Inertial Navigation System (INS).",
    "pdf_url": "https://arxiv.org/pdf/2511.11310v1",
    "github_url": null,
    "published": "2025-11-14T13:55:22+00:00",
    "updated": "2025-11-14T13:55:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11252v1",
    "title": "UAVBench: An Open Benchmark Dataset for Autonomous and Agentic AI UAV Systems via LLM-Generated Flight Scenarios",
    "authors": [
      "Ferrag",
      "Lakas",
      "Debbah"
    ],
    "summary": "Autonomous aerial systems increasingly rely on large language models (LLMs) for mission planning, perception, and decision-making, yet the lack of standardized and physically grounded benchmarks limits systematic evaluation of their reasoning capabilities. To address this gap, we introduce UAVBench, an open benchmark dataset comprising 50,000 validated UAV flight scenarios generated through taxonomy-guided LLM prompting and multi-stage safety validation. Each scenario is encoded in a structured JSON schema that includes mission objectives, vehicle configuration, environmental conditions, and quantitative risk labels, providing a unified representation of UAV operations across diverse domains. Building on this foundation, we present UAVBench_MCQ, a reasoning-oriented extension containing 50,000 multiple-choice questions spanning ten cognitive and ethical reasoning styles, ranging from aerodynamics and navigation to multi-agent coordination and integrated reasoning. This framework enables interpretable and machine-checkable assessment of UAV-specific cognition under realistic operational contexts. We evaluate 32 state-of-the-art LLMs, including GPT-5, ChatGPT-4o, Gemini 2.5 Flash, DeepSeek V3, Qwen3 235B, and ERNIE 4.5 300B, and find strong performance in perception and policy reasoning but persistent challenges in ethics-aware and resource-constrained decision-making. UAVBench establishes a reproducible and physically grounded foundation for benchmarking agentic AI in autonomous aerial systems and advancing next-generation UAV reasoning intelligence. To support open science and reproducibility, we release the UAVBench dataset, the UAVBench_MCQ benchmark, evaluation scripts, and all related materials on GitHub at https://github.com/maferrag/UAVBench",
    "pdf_url": "https://arxiv.org/pdf/2511.11252v1",
    "github_url": "https://github.com/maferrag/UAVBench",
    "published": "2025-11-14T12:51:48+00:00",
    "updated": "2025-11-14T12:51:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11169v1",
    "title": "Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA",
    "authors": [
      "Pandey",
      "Bardhan",
      "Jain"
    ],
    "summary": "In the context of Visual Question Answering (VQA) and Agentic AI, calibration refers to how closely an AI system's confidence in its answers reflects their actual correctness. This aspect becomes especially important when such systems operate autonomously and must make decisions under visual uncertainty. While modern VQA systems, powered by advanced vision-language models (VLMs), are increasingly used in high-stakes domains like medical diagnostics and autonomous navigation due to their improved accuracy, the reliability of their confidence estimates remains under-examined. Particularly, these systems often produce overconfident responses. To address this, we introduce AlignVQA, a debate-based multi-agent framework, in which diverse specialized VLM -- each following distinct prompting strategies -- generate candidate answers and then engage in two-stage interaction: generalist agents critique, refine and aggregate these proposals. This debate process yields confidence estimates that more accurately reflect the model's true predictive performance. We find that more calibrated specialized agents produce better aligned confidences. Furthermore, we introduce a novel differentiable calibration-aware loss function called aligncal designed to fine-tune the specialized agents by minimizing an upper bound on the calibration error. This objective explicitly improves the fidelity of each agent's confidence estimates. Empirical results across multiple benchmark VQA datasets substantiate the efficacy of our approach, demonstrating substantial reductions in calibration discrepancies. Furthermore, we propose a novel differentiable calibration-aware loss to fine-tune the specialized agents and improve the quality of their individual confidence estimates based on minimising upper bound calibration error.",
    "pdf_url": "https://arxiv.org/pdf/2511.11169v1",
    "github_url": null,
    "published": "2025-11-14T11:08:21+00:00",
    "updated": "2025-11-14T11:08:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00041v1",
    "title": "VISTAv2: World Imagination for Indoor Vision-and-Language Navigation",
    "authors": [
      "Huang",
      "Jiang",
      "Gao"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires agents to follow language instructions while acting in continuous real-world spaces. Prior image imagination based VLN work shows benefits for discrete panoramas but lacks online, action-conditioned predictions and does not produce explicit planning values; moreover, many methods replace the planner with long-horizon objectives that are brittle and slow. To bridge this gap, we propose VISTAv2, a generative world model that rolls out egocentric future views conditioned on past observations, candidate action sequences, and instructions, and projects them into an online value map for planning. Unlike prior approaches, VISTAv2 does not replace the planner. The online value map is fused at score level with the base objective, providing reachability and risk-aware guidance. Concretely, we employ an action-aware Conditional Diffusion Transformer video predictor to synthesize short-horizon futures, align them with the natural language instruction via a vision-language scorer, and fuse multiple rollouts in a differentiable imagination-to-value head to output an imagined egocentric value map. For efficiency, rollouts occur in VAE latent space with a distilled sampler and sparse decoding, enabling inference on a single consumer GPU. Evaluated on MP3D and RoboTHOR, VISTAv2 improves over strong baselines, and ablations show that action-conditioned imagination, instruction-guided value fusion, and the online value-map planner are all critical, suggesting that VISTAv2 offers a practical and interpretable route to robust VLN.",
    "pdf_url": "https://arxiv.org/pdf/2512.00041v1",
    "github_url": null,
    "published": "2025-11-14T10:20:22+00:00",
    "updated": "2025-11-14T10:20:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11043v2",
    "title": "Autonomous Vehicle Path Planning by Searching With Differentiable Simulation",
    "authors": [
      "Nachkov",
      "Zaech",
      "Paudel"
    ],
    "summary": "Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.",
    "pdf_url": "https://arxiv.org/pdf/2511.11043v2",
    "github_url": null,
    "published": "2025-11-14T07:56:34+00:00",
    "updated": "2025-11-24T09:43:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11011v1",
    "title": "Latent-Space Autoregressive World Model for Efficient and Robust Image-Goal Navigation",
    "authors": [
      "Zhang",
      "Zhang",
      "Chen"
    ],
    "summary": "Traditional navigation methods rely heavily on accurate localization and mapping. In contrast, world models that capture environmental dynamics in latent space have opened up new perspectives for navigation tasks, enabling systems to move beyond traditional multi-module pipelines. However, world model often suffers from high computational costs in both training and inference. To address this, we propose LS-NWM - a lightweight latent space navigation world model that is trained and operates entirely in latent space, compared to the state-of-the-art baseline, our method reduces training time by approximately 3.2x and planning time by about 447x,while further improving navigation performance with a 35% higher SR and an 11% higher SPL. The key idea is that accurate pixel-wise environmental prediction is unnecessary for navigation. Instead, the model predicts future latent states based on current observational features and action inputs, then performs path planning and decision-making within this compact representation, significantly improving computational efficiency. By incorporating an autoregressive multi-frame prediction strategy during training, the model effectively captures long-term spatiotemporal dependencies, thereby enhancing navigation performance in complex scenarios. Experimental results demonstrate that our method achieves state-of-the-art navigation performance while maintaining a substantial efficiency advantage over existing approaches.",
    "pdf_url": "https://arxiv.org/pdf/2511.11011v1",
    "github_url": null,
    "published": "2025-11-14T06:55:33+00:00",
    "updated": "2025-11-14T06:55:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10965v1",
    "title": "Mastering Rheology: A Strategic and practical guide for empowering all users",
    "authors": [
      "Suman"
    ],
    "summary": "Rheology, the study of flow, plays a vital role in diverse industries such as pharmaceuticals, cosmetics and food. In this work, we provide a comprehensive introduction to fundamental rheological experiments and offer a strategic approach to understand the rheological behavior of any soft condensed material. We emphasize the importance of design of a good rheological experiment, input parameter and analysis of the obtained output parameter. Through the standard design of the experiment and systematic conduction of rheological experiments, we can estimate a broad range of rheological parameters such as viscosity, modulus, stability, yield stress, nonlinear behavior of the material. Furthermore, we also discuss methods for detecting and good practices for reducing some of the common experimental errors that may arise. We also present a comprehensive range of advanced rheological characterization techniques that can be pursued to gain deeper insights into the mechanical behavior and structural evolution of soft materials. Through this work, we attempt to make rheology more accessible to researchers from various domains to incorporate rheology into their characterization study and enable them to confidently navigate rheological studies and contribute to material development.",
    "pdf_url": "https://arxiv.org/pdf/2511.10965v1",
    "github_url": null,
    "published": "2025-11-14T05:18:45+00:00",
    "updated": "2025-11-14T05:18:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10874v1",
    "title": "Collaborative Multi-Robot Non-Prehensile Manipulation via Flow-Matching Co-Generation",
    "authors": [
      "Shaoul",
      "Chen",
      "Mohamed"
    ],
    "summary": "Coordinating a team of robots to reposition multiple objects in cluttered environments requires reasoning jointly about where robots should establish contact, how to manipulate objects once contact is made, and how to navigate safely and efficiently at scale. Prior approaches typically fall into two extremes -- either learning the entire task or relying on privileged information and hand-designed planners -- both of which struggle to handle diverse objects in long-horizon tasks. To address these challenges, we present a unified framework for collaborative multi-robot, multi-object non-prehensile manipulation that integrates flow-matching co-generation with anonymous multi-robot motion planning. Within this framework, a generative model co-generates contact formations and manipulation trajectories from visual observations, while a novel motion planner conveys robots at scale. Crucially, the same planner also supports coordination at the object level, assigning manipulated objects to larger target structures and thereby unifying robot- and object-level reasoning within a single algorithmic framework. Experiments in challenging simulated environments demonstrate that our approach outperforms baselines in both motion planning and manipulation tasks, highlighting the benefits of generative co-design and integrated planning for scaling collaborative manipulation to complex multi-agent, multi-object settings. Visit gco-paper.github.io for code and demonstrations.",
    "pdf_url": "https://arxiv.org/pdf/2511.10874v1",
    "github_url": null,
    "published": "2025-11-14T01:05:58+00:00",
    "updated": "2025-11-14T01:05:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10816v2",
    "title": "Dynamically Extensible and Retractable Robotic Leg Linkages for Multi-task Execution in Search and Rescue Scenarios",
    "authors": [
      "Harris",
      "Yager",
      "Sylvester"
    ],
    "summary": "Search and rescue (SAR) robots are required to quickly traverse terrain and perform high-force rescue tasks, necessitating both terrain adaptability and controlled high-force output. Few platforms exist today for SAR, and fewer still have the ability to cover both tasks of terrain adaptability and high-force output when performing extraction. While legged robots offer significant ability to traverse uneven terrain, they typically are unable to incorporate mechanisms that provide variable high-force outputs, unlike traditional wheel-based drive trains. This work introduces a novel concept for a dynamically extensible and retractable robot leg. Leveraging a dynamically extensible and retractable five-bar linkage design, it allows for mechanically switching between height-advantaged and force-advantaged configurations via a geometric transformation. A testbed evaluated leg performance across linkage geometries and operating modes, with empirical and analytical analyses conducted on stride length, force output, and stability. The results demonstrate that the morphing leg offers a promising path toward SAR robots that can both navigate terrain quickly and perform rescue tasks effectively.",
    "pdf_url": "https://arxiv.org/pdf/2511.10816v2",
    "github_url": null,
    "published": "2025-11-13T21:27:39+00:00",
    "updated": "2025-11-17T19:02:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10810v1",
    "title": "HARNESS: Human-Agent Risk Navigation and Event Safety System for Proactive Hazard Forecasting in High-Risk DOE Environments",
    "authors": [
      "Elgedawy",
      "Das",
      "Seefried"
    ],
    "summary": "Operational safety at mission-critical work sites is a top priority given the complex and hazardous nature of daily tasks. This paper presents the Human-Agent Risk Navigation and Event Safety System (HARNESS), a modular AI framework designed to forecast hazardous events and analyze operational risks in U.S. Department of Energy (DOE) environments. HARNESS integrates Large Language Models (LLMs) with structured work data, historical event retrieval, and risk analysis to proactively identify potential hazards. A human-in-the-loop mechanism allows subject matter experts (SMEs) to refine predictions, creating an adaptive learning loop that enhances performance over time. By combining SME collaboration with iterative agentic reasoning, HARNESS improves the reliability and efficiency of predictive safety systems. Preliminary deployment shows promising results, with future work focusing on quantitative evaluation of accuracy, SME agreement, and decision latency reduction.",
    "pdf_url": "https://arxiv.org/pdf/2511.10810v1",
    "github_url": null,
    "published": "2025-11-13T21:22:53+00:00",
    "updated": "2025-11-13T21:22:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10808v1",
    "title": "Seasonality in the U.S. Housing Market: Post-Pandemic Shifts and Regional Dynamics",
    "authors": [
      "Hu",
      "Huang",
      "Wang"
    ],
    "summary": "Seasonality has traditionally shaped the U.S. housing market, with activity peaking in spring-summer and declining in autumn-winter. However, recent disruptions, particularly post-COVID-19, raise questions about shift in these patterns. This study analyzes housing market date (1991-2024) to examine evolving seasonality and regional heterogeneity. Using Housing Price Index (HPI), inventory and sales data from the Federal Housing Finance Agency and U.S. Census Bureau, seasonal components are extracted via the X-13-ARIMA procedure, and statistical tests assess variations across regions. The results confirm seasonal fluctuations in prices and volumes, with recent shifts toward earlier annual peak (March-April) and amplified seasonal effects. Regional variations align with differences in climate and market structure, while prices and sales volumes exhibit in-phase movement, suggesting thick-market momentum behaviour. These findings highlight key implications for policymakers, realtors and investors navigating post-pandemic market dynamics, offering insights into the timing and interpretation of housing market activities.",
    "pdf_url": "https://arxiv.org/pdf/2511.10808v1",
    "github_url": null,
    "published": "2025-11-13T21:20:54+00:00",
    "updated": "2025-11-13T21:20:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11755v1",
    "title": "Brazil Data Commons: A Platform for Unifying and Integrating Brazil's Public Data",
    "authors": [
      "Cristina",
      "Gonze",
      "Santos"
    ],
    "summary": "The fragmentation of public data in Brazil, coupled with inconsistent standards and limited interoperability, hinders effective research, evidence-based policymaking and access to data-driven insights. To address these issues, we introduce Brazil Data Commons, a platform that unifies various Brazilian datasets under a common semantic framework, enabling the seamless discovery, integration and visualization of information from different domains. By adopting globally recognized ontologies and interoperable data standards, Brazil Data Commons aligns with the principles of the broader Data Commons ecosystem and places Brazilian data in a global context. Through user-friendly interfaces, straightforward query mechanisms and flexible data access options, the platform democratizes data use and enables researchers, policy makers, and the public to gain meaningful insights and make informed decisions. This paper illustrates how Brazil Data Commons transforms scattered datasets into an integrated and easily navigable resource that allows a deeper understanding of Brazil's complex social, economic and environmental landscape.",
    "pdf_url": "https://arxiv.org/pdf/2511.11755v1",
    "github_url": null,
    "published": "2025-11-13T20:18:20+00:00",
    "updated": "2025-11-13T20:18:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10615v1",
    "title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals",
    "authors": [
      "Baghel",
      "Rathore",
      "Jena"
    ],
    "summary": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.",
    "pdf_url": "https://arxiv.org/pdf/2511.10615v1",
    "github_url": null,
    "published": "2025-11-13T18:45:39+00:00",
    "updated": "2025-11-13T18:45:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10598v2",
    "title": "Optimizing the flight path for a scouting Uncrewed Aerial Vehicle",
    "authors": [
      "Adhikari",
      "Khatiwada",
      "Poudel"
    ],
    "summary": "Post-disaster situations pose unique navigation challenges. One of those challenges is the unstructured nature of the environment, which makes it hard to layout paths for rescue vehicles. We propose the use of Uncrewed Aerial Vehicle (UAV) in such scenario to perform reconnaissance across the environment. To accomplish this, we propose an optimization-based approach to plan a path for the UAV at optimal height where the sensors of the UAV can cover the most area and collect data with minimum uncertainty.",
    "pdf_url": "https://arxiv.org/pdf/2511.10598v2",
    "github_url": null,
    "published": "2025-11-13T18:36:05+00:00",
    "updated": "2025-11-19T16:39:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10585v1",
    "title": "Textual understanding boost in the WikiRace",
    "authors": [
      "Ebrahimi",
      "Fuhrman",
      "Nguyen"
    ],
    "summary": "The WikiRace game, where players navigate between Wikipedia articles using only hyperlinks, serves as a compelling benchmark for goal-directed search in complex information networks. This paper presents a systematic evaluation of navigation strategies for this task, comparing agents guided by graph-theoretic structure (betweenness centrality), semantic meaning (language model embeddings), and hybrid approaches. Through rigorous benchmarking on a large Wikipedia subgraph, we demonstrate that a purely greedy agent guided by the semantic similarity of article titles is overwhelmingly effective. This strategy, when combined with a simple loop-avoidance mechanism, achieved a perfect success rate and navigated the network with an efficiency an order of magnitude better than structural or hybrid methods. Our findings highlight the critical limitations of purely structural heuristics for goal-directed search and underscore the transformative potential of large language models to act as powerful, zero-shot semantic navigators in complex information spaces.",
    "pdf_url": "https://arxiv.org/pdf/2511.10585v1",
    "github_url": null,
    "published": "2025-11-13T18:25:43+00:00",
    "updated": "2025-11-13T18:25:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10435v1",
    "title": "Neuronal Fluctuations: Learning Rates vs Participating Neurons",
    "authors": [
      "Pareek",
      "Kumar",
      "Rao"
    ],
    "summary": "Deep Neural Networks (DNNs) rely on inherent fluctuations in their internal parameters (weights and biases) to effectively navigate the complex optimization landscape and achieve robust performance. While these fluctuations are recognized as crucial for escaping local minima and improving generalization, their precise relationship with fundamental hyperparameters remains underexplored. A significant knowledge gap exists concerning how the learning rate, a critical parameter governing the training process, directly influences the dynamics of these neural fluctuations. This study systematically investigates the impact of varying learning rates on the magnitude and character of weight and bias fluctuations within a neural network. We trained a model using distinct learning rates and analyzed the corresponding parameter fluctuations in conjunction with the network's final accuracy. Our findings aim to establish a clear link between the learning rate's value, the resulting fluctuation patterns, and overall model performance. By doing so, we provide deeper insights into the optimization process, shedding light on how the learning rate mediates the crucial exploration-exploitation trade-off during training. This work contributes to a more nuanced understanding of hyperparameter tuning and the underlying mechanics of deep learning.",
    "pdf_url": "https://arxiv.org/pdf/2511.10435v1",
    "github_url": null,
    "published": "2025-11-13T15:58:53+00:00",
    "updated": "2025-11-13T15:58:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10408v1",
    "title": "Navigating the Ethics of Internet Measurement: Researchers' Perspectives from a Case Study in the EU",
    "authors": [
      "Amin",
      "Athar",
      "Feldmann"
    ],
    "summary": "Internet measurement research is essential for understanding, improving, and securing Internet infrastructure. However, its methods often involve large-scale data collection and user observation, raising complex ethical questions. While recent research has identified ethical challenges in Internet measurement research and laid out best practices, little is known about how researchers actually make ethical decisions in their research practice. To understand how these practices take shape day-to-day from the perspective of Internet measurement researchers, we interviewed 16 researchers from an Internet measurement research group in the EU. Through thematic analysis, we find that researchers deal with five main ethical challenges: privacy and consent issues, the possibility of unintended harm, balancing transparency with security and accountability, uncertain ethical boundaries, and hurdles in the ethics review process. Researchers address these by lab testing, rate limiting, setting up clear communication channels, and relying heavily on mentors and colleagues for guidance. Researchers express that ethical requirements vary across institutions, jurisdictions and conferences, and ethics review boards often lack the technical knowledge to evaluate Internet measurement research. We also highlight the invisible labor of Internet measurement researchers and describe their ethics practices as craft knowledge, both of which are crucial in upholding responsible research practices in the Internet measurement community.",
    "pdf_url": "https://arxiv.org/pdf/2511.10408v1",
    "github_url": null,
    "published": "2025-11-13T15:29:19+00:00",
    "updated": "2025-11-13T15:29:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10395v1",
    "title": "AgentEvolver: Towards Efficient Self-Evolving Agent System",
    "authors": [
      "Zhai",
      "Tao",
      "Chen"
    ],
    "summary": "Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.",
    "pdf_url": "https://arxiv.org/pdf/2511.10395v1",
    "github_url": null,
    "published": "2025-11-13T15:14:47+00:00",
    "updated": "2025-11-13T15:14:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10376v2",
    "title": "MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation",
    "authors": [
      "Huang",
      "Zhao",
      "Wang"
    ],
    "summary": "Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relation",
    "pdf_url": "https://arxiv.org/pdf/2511.10376v2",
    "github_url": null,
    "published": "2025-11-13T14:51:21+00:00",
    "updated": "2025-11-14T12:20:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00037v1",
    "title": "ICD-Net: Inertial Covariance Displacement Network for Drone Visual-Inertial SLAM",
    "authors": [
      "Shapira",
      "Klein"
    ],
    "summary": "Visual-inertial SLAM systems often exhibit suboptimal performance due to multiple confounding factors including imperfect sensor calibration, noisy measurements, rapid motion dynamics, low illumination, and the inherent limitations of traditional inertial navigation integration methods. These issues are particularly problematic in drone applications where robust and accurate state estimation is critical for safe autonomous operation. In this work, we present ICD-Net, a novel framework that enhances visual-inertial SLAM performance by learning to process raw inertial measurements and generating displacement estimates with associated uncertainty quantification. Rather than relying on analytical inertial sensor models that struggle with real-world sensor imperfections, our method directly extracts displacement maps from sensor data while simultaneously predicting measurement covariances that reflect estimation confidence. We integrate ICD-Net outputs as additional residual constraints into the VINS-Fusion optimization framework, where the predicted uncertainties appropriately weight the neural network contributions relative to traditional visual and inertial terms. The learned displacement constraints provide complementary information that compensates for various error sources in the SLAM pipeline. Our approach can be used under both normal operating conditions and in situations of camera inconsistency or visual degradation. Experimental evaluation on challenging high-speed drone sequences demonstrated that our approach significantly improved trajectory estimation accuracy compared to standard VINS-Fusion, with more than 38% improvement in mean APE and uncertainty estimates proving crucial for maintaining system robustness. Our method shows that neural network enhancement can effectively address multiple sources of SLAM degradation while maintaining real-time performance requirements.",
    "pdf_url": "https://arxiv.org/pdf/2512.00037v1",
    "github_url": null,
    "published": "2025-11-13T11:25:02+00:00",
    "updated": "2025-11-13T11:25:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10172v1",
    "title": "Equivalent Mechanical Models for Sloshing",
    "authors": [
      "Capolupo"
    ],
    "summary": "Propellant sloshing is a well-known, but not completely mastered phenomenon in space vehicles. It is particularly critical in both microgravity environments - such as interplanetary spacecraft requiring high pointing stability - and high-g conditions, as encountered during launch, re-entry, and landing. In both cases, sloshing can significantly affect vehicle performance and stability, and must often be explicitly considered in the design of the guidance, navigation, and control (GNC) subsystem.   For stability analysis and control design, the most common approach to modeling sloshing is through an equivalent mechanical representation, where the moving propellant is treated as a mechanical system interacting with the rigid (or flexible) spacecraft. Pendulum-based models and mass-spring-damper systems are widely used by control analysts to assess sloshing-induced perturbations on vehicles subjected to persistent non-gravitational acceleration along one of their body axes.   In this work, we present a rigorous mathematical formulation of pendulum dynamics, starting from a single spherical pendulum attached to a rigid spacecraft. We derive the nonlinear equations of motion for this 8-degree-of-freedom multi-body system, and then extend the formulation to include multiple pendulums, representing multiple sloshing modes within a tank and/or multiple tanks on the same vehicle. Furthermore, we derive the corresponding linearized equations of motion, explicitly accounting for a nominal longitudinal force acting on the vehicle - consistent with the high-g sloshing regime - expressed in either the inertial or body frame. Finally, we demonstrate the mathematical equivalence between the pendulum and mass-spring-damper models and validate the proposed models through time-domain simulation and frequency-domain analysis.",
    "pdf_url": "https://arxiv.org/pdf/2511.10172v1",
    "github_url": null,
    "published": "2025-11-13T10:33:17+00:00",
    "updated": "2025-11-13T10:33:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10158v1",
    "title": "Closed Form Modelling and Identification of Banking Effects in Confined Waters",
    "authors": [
      "Mikkelsen",
      "Enevoldsen",
      "Jensen"
    ],
    "summary": "Vessels navigating in confined waters are subject to banking effects, which are hydrodynamic forces and moments arising from pressure differentials between the vessel sides, significantly affecting manoeuvrability and safety. Existing numerical approaches such as computational fluid dynamics (CFD) can accurately capture these effects but are computationally expensive and unsuitable for real-time control or estimation. This paper presents a closed-form, first-principles model of banking effects. The model coefficients are identified using physics-informed regression on towing tank experiment data for a scaled container vessel. Validation through Shapley value analysis confirms the significance of the banking terms in reproducing the measured forces and moments. Lastly, the derived coefficients are shown to be non-dimensional, making the model applicable across different scales that preserve vessel geometry.",
    "pdf_url": "https://arxiv.org/pdf/2511.10158v1",
    "github_url": null,
    "published": "2025-11-13T10:14:52+00:00",
    "updated": "2025-11-13T10:14:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10027v2",
    "title": "ChEmREF: Evaluating Language Model Readiness for Chemical Emergency Response",
    "authors": [
      "Surana",
      "Ye",
      "Swayamdipta"
    ],
    "summary": "Emergency responders managing hazardous material HAZMAT incidents face critical, time-sensitive decisions, manually navigating extensive chemical guidelines. We investigate whether today's language models can assist responders by rapidly and reliably understanding critical information, identifying hazards, and providing recommendations. We introduce the Chemical Emergency Response Evaluation Framework (ChEmREF), a new benchmark comprising questions on 1,035 HAZMAT chemicals from the Emergency Response Guidebook and the PubChem Database. ChEmREF is organized into three tasks: (1) translation of chemical representation between structured and unstructured forms (e.g., converting C2H6O to ethanol), (2) emergency response generation (e.g., recommending appropriate evacuation distances) and (3) domain knowledge question answering from chemical safety and certification exams. Our best evaluated models received an exact match of 68.0% on unstructured HAZMAT chemical representation translation, a LLM Judge score of 52.7% on incident response recommendations, and a multiple-choice accuracy of 63.9% on HAMZAT examinations. These findings suggest that while language models show potential to assist emergency responders in various tasks, they require careful human oversight due to their current limitations.",
    "pdf_url": "https://arxiv.org/pdf/2511.10027v2",
    "github_url": null,
    "published": "2025-11-13T07:04:16+00:00",
    "updated": "2025-11-14T19:16:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10023v1",
    "title": "Efficient Automated Diagnosis of Retinopathy of Prematurity by Customize CNN Models",
    "authors": [
      "Saeedi",
      "Keshvari",
      "Shoeibi"
    ],
    "summary": "This paper encompasses an in-depth examination of Retinopathy of Prematurity (ROP) diagnosis, employing advanced deep learning methodologies. Our focus centers on refining and evaluating CNN-based approaches for precise and efficient ROP detection. We navigate the complexities of dataset curation, preprocessing strategies, and model architecture, aligning with research objectives encompassing model effectiveness, computational cost analysis, and time complexity assessment. Results underscore the supremacy of tailored CNN models over pre-trained counterparts, evident in heightened accuracy and F1-scores. Implementation of a voting system further enhances performance. Additionally, our study reveals the potential of the proposed customized CNN model to alleviate computational burdens associated with deep neural networks. Furthermore, we showcase the feasibility of deploying these models within dedicated software and hardware configurations, highlighting their utility as valuable diagnostic aids in clinical settings. In summary, our discourse significantly contributes to ROP diagnosis, unveiling the efficacy of deep learning models in enhancing diagnostic precision and efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2511.10023v1",
    "github_url": null,
    "published": "2025-11-13T07:00:54+00:00",
    "updated": "2025-11-13T07:00:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.09885v1",
    "title": "PuffyBot: An Untethered Shape Morphing Robot for Multi-environment Locomotion",
    "authors": [
      "Singh",
      "Si",
      "Temel"
    ],
    "summary": "Amphibians adapt their morphologies and motions to accommodate movement in both terrestrial and aquatic environments. Inspired by these biological features, we present PuffyBot, an untethered shape morphing robot capable of changing its body morphology to navigate multiple environments. Our robot design leverages a scissor-lift mechanism driven by a linear actuator as its primary structure to achieve shape morphing. The transformation enables a volume change from 255.00 cm3 to 423.75 cm3, modulating the buoyant force to counteract a downward force of 3.237 N due to 330 g mass of the robot. A bell-crank linkage is integrated with the scissor-lift mechanism, which adjusts the servo-actuated limbs by 90 degrees, allowing a seamless transition between crawling and swimming modes. The robot is fully waterproof, using thermoplastic polyurethane (TPU) fabric to ensure functionality in aquatic environments. The robot can operate untethered for two hours with an onboard battery of 1000 mA h. Our experimental results demonstrate multi-environment locomotion, including crawling on the land, crawling on the underwater floor, swimming on the water surface, and bimodal buoyancy adjustment to submerge underwater or resurface. These findings show the potential of shape morphing to create versatile and energy efficient robotic platforms suitable for diverse environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.09885v1",
    "github_url": null,
    "published": "2025-11-13T02:33:56+00:00",
    "updated": "2025-11-13T02:33:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.09827v1",
    "title": "AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting",
    "authors": [
      "Mir",
      "Wang",
      "Guler"
    ],
    "summary": "We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. Our key insight is that the rendering can be decoupled from the motion synthesis and each sub-problem can be addressed independently, without the need for paired human-scene data. Central to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human-scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework allows for novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with new animated humans, showcasing the unique advantage of 3DGS for monocular video-based human animation.",
    "pdf_url": "https://arxiv.org/pdf/2511.09827v1",
    "github_url": null,
    "published": "2025-11-13T00:19:18+00:00",
    "updated": "2025-11-13T00:19:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.09820v1",
    "title": "From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance",
    "authors": [
      "Min",
      "Kim",
      "Lee"
    ],
    "summary": "Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments. However, existing approaches often require supervised training on curated datasets and rely on panoramic or UAV-based images, which limits real-world deployment. In this paper, we present a simple yet effective cross-view image retrieval framework that leverages a pretrained vision encoder and a large language model (LLM), requiring no additional training. Given a monocular street-view image, our method extracts geographic cues through web-based image search and LLM-based location inference, generates a satellite query via geocoding API, and retrieves matching tiles using a pretrained vision encoder (e.g., DINOv2) with PCA-based whitening feature refinement. Despite using no ground-truth supervision or finetuning, our proposed method outperforms prior learning-based approaches on the benchmark dataset under zero-shot settings. Moreover, our pipeline enables automatic construction of semantically aligned street-to-satellite datasets, which is offering a scalable and cost-efficient alternative to manual annotation. All source codes will be made publicly available at https://jeonghomin.github.io/street2orbit.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2511.09820v1",
    "github_url": null,
    "published": "2025-11-12T23:51:46+00:00",
    "updated": "2025-11-12T23:51:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.09799v1",
    "title": "A Smooth Penalty-Based Feedback Law for Reactive Obstacle Avoidance with Convergence Guarantees",
    "authors": [
      "Smaili",
      "Berkane"
    ],
    "summary": "This paper addresses the problem of safe autonomous navigation in unknown obstacle-filled environments using only local sensory information. We propose a smooth feedback controller derived from an unconstrained penalty-based formulation that guarantees safety by construction. The controller modifies an arbitrary nominal input through a closed-form expression. The resulting closed-form feedback has a projection structure that interpolates between the nominal control and its orthogonal projection onto the obstacle boundary, ensuring forward invariance of a user-defined safety margin. The control law depends only on the distance and bearing to obstacles and requires no map, switching, or set construction. When the nominal input is a gradient descent of a navigation potential, we prove that the closed-loop system achieves almost global asymptotic stability (AGAS) to the goal. Undesired equilibria are shown to be unstable under a mild geometric curvature condition, which compares the normal curvature of the obstacle boundary with that of the potential level sets. We refer to the proposed method as SPF (Safe Penalty-based Feedback), which ensures safe and smooth navigation with minimal computational overhead, as demonstrated through simulations in complex 2D and 3D environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.09799v1",
    "github_url": null,
    "published": "2025-11-12T23:02:59+00:00",
    "updated": "2025-11-12T23:02:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.09724v1",
    "title": "PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model",
    "authors": [
      "Cheng",
      "Princen",
      "Manduchi"
    ],
    "summary": "Indoor localization in GPS-denied environments is crucial for applications like emergency response and assistive navigation. Vision-based methods such as PALMS enable infrastructure-free localization using only a floor plan and a stationary scan, but are limited by the short range of smartphone LiDAR and ambiguity in indoor layouts. We propose PALMS$+$, a modular, image-based system that addresses these challenges by reconstructing scale-aligned 3D point clouds from posed RGB images using a foundation monocular depth estimation model (Depth Pro), followed by geometric layout matching via convolution with the floor plan. PALMS$+$ outputs a posterior over the location and orientation, usable for direct or sequential localization. Evaluated on the Structured3D and a custom campus dataset consisting of 80 observations across four large campus buildings, PALMS$+$ outperforms PALMS and F3Loc in stationary localization accuracy -- without requiring any training. Furthermore, when integrated with a particle filter for sequential localization on 33 real-world trajectories, PALMS$+$ achieved lower localization errors compared to other methods, demonstrating robustness for camera-free tracking and its potential for infrastructure-free applications. Code and data are available at https://github.com/Head-inthe-Cloud/PALMS-Plane-based-Accessible-Indoor-Localization-Using-Mobile-Smartphones",
    "pdf_url": "https://arxiv.org/pdf/2511.09724v1",
    "github_url": "https://github.com/Head-inthe-Cloud/PALMS-Plane-based-Accessible-Indoor-Localization-Using-Mobile-Smartphones",
    "published": "2025-11-12T20:30:53+00:00",
    "updated": "2025-11-12T20:30:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.10699v1",
    "title": "DualVision ArthroNav: Investigating Opportunities to Enhance Localization and Reconstruction in Image-based Arthroscopy Navigation via External Cameras",
    "authors": [
      "Shu",
      "Seenivasan",
      "Liu"
    ],
    "summary": "Arthroscopic procedures can greatly benefit from navigation systems that enhance spatial awareness, depth perception, and field of view. However, existing optical tracking solutions impose strict workspace constraints and disrupt surgical workflow. Vision-based alternatives, though less invasive, often rely solely on the monocular arthroscope camera, making them prone to drift, scale ambiguity, and sensitivity to rapid motion or occlusion. We propose DualVision ArthroNav, a multi-camera arthroscopy navigation system that integrates an external camera rigidly mounted on the arthroscope. The external camera provides stable visual odometry and absolute localization, while the monocular arthroscope video enables dense scene reconstruction. By combining these complementary views, our system resolves the scale ambiguity and long-term drift inherent in monocular SLAM and ensures robust relocalization. Experiments demonstrate that our system effectively compensates for calibration errors, achieving an average absolute trajectory error of 1.09 mm. The reconstructed scenes reach an average target registration error of 2.16 mm, with high visual fidelity (SSIM = 0.69, PSNR = 22.19). These results indicate that our system provides a practical and cost-efficient solution for arthroscopic navigation, bridging the gap between optical tracking and purely vision-based systems, and paving the way toward clinically deployable, fully vision-based arthroscopic guidance.",
    "pdf_url": "https://arxiv.org/pdf/2511.10699v1",
    "github_url": null,
    "published": "2025-11-12T16:02:34+00:00",
    "updated": "2025-11-12T16:02:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.09443v1",
    "title": "BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation",
    "authors": [
      "Shu",
      "Soberanis-Mukul",
      "Xu"
    ],
    "summary": "Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.",
    "pdf_url": "https://arxiv.org/pdf/2511.09443v1",
    "github_url": null,
    "published": "2025-11-12T15:58:05+00:00",
    "updated": "2025-11-12T15:58:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.09331v1",
    "title": "CoRL-MPPI: Enhancing MPPI With Learnable Behaviours For Efficient And Provably-Safe Multi-Robot Collision Avoidance",
    "authors": [
      "Dergachev",
      "Pshenitsyn",
      "Panov"
    ],
    "summary": "Decentralized collision avoidance remains a core challenge for scalable multi-robot systems. One of the promising approaches to tackle this problem is Model Predictive Path Integral (MPPI) -- a framework that is naturally suited to handle any robot motion model and provides strong theoretical guarantees. Still, in practice MPPI-based controller may provide suboptimal trajectories as its performance relies heavily on uninformed random sampling. In this work, we introduce CoRL-MPPI, a novel fusion of Cooperative Reinforcement Learning and MPPI to address this limitation. We train an action policy (approximated as deep neural network) in simulation that learns local cooperative collision avoidance behaviors. This learned policy is then embedded into the MPPI framework to guide its sampling distribution, biasing it towards more intelligent and cooperative actions. Notably, CoRL-MPPI preserves all the theoretical guarantees of regular MPPI. We evaluate our approach in dense, dynamic simulation environments against state-of-the-art baselines, including ORCA, BVC, and a multi-agent MPPI implementation. Our results demonstrate that CoRL-MPPI significantly improves navigation efficiency (measured by success rate and makespan) and safety, enabling agile and robust multi-robot navigation.",
    "pdf_url": "https://arxiv.org/pdf/2511.09331v1",
    "github_url": null,
    "published": "2025-11-12T13:46:27+00:00",
    "updated": "2025-11-12T13:46:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.09044v1",
    "title": "Advancing Autonomous Emergency Response Systems: A Generative AI Perspective",
    "authors": [
      "Emami",
      "Reddy",
      "Pourkabirian"
    ],
    "summary": "Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.",
    "pdf_url": "https://arxiv.org/pdf/2511.09044v1",
    "github_url": null,
    "published": "2025-11-12T06:54:26+00:00",
    "updated": "2025-11-12T06:54:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08978v1",
    "title": "Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding",
    "authors": [
      "Ma",
      "Wang",
      "Zhao"
    ],
    "summary": "Nowadays, navigation and ride-sharing apps have collected numerous images with spatio-temporal data. A core technology for analyzing such images, associated with spatiotemporal information, is Traffic Scene Understanding (TSU), which aims to provide a comprehensive description of the traffic scene. Unlike traditional spatio-temporal data analysis tasks, the dependence on both spatio-temporal and visual-textual data introduces distinct challenges to TSU task. However, recent research often treats TSU as a common image understanding task, ignoring the spatio-temporal information and overlooking the interrelations between different aspects of the traffic scene. To address these issues, we propose a novel SpatioTemporal Enhanced Model based on CILP (ST-CLIP) for TSU. Our model uses the classic vision-language model, CLIP, as the backbone, and designs a Spatio-temporal Context Aware Multiaspect Prompt (SCAMP) learning method to incorporate spatiotemporal information into TSU. The prompt learning method consists of two components: A dynamic spatio-temporal context representation module that extracts representation vectors of spatio-temporal data for each traffic scene image, and a bi-level ST-aware multi-aspect prompt learning module that integrates the ST-context representation vectors into word embeddings of prompts for the CLIP model. The second module also extracts low-level visual features and image-wise high-level semantic features to exploit interactive relations among different aspects of traffic scenes. To the best of our knowledge, this is the first attempt to integrate spatio-temporal information into visionlanguage models to facilitate TSU task. Experiments on two realworld datasets demonstrate superior performance in the complex scene understanding scenarios with a few-shot learning strategy.",
    "pdf_url": "https://arxiv.org/pdf/2511.08978v1",
    "github_url": null,
    "published": "2025-11-12T04:55:38+00:00",
    "updated": "2025-11-12T04:55:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08942v1",
    "title": "Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning",
    "authors": [
      "Habibpour",
      "Afghah"
    ],
    "summary": "While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.",
    "pdf_url": "https://arxiv.org/pdf/2511.08942v1",
    "github_url": null,
    "published": "2025-11-12T03:38:50+00:00",
    "updated": "2025-11-12T03:38:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08935v1",
    "title": "Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation",
    "authors": [
      "Wang",
      "Chen",
      "Chen"
    ],
    "summary": "Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.",
    "pdf_url": "https://arxiv.org/pdf/2511.08935v1",
    "github_url": null,
    "published": "2025-11-12T03:23:09+00:00",
    "updated": "2025-11-12T03:23:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08900v1",
    "title": "An Improved Dual-Attention Transformer-LSTM for Small-Sample Prediction of Modal Frequency and Actual Anchor Radius in Micro Hemispherical Resonator Design",
    "authors": [
      "Yao",
      "Yang",
      "Xu"
    ],
    "summary": "The high-temperature glassblowing-fabricated micro hemispherical resonator (MHR) exhibits high symmetry and high Q-value for precision inertial navigation. However, MHR design entails a comprehensive evaluation of multiple possible configurations and demands extremely time-consuming simulation of key parameters combination. To address this problem, this paper proposed a rapid prediction method of modal frequency and actual anchor radius of designed MHR using an improved Transformer-LSTM (Long Short-Term Memory) model for rapid design sizing. High-temperature-induced softening deformation at the anchor point reduces the actual anchor radius below the designed value. By varying key parameters such as resonator height, anchor radius and edge thickness, finite element glassblowing simulation and modal analyse were conducted to obtain the first six modal frequencies and actual anchor radius. To address regression prediction challenges with limited data, dual multi-head self-attention (MHSA) mechanisms replaced the transformer's standard Feed Forward Network, to improve hidden information capture for high-accuracy predictions of modal frequencies and anchor radius. By checking fabricating feasibility of anchor radius and allowing rapid modal characteristics evaluation without interference, ablation and comparative experiments validated the method's superiority, as an effective support of MHR design. Design optimization experiments demonstrate a prediction accuracy of 96.35%, with computational time reduced to 1/48,000 of traditional finite element methods, significantly improving design efficiency. This study offers a new paradigm for intelligent Micro-Electro-Mechanical System (MEMS) device design under complex process conditions.",
    "pdf_url": "https://arxiv.org/pdf/2511.08900v1",
    "github_url": null,
    "published": "2025-11-12T02:20:47+00:00",
    "updated": "2025-11-12T02:20:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08863v1",
    "title": "XPRESS: X-Band Radar Place Recognition via Elliptical Scan Shaping",
    "authors": [
      "Jang",
      "Yang",
      "Kim"
    ],
    "summary": "X-band radar serves as the primary sensor on maritime vessels, however, its application in autonomous navigation has been limited due to low sensor resolution and insufficient information content. To enable X-band radar-only autonomous navigation in maritime environments, this paper proposes a place recognition algorithm specifically tailored for X-band radar, incorporating an object density-based rule for efficient candidate selection and intentional degradation of radar detections to achieve robust retrieval performance. The proposed algorithm was evaluated on both public maritime radar datasets and our own collected dataset, and its performance was compared against state-of-the-art radar place recognition methods. An ablation study was conducted to assess the algorithm's performance sensitivity with respect to key parameters.",
    "pdf_url": "https://arxiv.org/pdf/2511.08863v1",
    "github_url": null,
    "published": "2025-11-12T01:01:38+00:00",
    "updated": "2025-11-12T01:01:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08822v1",
    "title": "Low-cost Multi-agent Fleet for Acoustic Cooperative Localization Research",
    "authors": [
      "Durrant",
      "Meyers",
      "McMurray"
    ],
    "summary": "Real-world underwater testing for multi-agent autonomy presents substantial financial and engineering challenges. In this work, we introduce the Configurable Underwater Group of Autonomous Robots (CoUGARs) as a low-cost, configurable autonomous-underwater-vehicle (AUV) platform for multi-agent autonomy research. The base design costs less than $3,000 USD (as of May 2025) and is based on commercially-available and 3D-printed parts, enabling quick customization for various sensor payloads and configurations. Our current expanded model is equipped with a doppler velocity log (DVL) and ultra-short-baseline (USBL) acoustic array/transducer to support research on acoustic-based cooperative localization. State estimation, navigation, and acoustic communications software has been developed and deployed using a containerized software stack and is tightly integrated with the HoloOcean simulator. The system was tested both in simulation and via in-situ field trials in Utah lakes and reservoirs.",
    "pdf_url": "https://arxiv.org/pdf/2511.08822v1",
    "github_url": null,
    "published": "2025-11-11T22:37:37+00:00",
    "updated": "2025-11-11T22:37:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08763v1",
    "title": "Modeling multi-agent motion dynamics in immersive rooms",
    "authors": [
      "Mincong",
      "Huang",
      "Radev"
    ],
    "summary": "Immersive rooms are increasingly popular augmented reality systems that support multi-agent interactions within a virtual world. However, despite extensive content creation and technological developments, insights about perceptually-driven social dynamics, such as the complex movement patterns during virtual world navigation, remain largely underexplored. Computational models of motion dynamics can help us understand the underlying mechanism of human interaction in immersive rooms and develop applications that better support spatially distributed interaction. In this work, we propose a new agent-based model of emergent human motion dynamics. The model represents human agents as simple spatial geometries in the room that relocate and reorient themselves based on the salient virtual spatial objects they approach. Agent motion is modeled as an interactive process combining external diffusion-driven influences from the environment with internal self-propelling interactions among agents. Further, we leverage simulation-based inference (SBI) to show that the governing parameters of motion patterns can be estimated from simple observables. Our results indicate that the model successfully captures action-related agent properties but exposes local non-identifiability linked to environmental awareness. We argue that our simulation-based approach paves the way for creating adaptive, responsive immersive rooms -- spaces that adjust their interfaces and interactions based on human collective movement patterns and spatial attention.",
    "pdf_url": "https://arxiv.org/pdf/2511.08763v1",
    "github_url": null,
    "published": "2025-11-11T20:30:50+00:00",
    "updated": "2025-11-11T20:30:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08568v1",
    "title": "Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory",
    "authors": [
      "Ren",
      "Ma",
      "Yang"
    ],
    "summary": "Deep learning recommendation models (DLRMs) are widely used in industry, and their memory capacity requirements reach the terabyte scale. Tiered memory architectures provide a cost-effective solution but introduce challenges in embedding-vector placement due to complex embedding-access patterns. We propose RecMG, a machine learning (ML)-guided system for vector caching and prefetching on tiered memory. RecMG accurately predicts accesses to embedding vectors with long reuse distances or few reuses. The design of RecMG focuses on making ML feasible in the context of DLRM inference by addressing unique challenges in data labeling and navigating the search space for embedding-vector placement. By employing separate ML models for caching and prefetching, plus a novel differentiable loss function, RecMG narrows the prefetching search space and minimizes on-demand fetches. Compared to state-of-the-art temporal, spatial, and ML-based prefetchers, RecMG reduces on-demand fetches by 2.2x, 2.8x, and 1.5x, respectively. In industrial-scale DLRM inference scenarios, RecMG effectively reduces end-to-end DLRM inference time by up to 43%.",
    "pdf_url": "https://arxiv.org/pdf/2511.08568v1",
    "github_url": null,
    "published": "2025-11-11T18:49:53+00:00",
    "updated": "2025-11-11T18:49:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08545v1",
    "title": "RePose-NeRF: Robust Radiance Fields for Mesh Reconstruction under Noisy Camera Poses",
    "authors": [
      "Srinivasan",
      "Ramachandra"
    ],
    "summary": "Accurate 3D reconstruction from multi-view images is essential for downstream robotic tasks such as navigation, manipulation, and environment understanding. However, obtaining precise camera poses in real-world settings remains challenging, even when calibration parameters are known. This limits the practicality of existing NeRF-based methods that rely heavily on accurate extrinsic estimates. Furthermore, their implicit volumetric representations differ significantly from the widely adopted polygonal meshes, making rendering and manipulation inefficient in standard 3D software. In this work, we propose a robust framework that reconstructs high-quality, editable 3D meshes directly from multi-view images with noisy extrinsic parameters. Our approach jointly refines camera poses while learning an implicit scene representation that captures fine geometric detail and photorealistic appearance. The resulting meshes are compatible with common 3D graphics and robotics tools, enabling efficient downstream use. Experiments on standard benchmarks demonstrate that our method achieves accurate and robust 3D reconstruction under pose uncertainty, bridging the gap between neural implicit representations and practical robotic applications.",
    "pdf_url": "https://arxiv.org/pdf/2511.08545v1",
    "github_url": null,
    "published": "2025-11-11T18:25:58+00:00",
    "updated": "2025-11-11T18:25:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08502v1",
    "title": "Safe and Optimal Learning from Preferences via Weighted Temporal Logic with Applications in Robotics and Formula 1",
    "authors": [
      "Karagulle",
      "Vasile",
      "Ozay"
    ],
    "summary": "Autonomous systems increasingly rely on human feedback to align their behavior, expressed as pairwise comparisons, rankings, or demonstrations. While existing methods can adapt behaviors, they often fail to guarantee safety in safety-critical domains. We propose a safety-guaranteed, optimal, and efficient approach to solve the learning problem from preferences, rankings, or demonstrations using Weighted Signal Temporal Logic (WSTL). WSTL learning problems, when implemented naively, lead to multi-linear constraints in the weights to be learned. By introducing structural pruning and log-transform procedures, we reduce the problem size and recast the problem as a Mixed-Integer Linear Program while preserving safety guarantees. Experiments on robotic navigation and real-world Formula 1 data demonstrate that the method effectively captures nuanced preferences and models complex task objectives.",
    "pdf_url": "https://arxiv.org/pdf/2511.08502v1",
    "github_url": null,
    "published": "2025-11-11T17:35:23+00:00",
    "updated": "2025-11-11T17:35:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08416v1",
    "title": "Generative AI Meets 6G and Beyond: Diffusion Models for Semantic Communications",
    "authors": [
      "Qin",
      "Dai",
      "Lu"
    ],
    "summary": "Semantic communications mark a paradigm shift from bit-accurate transmission toward meaning-centric communication, essential as wireless systems approach theoretical capacity limits. The emergence of generative AI has catalyzed generative semantic communications, where receivers reconstruct content from minimal semantic cues by leveraging learned priors. Among generative approaches, diffusion models stand out for their superior generation quality, stable training dynamics, and rigorous theoretical foundations. However, the field currently lacks systematic guidance connecting diffusion techniques to communication system design, forcing researchers to navigate disparate literatures. This article provides the first comprehensive tutorial on diffusion models for generative semantic communications. We present score-based diffusion foundations and systematically review three technical pillars: conditional diffusion for controllable generation, efficient diffusion for accelerated inference, and generalized diffusion for cross-domain adaptation. In addition, we introduce an inverse problem perspective that reformulates semantic decoding as posterior inference, bridging semantic communications with computational imaging. Through analysis of human-centric, machine-centric, and agent-centric scenarios, we illustrate how diffusion models enable extreme compression while maintaining semantic fidelity and robustness. By bridging generative AI innovations with communication system design, this article aims to establish diffusion models as foundational components of next-generation wireless networks and beyond.",
    "pdf_url": "https://arxiv.org/pdf/2511.08416v1",
    "github_url": null,
    "published": "2025-11-11T16:27:43+00:00",
    "updated": "2025-11-11T16:27:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08343v2",
    "title": "JobSphere: An AI-Powered Multilingual Career Copilot for Government Employment Platforms",
    "authors": [
      "R",
      "B",
      "Hussain"
    ],
    "summary": "Users of government employment websites commonly face engagement and accessibility challenges linked to navigational complexity, a dearth of language options, and a lack of personalized support. This paper introduces JobSphere, an AI-powered career assistant that is redefining the employment platform in Punjab called PGRKAM. JobSphere employs Retrieval-Augmented Generation (RAG) architecture, and it is multilingual, available in English, Hindi and Punjabi. JobSphere technique uses 4-bit quantization, allowing the platform to deploy on consumer-grade GPUs (i.e., NVIDIA RTX 3050 4GB), making the implementation 89% cheaper than that of cloud-based systems. Key innovations include voice-enabled interaction with the assistant, automated mock tests, resume parsing with skills recognition, and embed-based job recommendation that achieves a precision@10 score of 68%. An evaluation of JobSphere's implementation reveals 94% factual accuracy, a median response time of 1.8 seconds, and a System Usability Scale score of 78.5/100, a 50% improvement compared to the baseline PGRKAM platform context. In conclusion, JobSphere effectively fills significant accessibility gaps for Punjab/Hindi-speaking users in rural locations, while also affirming the users access to trusted job content provided by government agencies.",
    "pdf_url": "https://arxiv.org/pdf/2511.08343v2",
    "github_url": null,
    "published": "2025-11-11T15:19:46+00:00",
    "updated": "2025-11-15T16:08:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08339v1",
    "title": "LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration",
    "authors": [
      "Qiu",
      "Wang",
      "Yang"
    ],
    "summary": "Lexicographic multi-objective problems, which consist of multiple conflicting subtasks with explicit priorities, are common in real-world applications. Despite the advantages of Reinforcement Learning (RL) in single tasks, extending conventional RL methods to prioritized multiple objectives remains challenging. In particular, traditional Safe RL and Multi-Objective RL (MORL) methods have difficulty enforcing priority orderings efficiently. Therefore, Lexicographic Multi-Objective RL (LMORL) methods have been developed to address these challenges. However, existing LMORL methods either rely on heuristic threshold tuning with prior knowledge or are restricted to discrete domains. To overcome these limitations, we propose Lexicographically Projected Policy Gradient RL (LPPG-RL), a novel LMORL framework which leverages sequential gradient projections to identify feasible policy update directions, thereby enabling LPPG-RL broadly compatible with all policy gradient algorithms in continuous spaces. LPPG-RL reformulates the projection step as an optimization problem, and utilizes Dykstra's projection rather than generic solvers to deliver great speedups, especially for small- to medium-scale instances. In addition, LPPG-RL introduces Subproblem Exploration (SE) to prevent gradient vanishing, accelerate convergence and enhance stability. We provide theoretical guarantees for convergence and establish a lower bound on policy improvement. Finally, through extensive experiments in a 2D navigation environment, we demonstrate the effectiveness of LPPG-RL, showing that it outperforms existing state-of-the-art continuous LMORL methods.",
    "pdf_url": "https://arxiv.org/pdf/2511.08339v1",
    "github_url": null,
    "published": "2025-11-11T15:14:41+00:00",
    "updated": "2025-11-11T15:14:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08338v1",
    "title": "Extended Time Varying Multi-Cluster Fluctuating Two-Ray Fading Model for Maritime Environment",
    "authors": [
      "Vié",
      "Galeazzi",
      "Papagergiou"
    ],
    "summary": "The recent advancements in autonomous and remote operation of maritime vessels necessitates the development of robust and reliable communication systems to support high-bandwidth applications such as real-time monitoring, navigation, and control. Existing communication channel models, including Rayleigh and Rician fading, are inadequate to accurately describe the dynamic and complex nature of maritime communication, particularly for high-speed vessels in coastal environments. This paper proposes an extension to the Multi-Cluster Fluctuating Two-Ray Fading (MFTR) model that also accounts for key phenomena such as large-scale fading, time-varying parameters and Doppler shifts. The extended MFTR model integrates Stochastic Differential Equations (SDEs) to capture the time-varying characteristics of the channel, such as phase shifts and delays, while considering physical factors like delay-induced power loss and path loss. The accuracy of the proposed model is assessed in simulation.",
    "pdf_url": "https://arxiv.org/pdf/2511.08338v1",
    "github_url": null,
    "published": "2025-11-11T15:14:24+00:00",
    "updated": "2025-11-11T15:14:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08325v1",
    "title": "AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress",
    "authors": [
      "Xi",
      "Liao",
      "Li"
    ],
    "summary": "Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.",
    "pdf_url": "https://arxiv.org/pdf/2511.08325v1",
    "github_url": null,
    "published": "2025-11-11T14:57:54+00:00",
    "updated": "2025-11-11T14:57:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08292v1",
    "title": "Distance by de-correlation: Computing distance with heterogeneous grid cells",
    "authors": [
      "Dasbehera",
      "Dogra",
      "Redman"
    ],
    "summary": "Encoding the distance between locations in space is essential for accurate navigation. Grid cells, a functional class of neurons in medial entorhinal cortex, are believed to support this computation. However, existing theories of how populations of grid cells code distance rely on complex coding schemes, with assumptions that may not be met by anatomical constraints. Inspired by recent work finding grid cells to have small, but robust heterogeneity in their grid properties, we hypothesize that distance coding can be achieved by a simple de-correlation of population activity. We develop a mathematical theory for describing this de-correlation in one-dimension, showing that its predictions are consistent with simulations of noisy grid cells. Our simulations highlight a non-intuitive prediction of such a distance by de-correlation framework. Namely, that some further distances are better encoded than some nearer distances. We find evidence of this \"sweet spot\" in previously published rodent behavioral experiments and demonstrate that a decoder which estimates distance from the de-correlation of populations of simulated noisy grid cells leads to a similar pattern of errors. Finally, by simulating noisy grid cells in two-dimensions, we find that there exists a trade-off between the range of distances that can be encoded by de-correlation of population activity and the distinguishability of different distances, which is controlled by the amount of variability in grid properties. We show that the previously observed average amount of grid property variability strikes a balance between the two, enabling the encoding of distances up to several meters. Our work provides new insight on how grid cells can underlie the coding of distance, without the assumptions previously needed, and why grid cells may have small amounts of heterogeneity in their grid properties.",
    "pdf_url": "https://arxiv.org/pdf/2511.08292v1",
    "github_url": null,
    "published": "2025-11-11T14:25:13+00:00",
    "updated": "2025-11-11T14:25:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08277v1",
    "title": "X-IONet: Cross-Platform Inertial Odometry Network with Dual-Stage Attention",
    "authors": [
      "Shen",
      "Chen"
    ],
    "summary": "Learning-based inertial odometry has achieved remarkable progress in pedestrian navigation. However, extending these methods to quadruped robots remains challenging due to their distinct and highly dynamic motion patterns. Models that perform well on pedestrian data often experience severe degradation when deployed on legged platforms. To tackle this challenge, we introduce X-IONet, a cross-platform inertial odometry framework that operates solely using a single Inertial Measurement Unit (IMU). X-IONet incorporates a rule-based expert selection module to classify motion platforms and route IMU sequences to platform-specific expert networks. The displacement prediction network features a dual-stage attention architecture that jointly models long-range temporal dependencies and inter-axis correlations, enabling accurate motion representation. It outputs both displacement and associated uncertainty, which are further fused through an Extended Kalman Filter (EKF) for robust state estimation. Extensive experiments on public pedestrian datasets and a self-collected quadruped robot dataset demonstrate that X-IONet achieves state-of-the-art performance, reducing Absolute Trajectory Error (ATE) by 14.3% and Relative Trajectory Error (RTE) by 11.4% on pedestrian data, and by 52.8% and 41.3% on quadruped robot data. These results highlight the effectiveness of X-IONet in advancing accurate and robust inertial navigation across both human and legged robot platforms.",
    "pdf_url": "https://arxiv.org/pdf/2511.08277v1",
    "github_url": null,
    "published": "2025-11-11T14:06:52+00:00",
    "updated": "2025-11-11T14:06:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08098v1",
    "title": "PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision",
    "authors": [
      "Patania",
      "Annese",
      "Pellegrini"
    ],
    "summary": "Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.",
    "pdf_url": "https://arxiv.org/pdf/2511.08098v1",
    "github_url": null,
    "published": "2025-11-11T10:54:15+00:00",
    "updated": "2025-11-11T10:54:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.08046v1",
    "title": "ProSona: Prompt-Guided Personalization for Multi-Expert Medical Image Segmentation",
    "authors": [
      "Elgebaly",
      "Delopoulos",
      "Hörner-Rieber"
    ],
    "summary": "Automated medical image segmentation suffers from high inter-observer variability, particularly in tasks such as lung nodule delineation, where experts often disagree. Existing approaches either collapse this variability into a consensus mask or rely on separate model branches for each annotator. We introduce ProSona, a two-stage framework that learns a continuous latent space of annotation styles, enabling controllable personalization via natural language prompts. A probabilistic U-Net backbone captures diverse expert hypotheses, while a prompt-guided projection mechanism navigates this latent space to generate personalized segmentations. A multi-level contrastive objective aligns textual and visual representations, promoting disentangled and interpretable expert styles. Across the LIDC-IDRI lung nodule and multi-institutional prostate MRI datasets, ProSona reduces the Generalized Energy Distance by 17% and improves mean Dice by more than one point compared with DPersona. These results demonstrate that natural-language prompts can provide flexible, accurate, and interpretable control over personalized medical image segmentation. Our implementation is available online 1 .",
    "pdf_url": "https://arxiv.org/pdf/2511.08046v1",
    "github_url": null,
    "published": "2025-11-11T09:50:36+00:00",
    "updated": "2025-11-11T09:50:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07927v1",
    "title": "Local Path Planning with Dynamic Obstacle Avoidance in Unstructured Environments",
    "authors": [
      "Guvenkaya",
      "Iz",
      "Unel"
    ],
    "summary": "Obstacle avoidance and path planning are essential for guiding unmanned ground vehicles (UGVs) through environments that are densely populated with dynamic obstacles. This paper develops a novel approach that combines tangentbased path planning and extrapolation methods to create a new decision-making algorithm for local path planning. In the assumed scenario, a UGV has a prior knowledge of its initial and target points within the dynamic environment. A global path has already been computed, and the robot is provided with waypoints along this path. As the UGV travels between these waypoints, the algorithm aims to avoid collisions with dynamic obstacles. These obstacles follow polynomial trajectories, with their initial positions randomized in the local map and velocities randomized between O and the allowable physical velocity limit of the robot, along with some random accelerations. The developed algorithm is tested in several scenarios where many dynamic obstacles move randomly in the environment. Simulation results show the effectiveness of the proposed local path planning strategy by gradually generating a collision free path which allows the robot to navigate safely between initial and the target locations.",
    "pdf_url": "https://arxiv.org/pdf/2511.07927v1",
    "github_url": null,
    "published": "2025-11-11T07:26:04+00:00",
    "updated": "2025-11-11T07:26:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07852v2",
    "title": "Silicon-photonic optomechanical magnetometer",
    "authors": [
      "Gottardo",
      "Carey",
      "Bawden"
    ],
    "summary": "Optomechanical sensors enable exquisitely sensitive force measurements, with emerging applications across quantum technologies, standards, fundamental science, and engineering. Magnetometry is among the most promising applications, where chip-scale optomechanical sensors offer high sensitivity without the cryogenics or magnetic shielding required by competing technologies. However, lack of compatibility with integrated photonics and electronics has posed a major barrier. Here we introduce silicon-on-insulator optomechanical magnetometers to address this barrier. A new post-release lithography process enables high-quality metallisation of released mechanical structures, overcoming the incompatibility between silicon-on-insulator fabrication and functional magnetic films. This allows us to employ photonic-crystal cavities that enhance motion-to-optical signal transduction by over an order of magnitude. The resulting devices achieve magnetic field sensitivity of 800 pT Hz^-1/2, three orders of magnitude beyond previous waveguide-integrated designs. The advances we report provide a path towards high-performance, room temperature and chip-integrated magnetometers for applications ranging from biomedical imaging and navigation to resource exploration.",
    "pdf_url": "https://arxiv.org/pdf/2511.07852v2",
    "github_url": null,
    "published": "2025-11-11T05:35:16+00:00",
    "updated": "2025-11-17T00:14:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07811v1",
    "title": "Virtual Traffic Lights for Multi-Robot Navigation: Decentralized Planning with Centralized Conflict Resolution",
    "authors": [
      "Gupta",
      "Nguyen",
      "Phan"
    ],
    "summary": "We present a hybrid multi-robot coordination framework that combines decentralized path planning with centralized conflict resolution. In our approach, each robot autonomously plans its path and shares this information with a centralized node. The centralized system detects potential conflicts and allows only one of the conflicting robots to proceed at a time, instructing others to stop outside the conflicting area to avoid deadlocks. Unlike traditional centralized planning methods, our system does not dictate robot paths but instead provides stop commands, functioning as a virtual traffic light. In simulation experiments with multiple robots, our approach increased the success rate of robots reaching their goals while reducing deadlocks. Furthermore, we successfully validated the system in real-world experiments with two quadruped robots and separately with wheeled Duckiebots.",
    "pdf_url": "https://arxiv.org/pdf/2511.07811v1",
    "github_url": null,
    "published": "2025-11-11T04:07:44+00:00",
    "updated": "2025-11-11T04:07:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07750v1",
    "title": "Navigating the Wild: Pareto-Optimal Visual Decision-Making in Image Space",
    "authors": [
      "Pushp",
      "Chen",
      "Chen"
    ],
    "summary": "Navigating complex real-world environments requires semantic understanding and adaptive decision-making. Traditional reactive methods without maps often fail in cluttered settings, map-based approaches demand heavy mapping effort, and learning-based solutions rely on large datasets with limited generalization. To address these challenges, we present Pareto-Optimal Visual Navigation, a lightweight image-space framework that combines data-driven semantics, Pareto-optimal decision-making, and visual servoing for real-time navigation.",
    "pdf_url": "https://arxiv.org/pdf/2511.07750v1",
    "github_url": null,
    "published": "2025-11-11T02:01:54+00:00",
    "updated": "2025-11-11T02:01:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07729v1",
    "title": "Designing Mental-Health Chatbots for Indian Adolescents: Mixed-Methods Evidence, a Boundary-Object Lens, and a Design-Tensions Framework",
    "authors": [
      "Sehgal",
      "Kambhamettu",
      "Matam"
    ],
    "summary": "Mental health challenges among Indian adolescents are shaped by unique cultural and systemic barriers, including high social stigma and limited professional support. We report a mixed-methods study of Indian adolescents (survey n=362; interviews n=14) examining how they navigate mental-health challenges and engage with digital tools. Quantitative results highlight low self-stigma but significant social stigma, a preference for text over voice interactions, and low utilization of mental health apps but high smartphone access. Our qualitative findings reveal that while adolescents value privacy, emotional support, and localized content in mental health tools, existing chatbots lack personalization and cultural relevance. We contribute (1) a Design-Tensions framework; (2) an artifact-level probe; and (3) a boundary-objects account that specifies how chatbots mediate adolescents, peers, families, and services. This work advances culturally sensitive chatbot design by centering on underrepresented populations, addressing critical gaps in accessibility and support for adolescents in India.",
    "pdf_url": "https://arxiv.org/pdf/2511.07729v1",
    "github_url": null,
    "published": "2025-11-11T01:17:45+00:00",
    "updated": "2025-11-11T01:17:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07727v1",
    "title": "LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models",
    "authors": [
      "Zhang",
      "Ding",
      "Hayamizu"
    ],
    "summary": "Task planning and motion planning are two of the most important problems in robotics, where task planning methods help robots achieve high-level goals and motion planning methods maintain low-level feasibility. Task and motion planning (TAMP) methods interleave the two processes of task planning and motion planning to ensure goal achievement and motion feasibility. Within the TAMP context, we are concerned with the mobile manipulation (MoMa) of multiple objects, where it is necessary to interleave actions for navigation and manipulation.   In particular, we aim to compute where and how each object should be placed given underspecified goals, such as ``set up dinner table with a fork, knife and plate.'' We leverage the rich common sense knowledge from large language models (LLMs), e.g., about how tableware is organized, to facilitate both task-level and motion-level planning. In addition, we use computer vision methods to learn a strategy for selecting base positions to facilitate MoMa behaviors, where the base position corresponds to the robot's ``footprint'' and orientation in its operating space. Altogether, this article provides a principled TAMP framework for MoMa tasks that accounts for common sense about object rearrangement and is adaptive to novel situations that include many objects that need to be moved. We performed quantitative experiments in both real-world settings and simulated environments. We evaluated the success rate and efficiency in completing long-horizon object rearrangement tasks. While the robot completed 84.4\\% real-world object rearrangement trials, subjective human evaluations indicated that the robot's performance is still lower than experienced human waiters.",
    "pdf_url": "https://arxiv.org/pdf/2511.07727v1",
    "github_url": null,
    "published": "2025-11-11T01:11:20+00:00",
    "updated": "2025-11-11T01:11:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07710v3",
    "title": "Cross Modal Fine-Grained Alignment via Granularity-Aware and Region-Uncertain Modeling",
    "authors": [
      "Liu",
      "Zhou",
      "Liu"
    ],
    "summary": "Fine-grained image-text alignment is a pivotal challenge in multimodal learning, underpinning key applications such as visual question answering, image captioning, and vision-language navigation. Unlike global alignment, fine-grained alignment requires precise correspondence between localized visual regions and textual tokens, often hindered by noisy attention mechanisms and oversimplified modeling of cross-modal relationships. In this work, we identify two fundamental limitations of existing approaches: the lack of robust intra-modal mechanisms to assess the significance of visual and textual tokens, leading to poor generalization in complex scenes; and the absence of fine-grained uncertainty modeling, which fails to capture the one-to-many and many-to-one nature of region-word correspondences. To address these issues, we propose a unified approach that incorporates significance-aware and granularity-aware modeling and region-level uncertainty modeling. Our method leverages modality-specific biases to identify salient features without relying on brittle cross-modal attention, and represents region features as a mixture of Gaussian distributions to capture fine-grained uncertainty. Extensive experiments on Flickr30K and MS-COCO demonstrate that our approach achieves state-of-the-art performance across various backbone architectures, significantly enhancing the robustness and interpretability of fine-grained image-text alignment.",
    "pdf_url": "https://arxiv.org/pdf/2511.07710v3",
    "github_url": null,
    "published": "2025-11-11T00:28:11+00:00",
    "updated": "2025-11-29T03:35:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07687v1",
    "title": "Testing and Evaluation of Underwater Vehicle Using Hardware-In-The-Loop Simulation with HoloOcean",
    "authors": [
      "Meyers",
      "Mangelson"
    ],
    "summary": "Testing marine robotics systems in controlled environments before field tests is challenging, especially when acoustic-based sensors and control surfaces only function properly underwater. Deploying robots in indoor tanks and pools often faces space constraints that complicate testing of control, navigation, and perception algorithms at scale. Recent developments of high-fidelity underwater simulation tools have the potential to address these problems. We demonstrate the utility of the recently released HoloOcean 2.0 simulator with improved dynamics for torpedo AUV vehicles and a new ROS 2 interface. We have successfully demonstrated a Hardware-in-the-Loop (HIL) and Software-in-the-Loop (SIL) setup for testing and evaluating a CougUV torpedo autonomous underwater vehicle (AUV) that was built and developed in our lab. With this HIL and SIL setup, simulations are run in HoloOcean using a ROS 2 bridge such that simulated sensor data is sent to the CougUV (mimicking sensor drivers) and control surface commands are sent back to the simulation, where vehicle dynamics and sensor data are calculated. We compare our simulated results to real-world field trial results.",
    "pdf_url": "https://arxiv.org/pdf/2511.07687v1",
    "github_url": null,
    "published": "2025-11-10T23:15:37+00:00",
    "updated": "2025-11-10T23:15:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07670v1",
    "title": "Effective SU(2) gadget: holonomic walk on higher-order Poincaré sphere",
    "authors": [
      "Umar",
      "Bansal",
      "Senthilkumaran"
    ],
    "summary": "In a manner commensurate to the SU(2) gadget for the Poincaré sphere, which involves a combination of two quarter-wave plates and one half-wave plate regardless of their sequential order, an analogous construct for the higher-order Poincaré sphere had long remained elusive. To address this, we recently demonstrated, by modifying Euler-angle parameterization, that an optical gadget consisting of two quarter-wave $q$-plates and one half-wave $q$-plate, each endowed with the same topological charge $q$, operates as a viable SU(2) gadget for the higher-order Poincaré sphere, contingent upon the fulfillment of the holonomy condition. This work presents the controlled navigation on the higher-order Poincaré sphere through the proposed architecture, formulated under the concept of an effective waveplate and thus can be referred to as an effective SU(2) gadget. The notion of an effective waveplate refers to a coaxial arrangement of multiple waveplates that, under specific constraints, can act as a single waveplate. In this gadget, the relative alignment of the offset angles of the constituent $q$-plates emerges as the decisive parameter governing systematic navigation on the higher-order Poincaré sphere. This study bear direct relevance to the deterministic control and engineering of structured light, encompassing polarization singularities, vector vortex beams and topological optical fields. Moreover, these results holds potential applications in the contemporary research frontiers in singular optics, spin-orbit photonics and quantum communication.",
    "pdf_url": "https://arxiv.org/pdf/2511.07670v1",
    "github_url": null,
    "published": "2025-11-10T22:24:31+00:00",
    "updated": "2025-11-10T22:24:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07634v1",
    "title": "Accessibility, Safety, and Accommodation Burden in U.S. Higher Education Syllabi for Blind and Low-Vision Students",
    "authors": [
      "Acharya"
    ],
    "summary": "Course syllabi are often the first and sometimes only structured artifact that explains how a class will run: deadlines, grading rules, safety procedures, and how to request disability accommodations. For blind and low-vision (BLV) students who use screen readers, independent access depends on whether the syllabus is machine readable and navigable. We audited publicly posted syllabi and master syllabi from five U.S. institutions spanning an elite private R1 university, large public R1s (including a UC campus), a large community college, and a workforce focused technical college. We coded each document on five dimensions: (1) machine-readability of core logistics, (2) readability of safety critical procedures, (3) accommodation framing (rights based vs. burden based), (4) governance model (instructor-authored vs. centralized \"master syllabus\"), and (5) presence of proactive universal design language. Across the sample, logistics and many safety expectations are published as selectable text. Accommodation language, however, shifts by institution type: research universities more often use rights based wording (while still requiring advance letters), whereas community/technical colleges emphasize disclosure, documentation, and institutional discretion in master syllabi that replicate across sections. We argue that accessibility is not only a PDF tagging problem but also a question of governance and equity, and we outline implications for HCI, including an \"accessible master syllabus\" template as a high leverage intervention.",
    "pdf_url": "https://arxiv.org/pdf/2511.07634v1",
    "github_url": null,
    "published": "2025-11-10T21:06:41+00:00",
    "updated": "2025-11-10T21:06:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07392v3",
    "title": "Voice-Interactive Surgical Agent for Multimodal Patient Data Control",
    "authors": [
      "Park",
      "Gu",
      "Lee"
    ],
    "summary": "In robotic surgery, surgeons fully engage their hands and visual attention in procedures, making it difficult to access and manipulate multimodal patient data without interrupting the workflow. To overcome this problem, we propose a Voice-Interactive Surgical Agent (VISA) built on a hierarchical multi-agent framework consisting of an orchestration agent and three task-specific agents driven by Large Language Models (LLMs). These LLM-based agents autonomously plan, refine, validate, and reason to interpret voice commands and execute tasks such as retrieving clinical information, manipulating CT scans, or navigating 3D anatomical models within surgical video. We construct a dataset of 240 user commands organized into hierarchical categories and introduce the Multi-level Orchestration Evaluation Metric (MOEM) that evaluates the performance and robustness at both the command and category levels. Experimental results demonstrate that VISA achieves high stage-level accuracy and workflow-level success rates, while also enhancing its robustness by correcting transcription errors, resolving linguistic ambiguity, and interpreting diverse free-form expressions. These findings highlight the strong potential of VISA to support robotic surgery and its scalability for integrating new functions and agents.",
    "pdf_url": "https://arxiv.org/pdf/2511.07392v3",
    "github_url": null,
    "published": "2025-11-10T18:47:24+00:00",
    "updated": "2025-12-17T21:32:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07277v1",
    "title": "Designing Beyond Language: Sociotechnical Barriers in AI Health Technologies for Limited English Proficiency",
    "authors": [
      "Huang",
      "Rodriguez",
      "Saha"
    ],
    "summary": "Limited English proficiency (LEP) patients in the U.S. face systemic barriers to healthcare beyond language and interpreter access, encompassing procedural and institutional constraints. AI advances may support communication and care through on-demand translation and visit preparation, but also risk exacerbating existing inequalities. We conducted storyboard-driven interviews with 14 patient navigators to explore how AI could shape care experiences for Spanish-speaking LEP individuals. We identified tensions around linguistic and cultural misunderstandings, privacy concerns, and opportunities and risks for AI to augment care workflows. Participants highlighted structural factors that can undermine trust in AI systems, including sensitive information disclosure, unstable technology access, and low digital literacy. While AI tools can potentially alleviate social barriers and institutional constraints, there are risks of misinformation and uprooting human camaraderie. Our findings contribute design considerations for AI that support LEP patients and care teams via rapport-building, education, and language support, and minimizing disruptions to existing practices.",
    "pdf_url": "https://arxiv.org/pdf/2511.07277v1",
    "github_url": null,
    "published": "2025-11-10T16:23:06+00:00",
    "updated": "2025-11-10T16:23:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07178v1",
    "title": "Trajectory Design for UAV-Assisted Logistics Collection in Low-Altitude Economy",
    "authors": [
      "Zhai",
      "Gao",
      "Ni"
    ],
    "summary": "Low-altitude economy (LAE) is rapidly emerging as a key driver of innovation, encompassing economic activities taking place in airspace below 500 meters. Unmanned aerial vehicles (UAVs) provide valuable tools for logistics collection within LAE systems, offering the ability to navigate through complex environments, avoid obstacles, and improve operational efficiency. However, logistics collection tasks involve UAVs flying through complex three-dimensional (3D) environments while avoiding obstacles, where traditional UAV trajectory design methods,typically developed under free-space conditions without explicitly accounting for obstacles, are not applicable. This paper presents, we propose a novel algorithm that combines the Lin-Kernighan-Helsgaun (LKH) and Deep Deterministic Policy Gradient (DDPG) methods to minimize the total collection time. Specifically, the LKH algorithm determines the optimal order of item collection, while the DDPG algorithm designs the flight trajectory between collection points. Simulations demonstrate that the proposed LKH-DDPG algorithm significantly reduces collection time by approximately 49 percent compared to baseline approaches, thereby highlighting its effectiveness in optimizing UAV trajectories and enhancing operational efficiency for logistics collection tasks in the LAE paradigm.",
    "pdf_url": "https://arxiv.org/pdf/2511.07178v1",
    "github_url": null,
    "published": "2025-11-10T15:07:03+00:00",
    "updated": "2025-11-10T15:07:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07158v1",
    "title": "Guiding Generative Models to Uncover Diverse and Novel Crystals via Reinforcement Learning",
    "authors": [
      "Park",
      "Walsh"
    ],
    "summary": "Discovering functional crystalline materials entails navigating an immense combinatorial design space. While recent advances in generative artificial intelligence have enabled the sampling of chemically plausible compositions and structures, a fundamental challenge remains: the objective misalignment between likelihood-based sampling in generative modelling and targeted focus on underexplored regions where novel compounds reside. Here, we introduce a reinforcement learning framework that guides latent denoising diffusion models toward diverse and novel, yet thermodynamically viable crystalline compounds. Our approach integrates group relative policy optimisation with verifiable, multi-objective rewards that jointly balance creativity, stability, and diversity. Beyond de novo generation, we demonstrate enhanced property-guided design that preserves chemical validity, while targeting desired functional properties. This approach establishes a modular foundation for controllable AI-driven inverse design that addresses the novelty-validity trade-off across scientific discovery applications of generative models.",
    "pdf_url": "https://arxiv.org/pdf/2511.07158v1",
    "github_url": null,
    "published": "2025-11-10T14:48:49+00:00",
    "updated": "2025-11-10T14:48:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.07118v1",
    "title": "On the Joint Minimization of Regularization Loss Functions in Deep Variational Bayesian Methods for Attribute-Controlled Symbolic Music Generation",
    "authors": [
      "Pettenó",
      "Mezza",
      "Bernardini"
    ],
    "summary": "Explicit latent variable models provide a flexible yet powerful framework for data synthesis, enabling controlled manipulation of generative factors. With latent variables drawn from a tractable probability density function that can be further constrained, these models enable continuous and semantically rich exploration of the output space by navigating their latent spaces. Structured latent representations are typically obtained through the joint minimization of regularization loss functions. In variational information bottleneck models, reconstruction loss and Kullback-Leibler Divergence (KLD) are often linearly combined with an auxiliary Attribute-Regularization (AR) loss. However, balancing KLD and AR turns out to be a very delicate matter. When KLD dominates over AR, generative models tend to lack controllability; when AR dominates over KLD, the stochastic encoder is encouraged to violate the standard normal prior. We explore this trade-off in the context of symbolic music generation with explicit control over continuous musical attributes. We show that existing approaches struggle to jointly minimize both regularization objectives, whereas suitable attribute transformations can help achieve both controllability and regularization of the target latent dimensions.",
    "pdf_url": "https://arxiv.org/pdf/2511.07118v1",
    "github_url": null,
    "published": "2025-11-10T14:09:25+00:00",
    "updated": "2025-11-10T14:09:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06998v1",
    "title": "Raspi$^2$USBL: An open-source Raspberry Pi-Based Passive Inverted Ultra-Short Baseline Positioning System for Underwater Robotics",
    "authors": [
      "Huang",
      "Wang",
      "Chen"
    ],
    "summary": "Precise underwater positioning remains a fundamental challenge for underwater robotics since global navigation satellite system (GNSS) signals cannot penetrate the sea surface. This paper presents Raspi$^2$USBL, an open-source, Raspberry Pi-based passive inverted ultra-short baseline (piUSBL) positioning system designed to provide a low-cost and accessible solution for underwater robotic research. The system comprises a passive acoustic receiver and an active beacon. The receiver adopts a modular hardware architecture that integrates a hydrophone array, a multichannel preamplifier, an oven-controlled crystal oscillator (OCXO), a Raspberry Pi 5, and an MCC-series data acquisition (DAQ) board. Apart from the Pi 5, OCXO, and MCC board, the beacon comprises an impedance-matching network, a power amplifier, and a transmitting transducer. An open-source C++ software framework provides high-precision clock synchronization and triggering for one-way travel-time (OWTT) messaging, while performing real-time signal processing, including matched filtering, array beamforming, and adaptive gain control, to estimate the time of flight (TOF) and direction of arrival (DOA) of received signals. The Raspi$^2$USBL system was experimentally validated in an anechoic tank, freshwater lake, and open-sea trials. Results demonstrate a slant-range accuracy better than 0.1%, a bearing accuracy within 0.1$^\\circ$, and stable performance over operational distances up to 1.3 km. These findings confirm that low-cost, reproducible hardware can deliver research-grade underwater positioning accuracy. By releasing both the hardware and software as open-source, Raspi$^2$USBL provides a unified reference platform that lowers the entry barrier for underwater robotics laboratories, fosters reproducibility, and promotes collaborative innovation in underwater acoustic navigation and swarm robotics.",
    "pdf_url": "https://arxiv.org/pdf/2511.06998v1",
    "github_url": null,
    "published": "2025-11-10T11:51:22+00:00",
    "updated": "2025-11-10T11:51:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06990v1",
    "title": "Koopman-Based Dynamic Environment Prediction for Safe UAV Navigation",
    "authors": [
      "Bueno",
      "Azarbahram",
      "Farina"
    ],
    "summary": "This paper presents a Koopman-based model predictive control (MPC) framework for safe UAV navigation in dynamic environments using real-time LiDAR data. By leveraging the Koopman operator to linearly approximate the dynamics of surrounding objets, we enable efficient and accurate prediction of the position of moving obstacles. Embedding this into an MPC formulation ensures robust, collision-free trajectory planning suitable for real-time execution. The method is validated through simulation and ROS2-Gazebo implementation, demonstrating reliable performance under sensor noise, actuation delays, and environmental uncertainty.",
    "pdf_url": "https://arxiv.org/pdf/2511.06990v1",
    "github_url": null,
    "published": "2025-11-10T11:42:09+00:00",
    "updated": "2025-11-10T11:42:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06954v1",
    "title": "Personalizing Emotion-aware Conversational Agents? Exploring User Traits-driven Conversational Strategies for Enhanced Interaction",
    "authors": [
      "Zhang",
      "Ma",
      "Fu"
    ],
    "summary": "Conversational agents (CAs) are increasingly embedded in daily life, yet their ability to navigate user emotions efficiently is still evolving. This study investigates how users with varying traits -- gender, personality, and cultural background -- adapt their interaction strategies with emotion-aware CAs in specific emotional scenarios. Using an emotion-aware CA prototype expressing five distinct emotions (neutral, happy, sad, angry, and fear) through male and female voices, we examine how interaction dynamics shift across different voices and emotional contexts through empirical studies. Our findings reveal distinct variations in user engagement and conversational strategies based on individual traits, emphasizing the value of personalized, emotion-sensitive interactions. By analyzing both qualitative and quantitative data, we demonstrate that tailoring CAs to user characteristics can enhance user satisfaction and interaction quality. This work underscores the critical need for ongoing research to design CAs that not only recognize but also adaptively respond to emotional needs, ultimately supporting a diverse user groups more effectively.",
    "pdf_url": "https://arxiv.org/pdf/2511.06954v1",
    "github_url": null,
    "published": "2025-11-10T11:03:31+00:00",
    "updated": "2025-11-10T11:03:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06856v2",
    "title": "Contact Wasserstein Geodesics for Non-Conservative Schrödinger Bridges",
    "authors": [
      "Testa",
      "Hauberg",
      "Asfour"
    ],
    "summary": "The Schrödinger Bridge provides a principled framework for modeling stochastic processes between distributions; however, existing methods are limited by energy-conservation assumptions, which constrains the bridge's shape preventing it from model varying-energy phenomena. To overcome this, we introduce the non-conservative generalized Schrödinger bridge (NCGSB), a novel, energy-varying reformulation based on contact Hamiltonian mechanics. By allowing energy to change over time, the NCGSB provides a broader class of real-world stochastic processes, capturing richer and more faithful intermediate dynamics. By parameterizing the Wasserstein manifold, we lift the bridge problem to a tractable geodesic computation in a finite-dimensional space. Unlike computationally expensive iterative solutions, our contact Wasserstein geodesic (CWG) is naturally implemented via a ResNet architecture and relies on a non-iterative solver with near-linear complexity. Furthermore, CWG supports guided generation by modulating a task-specific distance metric. We validate our framework on tasks including manifold navigation, molecular dynamics predictions, and image generation, demonstrating its practical benefits and versatility.",
    "pdf_url": "https://arxiv.org/pdf/2511.06856v2",
    "github_url": null,
    "published": "2025-11-10T08:56:21+00:00",
    "updated": "2025-11-11T13:28:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06840v1",
    "title": "PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory",
    "authors": [
      "Jin",
      "Wu",
      "Chen"
    ],
    "summary": "Zero-shot object navigation (ZSON) in unseen environments remains a challenging problem for household robots, requiring strong perceptual understanding and decision-making capabilities. While recent methods leverage metric maps and Large Language Models (LLMs), they often depend on depth sensors or prebuilt maps, limiting the spatial reasoning ability of Multimodal Large Language Models (MLLMs). Mapless ZSON approaches have emerged to address this, but they typically make short-sighted decisions, leading to local deadlocks due to a lack of historical context. We propose PanoNav, a fully RGB-only, mapless ZSON framework that integrates a Panoramic Scene Parsing module to unlock the spatial parsing potential of MLLMs from panoramic RGB inputs, and a Memory-guided Decision-Making mechanism enhanced by a Dynamic Bounded Memory Queue to incorporate exploration history and avoid local deadlocks. Experiments on the public navigation benchmark show that PanoNav significantly outperforms representative baselines in both SR and SPL metrics.",
    "pdf_url": "https://arxiv.org/pdf/2511.06840v1",
    "github_url": null,
    "published": "2025-11-10T08:31:32+00:00",
    "updated": "2025-11-10T08:31:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06817v3",
    "title": "TiS-TSL: Image-Label Supervised Surgical Video Stereo Matching via Time-Switchable Teacher-Student Learning",
    "authors": [
      "Wang",
      "Zhou",
      "Wang"
    ],
    "summary": "Stereo matching in minimally invasive surgery (MIS) is essential for next-generation navigation and augmented reality. Yet, dense disparity supervision is nearly impossible due to anatomical constraints, typically limiting annotations to only a few image-level labels acquired before the endoscope enters deep body cavities. Teacher-Student Learning (TSL) offers a promising solution by leveraging a teacher trained on sparse labels to generate pseudo labels and associated confidence maps from abundant unlabeled surgical videos. However, existing TSL methods are confined to image-level supervision, providing only spatial confidence and lacking temporal consistency estimation. This absence of spatio-temporal reliability results in unstable disparity predictions and severe flickering artifacts across video frames. To overcome these challenges, we propose TiS-TSL, a novel time-switchable teacher-student learning framework for video stereo matching under minimal supervision. At its core is a unified model that operates in three distinct modes: Image-Prediction (IP), Forward Video-Prediction (FVP), and Backward Video-Prediction (BVP), enabling flexible temporal modeling within a single architecture. Enabled by this unified model, TiS-TSL adopts a two-stage learning strategy. The Image-to-Video (I2V) stage transfers sparse image-level knowledge to initialize temporal modeling. The subsequent Video-to-Video (V2V) stage refines temporal disparity predictions by comparing forward and backward predictions to calculate bidirectional spatio-temporal consistency. This consistency identifies unreliable regions across frames, filters noisy video-level pseudo labels, and enforces temporal coherence. Experimental results on two public datasets demonstrate that TiS-TSL exceeds other image-based state-of-the-arts by improving TEPE and EPE by at least 2.11% and 4.54%, respectively.",
    "pdf_url": "https://arxiv.org/pdf/2511.06817v3",
    "github_url": null,
    "published": "2025-11-10T08:01:26+00:00",
    "updated": "2025-11-12T02:36:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06801v1",
    "title": "Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots",
    "authors": [
      "Kumar",
      "Sandhan"
    ],
    "summary": "The deployment of autonomous service robots in human-centric environments is hindered by a critical gap in perception and planning. Traditional navigation systems rely on expensive LiDARs that, while geometrically precise, are semantically unaware, they cannot distinguish a important document on an office floor from a harmless piece of litter, treating both as physically traversable. While advanced semantic segmentation exists, no prior work has successfully integrated this visual intelligence into a real-time path planner that is efficient enough for low-cost, embedded hardware. This paper presents a framework to bridge this gap, delivering context-aware navigation on an affordable robotic platform. Our approach centers on a novel, tight integration of a lightweight perception module with an online A* planner. The perception system employs a semantic segmentation model to identify user-defined visual constraints, enabling the robot to navigate based on contextual importance rather than physical size alone. This adaptability allows an operator to define what is critical for a given task, be it sensitive papers in an office or safety lines in a factory, thus resolving the ambiguity of what to avoid. This semantic perception is seamlessly fused with geometric data. The identified visual constraints are projected as non-geometric obstacles onto a global map that is continuously updated from sensor data, enabling robust navigation through both partially known and unknown environments. We validate our framework through extensive experiments in high-fidelity simulations and on a real-world robotic platform. The results demonstrate robust, real-time performance, proving that a cost-effective robot can safely navigate complex environments while respecting critical visual cues invisible to traditional planners.",
    "pdf_url": "https://arxiv.org/pdf/2511.06801v1",
    "github_url": null,
    "published": "2025-11-10T07:44:22+00:00",
    "updated": "2025-11-10T07:44:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06791v1",
    "title": "Coupling Agent-based Modeling and Life Cycle Assessment to Analyze Trade-offs in Resilient Energy Transitions",
    "authors": [
      "Zhang",
      "Zaki",
      "Breunig"
    ],
    "summary": "Transitioning to sustainable and resilient energy systems requires navigating complex and interdependent trade-offs across environmental, social, and resource dimensions. Neglecting these trade-offs can lead to unintended consequences across sectors. However, existing assessments often evaluate emerging energy pathways and their impacts in silos, overlooking critical interactions such as regional resource competition and cumulative impacts. We present an integrated modeling framework that couples agent-based modeling and Life Cycle Assessment (LCA) to simulate how energy transition pathways interact with regional resource competition, ecological constraints, and community-level burdens. We apply the model to a case study in Southern California. The results demonstrate how integrated and multiscale decision making can shape energy pathway deployment and reveal spatially explicit trade-offs under scenario-driven constraints. This modeling framework can further support more adaptive and resilient energy transition planning on spatial and institutional scales.",
    "pdf_url": "https://arxiv.org/pdf/2511.06791v1",
    "github_url": null,
    "published": "2025-11-10T07:28:03+00:00",
    "updated": "2025-11-10T07:28:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06565v1",
    "title": "FPGA or GPU? Analyzing comparative research for application-specific guidance",
    "authors": [
      "Purkayastha",
      "Tharwani",
      "Aggarwal"
    ],
    "summary": "The growing complexity of computational workloads has amplified the need for efficient and specialized hardware accelerators. Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent solutions, each excelling in specific domains. Although there is substantial research comparing FPGAs and GPUs, most of the work focuses primarily on performance metrics, offering limited insight into the specific types of applications that each accelerator benefits the most. This paper aims to bridge this gap by synthesizing insights from various research articles to guide users in selecting the appropriate accelerator for domain-specific applications. By categorizing the reviewed studies and analyzing key performance metrics, this work highlights the strengths, limitations, and ideal use cases for FPGAs and GPUs. The findings offer actionable recommendations, helping researchers and practitioners navigate trade-offs in performance, energy efficiency, and programmability.",
    "pdf_url": "https://arxiv.org/pdf/2511.06565v1",
    "github_url": null,
    "published": "2025-11-09T22:54:28+00:00",
    "updated": "2025-11-09T22:54:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06365v1",
    "title": "V-Shuffle: Zero-Shot Style Transfer via Value Shuffle",
    "authors": [
      "Tang",
      "Lin",
      "Xu"
    ],
    "summary": "Attention injection-based style transfer has achieved remarkable progress in recent years. However, existing methods often suffer from content leakage, where the undesired semantic content of the style image mistakenly appears in the stylized output. In this paper, we propose V-Shuffle, a zero-shot style transfer method that leverages multiple style images from the same style domain to effectively navigate the trade-off between content preservation and style fidelity. V-Shuffle implicitly disrupts the semantic content of the style images by shuffling the value features within the self-attention layers of the diffusion model, thereby preserving low-level style representations. We further introduce a Hybrid Style Regularization that complements these low-level representations with high-level style textures to enhance style fidelity. Empirical results demonstrate that V-Shuffle achieves excellent performance when utilizing multiple style images. Moreover, when applied to a single style image, V-Shuffle outperforms previous state-of-the-art methods.",
    "pdf_url": "https://arxiv.org/pdf/2511.06365v1",
    "github_url": null,
    "published": "2025-11-09T13:07:23+00:00",
    "updated": "2025-11-09T13:07:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06337v1",
    "title": "BuildingWorld: A Structured 3D Building Dataset for Urban Foundation Models",
    "authors": [
      "Huang",
      "Wang",
      "Wang"
    ],
    "summary": "As digital twins become central to the transformation of modern cities, accurate and structured 3D building models emerge as a key enabler of high-fidelity, updatable urban representations. These models underpin diverse applications including energy modeling, urban planning, autonomous navigation, and real-time reasoning. Despite recent advances in 3D urban modeling, most learning-based models are trained on building datasets with limited architectural diversity, which significantly undermines their generalizability across heterogeneous urban environments. To address this limitation, we present BuildingWorld, a comprehensive and structured 3D building dataset designed to bridge the gap in stylistic diversity. It encompasses buildings from geographically and architecturally diverse regions -- including North America, Europe, Asia, Africa, and Oceania -- offering a globally representative dataset for urban-scale foundation modeling and analysis. Specifically, BuildingWorld provides about five million LOD2 building models collected from diverse sources, accompanied by real and simulated airborne LiDAR point clouds. This enables comprehensive research on 3D building reconstruction, detection and segmentation. Cyber City, a virtual city model, is introduced to enable the generation of unlimited training data with customized and structurally diverse point cloud distributions. Furthermore, we provide standardized evaluation metrics tailored for building reconstruction, aiming to facilitate the training, evaluation, and comparison of large-scale vision models and foundation models in structured 3D urban environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.06337v1",
    "github_url": null,
    "published": "2025-11-09T11:40:34+00:00",
    "updated": "2025-11-09T11:40:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06273v1",
    "title": "COTN: A Chaotic Oscillatory Transformer Network for Complex Volatile Systems under Extreme Conditions",
    "authors": [
      "Tang",
      "Zeng",
      "Ren"
    ],
    "summary": "Accurate prediction of financial and electricity markets, especially under extreme conditions, remains a significant challenge due to their intrinsic nonlinearity, rapid fluctuations, and chaotic patterns. To address these limitations, we propose the Chaotic Oscillatory Transformer Network (COTN). COTN innovatively combines a Transformer architecture with a novel Lee Oscillator activation function, processed through Max-over-Time pooling and a lambda-gating mechanism. This design is specifically tailored to effectively capture chaotic dynamics and improve responsiveness during periods of heightened volatility, where conventional activation functions (e.g., ReLU, GELU) tend to saturate. Furthermore, COTN incorporates an Autoencoder Self-Regressive (ASR) module to detect and isolate abnormal market patterns, such as sudden price spikes or crashes, thereby preventing corruption of the core prediction process and enhancing robustness. Extensive experiments across electricity spot markets and financial markets demonstrate the practical applicability and resilience of COTN. Our approach outperforms state-of-the-art deep learning models like Informer by up to 17% and traditional statistical methods like GARCH by as much as 40%. These results underscore COTN's effectiveness in navigating real-world market uncertainty and complexity, offering a powerful tool for forecasting highly volatile systems under duress.",
    "pdf_url": "https://arxiv.org/pdf/2511.06273v1",
    "github_url": null,
    "published": "2025-11-09T08:17:19+00:00",
    "updated": "2025-11-09T08:17:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06257v1",
    "title": "Fast Reconstruction of Motion-Corrupted Data with Mobile-GRAPPA: Motion and dB0 Inhomogeneity Correction Leveraging Efficient GRAPPA",
    "authors": [
      "Lin",
      "Wang",
      "Abraham"
    ],
    "summary": "Advanced motion navigations now enable rapid tracking of subject motion and dB0-induced phase, but accurately incorporating this high-temporal-resolution information into SENSE (Aligned-SENSE) is often computationally prohibitive. We propose \"Mobile-GRAPPA\", a k-space \"cleaning\" approach that uses local GRAPPA operators to remove motion and dB0 related corruption so that the resulting data can be reconstructed with standard SENSE. We efficiently train a family of k-space-position-specific Mobile-GRAPPA kernels via a lightweight multilayer perceptron (MLP) and apply them across k-space to generate clean data. In experiments on highly motion-corrupted 1-mm whole-brain GRE (Tacq = 10 min; 1,620 motion/dB0 trackings) and EPTI (Tacq = 2 min; 544 trackings), Mobile-GRAPPA enabled accurate reconstruction with negligible time penalty, whereas full Aligned-SENSE was impractical (reconstruction times > 10 h for GRE and > 10 days for EPTI). These results show that Mobile-GRAPPA incorporates detailed motion and dB0 tracking into SENSE with minimal computational overhead, enabling fast, high-quality reconstructions of challenging data.",
    "pdf_url": "https://arxiv.org/pdf/2511.06257v1",
    "github_url": null,
    "published": "2025-11-09T07:16:41+00:00",
    "updated": "2025-11-09T07:16:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06240v1",
    "title": "Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation",
    "authors": [
      "Lin",
      "Yeh",
      "Su"
    ],
    "summary": "In open-vocabulary mobile manipulation (OVMM), task success often hinges on the selection of an appropriate base placement for the robot. Existing approaches typically navigate to proximity-based regions without considering affordances, resulting in frequent manipulation failures. We propose Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base placement that integrates semantic understanding from vision-language models (VLMs) with geometric feasibility through an iterative optimization process. Our method constructs cross-modal representations, namely Affordance RGB and Obstacle Map+, to align semantics with spatial context. This enables reasoning that extends beyond the egocentric limitations of RGB perception. To ensure interaction is guided by task-relevant affordances, we leverage coarse semantic priors from VLMs to guide the search toward task-relevant regions and refine placements with geometric constraints, thereby reducing the risk of convergence to local optima. Evaluated on five diverse open-vocabulary mobile manipulation tasks, our system achieves an 85% success rate, significantly outperforming classical geometric planners and VLM-based methods. This demonstrates the promise of affordance-aware and multimodal reasoning for generalizable, instruction-conditioned planning in OVMM.",
    "pdf_url": "https://arxiv.org/pdf/2511.06240v1",
    "github_url": null,
    "published": "2025-11-09T05:52:22+00:00",
    "updated": "2025-11-09T05:52:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06182v2",
    "title": "OpenVLN: Open-world Aerial Vision-Language Navigation",
    "authors": [
      "Lin",
      "Sun",
      "Liu"
    ],
    "summary": "Vision-language models (VLMs) have been widely-applied in ground-based vision-language navigation (VLN). However, the vast complexity of outdoor aerial environments compounds data acquisition challenges and imposes long-horizon trajectory planning requirements on Unmanned Aerial Vehicles (UAVs), introducing novel complexities for aerial VLN. To address these challenges, we propose a data-efficient Open-world aerial Vision-Language Navigation (i.e., OpenVLN) framework, which could execute language-guided flight with limited data constraints and enhance long-horizon trajectory planning capabilities in complex aerial environments. Specifically, we reconfigure a reinforcement learning framework to optimize the VLM for UAV navigation tasks, which can efficiently fine-tune VLM by using rule-based policies under limited training data. Concurrently, we introduce a long-horizon planner for trajectory synthesis that dynamically generates precise UAV actions via value-based rewards. To the end, we conduct sufficient navigation experiments on the TravelUAV benchmark with dataset scaling across diverse reward settings. Our method demonstrates consistent performance gains of up to 4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success weighted by Path Length over baseline methods, validating its deployment efficacy for long-horizon UAV navigation in complex aerial environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.06182v2",
    "github_url": null,
    "published": "2025-11-09T01:50:53+00:00",
    "updated": "2025-11-21T00:57:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06152v1",
    "title": "Real-Time Bundle Adjustment for Ultra-High-Resolution UAV Imagery Using Adaptive Patch-Based Feature Tracking",
    "authors": [
      "Iz",
      "Nex",
      "Kerle"
    ],
    "summary": "Real-time processing of UAV imagery is crucial for applications requiring urgent geospatial information, such as disaster response, where rapid decision-making and accurate spatial data are essential. However, processing high-resolution imagery in real time presents significant challenges due to the computational demands of feature extraction, matching, and bundle adjustment (BA). Conventional BA methods either downsample images, sacrificing important details, or require extensive processing time, making them unsuitable for time-critical missions. To overcome these limitations, we propose a novel real-time BA framework that operates directly on fullresolution UAV imagery without downsampling. Our lightweight, onboard-compatible approach divides each image into user-defined patches (e.g., NxN grids, default 150x150 pixels) and dynamically tracks them across frames using UAV GNSS/IMU data and a coarse, globally available digital surface model (DSM). This ensures spatial consistency for robust feature extraction and matching between patches. Overlapping relationships between images are determined in real time using UAV navigation system, enabling the rapid selection of relevant neighbouring images for localized BA. By limiting optimization to a sliding cluster of overlapping images, including those from adjacent flight strips, the method achieves real-time performance while preserving the accuracy of global BA. The proposed algorithm is designed for seamless integration into the DLR Modular Aerial Camera System (MACS), supporting largearea mapping in real time for disaster response, infrastructure monitoring, and coastal protection. Validation on MACS datasets with 50MP images demonstrates that the method maintains precise camera orientations and high-fidelity mapping across multiple strips, running full bundle adjustment in under 2 seconds without GPU acceleration.",
    "pdf_url": "https://arxiv.org/pdf/2511.06152v1",
    "github_url": null,
    "published": "2025-11-08T22:09:32+00:00",
    "updated": "2025-11-08T22:09:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06134v1",
    "title": "Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs",
    "authors": [
      "Yang",
      "Pang",
      "Li"
    ],
    "summary": "Multi-agent systems (MAS) built on Large Language Models (LLMs) are being used to approach complex problems and can surpass single model inference. However, their success hinges on navigating a fundamental cognitive tension: the need to balance broad, divergent exploration of the solution space with a principled, convergent synthesis to the optimal solution. Existing paradigms often struggle to manage this duality, leading to premature consensus, error propagation, and a critical credit assignment problem that fails to distinguish between genuine reasoning and superficially plausible arguments. To resolve this core challenge, we propose the Multi-Agent Exploration-Synthesis framework Through Role Orchestration (Maestro), a principled paradigm for collaboration that structurally decouples these cognitive modes. Maestro uses a collective of parallel Execution Agents for diverse exploration and a specialized Central Agent for convergent, evaluative synthesis. To operationalize this critical synthesis phase, we introduce Conditional Listwise Policy Optimization (CLPO), a reinforcement learning objective that disentangles signals for strategic decisions and tactical rationales. By combining decision-focused policy gradients with a list-wise ranking loss over justifications, CLPO achieves clean credit assignment and stronger comparative supervision. Experiments on mathematical reasoning and general problem-solving benchmarks demonstrate that Maestro, coupled with CLPO, consistently outperforms existing state-of-the-art multi-agent approaches, delivering absolute accuracy gains of 6% on average and up to 10% at best.",
    "pdf_url": "https://arxiv.org/pdf/2511.06134v1",
    "github_url": null,
    "published": "2025-11-08T21:01:27+00:00",
    "updated": "2025-11-08T21:01:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06080v3",
    "title": "AIDEN: Design and Pilot Study of an AI Assistant for the Visually Impaired",
    "authors": [
      "Marquez-Carpintero",
      "Gomez-Donoso",
      "Bauer"
    ],
    "summary": "This paper presents AIDEN, an artificial intelligence-based assistant designed to enhance the autonomy and daily quality of life of visually impaired individuals, who often struggle with object identification, text reading, and navigation in unfamiliar environments. Existing solutions such as screen readers or audio-based assistants facilitate access to information but frequently lead to auditory overload and raise privacy concerns in open environments. AIDEN addresses these limitations with a hybrid architecture that integrates You Only Look Once (YOLO) for real-time object detection and a Large Language and Vision Assistant (LLaVA) for scene description and Optical Character Recognition (OCR). A key novelty of the system is a continuous haptic guidance mechanism based on a Geiger-counter metaphor, which supports object centering without occupying the auditory channel, while privacy is preserved by ensuring that no personal data are stored. Empirical evaluations with visually impaired participants assessed perceived ease of use and acceptance using the Technology Acceptance Model (TAM). Results indicate high user satisfaction, particularly regarding intuitiveness and perceived autonomy. Moreover, the ``Find an Object'' achieved effective real-time performance. These findings provide promising evidence that multimodal haptic-visual feedback can improve daily usability and independence compared to traditional audio-centric methods, motivating larger-scale clinical validations.",
    "pdf_url": "https://arxiv.org/pdf/2511.06080v3",
    "github_url": null,
    "published": "2025-11-08T17:23:51+00:00",
    "updated": "2025-12-02T16:23:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.06060v1",
    "title": "Positioning Using LEO Satellite Communication Signals Under Orbital Errors",
    "authors": [
      "Ma",
      "Zheng",
      "Liu"
    ],
    "summary": "Low Earth orbit (LEO) satellites offer a promising alternative to global navigation satellite systems for precise positioning; however, their relatively low altitudes make them more susceptible to orbital perturbations, which in turn degrade positioning accuracy. In this work, we study LEO-based positioning under orbital errors within a signal-of-opportunity framework. First, we introduce a LEO orbit model that accounts for Earth's non-sphericity and derive a wideband communication model that captures fast- and slow-time Doppler effects and multipath propagation. Subsequently, we perform a misspecified Cramér-Rao bound (MCRB) analysis to evaluate the impact of orbital errors on positioning performance. Then, we propose a two-stage positioning method starting with a (i) MCRB-based weighted orbit calibration, followed by (ii) least-squares user positioning using the corrected orbit. The MCRB analysis indicates that orbital errors can induce kilometer-level position biases. Extensive simulations show that the proposed estimator can considerably enhance the positioning accuracy relative to the orbit-mismatched baseline, yielding errors on the order of a few meters.",
    "pdf_url": "https://arxiv.org/pdf/2511.06060v1",
    "github_url": null,
    "published": "2025-11-08T16:03:20+00:00",
    "updated": "2025-11-08T16:03:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05933v1",
    "title": "Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs",
    "authors": [
      "Zhang",
      "Kaniselvan",
      "Mireshghallah"
    ],
    "summary": "Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement \"code 57.95 refers to urinary infection\") maintain high cosine similarity between SFT and RL models, query representations (e.g., \"what is code 57.95\") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.",
    "pdf_url": "https://arxiv.org/pdf/2511.05933v1",
    "github_url": null,
    "published": "2025-11-08T08:56:29+00:00",
    "updated": "2025-11-08T08:56:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05900v2",
    "title": "Disentangled Control of Multi-Agent Systems",
    "authors": [
      "Lin",
      "Notomista",
      "Egerstedt"
    ],
    "summary": "This paper develops a general framework for multi-agent control synthesis, which applies to a wide range of problems with convergence guarantees, regardless of the complexity of the underlying graph topology and the explicit time dependence of the objective function. The proposed framework systematically addresses a particularly challenging problem in multi-agent systems, i.e., decentralization of entangled dynamics among different agents, and it naturally supports multi-objective robotics and real-time implementations. To demonstrate its generality and effectiveness, the framework is implemented across three experiments, namely time-varying leader-follower formation control, decentralized coverage control for time-varying density functions without any approximations, which is a long-standing open problem, and safe formation navigation in dense environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.05900v2",
    "github_url": null,
    "published": "2025-11-08T07:50:53+00:00",
    "updated": "2025-11-25T06:39:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05889v1",
    "title": "From Words to Safety: Language-Conditioned Safety Filtering for Robot Navigation",
    "authors": [
      "Feng",
      "Zhang",
      "Bansal"
    ],
    "summary": "As robots become increasingly integrated into open-world, human-centered environments, their ability to interpret natural language instructions and adhere to safety constraints is critical for effective and trustworthy interaction. Existing approaches often focus on mapping language to reward functions instead of safety specifications or address only narrow constraint classes (e.g., obstacle avoidance), limiting their robustness and applicability. We propose a modular framework for language-conditioned safety in robot navigation. Our framework is composed of three core components: (1) a large language model (LLM)-based module that translates free-form instructions into structured safety specifications, (2) a perception module that grounds these specifications by maintaining object-level 3D representations of the environment, and (3) a model predictive control (MPC)-based safety filter that enforces both semantic and geometric constraints in real time. We evaluate the effectiveness of the proposed framework through both simulation studies and hardware experiments, demonstrating that it robustly interprets and enforces diverse language-specified constraints across a wide range of environments and scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2511.05889v1",
    "github_url": null,
    "published": "2025-11-08T07:07:11+00:00",
    "updated": "2025-11-08T07:07:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00030v1",
    "title": "Perturbation-mitigated USV Navigation with Distributionally Robust Reinforcement Learning",
    "authors": [
      "Zhang",
      "Yang",
      "Xie"
    ],
    "summary": "The robustness of Unmanned Surface Vehicles (USV) is crucial when facing unknown and complex marine environments, especially when heteroscedastic observational noise poses significant challenges to sensor-based navigation tasks. Recently, Distributional Reinforcement Learning (DistRL) has shown promising results in some challenging autonomous navigation tasks without prior environmental information. However, these methods overlook situations where noise patterns vary across different environmental conditions, hindering safe navigation and disrupting the learning of value functions. To address the problem, we propose DRIQN to integrate Distributionally Robust Optimization (DRO) with implicit quantile networks to optimize worst-case performance under natural environmental conditions. Leveraging explicit subgroup modeling in the replay buffer, DRIQN incorporates heterogeneous noise sources and target robustness-critical scenarios. Experimental results based on the risk-sensitive environment demonstrate that DRIQN significantly outperforms state-of-the-art methods, achieving +13.51\\% success rate, -12.28\\% collision rate and +35.46\\% for time saving, +27.99\\% for energy saving, compared with the runner-up.",
    "pdf_url": "https://arxiv.org/pdf/2512.00030v1",
    "github_url": null,
    "published": "2025-11-08T04:56:38+00:00",
    "updated": "2025-11-08T04:56:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05819v1",
    "title": "(Working Paper) Good Faith Design: Religion as a Resource for Technologists",
    "authors": [
      "Lutz",
      "Olsen",
      "Liu"
    ],
    "summary": "Previous work has found a lack of research in HCI on religion, partly driven by misunderstandings of values and practices between religious and technical communities. To bridge this divide in an empirically rigorous way, we conducted an interview study with 48 religious people and/or experts from 11 faiths, and we document how religious people experience, understand, and imagine technologies. We show that religious stakeholders find non-neutral secular embeddings in technologies and the firms and people that design them, and how these manifest in unintended harms for religious and nonreligious users. Our findings reveal how users navigate technoreligious practices with religiously informed mental models and what they desire from technologies. Informed by this, we distill six design values -- wonder, humility, space, embodiedness, community, and eternity -- to guide technologists in considering and leveraging religion as an additional, valid sociocultural resource when designing for a holistic user. We further spell out directions for future research.",
    "pdf_url": "https://arxiv.org/pdf/2511.05819v1",
    "github_url": null,
    "published": "2025-11-08T03:08:26+00:00",
    "updated": "2025-11-08T03:08:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05798v1",
    "title": "An Open-Source, Reproducible Tensegrity Robot that can Navigate Among Obstacles",
    "authors": [
      "Johnson",
      "Meng",
      "Chen"
    ],
    "summary": "Tensegrity robots, composed of rigid struts and elastic tendons, provide impact resistance, low mass, and adaptability to unstructured terrain. Their compliance and complex, coupled dynamics, however, present modeling and control challenges, hindering path planning and obstacle avoidance. This paper presents a complete, open-source, and reproducible system that enables navigation for a 3-bar tensegrity robot. The system comprises: (i) an inexpensive, open-source hardware design, and (ii) an integrated, open-source software stack for physics-based modeling, system identification, state estimation, path planning, and control. All hardware and software are publicly available at https://sites.google.com/view/tensegrity-navigation/. The proposed system tracks the robot's pose and executes collision-free paths to a specified goal among known obstacle locations. System robustness is demonstrated through experiments involving unmodeled environmental challenges, including a vertical drop, an incline, and granular media, culminating in an outdoor field demonstration. To validate reproducibility, experiments were conducted using robot instances at two different laboratories. This work provides the robotics community with a complete navigation system for a compliant, impact-resistant, and shape-morphing robot. This system is intended to serve as a springboard for advancing the navigation capabilities of other unconventional robotic platforms.",
    "pdf_url": "https://arxiv.org/pdf/2511.05798v1",
    "github_url": null,
    "published": "2025-11-08T02:07:07+00:00",
    "updated": "2025-11-08T02:07:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05297v1",
    "title": "Building Specialized Software-Assistant ChatBot with Graph-Based Retrieval-Augmented Generation",
    "authors": [
      "Hilel",
      "Karmim",
      "Bodinat"
    ],
    "summary": "Digital Adoption Platforms (DAPs) have become essential tools for helping employees navigate complex enterprise software such as CRM, ERP, or HRMS systems. Companies like LemonLearning have shown how digital guidance can reduce training costs and accelerate onboarding. However, building and maintaining these interactive guides still requires extensive manual effort. Leveraging Large Language Models as virtual assistants is an appealing alternative, yet without a structured understanding of the target software, LLMs often hallucinate and produce unreliable answers. Moreover, most production-grade LLMs are black-box APIs, making fine-tuning impractical due to the lack of access to model weights. In this work, we introduce a Graph-based Retrieval-Augmented Generation framework that automatically converts enterprise web applications into state-action knowledge graphs, enabling LLMs to generate grounded and context-aware assistance. The framework was co-developed with the AI enterprise RAKAM, in collaboration with Lemon Learning. We detail the engineering pipeline that extracts and structures software interfaces, the design of the graph-based retrieval process, and the integration of our approach into production DAP workflows. Finally, we discuss scalability, robustness, and deployment lessons learned from industrial use cases.",
    "pdf_url": "https://arxiv.org/pdf/2511.05297v1",
    "github_url": null,
    "published": "2025-11-07T14:56:45+00:00",
    "updated": "2025-11-07T14:56:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05253v1",
    "title": "Automatic segmentation of colorectal liver metastases for ultrasound-based navigated resection",
    "authors": [
      "Natali",
      "Olthof",
      "Kok"
    ],
    "summary": "Introduction: Accurate intraoperative delineation of colorectal liver metastases (CRLM) is crucial for achieving negative resection margins but remains challenging using intraoperative ultrasound (iUS) due to low contrast, noise, and operator dependency. Automated segmentation could enhance precision and efficiency in ultrasound-based navigation workflows.   Methods: Eighty-five tracked 3D iUS volumes from 85 CRLM patients were used to train and evaluate a 3D U-Net implemented via the nnU-Net framework. Two variants were compared: one trained on full iUS volumes and another on cropped regions around tumors. Segmentation accuracy was assessed using Dice Similarity Coefficient (DSC), Hausdorff Distance (HDist.), and Relative Volume Difference (RVD) on retrospective and prospective datasets. The workflow was integrated into 3D Slicer for real-time intraoperative use.   Results: The cropped-volume model significantly outperformed the full-volume model across all metrics (AUC-ROC = 0.898 vs 0.718). It achieved median DSC = 0.74, recall = 0.79, and HDist. = 17.1 mm comparable to semi-automatic segmentation but with ~4x faster execution (~ 1 min). Prospective intraoperative testing confirmed robust and consistent performance, with clinically acceptable accuracy for real-time surgical guidance.   Conclusion: Automatic 3D segmentation of CRLM in iUS using a cropped 3D U-Net provides reliable, near real-time results with minimal operator input. The method enables efficient, registration-free ultrasound-based navigation for hepatic surgery, approaching expert-level accuracy while substantially reducing manual workload and procedure time.",
    "pdf_url": "https://arxiv.org/pdf/2511.05253v1",
    "github_url": null,
    "published": "2025-11-07T14:13:31+00:00",
    "updated": "2025-11-07T14:13:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05158v1",
    "title": "Follow-Me in Micro-Mobility with End-to-End Imitation Learning",
    "authors": [
      "Salimpour",
      "Catalano",
      "Westerlund"
    ],
    "summary": "Autonomous micro-mobility platforms face challenges from the perspective of the typical deployment environment: large indoor spaces or urban areas that are potentially crowded and highly dynamic. While social navigation algorithms have progressed significantly, optimizing user comfort and overall user experience over other typical metrics in robotics (e.g., time or distance traveled) is understudied. Specifically, these metrics are critical in commercial applications. In this paper, we show how imitation learning delivers smoother and overall better controllers, versus previously used manually-tuned controllers. We demonstrate how DAAV's autonomous wheelchair achieves state-of-the-art comfort in follow-me mode, in which it follows a human operator assisting persons with reduced mobility (PRM). This paper analyzes different neural network architectures for end-to-end control and demonstrates their usability in real-world production-level deployments.",
    "pdf_url": "https://arxiv.org/pdf/2511.05158v1",
    "github_url": null,
    "published": "2025-11-07T11:24:20+00:00",
    "updated": "2025-11-07T11:24:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05100v1",
    "title": "TRICK: Time and Range Integrity ChecK using Low Earth Orbiting Satellite for Securing GNSS",
    "authors": [
      "Mumtaz",
      "Singh"
    ],
    "summary": "Global Navigation Satellite Systems (GNSS) provide Positioning, Navigation, and Timing (PNT) information to over 4 billion devices worldwide. Despite its pervasive use in safety critical and high precision applications, GNSS remains vulnerable to spoofing attacks. Cryptographic enhancements, such as the use of TESLA protocol in Galileo, to provide navigation message authentication do not mitigate time of arrival manipulations. In this paper, we propose TRICK, a primitive for secure positioning that closes this gap by introducing a fundamentally new approach that only requires two way communications with a single reference node along with multiple broadcast signals. Unlike classical Verifiable Multilateration (VM), which requires establishing two way communication with each reference nodes, our solution relies on only two measurements with a trusted Low Earth Orbiting (LEO) satellite and combines broadcast navigation signals. We rigorously prove that combining the LEO satellite based two way range measurements and multiple one way ranges such as from broadcast signals of GNSS into ellipsoidal constraint restores the same guarantees as offered by VM whilst using minimal infrastructure and message exchanges. Through detailed analysis, we show that our approach reliably detects spoofing attempts while adding negligible computation overhead.",
    "pdf_url": "https://arxiv.org/pdf/2511.05100v1",
    "github_url": null,
    "published": "2025-11-07T09:30:18+00:00",
    "updated": "2025-11-07T09:30:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05080v2",
    "title": "An Architectural Advantage of The Instruction-Tuned LLM in Containing The Readability-Accuracy Tension in Text Simplification",
    "authors": [
      "Githinji",
      "Meilliou",
      "Liang"
    ],
    "summary": "The increasing health-seeking behavior and digital consumption of biomedical information by the general public necessitate scalable solutions for automatically adapting complex scientific and technical documents into plain language. Automatic text simplification solutions, including advanced large language models (LLMs), however, continue to face challenges in reliably arbitrating the tension between optimizing readability performance and ensuring preservation of discourse fidelity. This report empirically assesses two major classes of general-purpose LLMs, demonstrating how they navigate the readability-accuracy tension compared to a human benchmark. Using a comparative analysis of the instruction-tuned Mistral-Small 3 24B and the reasoning-augmented QWen2.5 32B, we identify an architectural advantage in the instruction-tuned LLM. Mistral exhibits a tempered lexical simplification strategy that enhances readability across a suite of metrics while preserving human-level discourse with a BERTScore of 0.91. QWen also attains enhanced readability performance and a reasonable BERTScore of 0.89, but its operational strategy shows a disconnect in balancing between readability and accuracy. Additionally, a comprehensive correlation analysis of a suite of 21 metrics spanning readability, discourse fidelity, content safety, and underlying distributional measures for mechanistic insights, confirms strong functional redundancies, and informs metric selection and domain adaptation for text simplification.",
    "pdf_url": "https://arxiv.org/pdf/2511.05080v2",
    "github_url": null,
    "published": "2025-11-07T08:53:39+00:00",
    "updated": "2025-11-21T23:48:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05052v1",
    "title": "TAPOM: Task-Space Topology-Guided Motion Planning for Manipulating Elongated Object in Cluttered Environments",
    "authors": [
      "Li",
      "Zhu",
      "Zhong"
    ],
    "summary": "Robotic manipulation in complex, constrained spaces is vital for widespread applications but challenging, particularly when navigating narrow passages with elongated objects. Existing planning methods often fail in these low-clearance scenarios due to the sampling difficulties or the local minima. This work proposes Topology-Aware Planning for Object Manipulation (TAPOM), which explicitly incorporates task-space topological analysis to enable efficient planning. TAPOM uses a high-level analysis to identify critical pathways and generate guiding keyframes, which are utilized in a low-level planner to find feasible configuration space trajectories. Experimental validation demonstrates significantly high success rates and improved efficiency over state-of-the-art methods on low-clearance manipulation tasks. This approach offers broad implications for enhancing manipulation capabilities of robots in complex real-world environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.05052v1",
    "github_url": null,
    "published": "2025-11-07T07:49:54+00:00",
    "updated": "2025-11-07T07:49:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.04886v1",
    "title": "Beta Distribution Learning for Reliable Roadway Crash Risk Assessment",
    "authors": [
      "Elallaf",
      "Jacobs",
      "Ye"
    ],
    "summary": "Roadway traffic accidents represent a global health crisis, responsible for over a million deaths annually and costing many countries up to 3% of their GDP. Traditional traffic safety studies often examine risk factors in isolation, overlooking the spatial complexity and contextual interactions inherent in the built environment. Furthermore, conventional Neural Network-based risk estimators typically generate point estimates without conveying model uncertainty, limiting their utility in critical decision-making. To address these shortcomings, we introduce a novel geospatial deep learning framework that leverages satellite imagery as a comprehensive spatial input. This approach enables the model to capture the nuanced spatial patterns and embedded environmental risk factors that contribute to fatal crash risks. Rather than producing a single deterministic output, our model estimates a full Beta probability distribution over fatal crash risk, yielding accurate and uncertainty-aware predictions--a critical feature for trustworthy AI in safety-critical applications. Our model outperforms baselines by achieving a 17-23% improvement in recall, a key metric for flagging potential dangers, while delivering superior calibration. By providing reliable and interpretable risk assessments from satellite imagery alone, our method enables safer autonomous navigation and offers a highly scalable tool for urban planners and policymakers to enhance roadway safety equitably and cost-effectively.",
    "pdf_url": "https://arxiv.org/pdf/2511.04886v1",
    "github_url": null,
    "published": "2025-11-07T00:08:55+00:00",
    "updated": "2025-11-07T00:08:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.04847v2",
    "title": "Grounded Test-Time Adaptation for LLM Agents",
    "authors": [
      "Chen",
      "Liu",
      "Zhang"
    ],
    "summary": "Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.",
    "pdf_url": "https://arxiv.org/pdf/2511.04847v2",
    "github_url": null,
    "published": "2025-11-06T22:24:35+00:00",
    "updated": "2025-12-03T05:19:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.04730v1",
    "title": "SGNL: Scalable Low-Latency Gravitational Wave Detection Pipeline for Compact Binary Mergers",
    "authors": [
      "Huang",
      "Hanna",
      "Tsukada"
    ],
    "summary": "We present SGNL, a scalable, low-latency gravitational-wave search pipeline. It reimplements the core matched-filtering principles of the GstLAL pipeline within a modernized framework. The Streaming Graph Navigator library, a lightweight Python streaming framework, replaces GstLAL's GStreamer infrastructure, simplifying pipeline construction and enabling flexible, modular graph design. The filtering core is reimplemented in PyTorch, allowing SGNL to leverage GPU acceleration for improved computational scalability. We describe the pipeline architecture and introduce a novel implementation of the Low-Latency Online Inspiral Detection algorithm in which components are pre-synchronized to reduce latency. Results from a 40-day Mock Data Challenge show that SGNL's event recovery and sensitivity are consistent with GstLAL's within statistical and systematic uncertainties. Notably, SGNL achieves a median latency of 5.4 seconds, a 42\\% reduction compared to GstLAL's 9.3 seconds.",
    "pdf_url": "https://arxiv.org/pdf/2511.04730v1",
    "github_url": null,
    "published": "2025-11-06T18:20:54+00:00",
    "updated": "2025-11-06T18:20:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.04481v1",
    "title": "Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis",
    "authors": [
      "Krupp",
      "Geißler",
      "Banwari"
    ],
    "summary": "Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful agentic systems pushing the boundaries of Large Language Models (LLM). They can autonomously interact with the internet at the user's behest, such as navigating websites, filling search masks, and comparing price lists. Though web agent research is thriving, induced sustainability issues remain largely unexplored. To highlight the urgency of this issue, we provide an initial exploration of the energy and $CO_2$ cost associated with web agents from both a theoretical -via estimation- and an empirical perspective -by benchmarking. Our results show how different philosophies in web agent creation can severely impact the associated expended energy, and that more energy consumed does not necessarily equate to better results. We highlight a lack of transparency regarding disclosing model parameters and processes used for some web agents as a limiting factor when estimating energy consumption. Our work contributes towards a change in thinking of how we evaluate web agents, advocating for dedicated metrics measuring energy consumption in benchmarks.",
    "pdf_url": "https://arxiv.org/pdf/2511.04481v1",
    "github_url": null,
    "published": "2025-11-06T15:59:59+00:00",
    "updated": "2025-11-06T15:59:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.04347v1",
    "title": "Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection",
    "authors": [
      "Kumar",
      "Brophy",
      "Grua"
    ],
    "summary": "Accurate 3D object detection is essential for automated vehicles to navigate safely in complex real-world environments. Bird's Eye View (BEV) representations, which project multi-sensor data into a top-down spatial format, have emerged as a powerful approach for robust perception. Although BEV-based fusion architectures have demonstrated strong performance through multimodal integration, the effects of sensor occlusions, caused by environmental conditions such as fog, haze, or physical obstructions, on 3D detection accuracy remain underexplored. In this work, we investigate the impact of occlusions on both camera and Light Detection and Ranging (LiDAR) outputs using the BEVFusion architecture, evaluated on the nuScenes dataset. Detection performance is measured using mean Average Precision (mAP) and the nuScenes Detection Score (NDS). Our results show that moderate camera occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is based only on the camera. On the other hand, LiDAR sharply drops in performance only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%), with a severe impact on long-range detection. In fused settings, the effect depends on which sensor is occluded: occluding the camera leads to a minor 4.1% drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8% drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task of 3D object detection. Our results highlight the need for future research into occlusion-aware evaluation methods and improved sensor fusion techniques that can maintain detection accuracy in the presence of partial sensor failure or degradation due to adverse environmental conditions.",
    "pdf_url": "https://arxiv.org/pdf/2511.04347v1",
    "github_url": null,
    "published": "2025-11-06T13:25:18+00:00",
    "updated": "2025-11-06T13:25:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.04320v1",
    "title": "MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments",
    "authors": [
      "Sima",
      "Tang",
      "Ma"
    ],
    "summary": "Autonomous navigation in unknown environments requires compact yet expressive spatial understanding under partial observability to support high-level decision making. Existing approaches struggle to balance rich contextual representation with navigation efficiency. We present MacroNav, a learning-based navigation framework featuring two key components: (1) a lightweight context encoder trained via multi-task self-supervised learning to capture multi-scale, navigation-centric spatial representations; and (2) a reinforcement learning policy that seamlessly integrates these representations with graph-based reasoning for efficient action selection. Extensive experiments demonstrate the context encoder's efficient and robust environmental understanding. Real-world deployments further validate MacroNav's effectiveness, yielding significant gains over state-of-the-art navigation methods in both Success Rate (SR) and Success weighted by Path Length (SPL), while maintaining low computational cost. Code will be released upon acceptance.",
    "pdf_url": "https://arxiv.org/pdf/2511.04320v1",
    "github_url": null,
    "published": "2025-11-06T12:47:33+00:00",
    "updated": "2025-11-06T12:47:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.04262v1",
    "title": "Vitessce Link: A Mixed Reality and 2D Display Hybrid Approach for Visual Analysis of 3D Tissue Maps",
    "authors": [
      "Mörth",
      "Turner",
      "Nielsen"
    ],
    "summary": "Advances in spatial omics and high-resolution imaging enable the creation of three-dimensional (3D) tissue maps that capture cellular organization and interactions in situ. While these data provide critical insights into tissue function and disease, their exploration is often constrained by tools limited to 2D displays or stereoscopic rendering without analytical integration. We present Vitessce Link, a web-based hybrid framework that unites a 3D stereoscopic view in mixed reality with a synchronized 2D display environment. Users can navigate volumetric data with intuitive hand gestures while controlling channels, filters, and derived data views through the Vitessce platform. Built on open standards and running entirely in the browser, Vitessce Link minimizes friction, supports integration with computational notebooks, and synchronizes interactions across devices via a lightweight WebSocket architecture. Case studies in nephrology and oncology demonstrate how the hybrid approach enhances segmentation evaluation, distance measurement, and interpretation of spatial relationships. Vitessce Link establishes a paradigm for integrative, web-native analysis of 3D tissue maps.",
    "pdf_url": "https://arxiv.org/pdf/2511.04262v1",
    "github_url": null,
    "published": "2025-11-06T10:51:28+00:00",
    "updated": "2025-11-06T10:51:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.04221v1",
    "title": "Coordination-Free Lane Partitioning for Convergent ANN Search",
    "authors": [
      "Kugblenu",
      "Vuorimaa"
    ],
    "summary": "Production vector search systems often fan out each query across parallel lanes (threads, replicas, or shards) to meet latency service-level objectives (SLOs). In practice, these lanes rediscover the same candidates, so extra compute does not increase coverage. We present a coordination-free lane partitioner that turns duplication into complementary work at the same cost and deadline. For each query we (1) build a deterministic candidate pool sized to the total top-k budget, (2) apply a per-query pseudorandom permutation, and (3) assign each lane a disjoint slice of positions. Lanes then return different results by construction, with no runtime coordination.   At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT feature vectors) with Hierarchical Navigable Small World graphs (HNSW) recall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100% to 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to 0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted file (IVF) indexes we see smaller but consistent gains (for example, +11% on MS MARCO) by de-duplicating list routing. A microbenchmark shows planner overhead of ~37 microseconds per query (mean at the main setting) with linear growth in the number of merged candidates.   These results yield a simple operational guideline: size the per-query pool to the total budget, deterministically partition positions across lanes, and turn redundant fan-out into complementary coverage without changing budget or deadline.",
    "pdf_url": "https://arxiv.org/pdf/2511.04221v1",
    "github_url": null,
    "published": "2025-11-06T09:36:18+00:00",
    "updated": "2025-11-06T09:36:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.04156v1",
    "title": "Deep reinforcement learning based navigation of a jellyfish-like swimmer in flows with obstacles",
    "authors": [
      "Chen",
      "Yang"
    ],
    "summary": "We develop a deep reinforcement learning framework for controlling a bio-inspired jellyfish swimmer to navigate complex fluid environments with obstacles. While existing methods often rely on kinematic and geometric states, a key challenge remains in achieving efficient obstacle avoidance under strong fluid-structure interactions and near-wall effects. We augment the agent's state representation within a soft actor-critic algorithm to include the real-time forces and torque experienced by the swimmer, providing direct mechanical feedback from vortex-wall interactions. This augmented state space enables the swimmer to perceive and interpret wall proximity and orientation through distinct hydrodynamic force signatures. We analyze how these force and torque patterns, generated by walls at different positions influence the swimmer's decision-making policy. Comparative experiments with a baseline model without force feedback demonstrate that the present one with force feedback achieves higher navigation efficiency in two-dimensional obstacle-avoidance tasks. The results show that explicit force feedback facilitates earlier, smoother maneuvers and enables the exploitation of wall effects for efficient turning behaviors. With an application to autonomous cave mapping, this work underscores the critical role of direct mechanical feedback in fluid environments and presents a physics-aware machine learning framework for advancing robust underwater exploration systems.",
    "pdf_url": "https://arxiv.org/pdf/2511.04156v1",
    "github_url": null,
    "published": "2025-11-06T08:01:37+00:00",
    "updated": "2025-11-06T08:01:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00027v1",
    "title": "A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation",
    "authors": [
      "Yakolli",
      "Gautam",
      "Das"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is a multi-modal, cooperative task requiring agents to interpret human instructions, navigate 3D environments, and communicate effectively under ambiguity. This paper presents a comprehensive review of recent VLN advancements in robotics and outlines promising directions to improve multi-robot coordination. Despite progress, current models struggle with bidirectional communication, ambiguity resolution, and collaborative decision-making in the multi-agent systems. We review approximately 200 relevant articles to provide an in-depth understanding of the current landscape. Through this survey, we aim to provide a thorough resource that inspires further research at the intersection of VLN and robotics. We advocate that the future VLN systems should support proactive clarification, real-time feedback, and contextual reasoning through advanced natural language understanding (NLU) techniques. Additionally, decentralized decision-making frameworks with dynamic role assignment are essential for scalable, efficient multi-robot collaboration. These innovations can significantly enhance human-robot interaction (HRI) and enable real-world deployment in domains such as healthcare, logistics, and disaster response.",
    "pdf_url": "https://arxiv.org/pdf/2512.00027v1",
    "github_url": null,
    "published": "2025-11-06T07:52:56+00:00",
    "updated": "2025-11-06T07:52:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.04128v2",
    "title": "DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms",
    "authors": [
      "Tang",
      "Lu",
      "Dong"
    ],
    "summary": "Accurate perception of the marine environment through robust multi-object tracking (MOT) is essential for ensuring safe vessel navigation and effective maritime surveillance. However, the complicated maritime environment often causes camera motion and subsequent visual degradation, posing significant challenges to MOT. To address this challenge, we propose an efficient Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the framework is a parallel tracker with affine compensation, which incorporates an object detection and re-identification (ReID) branch, along with a dedicated branch for dynamic camera motion estimation. Specifically, a Reversible Columnar Detection Network (RCDN) is integrated into the detection module to leverage multi-level visual features for robust object detection. Furthermore, a lightweight Transformer-based appearance extractor (Li-TAE) is designed to capture global contextual information and generate robust appearance features. Another branch decouples platform-induced and target-intrinsic motion by constructing a projective transformation, applying platform-motion compensation within the Kalman filter, and thereby stabilizing true object trajectories. Finally, a clustering-optimized feature fusion module effectively combines motion and appearance cues to ensure identity consistency under noise, occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT attains the fastest runtime among existing ReID-based MOT frameworks while maintaining high identity consistency and robustness to jitter and occlusion. Code is available at: https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.",
    "pdf_url": "https://arxiv.org/pdf/2511.04128v2",
    "github_url": "https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-",
    "published": "2025-11-06T07:20:36+00:00",
    "updated": "2025-11-16T01:10:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.04052v1",
    "title": "Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors",
    "authors": [
      "Yun",
      "Bayard",
      "Kubiak"
    ],
    "summary": "Future planetary exploration missions demand high-performance, fault-tolerant computing to enable autonomous Guidance, Navigation, and Control (GNC) and Lander Vision System (LVS) operations during Entry, Descent, and Landing (EDL). This paper evaluates the deployment of GNC and LVS algorithms on next-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx Versal--demonstrating up to 15x speedup for LVS image processing and over 250x speedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory optimization compared to legacy spaceflight hardware. To ensure computational reliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for Trusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that performs real-time fault detection and correction across redundant cores. ARBITER is validated in both static optimization tasks (GFOLD) and dynamic closed-loop control (Attitude Control System). A fault injection study further identifies the gradient computation stage in GFOLD as the most sensitive to bit-level errors, motivating selective protection strategies and vector-based output arbitration. This work establishes a scalable and energy-efficient architecture for future missions, including Mars Sample Return, Enceladus Orbilander, and Ceres Sample Return, where onboard autonomy, low latency, and fault resilience are critical.",
    "pdf_url": "https://arxiv.org/pdf/2511.04052v1",
    "github_url": null,
    "published": "2025-11-06T04:45:44+00:00",
    "updated": "2025-11-06T04:45:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03928v2",
    "title": "SynQuE: Estimating Synthetic Dataset Quality Without Annotations",
    "authors": [
      "Chen",
      "Zhong"
    ],
    "summary": "We introduce and formalize the Synthetic Dataset Quality Estimation (SynQuE) problem: ranking synthetic datasets by their expected real-world task performance using only limited unannotated real data. This addresses a critical and open challenge where data is scarce due to collection costs or privacy constraints. We establish the first comprehensive benchmarks for this problem by introducing and evaluating proxy metrics that choose synthetic data for training to maximize task performance on real data. We introduce the first proxy metrics for SynQuE by adapting distribution and diversity-based distance measures to our context via embedding models. To address the shortcomings of these metrics on complex planning tasks, we propose LENS, a novel proxy that leverages large language model reasoning. Our results show that SynQuE proxies correlate with real task performance across diverse tasks, including sentiment analysis, Text2SQL, web navigation, and image classification, with LENS consistently outperforming others on complex tasks by capturing nuanced characteristics. For instance, on text-to-SQL parsing, training on the top-3 synthetic datasets selected via SynQuE proxies can raise accuracy from 30.4% to 38.4 (+8.1)% on average compared to selecting data indiscriminately. This work establishes SynQuE as a practical framework for synthetic data selection under real-data scarcity and motivates future research on foundation model-based data characterization and fine-grained data selection.",
    "pdf_url": "https://arxiv.org/pdf/2511.03928v2",
    "github_url": null,
    "published": "2025-11-06T00:09:33+00:00",
    "updated": "2025-12-04T02:23:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03882v1",
    "title": "Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures",
    "authors": [
      "Klitzner",
      "Inigo",
      "Killeen"
    ],
    "summary": "Imitation learning-based robot control policies are enjoying renewed interest in video-based robotics. However, it remains unclear whether this approach applies to X-ray-guided procedures, such as spine instrumentation. This is because interpretation of multi-view X-rays is complex. We examine opportunities and challenges for imitation policy learning in bi-plane-guided cannula insertion. We develop an in silico sandbox for scalable, automated simulation of X-ray-guided spine procedures with a high degree of realism. We curate a dataset of correct trajectories and corresponding bi-planar X-ray sequences that emulate the stepwise alignment of providers. We then train imitation learning policies for planning and open-loop control that iteratively align a cannula solely based on visual information. This precisely controlled setup offers insights into limitations and capabilities of this method. Our policy succeeded on the first attempt in 68.5% of cases, maintaining safe intra-pedicular trajectories across diverse vertebral levels. The policy generalized to complex anatomy, including fractures, and remained robust to varied initializations. Rollouts on real bi-planar X-rays further suggest that the model can produce plausible trajectories, despite training exclusively in simulation. While these preliminary results are promising, we also identify limitations, especially in entry point precision. Full closed-look control will require additional considerations around how to provide sufficiently frequent feedback. With more robust priors and domain knowledge, such models may provide a foundation for future efforts toward lightweight and CT-free robotic intra-operative spinal navigation.",
    "pdf_url": "https://arxiv.org/pdf/2511.03882v1",
    "github_url": null,
    "published": "2025-11-05T22:00:48+00:00",
    "updated": "2025-11-05T22:00:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03859v1",
    "title": "Levers of Power in the Field of AI",
    "authors": [
      "Mackenzie",
      "Punj",
      "Perez"
    ],
    "summary": "This paper examines how decision makers in academia, government, business, and civil society navigate questions of power in implementations of artificial intelligence. The study explores how individuals experience and exercise levers of power, which are presented as social mechanisms that shape institutional responses to technological change. The study reports on the responses of personalized questionnaires designed to gather insight on a decision maker's institutional purview, based on an institutional governance framework developed from the work of Neo-institutionalists. Findings present the anonymized, real responses and circumstances of respondents in the form of twelve fictional personas of high-level decision makers from North America and Europe. These personas illustrate how personal agency, organizational logics, and institutional infrastructures may intersect in the governance of AI. The decision makers' responses to the questionnaires then inform a discussion of the field-level personal power of decision makers, methods of fostering institutional stability in times of change, and methods of influencing institutional change in the field of AI. The final section of the discussion presents a table of the dynamics of the levers of power in the field of AI for change makers and five testable hypotheses for institutional and social movement researchers. In summary, this study provides insight on the means for policymakers within institutions and their counterparts in civil society to personally engage with AI governance.",
    "pdf_url": "https://arxiv.org/pdf/2511.03859v1",
    "github_url": null,
    "published": "2025-11-05T21:03:57+00:00",
    "updated": "2025-11-05T21:03:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03651v1",
    "title": "Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural",
    "authors": [
      "Korigodskii",
      "Kalachev",
      "Vasiunik"
    ],
    "summary": "This paper presents the innovative design and successful deployment of a pioneering autonomous unmanned aerial system developed for executing the world's largest mural painted by a drone. Addressing the dual challenges of maintaining artistic precision and operational reliability under adverse outdoor conditions such as wind and direct sunlight, our work introduces a robust system capable of navigating and painting outdoors with unprecedented accuracy. Key to our approach is a novel navigation system that combines an infrared (IR) motion capture camera and LiDAR technology, enabling precise location tracking tailored specifically for largescale artistic applications. We employ a unique control architecture that uses different regulation in tangential and normal directions relative to the planned path, enabling precise trajectory tracking and stable line rendering. We also present algorithms for trajectory planning and path optimization, allowing for complex curve drawing and area filling. The system includes a custom-designed paint spraying mechanism, specifically engineered to function effectively amidst the turbulent airflow generated by the drone's propellers, which also protects the drone's critical components from paint-related damage, ensuring longevity and consistent performance. Experimental results demonstrate the system's robustness and precision in varied conditions, showcasing its potential for autonomous large-scale art creation and expanding the functional applications of robotics in creative fields.",
    "pdf_url": "https://arxiv.org/pdf/2511.03651v1",
    "github_url": null,
    "published": "2025-11-05T17:09:16+00:00",
    "updated": "2025-11-05T17:09:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03591v1",
    "title": "Manifold-constrained Hamilton-Jacobi Reachability Learning for Decentralized Multi-Agent Motion Planning",
    "authors": [
      "Chen",
      "Ni",
      "Kim"
    ],
    "summary": "Safe multi-agent motion planning (MAMP) under task-induced constraints is a critical challenge in robotics. Many real-world scenarios require robots to navigate dynamic environments while adhering to manifold constraints imposed by tasks. For example, service robots must carry cups upright while avoiding collisions with humans or other robots. Despite recent advances in decentralized MAMP for high-dimensional systems, incorporating manifold constraints remains difficult. To address this, we propose a manifold-constrained Hamilton-Jacobi reachability (HJR) learning framework for decentralized MAMP. Our method solves HJR problems under manifold constraints to capture task-aware safety conditions, which are then integrated into a decentralized trajectory optimization planner. This enables robots to generate motion plans that are both safe and task-feasible without requiring assumptions about other agents' policies. Our approach generalizes across diverse manifold-constrained tasks and scales effectively to high-dimensional multi-agent manipulation problems. Experiments show that our method outperforms existing constrained motion planners and operates at speeds suitable for real-world applications. Video demonstrations are available at https://youtu.be/RYcEHMnPTH8 .",
    "pdf_url": "https://arxiv.org/pdf/2511.03591v1",
    "github_url": null,
    "published": "2025-11-05T16:11:12+00:00",
    "updated": "2025-11-05T16:11:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03576v2",
    "title": "Multi-User Personalisation in Human-Robot Interaction: Resolving Preference Conflicts Using Gradual Argumentation",
    "authors": [
      "Civit",
      "Andriella",
      "Sierra"
    ],
    "summary": "While personalisation in Human-Robot Interaction (HRI) has advanced significantly, most existing approaches focus on single-user adaptation, overlooking scenarios involving multiple stakeholders with potentially conflicting preferences. To address this, we propose the Multi-User Preferences Quantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user personalisation framework based on Quantitative Bipolar Argumentation Frameworks (QBAFs) that explicitly models and resolves multi-user preference conflicts. Unlike prior work in Argumentation Frameworks, which typically assumes static inputs, our approach is tailored to robotics: it incorporates both users' arguments and the robot's dynamic observations of the environment, allowing the system to adapt over time and respond to changing contexts. Preferences, both positive and negative, are represented as arguments whose strength is recalculated iteratively based on new information. The framework's properties and capabilities are presented and validated through a realistic case study, where an assistive robot mediates between the conflicting preferences of a caregiver and a care recipient during a frailty assessment task. This evaluation further includes a sensitivity analysis of argument base scores, demonstrating how preference outcomes can be shaped by user input and contextual observations. By offering a transparent, structured, and context-sensitive approach to resolving competing user preferences, this work advances the field of multi-user HRI. It provides a principled alternative to data-driven methods, enabling robots to navigate conflicts in real-world environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.03576v2",
    "github_url": null,
    "published": "2025-11-05T15:59:30+00:00",
    "updated": "2025-12-02T10:59:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03763v1",
    "title": "Dexterous Intramyocardial Needle Ablation (d-INA): Design, Fabrication, and In-Vivo Validation",
    "authors": [
      "Zhou",
      "Hong",
      "Wang"
    ],
    "summary": "Radiofrequency ablation is widely used to prevent ventricular tachycardia (VT) by creating lesions to inhibit arrhythmias; however, the current surface ablation catheters are limited in creating lesions that are deeper within the left ventricle (LV) wall. Intramyocardial needle ablation (INA) addresses this limitation by penetrating the myocardium and delivering energy from within. Yet, existing INA catheters lack adequate dexterity to navigate the highly asymmetric, trabeculated LV chamber and steer around papillary structures, limiting precise targeting. This work presents a novel dexterous INA (d-INA) toolset designed to enable effective manipulation and creation of deep ablation lesions. The system consists of an outer sheath and an inner catheter, both bidirectionally steerable, along with an integrated ablation needle assembly. Benchtop tests demonstrated that the sheath and catheter reached maximum bending curvatures of 0.088~mm$^{-1}$ and 0.114~mm$^{-1}$, respectively, and achieved stable C-, S-, and non-planar S-shaped configurations. Ex-vivo studies validated the system's stiffness modulation and lesion-creation capabilities. In-vivo experiments in two swine demonstrated the device's ability to reach previously challenging regions such as the LV summit, and achieved a 219\\% increase in ablation depth compared with a standard ablation catheter. These results establish the proposed d-INA as a promising platform for achieving deep ablation with enhanced dexterity, advancing VT treatment.",
    "pdf_url": "https://arxiv.org/pdf/2511.03763v1",
    "github_url": null,
    "published": "2025-11-05T13:45:21+00:00",
    "updated": "2025-11-05T13:45:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03375v1",
    "title": "I Prompt, it Generates, we Negotiate. Exploring Text-Image Intertextuality in Human-AI Co-Creation of Visual Narratives with VLMs",
    "authors": [
      "Guo",
      "Nie",
      "Gao"
    ],
    "summary": "Creating meaningful visual narratives through human-AI collaboration requires understanding how text-image intertextuality emerges when textual intentions meet AI-generated visuals. We conducted a three-phase qualitative study with 15 participants using GPT-4o to investigate how novices navigate sequential visual narratives. Our findings show that users develop strategies to harness AI's semantic surplus by recognizing meaningful visual content beyond literal descriptions, iteratively refining prompts, and constructing narrative significance through complementary text-image relationships. We identified four distinct collaboration patterns and, through fsQCA's analysis, discovered three pathways to successful intertextual collaboration: Educational Collaborator, Technical Expert, and Visual Thinker. However, participants faced challenges, including cultural representation gaps, visual consistency issues, and difficulties translating narrative concepts into visual prompts. These findings contribute to HCI research by providing an empirical account of \\textit{text-image intertextuality} in human-AI co-creation and proposing design implications for role-based AI assistants that better support iterative, human-led creative processes in visual storytelling.",
    "pdf_url": "https://arxiv.org/pdf/2511.03375v1",
    "github_url": null,
    "published": "2025-11-05T11:30:58+00:00",
    "updated": "2025-11-05T11:30:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03324v2",
    "title": "Isolated quantum-state networks in ultracold molecules",
    "authors": [
      "Hepworth",
      "Cornish",
      "Gregory"
    ],
    "summary": "Precise control over rotational angular momentum is at the heart of recent advances in quantum chemistry, quantum simulation, and quantum computation with ultracold bialkali molecules. Each rotational state comprises a rich manifold of hyperfine states arising from combinations of rotation and nuclear spins; this often yields hundreds of transitions available between a given pair of rotational states, and the efficient navigation of this complex space is a current challenge for experiments. Here, we describe a general approach based on a simple heuristic and graph theory to quickly identify optimal sets of states in ultracold bialkali molecules. We explain how to find pathways through the many available transitions to prepare the molecule in a specific state with maximum speed for any desired fidelity. We then examine networks of states where multiple couplings are present at the same time. As example applications, we first identify a closed loop of four states in the RbCs molecule where there is minimal population leakage out of the loop during simultaneous microwave coupling; we then extend the optimisation procedure to account for decoherence induced by magnetic-field noise and obtain an optimal set of 3 states for quantum computation applications.",
    "pdf_url": "https://arxiv.org/pdf/2511.03324v2",
    "github_url": null,
    "published": "2025-11-05T09:38:05+00:00",
    "updated": "2025-11-09T19:57:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03173v1",
    "title": "Optimizing Earth-Moon Transfer and Cislunar Navigation: Integrating Low-Energy Trajectories, AI Techniques and GNSS-R Technologies",
    "authors": [
      "Muhammad",
      "Ahmed",
      "Ojonugwa"
    ],
    "summary": "The rapid growth of cislunar activities, including lunar landings, the Lunar Gateway, and in-space refueling stations, requires advances in cost-efficient trajectory design and reliable integration of navigation and remote sensing. Traditional Earth-Moon transfers suffer from rigid launch windows and high propellant demands, while Earth-based GNSS systems provide little to no coverage beyond geostationary orbit. This limits autonomy and environmental awareness in cislunar space. This review compares four major transfer strategies by evaluating velocity requirements, flight durations, and fuel efficiency, and by identifying their suitability for both crewed and robotic missions. The emerging role of artificial intelligence and machine learning is highlighted: convolutional neural networks support automated crater recognition and digital terrain model generation, while deep reinforcement learning enables adaptive trajectory refinement during descent and landing to reduce risk and decision latency. The study also examines how GNSS-Reflectometry and advanced Positioning, Navigation, and Timing architectures can extend navigation capabilities beyond current limits. GNSS-R can act as a bistatic radar for mapping lunar ice, soil properties, and surface topography, while PNT systems support autonomous rendezvous, Lagrange point station-keeping, and coordinated satellite swarm operations. Combining these developments establishes a scalable framework for sustainable cislunar exploration and long-term human and robotic presence.",
    "pdf_url": "https://arxiv.org/pdf/2511.03173v1",
    "github_url": null,
    "published": "2025-11-05T04:41:43+00:00",
    "updated": "2025-11-05T04:41:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03167v1",
    "title": "Learning Natural and Robust Hexapod Locomotion over Complex Terrains via Motion Priors based on Deep Reinforcement Learning",
    "authors": [
      "Liu",
      "Wu",
      "Li"
    ],
    "summary": "Multi-legged robots offer enhanced stability to navigate complex terrains with their multiple legs interacting with the environment. However, how to effectively coordinate the multiple legs in a larger action exploration space to generate natural and robust movements is a key issue. In this paper, we introduce a motion prior-based approach, successfully applying deep reinforcement learning algorithms to a real hexapod robot. We generate a dataset of optimized motion priors, and train an adversarial discriminator based on the priors to guide the hexapod robot to learn natural gaits. The learned policy is then successfully transferred to a real hexapod robot, and demonstrate natural gait patterns and remarkable robustness without visual information in complex terrains. This is the first time that a reinforcement learning controller has been used to achieve complex terrain walking on a real hexapod robot.",
    "pdf_url": "https://arxiv.org/pdf/2511.03167v1",
    "github_url": null,
    "published": "2025-11-05T04:32:07+00:00",
    "updated": "2025-11-05T04:32:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03165v1",
    "title": "SENT Map -- Semantically Enhanced Topological Maps with Foundation Models",
    "authors": [
      "Kathirvel",
      "Chavis",
      "Guy"
    ],
    "summary": "We introduce SENT-Map, a semantically enhanced topological map for representing indoor environments, designed to support autonomous navigation and manipulation by leveraging advancements in foundational models (FMs). Through representing the environment in a JSON text format, we enable semantic information to be added and edited in a format that both humans and FMs understand, while grounding the robot to existing nodes during planning to avoid infeasible states during deployment. Our proposed framework employs a two stage approach, first mapping the environment alongside an operator with a Vision-FM, then using the SENT-Map representation alongside a natural-language query within an FM for planning. Our experimental results show that semantic-enhancement enables even small locally-deployable FMs to successfully plan over indoor environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.03165v1",
    "github_url": null,
    "published": "2025-11-05T04:22:04+00:00",
    "updated": "2025-11-05T04:22:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.03032v1",
    "title": "Leveraging Discrete Function Decomposability for Scientific Design",
    "authors": [
      "Bowden",
      "Levine",
      "Listgarten"
    ],
    "summary": "In the era of AI-driven science and engineering, we often want to design discrete objects in silico according to user-specified properties. For example, we may wish to design a protein to bind its target, arrange components within a circuit to minimize latency, or find materials with certain properties. Given a property predictive model, in silico design typically involves training a generative model over the design space (e.g., protein sequence space) to concentrate on designs with the desired properties. Distributional optimization -- which can be formalized as an estimation of distribution algorithm or as reinforcement learning policy optimization -- finds the generative model that maximizes an objective function in expectation. Optimizing a distribution over discrete-valued designs is in general challenging because of the combinatorial nature of the design space. However, many property predictors in scientific applications are decomposable in the sense that they can be factorized over design variables in a way that could in principle enable more effective optimization. For example, amino acids at a catalytic site of a protein may only loosely interact with amino acids of the rest of the protein to achieve maximal catalytic activity. Current distributional optimization algorithms are unable to make use of such decomposability structure. Herein, we propose and demonstrate use of a new distributional optimization algorithm, Decomposition-Aware Distributional Optimization (DADO), that can leverage any decomposability defined by a junction tree on the design variables, to make optimization more efficient. At its core, DADO employs a soft-factorized \"search distribution\" -- a learned generative model -- for efficient navigation of the search space, invoking graph message-passing to coordinate optimization across linked factors.",
    "pdf_url": "https://arxiv.org/pdf/2511.03032v1",
    "github_url": null,
    "published": "2025-11-04T21:57:51+00:00",
    "updated": "2025-11-04T21:57:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.02979v1",
    "title": "Systematizing LLM Persona Design: A Four-Quadrant Technical Taxonomy for AI Companion Applications",
    "authors": [
      "Sun",
      "Wu"
    ],
    "summary": "The design and application of LLM-based personas in AI companionship is a rapidly expanding but fragmented field, spanning from virtual emotional companions and game NPCs to embodied functional robots. This diversity in objectives, modality, and technical stacks creates an urgent need for a unified framework. To address this gap, this paper systematizes the field by proposing a Four-Quadrant Technical Taxonomy for AI companion applications. The framework is structured along two critical axes: Virtual vs. Embodied and Emotional Companionship vs. Functional Augmentation. Quadrant I (Virtual Companionship) explores virtual idols, romantic companions, and story characters, introducing a four-layer technical framework to analyze their challenges in maintaining long-term emotional consistency. Quadrant II (Functional Virtual Assistants) analyzes AI applications in work, gaming, and mental health, highlighting the shift from \"feeling\" to \"thinking and acting\" and pinpointing key technologies like enterprise RAG and on-device inference. Quadrants III & IV (Embodied Intelligence) shift from the virtual to the physical world, analyzing home robots and vertical-domain assistants, revealing core challenges in symbol grounding, data privacy, and ethical liability. This taxonomy provides not only a systematic map for researchers and developers to navigate the complex persona design space but also a basis for policymakers to identify and address the unique risks inherent in different application scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2511.02979v1",
    "github_url": null,
    "published": "2025-11-04T20:37:13+00:00",
    "updated": "2025-11-04T20:37:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.02709v1",
    "title": "Lunar Time",
    "authors": [
      "Defraigne",
      "Meynadier",
      "Bourgoin"
    ],
    "summary": "The regain of interest in Moon exploration has substantially grown in the last years. For this reason, the space agencies consider the development of a precise navigation and positioning service similar to the Earth GNSS. Aiming at some meter accuracy, this requires to set up a relativistic lunar reference frame, with an associated coordinate time. If the IAU already defined the Lunar Coordinate Time TCL, there is still some freedom in the choice of the coordinate timescale to be adopted as reference on or around the Moon. This paper proposes a trade-off analysis of different possible options for this reference time scale. It shows that TCL is the best option to be used as practical time reference on the Moon, without the need to define a new time scale based on a scaling of TCL.",
    "pdf_url": "https://arxiv.org/pdf/2511.02709v1",
    "github_url": null,
    "published": "2025-11-04T16:27:01+00:00",
    "updated": "2025-11-04T16:27:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.02690v1",
    "title": "Curriculum Design for Trajectory-Constrained Agent: Compressing Chain-of-Thought Tokens in LLMs",
    "authors": [
      "Tzannetos",
      "Kamalaruban",
      "Singla"
    ],
    "summary": "Training agents to operate under strict constraints during deployment, such as limited resource budgets or stringent safety requirements, presents significant challenges, especially when these constraints render the task complex. In this work, we propose a curriculum learning strategy that gradually tightens constraints during training, enabling the agent to incrementally master the deployment requirements. Inspired by self-paced learning techniques in unconstrained reinforcement learning (RL), our approach facilitates a smoother transition to challenging environments by initially training on simplified versions of the constraints and progressively introducing the full deployment conditions. We provide a theoretical analysis using an RL agent in a binary-tree Markov Decision Process (MDP) to demonstrate that our curriculum strategy can accelerate training relative to a baseline approach that imposes the trajectory constraints from the outset. Moreover, we empirically validate the effectiveness and generality of our method across both RL and large language model (LLM) agents in diverse settings, including a binary-tree MDP, a multi-task navigation domain, and a math reasoning task with two benchmarks. These results highlight the potential of curriculum design in enhancing the efficiency and performance of agents operating under complex trajectory constraints during deployment. Moreover, when applied to LLMs, our strategy enables compression of output chain-of-thought tokens, achieving a substantial inference speedup on consumer hardware, demonstrating its effectiveness for resource-constrained deployment.",
    "pdf_url": "https://arxiv.org/pdf/2511.02690v1",
    "github_url": null,
    "published": "2025-11-04T16:14:56+00:00",
    "updated": "2025-11-04T16:14:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.02526v1",
    "title": "Many-vs-Many Missile Guidance via Virtual Targets",
    "authors": [
      "Schneider",
      "Fichter"
    ],
    "summary": "This paper presents a novel approach to many-vs-many missile guidance using virtual targets (VTs) generated by a Normalizing Flows-based trajectory predictor. Rather than assigning n interceptors directly to m physical targets through conventional weapon target assignment algorithms, we propose a centralized strategy that constructs n VT trajectories representing probabilistic predictions of maneuvering target behavior. Each interceptor is guided toward its assigned VT using Zero-Effort-Miss guidance during midcourse flight, transitioning to Proportional Navigation guidance for terminal interception. This approach treats many-vs-many engagements as many-vs-distribution scenarios, exploiting numerical superiority (n > m) by distributing interceptors across diverse trajectory hypotheses rather than pursuing identical deterministic predictions. Monte Carlo simulations across various target-interceptor configurations (1-6 targets, 1-8 interceptors) demonstrate that the VT method matches or exceeds baseline straight-line prediction performance by 0-4.1% when n = m, with improvements increasing to 5.8-14.4% when n > m. The results confirm that probabilistic VTs enable effective exploitation of numerical superiority, significantly increasing interception probability in many-vs-many scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2511.02526v1",
    "github_url": null,
    "published": "2025-11-04T12:24:56+00:00",
    "updated": "2025-11-04T12:24:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.02471v1",
    "title": "Vorticity-induced surfing and trapping in porous media",
    "authors": [
      "Das",
      "Residori",
      "Voigt"
    ],
    "summary": "Microorganisms often encounter strong confinement and complex hydrodynamic flows while navigating their habitats. Combining finite-element methods and stochastic simulations, we study the interplay of active transport and heterogeneous flows in dense porous channels. We find that swimming always slows down the traversal of agents across the channel, giving rise to robust power-law tails of their exit-time distributions. These exit-time distributions collapse onto a universal master curve with a scaling exponent of $\\approx 3/2$ across a wide range of packing fractions and motility parameters, which can be rationalized by a scaling relation. We further identify a new motility pattern where agents alternate between surfing along fast streams and extended trapping phases, the latter determining the power-law exponent. Unexpectedly, trapping occurs in the flow backbone itself -- not only at obstacle boundaries -- due to vorticity-induced reorientation in the highly-heterogeneous fluid environment. These findings provide a fundamentally new active transport mechanism with direct implications for biofilm clogging and the design of novel microrobots capable of operating in heterogeneous media.",
    "pdf_url": "https://arxiv.org/pdf/2511.02471v1",
    "github_url": null,
    "published": "2025-11-04T10:58:54+00:00",
    "updated": "2025-11-04T10:58:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.02445v1",
    "title": "When Continuous Delivery Is Not an Option: Practical Paths to Continuous Engineering in Complex Organizations",
    "authors": [
      "Klotins",
      "Ahlgren",
      "Vivaldi"
    ],
    "summary": "Purpose: Continuous Software Engineering (CSE) promises improved efficiency, quality, and responsiveness in software-intensive organizations. However, fully adopting CSE is often constrained by complex products, legacy systems, organizational inertia, and regulatory requirements. In this paper, we examine four industrial cases from the automation, automotive, retail, and chemical sectors to explore how such constraints shape CSE adoption in practice. Methods: We apply and extend a previously proposed CSE Industry Readiness Model to assess the current and potential levels of adoption in each case. Through expert interviews and narrative synthesis, we identify common driving forces and adoption barriers, including organizational preparedness, cross-organizational dependencies, and limited customer demand for continuous delivery. Results: Based on our findings, we propose an updated readiness model that introduces additional levels of internal and external feedback, distinguishes market- and organization-facing constraints, and better guides practitioners in setting realistic CSE adoption goals. Conclusions: Our results highlight that while full end-to-end CSE adoption may not always be feasible, meaningful internal improvements are still possible and beneficial. This study provides empirically grounded guidance for organizations navigating partial or constrained CSE transformations.",
    "pdf_url": "https://arxiv.org/pdf/2511.02445v1",
    "github_url": null,
    "published": "2025-11-04T10:18:35+00:00",
    "updated": "2025-11-04T10:18:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.02441v2",
    "title": "On the supra-linear storage in dense networks of grid and place cells",
    "authors": [
      "Barra",
      "Centonze",
      "Solazzo"
    ],
    "summary": "Place-cell networks, typically forced to pairwise synaptic interactions, are widely studied as models of cognitive maps: such models, however, share a severely limited storage capacity, scaling linearly with network size and with a very small critical storage. This limitation is a challenge for navigation in 3-dimensional space because, oversimplifying, if encoding motion along a one-dimensional trajectory embedded in 2-dimensions requires $O(K)$ patterns (interpreted as bins), extending this to a 2-dimensional manifold embedded in a 3-dimensional space -- yet preserving the same resolution -- requires roughly $O(K^2)$ patterns, namely a supra-linear amount of patterns. In these regards, dense Hebbian architectures, where higher-order neural assemblies mediate memory retrieval, display much larger capacities and are increasingly recognized as biologically plausible, but have never linked to place cells so far. Here we propose a minimal two-layer model, with place cells building a layer and leaving the other layer populated by neural units that account for the internal representations (so to qualitatively resemble grid cells in the MEC of mammals): crucially, by assuming that each place cell interacts with pairs of grid cells, we show how such a model is formally equivalent to a dense Battaglia-Treves-like Hebbian network of grid cells only endowed with four-body interactions. By studying its emergent computational properties by means of statistical mechanics of disordered systems, we prove -- analytically -- that such effective higher-order assemblies (constructed under the guise of biological plausibility) can support supra-linear storage of continuous attractors; furthermore, we prove -- numerically -- that the present neural network is capable of recognition and navigation on general surfaces embedded in a 3-dimensional space.",
    "pdf_url": "https://arxiv.org/pdf/2511.02441v2",
    "github_url": null,
    "published": "2025-11-04T10:15:38+00:00",
    "updated": "2025-11-21T10:28:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.02233v1",
    "title": "Learning Spatial Awareness for Laparoscopic Surgery with AI Assisted Visual Feedback",
    "authors": [
      "Liu",
      "Tan",
      "Li"
    ],
    "summary": "Laparoscopic surgery constrains surgeons spatial awareness because procedures are performed through a monocular, two-dimensional (2D) endoscopic view. Conventional training methods using dry-lab models or recorded videos provide limited depth cues, often leading trainees to misjudge instrument position and perform ineffective or unsafe maneuvers. To address this limitation, we present an AI-assisted training framework developed in NVIDIA Isaac Sim that couples the standard 2D laparoscopic feed with synchronized three-dimensional (3D) visual feedback delivered through a mixed-reality (MR) interface. While trainees operate using the clinical 2D view, validated AI modules continuously localize surgical instruments and detect instrument-tissue interactions in the background. When spatial misjudgments are detected, 3D visual feedback are displayed to trainees, while preserving the original operative perspective. Our framework considers various surgical tasks including navigation, manipulation, transfer, cutting, and suturing. Visually similar 2D cases can be disambiguated through the added 3D context, improving depth perception, contact awareness, and tool orientation understanding.",
    "pdf_url": "https://arxiv.org/pdf/2511.02233v1",
    "github_url": null,
    "published": "2025-11-04T03:53:55+00:00",
    "updated": "2025-11-04T03:53:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01797v1",
    "title": "Hybrid Neural Network-Based Indoor Localisation System for Mobile Robots Using CSI Data in a Robotics Simulator",
    "authors": [
      "Ballesteros-Jerez",
      "Martínez-Gómez",
      "García-Varea"
    ],
    "summary": "We present a hybrid neural network model for inferring the position of mobile robots using Channel State Information (CSI) data from a Massive MIMO system. By leveraging an existing CSI dataset, our approach integrates a Convolutional Neural Network (CNN) with a Multilayer Perceptron (MLP) to form a Hybrid Neural Network (HyNN) that estimates 2D robot positions. CSI readings are converted into synthetic images using the TINTO tool. The localisation solution is integrated with a robotics simulator, and the Robot Operating System (ROS), which facilitates its evaluation through heterogeneous test cases, and the adoption of state estimators like Kalman filters. Our contributions illustrate the potential of our HyNN model in achieving precise indoor localisation and navigation for mobile robots in complex environments. The study follows, and proposes, a generalisable procedure applicable beyond the specific use case studied, making it adaptable to different scenarios and datasets.",
    "pdf_url": "https://arxiv.org/pdf/2511.01797v1",
    "github_url": null,
    "published": "2025-11-03T17:57:18+00:00",
    "updated": "2025-11-03T17:57:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01775v1",
    "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment",
    "authors": [
      "Chen",
      "Xu",
      "Wu"
    ],
    "summary": "Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.",
    "pdf_url": "https://arxiv.org/pdf/2511.01775v1",
    "github_url": null,
    "published": "2025-11-03T17:28:54+00:00",
    "updated": "2025-11-03T17:28:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01736v1",
    "title": "Cobble: Compiling Block Encodings for Quantum Computational Linear Algebra",
    "authors": [
      "Yuan"
    ],
    "summary": "Quantum algorithms for computational linear algebra promise up to exponential speedups for applications such as simulation and regression, making them prime candidates for hardware realization. But these algorithms execute in a model that cannot efficiently store matrices in memory like a classical algorithm does, instead requiring developers to implement complex expressions for matrix arithmetic in terms of correct and efficient quantum circuits. Among the challenges for the developer is navigating a cost model in which conventional optimizations for linear algebra, such as subexpression reuse, can be inapplicable or unprofitable.   In this work, we present Cobble, a language for programming with quantum computational linear algebra. Cobble enables developers to express and manipulate the quantum representations of matrices, known as block encodings, using high-level notation that automatically compiles to correct quantum circuits. Cobble features analyses that estimate leading factors in time and space usage of programs, as well as optimizations that reduce overhead and generate efficient circuits using leading techniques such as the quantum singular value transformation. We evaluate Cobble on benchmark kernels for simulation, regression, search, and other applications, showing 2.6x-25.4x speedups not achieved by existing circuit optimizers on these benchmarks.",
    "pdf_url": "https://arxiv.org/pdf/2511.01736v1",
    "github_url": null,
    "published": "2025-11-03T16:48:13+00:00",
    "updated": "2025-11-03T16:48:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01643v1",
    "title": "A Graph-based RAG for Energy Efficiency Question Answering",
    "authors": [
      "Campi",
      "Vago",
      "Giudici"
    ],
    "summary": "In this work, we investigate the use of Large Language Models (LLMs) within a graph-based Retrieval Augmented Generation (RAG) architecture for Energy Efficiency (EE) Question Answering. First, the system automatically extracts a Knowledge Graph (KG) from guidance and regulatory documents in the energy field. Then, the generated graph is navigated and reasoned upon to provide users with accurate answers in multiple languages. We implement a human-based validation using the RAGAs framework properties, a validation dataset comprising 101 question-answer pairs, and domain experts. Results confirm the potential of this architecture and identify its strengths and weaknesses. Validation results show how the system correctly answers in about three out of four of the cases (75.2 +- 2.7%), with higher results on questions related to more general EE answers (up to 81.0 +- 4.1%), and featuring promising multilingual abilities (4.4% accuracy loss due to translation).",
    "pdf_url": "https://arxiv.org/pdf/2511.01643v1",
    "github_url": null,
    "published": "2025-11-03T14:55:34+00:00",
    "updated": "2025-11-03T14:55:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01527v1",
    "title": "TPS-Bench: Evaluating AI Agents' Tool Planning \\& Scheduling Abilities in Compounding Tasks",
    "authors": [
      "Xu",
      "Huang",
      "Liu"
    ],
    "summary": "Large language model (LLM) agents have exhibited strong problem-solving competence across domains like research and coding. Yet, it remains underexplored whether LLM agents can tackle compounding real-world problems that require a diverse set of tools to complete. Given a broad, heterogeneous tool repository, LLM agents must not only select appropriate tools based on task planning analysis but also strategically schedule the execution order to ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of LLM agents in solving such problems that demand Tool Planning and Scheduling. TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a tool repository containing hundreds of model context protocol (MCP) tools. In particular, each task is composed of multiple subtasks, such as web search, map navigation, calendar checking, etc., and each subtask can be completed by a basic tool. Our evaluation emphasizes both task completion rate and efficiency. The empirical studies on popular closed-source and open-source LLMs indicate that most models can perform reasonable tool planning, but differ in scheduling. For example, GLM-4.5 achieves an outperforming task completion rate of 64.72% with extensive sequential tool calls, hence suffering from significantly long execution time. By contrast, GPT-4o prioritizes parallel tool calls but achieves only a 45.08% completion rate. Considering reinforcement learning (RL) can be a viable way to improve the scheduling efficiency without compromising performance, we perform an initial study on Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in task completion rate based on rarely 100 RL training samples. Our code is available https://github.com/hanwenxu1/mcp-agent.",
    "pdf_url": "https://arxiv.org/pdf/2511.01527v1",
    "github_url": "https://github.com/hanwenxu1/mcp-agent",
    "published": "2025-11-03T12:45:39+00:00",
    "updated": "2025-11-03T12:45:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01493v2",
    "title": "Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues",
    "authors": [
      "Huang",
      "Li",
      "Wan"
    ],
    "summary": "Guiding an agent to a specific target in indoor environments based solely on RGB inputs and a floor plan is a promising yet challenging problem. Although existing methods have made significant progress, two challenges remain unresolved. First, the modality gap between egocentric RGB observations and the floor plan hinders the integration of visual and spatial information for both local obstacle avoidance and global planning. Second, accurate localization is critical for navigation performance, but remains challenging at deployment in unseen environments due to the lack of explicit geometric alignment between RGB inputs and floor plans. We propose a novel diffusion-based policy, denoted as GlocDiff, which integrates global path planning from the floor plan with local depth-aware features derived from RGB observations. The floor plan offers explicit global guidance, while the depth features provide implicit geometric cues, collectively enabling precise prediction of optimal navigation directions and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation during training to enhance robustness against pose estimation errors, and we find that combining this with a relatively stable VO module during inference results in significantly improved navigation performance. Extensive experiments on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in achieving superior navigation performance, and the success of real-world deployments also highlights its potential for widespread practical applications.",
    "pdf_url": "https://arxiv.org/pdf/2511.01493v2",
    "github_url": null,
    "published": "2025-11-03T12:00:15+00:00",
    "updated": "2025-11-26T11:12:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01378v1",
    "title": "Building granular structures with elasto-active systems",
    "authors": [
      "Xi",
      "Marzin",
      "Brun"
    ],
    "summary": "Natural active systems routinely reshape and reorganize their environments through sustained local interactions. Examples of decentralized collective construction are common in nature, e.g., many insects achieve large-scale constructions through indirect communication. While synthetic realizations of self-organization exist, they typically rely on rigid agents that require some kind of sensors and direct programming to achieve their function. Understanding how soft, deformable active matter navigates and remodels crowded landscapes remains an open challenge. Here we show that connecting rigid microbots to elastic beams yields elasto-active structures that can restructure and adapt to heterogeneous surroundings. We investigate the dynamics of these agents in environments with varying granular densities, rationalizing how they can aggregate or carve the medium through gentle interactions. At low density, the system compacts dispersed obstacles into clusters, a process modeled by a modified Smoluchowski coagulation theory. At high density, our agents carve voids whose size is predicted by a force-limited argument. These results establish a framework for understanding how activity, elasticity, and deformability can influence active navigation and environmental reconfiguration in granular media.",
    "pdf_url": "https://arxiv.org/pdf/2511.01378v1",
    "github_url": null,
    "published": "2025-11-03T09:23:07+00:00",
    "updated": "2025-11-03T09:23:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01336v1",
    "title": "Beyond Permissions: Investigating Mobile Personalization with Simulated Personas",
    "authors": [
      "Khalilov",
      "Chen",
      "Xiao"
    ],
    "summary": "Mobile applications increasingly rely on sensor data to infer user context and deliver personalized experiences. Yet the mechanisms behind this personalization remain opaque to users and researchers alike. This paper presents a sandbox system that uses sensor spoofing and persona simulation to audit and visualize how mobile apps respond to inferred behaviors. Rather than treating spoofing as adversarial, we demonstrate its use as a tool for behavioral transparency and user empowerment. Our system injects multi-sensor profiles - generated from structured, lifestyle-based personas - into Android devices in real time, enabling users to observe app responses to contexts such as high activity, location shifts, or time-of-day changes. With automated screenshot capture and GPT-4 Vision-based UI summarization, our pipeline helps document subtle personalization cues. Preliminary findings show measurable app adaptations across fitness, e-commerce, and everyday service apps such as weather and navigation. We offer this toolkit as a foundation for privacy-enhancing technologies and user-facing transparency interventions.",
    "pdf_url": "https://arxiv.org/pdf/2511.01336v1",
    "github_url": null,
    "published": "2025-11-03T08:39:38+00:00",
    "updated": "2025-11-03T08:39:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01305v1",
    "title": "DeepSpecs: Expert-Level Questions Answering in 5G",
    "authors": [
      "Manvattira",
      "Xu",
      "Dang"
    ],
    "summary": "5G technology enables mobile Internet access for billions of users. Answering expert-level questions about 5G specifications requires navigating thousands of pages of cross-referenced standards that evolve across releases. Existing retrieval-augmented generation (RAG) frameworks, including telecom-specific approaches, rely on semantic similarity and cannot reliably resolve cross-references or reason about specification evolution. We present DeepSpecs, a RAG system enhanced by structural and temporal reasoning via three metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB (line-level version diffs), and TDocDB (standardization meeting documents). DeepSpecs explicitly resolves cross-references by recursively retrieving referenced clauses through metadata lookup, and traces specification evolution by mining changes and linking them to Change Requests that document design rationale. We curate two 5G QA datasets: 573 expert-annotated real-world questions from practitioner forums and educational resources, and 350 evolution-focused questions derived from approved Change Requests. Across multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art telecom RAG systems; ablations confirm that explicit cross-reference resolution and evolution-aware retrieval substantially improve answer quality, underscoring the value of modeling the structural and temporal properties of 5G standards.",
    "pdf_url": "https://arxiv.org/pdf/2511.01305v1",
    "github_url": null,
    "published": "2025-11-03T07:39:22+00:00",
    "updated": "2025-11-03T07:39:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01269v1",
    "title": "NIR-II Fluorescence Project Technology for Augmented Reality Surgical Navigation",
    "authors": [
      "Zhang",
      "Liu",
      "Liu"
    ],
    "summary": "NIR-II fluorescence imaging provides superior tissue penetration and clarity, yet its clinical use in surgical navigation is hindered by a critical workflow issue. Surgeons must divert their attention between the operative field and external monitors, increasing cognitive load and disrupting procedures. Current strategies have failed to resolve this fundamental problem. Here, we developed a co-axial NIR-II fluorescence projection navigation system to enable real-time, in situ visualization. This system creates an intraoperative augmented reality by directly projecting high-precision, pseudocolored fluorescence images onto the surgical field, spatially integrating functional signals with patient anatomy. Validated through in vitro, in vivo, and clinical patient studies, our system eliminates visual field switching, reduces intraoperative distraction, and preserves natural stereoscopic vision. This approach represents a paradigm shift toward a more coherent, efficient, and ergonomically optimized optical imaging modality for surgical navigation.",
    "pdf_url": "https://arxiv.org/pdf/2511.01269v1",
    "github_url": null,
    "published": "2025-11-03T06:42:37+00:00",
    "updated": "2025-11-03T06:42:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01234v1",
    "title": "A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization",
    "authors": [
      "Gan",
      "Chen",
      "Yi"
    ],
    "summary": "The proliferation of saddle points, rather than poor local minima, is increasingly understood to be a primary obstacle in large-scale non-convex optimization for machine learning. Variable elimination algorithms, like Variable Projection (VarPro), have long been observed to exhibit superior convergence and robustness in practice, yet a principled understanding of why they so effectively navigate these complex energy landscapes has remained elusive. In this work, we provide a rigorous geometric explanation by comparing the optimization landscapes of the original and reduced formulations. Through a rigorous analysis based on Hessian inertia and the Schur complement, we prove that variable elimination fundamentally reshapes the critical point structure of the objective function, revealing that local maxima in the reduced landscape are created from, and correspond directly to, saddle points in the original formulation. Our findings are illustrated on the canonical problem of non-convex matrix factorization, visualized directly on two-parameter neural networks, and finally validated in training deep Residual Networks, where our approach yields dramatic improvements in stability and convergence to superior minima. This work goes beyond explaining an existing method; it establishes landscape simplification via saddle point transformation as a powerful principle that can guide the design of a new generation of more robust and efficient optimization algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2511.01234v1",
    "github_url": null,
    "published": "2025-11-03T05:19:43+00:00",
    "updated": "2025-11-03T05:19:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01199v1",
    "title": "Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures",
    "authors": [
      "McCandless",
      "Hamid",
      "Elmariah"
    ],
    "summary": "To move away from open-heart surgery towards safer transcatheter procedures, there is a growing need for improved imaging techniques and robotic solutions to enable simple, accurate tool navigation. Common imaging modalities, such as fluoroscopy and ultrasound, have limitations that can be overcome using cardioscopy, i.e., direct optical visualization inside the beating heart. We present a cardioscope designed as a steerable balloon. As a balloon, it can be collapsed to pass through the vasculature and subsequently inflated inside the heart for visualization and tool delivery through an integrated working channel. Through careful design of balloon wall thickness, a single input, balloon inflation pressure, is used to independently control two outputs, balloon diameter (corresponding to field of view diameter) and balloon bending angle (enabling precise working channel positioning). This balloon technology can be tuned to produce cardioscopes designed for a range of intracardiac tasks. To illustrate this approach, a balloon design is presented for the specific task of aortic leaflet laceration. Image-based closed-loop control of bending angle is also demonstrated as a means of enabling stable orientation control during tool insertion and removal.",
    "pdf_url": "https://arxiv.org/pdf/2511.01199v1",
    "github_url": null,
    "published": "2025-11-03T03:47:02+00:00",
    "updated": "2025-11-03T03:47:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01186v1",
    "title": "LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping",
    "authors": [
      "Wang",
      "Guo",
      "Xu"
    ],
    "summary": "Reconstructing large-scale colored point clouds is an important task in robotics, supporting perception, navigation, and scene understanding. Despite advances in LiDAR inertial visual odometry (LIVO), its performance remains highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation models, such as VGGT, suffer from limited scalability in large environments and inherently lack metric scale. To overcome these limitations, we propose LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion pipeline: First, a pre-fusion module with robust initialization refinement efficiently estimates VGGT poses and point clouds with coarse metric scale within each session. Then, a post-fusion module enhances cross-modal 3D similarity transformation, using bounding-box-based regularization to reduce scale distortions caused by inconsistent FOVs between LiDAR and camera sensors. Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT achieves dense, globally consistent colored point clouds and outperforms both VGGT-based methods and LIVO baselines. The implementation of our proposed novel color point cloud evaluation toolkit will be released as open source.",
    "pdf_url": "https://arxiv.org/pdf/2511.01186v1",
    "github_url": null,
    "published": "2025-11-03T03:24:28+00:00",
    "updated": "2025-11-03T03:24:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01083v1",
    "title": "Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment",
    "authors": [
      "Wang",
      "Li",
      "Wu"
    ],
    "summary": "Rivers are critical corridors for environmental monitoring and disaster response, where Unmanned Aerial Vehicles (UAVs) guided by vision-driven policies can provide fast, low-cost coverage. However, deployment exposes simulation-trained policies with distribution shift and safety risks and requires efficient adaptation from limited human interventions. We study human-in-the-loop (HITL) learning with a conservative overseer who vetoes unsafe or inefficient actions and provides statewise preferences by comparing the agent's proposal with a corrective override. We introduce Statewise Hybrid Preference Alignment for Robotics (SPAR-H), which fuses direct preference optimization on policy logits with a reward-based pathway that trains an immediate-reward estimator from the same preferences and updates the policy using a trust-region surrogate. With five HITL rollouts collected from a fixed novice policy, SPAR-H achieves the highest final episodic reward and the lowest variance across initial conditions among tested methods. The learned reward model aligns with human-preferred actions and elevates nearby non-intervened choices, supporting stable propagation of improvements. We benchmark SPAR-H against imitation learning (IL), direct preference variants, and evaluative reinforcement learning (RL) in the HITL setting, and demonstrate real-world feasibility of continual preference alignment for UAV river following. Overall, dual statewise preferences empirically provide a practical route to data-efficient online adaptation in riverine navigation.",
    "pdf_url": "https://arxiv.org/pdf/2511.01083v1",
    "github_url": null,
    "published": "2025-11-02T21:30:29+00:00",
    "updated": "2025-11-02T21:30:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01009v1",
    "title": "Equality Graph Assisted Symbolic Regression",
    "authors": [
      "Franca",
      "Kronberger"
    ],
    "summary": "In Symbolic Regression (SR), Genetic Programming (GP) is a popular search algorithm that delivers state-of-the-art results in term of accuracy. Its success relies on the concept of neutrality, which induces large plateaus that the search can safely navigate to more promising regions. Navigating these plateaus, while necessary, requires the computation of redundant expressions, up to 60% of the total number of evaluation, as noted in a recent study. The equality graph (e-graph) structure can compactly store and group equivalent expressions enabling us to verify if a given expression and their variations were already visited by the search, thus enabling us to avoid unnecessary computation. We propose a new search algorithm for symbolic regression called SymRegg that revolves around the e-graph structure following simple steps: perturb solutions sampled from a selection of expressions stored in the e-graph, if it generates an unvisited expression, insert it into the e-graph and generates its equivalent forms. We show that SymRegg is capable of improving the efficiency of the search, maintaining consistently accurate results across different datasets while requiring a choice of a minimalist set of hyperparameters.",
    "pdf_url": "https://arxiv.org/pdf/2511.01009v1",
    "github_url": null,
    "published": "2025-11-02T16:57:22+00:00",
    "updated": "2025-11-02T16:57:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00945v1",
    "title": "\"Less is More\": Reducing Cognitive Load and Task Drift in Real-Time Multimodal Assistive Agents for the Visually Impaired",
    "authors": [
      "Zhao",
      "Wang",
      "Geng"
    ],
    "summary": "Vision-Language Models (VLMs) enable on-demand visual assistance, yet current applications for people with visual impairments (PVI) impose high cognitive load and exhibit task drift, limiting real-world utility. We first conducted a formative study with 15 PVI and identified three requirements for visually impaired assistance (VIA): low latency for real-time use, minimal cognitive load, and hallucination-resistant responses to sustain trust. Informed by the formative study, we present VIA-Agent, a prototype that co-optimizes its cognitive 'brain' and interactive 'body'. The brain implements a goal-persistent design with calibrated conciseness to produce brief, actionable guidance; the body adopts a real-time communication (RTC) embodiment-evolving from a request-response model Context Protocol (MCP) pipeline-to-support fluid interaction. We evaluated VIA-Agent with 9 PVI across navigation and object retrieval in the wild against BeMyAI and Doubao. VIA-Agent significantly outperformed BeMyAI both quantitatively and qualitatively. While achieving success rates comparable to Doubao, it reduced mean task time by 39.9% (70.1 s vs. 110.7 s), required fewer conversational turns (4.3 vs. 5.0), and lowered perceived cognitive load and task drift. System Usability Scale (SUS) results aligned with these findings, with VIA-Agent achieving the highest usability. We hope this work inspires the development of more human-centered VIA systems.",
    "pdf_url": "https://arxiv.org/pdf/2511.00945v1",
    "github_url": null,
    "published": "2025-11-02T14:08:28+00:00",
    "updated": "2025-11-02T14:08:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00934v1",
    "title": "pacSTL: PAC-Bounded Signal Temporal Logic from Data-Driven Reachability Analysis",
    "authors": [
      "Dietrich",
      "Krasowski",
      "Gezer"
    ],
    "summary": "Real-world robotic systems must comply with safety requirements in the presence of uncertainty. To define and measure requirement adherence, Signal Temporal Logic (STL) offers a mathematically rigorous and expressive language. However, standard STL cannot account for uncertainty. We address this problem by presenting pacSTL, a framework that combines Probably Approximately Correct (PAC) bounded set predictions with an interval extension of STL through optimization problems on the atomic proposition level. pacSTL provides PAC-bounded robustness intervals on the specification level that can be utilized in monitoring. We demonstrate the effectiveness of this approach through maritime navigation and analyze the efficiency and scalability of pacSTL through simulation and real-world experimentation on model vessels.",
    "pdf_url": "https://arxiv.org/pdf/2511.00934v1",
    "github_url": null,
    "published": "2025-11-02T13:22:01+00:00",
    "updated": "2025-11-02T13:22:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00933v1",
    "title": "Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation",
    "authors": [
      "Shi",
      "Li",
      "Qiao"
    ],
    "summary": "Recent advances in Vision-and-Language Navigation in Continuous Environments (VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve zero-shot navigation. However, existing methods often rely on panoramic observations and two-stage pipelines involving waypoint predictors, which introduce significant latency and limit real-world applicability. In this work, we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that eliminates the need for panoramic views and waypoint predictors. Our approach uses only three frontal RGB-D images combined with natural language instructions, enabling MLLMs to directly predict actions. To enhance decision robustness, we introduce an Uncertainty-Aware Reasoning module that integrates (i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past Bidirectional Reasoning mechanism for globally coherent planning. Experiments on both simulated and real-robot environments demonstrate that our method significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines. These results demonstrate the practicality and effectiveness of Fast-SmartWay for real-world zero-shot embodied navigation.",
    "pdf_url": "https://arxiv.org/pdf/2511.00933v1",
    "github_url": null,
    "published": "2025-11-02T13:21:54+00:00",
    "updated": "2025-11-02T13:21:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00884v1",
    "title": "A Meta-Cognitive Swarm Intelligence Framework for Resilient UAV Navigation in GPS-Denied and Cluttered Environments",
    "authors": [
      "Mankoe",
      "Lu",
      "Bi"
    ],
    "summary": "Autonomous navigation of UAV swarms in perceptually-degraded environments, where GPS is unavailable and terrain is densely cluttered, presents a critical bottleneck for real-world deployment. Existing optimization-based planners lack the resilience to avoid catastrophic convergence to local optima under such uncertainty. Inspired by principles of computational meta-cognition, this paper introduces a novel swarm intelligence framework that enables a fleet of UAVs to autonomously sense, adapt, and recover from planning failures in real-time. At its core is the Self-Learning Slime Mould Algorithm (SLSMA), which integrates three meta-cognitive layers: a situation-aware search strategy that dynamically selects between exploration and exploitation based on perceived search stagnation; a collective memory mechanism that allows the swarm to learn from and avoid previously failed trajectories; and an adaptive recovery behavior that triggers global re-exploration upon entrapment. We formulate the multi-UAV trajectory problem as a resilient planning challenge, with a cost function that penalizes not only path length and collisions but also navigational uncertainty and proximity to failure states. Extensive simulations in synthetically complex 3D worlds and against the CEC 2017 benchmark suite demonstrate the framework's superior performance. The SLSMA does not merely optimize paths; it generates resilient trajectories, demonstrating a 99.5% mission success rate and significantly outperforming state-of-the-art metaheuristics in recovery speed and solution reliability. This work provides a foundational step towards truly autonomous swarms capable of persistent operation in denied and dynamic environments.",
    "pdf_url": "https://arxiv.org/pdf/2511.00884v1",
    "github_url": null,
    "published": "2025-11-02T10:47:47+00:00",
    "updated": "2025-11-02T10:47:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00858v1",
    "title": "Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction",
    "authors": [
      "Liu",
      "Liu",
      "Yang"
    ],
    "summary": "Predicting pedestrian crossing intentions is crucial for the navigation of mobile robots and intelligent vehicles. Although recent deep learning-based models have shown significant success in forecasting intentions, few consider incomplete observation under occlusion scenarios. To tackle this challenge, we propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded motion patterns and leverages them to guide future intention prediction. During the denoising stage, we introduce an occlusion-aware diffusion transformer architecture to estimate noise features associated with occluded patterns, thereby enhancing the model's ability to capture contextual relationships in occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse process is introduced to effectively utilize observation information, reducing the accumulation of prediction errors and enhancing the accuracy of reconstructed motion features. The performance of the proposed method under various occlusion scenarios is comprehensively evaluated and compared with existing methods on popular benchmarks, namely PIE and JAAD. Extensive experimental results demonstrate that the proposed method achieves more robust performance than existing methods in the literature.",
    "pdf_url": "https://arxiv.org/pdf/2511.00858v1",
    "github_url": null,
    "published": "2025-11-02T08:49:07+00:00",
    "updated": "2025-11-02T08:49:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00783v2",
    "title": "When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage",
    "authors": [
      "Xu",
      "Zhang",
      "Li"
    ],
    "summary": "Underwater multi-robot cooperative coverage remains challenging due to partial observability, limited communication, environmental uncertainty, and the lack of access to global localization. To address these issues, this paper presents a semantics-guided fuzzy control framework that couples Large Language Models (LLMs) with interpretable control and lightweight coordination. Raw multimodal observations are compressed by the LLM into compact, human-interpretable semantic tokens that summarize obstacles, unexplored regions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy inference system with pre-defined membership functions then maps these tokens into smooth and stable steering and gait commands, enabling reliable navigation without relying on global positioning. Then, we further coordinate multiple robots by introducing semantic communication that shares intent and local context in linguistic form, enabling agreement on who explores where while avoiding redundant revisits. Extensive simulations in unknown reef-like environments show that, under limited sensing and communication, the proposed framework achieves robust OOI-oriented navigation and cooperative coverage with improved efficiency and adaptability, narrowing the gap between semantic cognition and distributed underwater control in GPS-denied, map-free conditions.",
    "pdf_url": "https://arxiv.org/pdf/2511.00783v2",
    "github_url": null,
    "published": "2025-11-02T03:34:44+00:00",
    "updated": "2025-11-06T15:24:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00710v3",
    "title": "Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries",
    "authors": [
      "Shen",
      "Zhi",
      "Liu"
    ],
    "summary": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning (RL) show impressive general reasoning, their evaluation is often confined to language-dominant tasks (e.g., math). This raises a critical question: can RL post-training truly extend the inherent capability boundary of a base VLM, particularly for visual-centric spatial tasks where it initially fails? To investigate this, we introduce Ariadne, a framework utilizing synthetic mazes for multi-step spatial reasoning where task difficulty (e.g., path length, turns) is precisely controlled. We leverage this controllable environment to train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves over 50% accuracy on a problem set where the base model scored 0%, demonstrating that our approach expands the model's initial capability boundary. To assess real-world viability, we evaluate out-of-distribution (OOD) generalization on practical benchmarks. Despite training only on synthetic maze samples, Ariadne achieves significant zero-shot improvements, averaging 16% on MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer tasks). These results confirm that our method not only broadens the model's fundamental limits but also enhances its generalization to real-world spatial reasoning. We acknowledge our study is limited to the post-training phase, given the opaqueness of pre-training data, and hope our research motivates further work on specialized, capability-extending alignment.",
    "pdf_url": "https://arxiv.org/pdf/2511.00710v3",
    "github_url": null,
    "published": "2025-11-01T21:19:41+00:00",
    "updated": "2025-11-11T19:06:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00627v1",
    "title": "Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature",
    "authors": [
      "Barré",
      "Seminck",
      "Bourgois"
    ],
    "summary": "This research explores the evolution of the detective archetype in French detective fiction through computational analysis. Using quantitative methods and character-level embeddings, we show that a supervised model is able to capture the unity of the detective archetype across 150 years of literature, from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding, the study demonstrates how the detective figure evolves from a secondary narrative role to become the central character and the \"reasoning machine\" of the classical detective story. In the aftermath of the Second World War, with the importation of the hardboiled tradition into France, the archetype becomes more complex, navigating the genre's turn toward social violence and moral ambiguity.",
    "pdf_url": "https://arxiv.org/pdf/2511.00627v1",
    "github_url": null,
    "published": "2025-11-01T17:09:05+00:00",
    "updated": "2025-11-01T17:09:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00598v1",
    "title": "GDROS: A Geometry-Guided Dense Registration Framework for Optical-SAR Images under Large Geometric Transformations",
    "authors": [
      "Sun",
      "Zhi",
      "Li"
    ],
    "summary": "Registration of optical and synthetic aperture radar (SAR) remote sensing images serves as a critical foundation for image fusion and visual navigation tasks. This task is particularly challenging because of their modal discrepancy, primarily manifested as severe nonlinear radiometric differences (NRD), geometric distortions, and noise variations. Under large geometric transformations, existing classical template-based and sparse keypoint-based strategies struggle to achieve reliable registration results for optical-SAR image pairs. To address these limitations, we propose GDROS, a geometry-guided dense registration framework leveraging global cross-modal image interactions. First, we extract cross-modal deep features from optical and SAR images through a CNN-Transformer hybrid feature extraction module, upon which a multi-scale 4D correlation volume is constructed and iteratively refined to establish pixel-wise dense correspondences. Subsequently, we implement a least squares regression (LSR) module to geometrically constrain the predicted dense optical flow field. Such geometry guidance mitigates prediction divergence by directly imposing an estimated affine transformation on the final flow predictions. Extensive experiments have been conducted on three representative datasets WHU-Opt-SAR dataset, OS dataset, and UBCv2 dataset with different spatial resolutions, demonstrating robust performance of our proposed method across different imaging resolutions. Qualitative and quantitative results show that GDROS significantly outperforms current state-of-the-art methods in all metrics. Our source code will be released at: https://github.com/Zi-Xuan-Sun/GDROS.",
    "pdf_url": "https://arxiv.org/pdf/2511.00598v1",
    "github_url": "https://github.com/Zi-Xuan-Sun/GDROS",
    "published": "2025-11-01T15:40:34+00:00",
    "updated": "2025-11-01T15:40:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.02859v1",
    "title": "The Evolution of Agile and Hybrid Project Management Methodologies: A Systematic Literature Review",
    "authors": [
      "Leech",
      "Hanslo"
    ],
    "summary": "The rapid evolution of IT projects has driven the transformation of project management methodologies, from traditional waterfall approaches to agile frameworks and, more recently, hybrid models. This systematic literature review investigates the evolution of agile methodologies into hybrid frameworks, analysing their implementation challenges and success factors. We identify key trends through PRISMA-guided analysis of peer-reviewed studies from the last 8 years. Hybrid methodologies emerge from agile limitations in large-scale and regulated environments, combining iterative flexibility with structured governance. Agile has several implementation challenges, leading to hybrid methods, and the success hinges on leadership support, tailored process integration, and continuous improvement mechanisms. The study explores the need for contextual adaptation over rigid frameworks, offering practical insights for organisations navigating hybrid transitions.",
    "pdf_url": "https://arxiv.org/pdf/2511.02859v1",
    "github_url": null,
    "published": "2025-11-01T12:13:47+00:00",
    "updated": "2025-11-01T12:13:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00491v1",
    "title": "Meta-Learning Based Radio Frequency Fingerprinting for GNSS Spoofing Detection",
    "authors": [
      "Marata",
      "Sankari",
      "Eldeeb"
    ],
    "summary": "The rapid development of technology has led to an increase in the number of devices that rely on position, velocity, and time (PVT) information to perform their functions. As such, the Global Navigation Satellite Systems (GNSS) have been adopted as one of the most promising solutions to provide PVT. Consequently, there are renewed efforts aimed at enhancing GNSS capabilities to meet emerging use cases and their requirements. For example, GNSS is evolving to rely on low-earth-orbit satellites, shifting the focus from traditional medium-earth-orbit satellites. Unfortunately, these developments also bring forth higher risks of interference signals such as spoofers, which pose serious security threats. To address this challenge, artificial intelligence (AI)-inspired solutions are being developed to overcome the limitations of conventional mathematics-based approaches, which have proven inflexible when dealing with diverse forms of interference. In this paper, we advance this direction by proposing a meta-learning framework that enables GNSS receivers to detect various types of spoofers. Specifically, our approach exploits the radio frequency fingerprints present in the signal at both the pre-correlation and post-correlation stages of the receiver. The proposed solution has superior generalization properties compared to the state-of-the-art solutions. Numerical results demonstrate that our proposed solution significantly detects spoofers of different forms, with spoofing detection accuracies of more than 95% on multiple datasets from the Texas Spoofing Test Battery (TEXBAT) and the Oak Ridge Spoofing and Interference Test Battery (OAKBAT) repositories",
    "pdf_url": "https://arxiv.org/pdf/2511.00491v1",
    "github_url": null,
    "published": "2025-11-01T10:57:20+00:00",
    "updated": "2025-11-01T10:57:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00453v1",
    "title": "CT-ESKF: A General Framework of Covariance Transformation-Based Error-State Kalman Filter",
    "authors": [
      "Han",
      "Ouyang",
      "Zhu"
    ],
    "summary": "Invariant extended Kalman filter (InEKF) possesses excellent trajectory-independent property and better consistency compared to conventional extended Kalman filter (EKF). However, when applied to scenarios involving both global-frame and body-frame observations, InEKF may fail to preserve its trajectory-independent property. This work introduces the concept of equivalence between error states and covariance matrices among different error-state Kalman filters, and shows that although InEKF exhibits trajectory independence, its covariance propagation is actually equivalent to EKF. A covariance transformation-based error-state Kalman filter (CT-ESKF) framework is proposed that unifies various error-state Kalman filtering algorithms. The framework gives birth to novel filtering algorithms that demonstrate improved performance in integrated navigation systems that incorporate both global and body-frame observations. Experimental results show that the EKF with covariance transformation outperforms both InEKF and original EKF in a representative INS/GNSS/Odometer integrated navigation system.",
    "pdf_url": "https://arxiv.org/pdf/2511.00453v1",
    "github_url": null,
    "published": "2025-11-01T08:36:03+00:00",
    "updated": "2025-11-01T08:36:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00417v2",
    "title": "Human-AI Programming Role Optimization: Developing a Personality-Driven Self-Determination Framework",
    "authors": [
      "Valovy"
    ],
    "summary": "As artificial intelligence transforms software development, a critical question emerges: how can developers and AI systems collaborate most effectively? This dissertation optimizes human-AI programming roles through self-determination theory and personality psychology, introducing the Role Optimization Motivation Alignment (ROMA) framework.   Through Design Science Research spanning five cycles, this work establishes empirically-validated connections between personality traits, programming role preferences, and collaborative outcomes, engaging 200 experimental participants and 46 interview respondents.   Key findings demonstrate that personality-driven role optimization significantly enhances self-determination and team dynamics, yielding 23% average motivation increases among professionals and up to 65% among undergraduates. Five distinct personality archetypes emerge: The Explorer (high Openness/low Agreeableness), The Orchestrator (high Extraversion/Agreeableness), The Craftsperson (high Neuroticism/low Extraversion), The Architect (high Conscientiousness), and The Adapter (balanced profile). Each exhibits distinct preferences for programming roles (Co-Pilot, Co-Navigator, Agent), with assignment modes proving crucial for satisfaction.   The dissertation contributes: (1) an empirically-validated framework linking personality traits to role preferences and self-determination outcomes; (2) a taxonomy of AI collaboration modalities mapped to personality profiles while preserving human agency; and (3) an ISO/IEC 29110 extension enabling Very Small Entities to implement personality-driven role optimization within established standards.   Keywords: artificial intelligence, human-computer interaction, behavioral software engineering, self-determination theory, personality psychology, phenomenology, intrinsic motivation, pair programming, design science research, ISO/IEC 29110",
    "pdf_url": "https://arxiv.org/pdf/2511.00417v2",
    "github_url": null,
    "published": "2025-11-01T06:00:14+00:00",
    "updated": "2025-11-27T03:36:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00412v1",
    "title": "Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory",
    "authors": [
      "Christian",
      "Walker",
      "Bridgman"
    ],
    "summary": "The integration of gyroscope measurements is an essential task for most navigation systems. Modern vehicles typically use strapdown systems, such that gyro integration requires coning compensation to account for the sensor's rotation during the integration. Many coning compensation algorithms have been developed and a few are reviewed. This work introduces a new class of coning correction algorithm built directly from the classical Runge-Kutta integration routines. A simple case is shown to collapse to one of the most popular coning algorithms and a clear procedure for generating higher-order algorithms is presented.",
    "pdf_url": "https://arxiv.org/pdf/2511.00412v1",
    "github_url": null,
    "published": "2025-11-01T05:46:04+00:00",
    "updated": "2025-11-01T05:46:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00387v1",
    "title": "Spatial Crowdsourcing-based Task Allocation for UAV-assisted Maritime Data Collection",
    "authors": [
      "Han",
      "Lin",
      "Na"
    ],
    "summary": "Driven by the unceasing development of maritime services, tasks of unmanned aerial vehicle (UAV)-assisted maritime data collection (MDC) are becoming increasingly diverse, complex and personalized. As a result, effective task allocation for MDC is becoming increasingly critical. In this work, integrating the concept of spatial crowdsourcing (SC), we develop an SC-based MDC network model and investigate the task allocation problem for UAV-assisted MDC. In variable maritime service scenarios, tasks are allocated to UAVs based on the spatial and temporal requirements of the tasks, as well as the mobility of the UAVs. To address this problem, we design an SC-based task allocation algorithm for the MDC (SC-MDC-TA). The quality estimation is utilized to assess and regulate task execution quality by evaluating signal to interference plus noise ratio and the UAV energy consumption. The reverse auction is employed to potentially reduce the task waiting time as much as possible while ensuring timely completion. Additionally, we establish typical task allocation scenarios based on maritime service requirements indicated by electronic navigational charts. Simulation results demonstrate that the proposed SC-MDC-TA algorithm effectively allocates tasks for various MDC scenarios. Furthermore, compared to the benchmark, the SC-MDC-TA algorithm can also reduce the task completion time and lower the UAV energy consumption.",
    "pdf_url": "https://arxiv.org/pdf/2511.00387v1",
    "github_url": null,
    "published": "2025-11-01T03:41:20+00:00",
    "updated": "2025-11-01T03:41:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00306v1",
    "title": "FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications",
    "authors": [
      "Song",
      "Xu",
      "Hsu"
    ],
    "summary": "Sliding window-factor graph optimization (SW-FGO) has gained more and more attention in navigation research due to its robust approximation to non-Gaussian noises and nonlinearity of measuring models. There are lots of works focusing on its application performance compared to extended Kalman filter (EKF) but there is still a myth at the theoretical relationship between the SW-FGO and EKF. In this paper, we find the necessarily fair condition to connect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF (IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the conditions, we propose a recursive FGO (Re-FGO) framework to represent KFV under SW-FGO formulation. Under explicit conditions (Markov assumption, Gaussian noise with L2 loss, and a one-state window), Re-FGO regenerates exactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in nonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after clarifying the connection between them, we highlight the unique advantages of SW-FGO in practical phases, especially on numerical estimation and deep learning integration. The code and data used in this work is open sourced at https://github.com/Baoshan-Song/KFV-FGO-Comparison.",
    "pdf_url": "https://arxiv.org/pdf/2511.00306v1",
    "github_url": "https://github.com/Baoshan-Song/KFV-FGO-Comparison",
    "published": "2025-10-31T23:10:20+00:00",
    "updated": "2025-10-31T23:10:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00290v2",
    "title": "NOMAD -- Navigating Optimal Model Application to Datastreams",
    "authors": [
      "Colaco",
      "Mehrotra",
      "Lucia"
    ],
    "summary": "NOMAD (Navigating Optimal Model Application for Datastreams) is an intelligent framework for data enrichment during ingestion that optimizes realtime multiclass classification by dynamically constructing model chains, i.e ,sequences of machine learning models with varying cost-quality tradeoffs, selected via a utilitybased criterion. Inspired by predicate ordering techniques from database query processing, NOMAD leverages cheaper models as initial filters, proceeding to more expensive models only when necessary, while guaranteeing classification quality remains comparable to a designated role model through a formal chain safety mechanism. It employs a dynamic belief update strategy to adapt model selection based on per event predictions and shifting data distributions, and extends to scenarios with dependent models such as earlyexit DNNs and stacking ensembles. Evaluation across multiple datasets demonstrates that NOMAD achieves significant computational savings compared to static and naive approaches while maintaining classification quality comparable to that achieved by the most accurate (and often the most expensive) model.",
    "pdf_url": "https://arxiv.org/pdf/2511.00290v2",
    "github_url": null,
    "published": "2025-10-31T22:17:50+00:00",
    "updated": "2025-11-14T05:56:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00260v1",
    "title": "MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba",
    "authors": [
      "Jiang",
      "Huang",
      "Bano"
    ],
    "summary": "Accurate 3D point cloud registration underpins reliable image-guided colonoscopy, directly affecting lesion localization, margin assessment, and navigation safety. However, biological tissue exhibits repetitive textures and locally homogeneous geometry that cause feature degeneracy, while substantial domain shifts between pre-operative anatomy and intra-operative observations further degrade alignment stability. To address these clinically critical challenges, we introduce a novel 3D registration method tailored for endoscopic navigation and a high-quality, clinically grounded dataset to support rigorous and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale benchmark dataset with 10,014 geometrically aligned point cloud pairs derived from clinical CT data. We propose MambaNetLK, a novel correspondence-free registration framework, which enhances the PointNetLK architecture by integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor. As a result, the proposed framework efficiently captures long-range dependencies with linear-time complexity. The alignment is achieved iteratively using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k, MambaNetLK achieves the best performance compared with the state-of-the-art methods, reducing median rotation error by 56.04% and RMSE translation error by 26.19% over the second-best method. The model also demonstrates strong generalization on ModelNet40 and superior robustness to initial pose perturbations. MambaNetLK provides a robust foundation for 3D registration in surgical navigation. The combination of a globally expressive SSM-based feature extractor and a large-scale clinical dataset enables more accurate and reliable guidance systems in minimally invasive procedures like colonoscopy.",
    "pdf_url": "https://arxiv.org/pdf/2511.00260v1",
    "github_url": null,
    "published": "2025-10-31T21:14:25+00:00",
    "updated": "2025-10-31T21:14:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00140v1",
    "title": "Supply Chain Exploitation of Secure ROS 2 Systems: A Proof-of-Concept on Autonomous Platform Compromise via Keystore Exfiltration",
    "authors": [
      "Sakib",
      "Martinez",
      "Brady"
    ],
    "summary": "This paper presents a proof-of-concept supply chain attack against the Secure ROS 2 (SROS 2) framework, demonstrated on a Quanser QCar2 autonomous vehicle platform. A Trojan-infected Debian package modifies core ROS 2 security commands to exfiltrate newly generated keystore credentials via DNS in base64-encoded chunks to an attacker-controlled nameserver. Possession of these credentials enables the attacker to rejoin the SROS 2 network as an authenticated participant and publish spoofed control or perception messages without triggering authentication failures. We evaluate this capability on a secure ROS 2 Humble testbed configured for a four-stop-sign navigation routine using an Intel RealSense camera for perception. Experimental results show that control-topic injections can cause forced braking, sustained high-speed acceleration, and continuous turning loops, while perception-topic spoofing can induce phantom stop signs or suppress real detections. The attack generalizes to any data distribution service (DDS)-based robotic system using SROS 2, highlighting the need for both supply chain integrity controls and runtime semantic validation to safeguard autonomous systems against insider and impersonation threats.",
    "pdf_url": "https://arxiv.org/pdf/2511.00140v1",
    "github_url": null,
    "published": "2025-10-31T17:27:10+00:00",
    "updated": "2025-10-31T17:27:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27596v1",
    "title": "Navigated hepatic tumor resection using intraoperative ultrasound imaging",
    "authors": [
      "Olthof",
      "Ruers",
      "Natali"
    ],
    "summary": "Purpose: This proof-of-concept study evaluates feasibility and accuracy of an ultrasound-based navigation system for open liver surgery. Unlike most conventional systems that rely on registration to preoperative imaging, the proposed system provides navigation-guided resection using 3D models generated from intraoperative ultrasound.   Methods: A pilot study was conducted in 25 patients undergoing resection of liver metastases. The first five cases served to optimize the workflow. Intraoperatively, an electromagnetic sensor compensated for organ motion, after which an ultrasound volume was acquired. Vasculature was segmented automatically and tumors semi-automatically using region-growing (n=15) or a deep learning algorithm (n=5). The resulting 3D model was visualized alongside tracked surgical instruments. Accuracy was assessed by comparing the distance between surgical clips and tumors in the navigation software with the same distance on a postoperative CT of the resected specimen.   Results: Navigation was successfully established in all 20 patients. However, four cases were excluded from accuracy assessment due to intraoperative sensor detachment (n=3) or incorrect data recording (n=1). The complete navigation workflow was operational within 5-10 minutes. In 16 evaluable patients, 78 clip-to-tumor distances were analyzed. The median navigation accuracy was 3.2 mm [IQR: 2.8-4.8 mm], and an R0 resection was achieved in 15/16 (93.8%) patients and one patient had an R1 vascular resection.   Conclusion: Navigation based solely on intra-operative ultrasound is feasible and accurate for liver surgery. This registration-free approach paves the way for simpler and more accurate image guidance systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.27596v1",
    "github_url": null,
    "published": "2025-10-31T16:21:18+00:00",
    "updated": "2025-10-31T16:21:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27576v1",
    "title": "Trends and Challenges in Next-Generation GNSS Interference Management",
    "authors": [
      "Marata",
      "Jaramillo-Civill",
      "Imbiriba"
    ],
    "summary": "The global navigation satellite system (GNSS) continues to evolve in order to meet the demands of emerging applications such as autonomous driving and smart environmental monitoring. However, these advancements are accompanied by a rise in interference threats, which can significantly compromise the reliability and safety of GNSS. Such interference problems are typically addressed through signal-processing techniques that rely on physics-based mathematical models. Unfortunately, solutions of this nature can often fail to fully capture the complex forms of interference. To address this, artificial intelligence (AI)-inspired solutions are expected to play a key role in future interference management solutions, thanks to their ability to exploit data in addition to physics-based models. This magazine paper discusses the main challenges and tasks required to secure GNSS and present a research vision on how AI can be leveraged towards achieving more robust GNSS-based positioning.",
    "pdf_url": "https://arxiv.org/pdf/2510.27576v1",
    "github_url": null,
    "published": "2025-10-31T15:57:57+00:00",
    "updated": "2025-10-31T15:57:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27542v1",
    "title": "Beyond Demographics: Behavioural Segmentation and Spatial Analytics to Enhance Visitor Experience at The British Museum",
    "authors": [
      "Muggleton",
      "Monteath",
      "Yasseri"
    ],
    "summary": "This study explores visitor behaviour at The British Museum using data science methods applied to novel sources, including audio guide usage logs and TripAdvisor reviews. Analysing 42,000 visitor journeys and over 50,000 reviews, we identify key drivers of satisfaction, segment visitors by behavioural patterns, examine tour engagement, model spatial navigation, and investigate room popularity. Behavioural clustering uncovered four distinct visitor types: Committed Trekkers, Leisurely Explorers, Targeted Visitors, and Speedy Samplers, each characterised by different levels of engagement and movement. Tour usage analysis revealed high drop-off rates and variation in completion rates across different language groups. Spatial flow modelling revealed that accessibility and proximity, particularly aversion to stairs, shaped visitor paths more than thematic organisation. Room popularity was more strongly predicted by physical accessibility than curatorial content. We propose practical strategies for improving engagement and flow, offering a scalable framework for visitor-centred, data-informed museum planning.",
    "pdf_url": "https://arxiv.org/pdf/2510.27542v1",
    "github_url": null,
    "published": "2025-10-31T15:16:50+00:00",
    "updated": "2025-10-31T15:16:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27410v1",
    "title": "Dialogue as Discovery: Navigating Human Intent Through Principled Inquiry",
    "authors": [
      "Sun",
      "Feng",
      "Chang"
    ],
    "summary": "A fundamental bottleneck in human-AI collaboration is the \"intention expression gap,\" the difficulty for humans to effectively convey complex, high-dimensional thoughts to AI. This challenge often traps users in inefficient trial-and-error loops and is exacerbated by the diverse expertise levels of users. We reframe this problem from passive instruction following to a Socratic collaboration paradigm, proposing an agent that actively probes for information to resolve its uncertainty about user intent. we name the proposed agent Nous, trained to acquire proficiency in this inquiry policy. The core mechanism of Nous is a training framework grounded in the first principles of information theory. Within this framework, we define the information gain from dialogue as an intrinsic reward signal, which is fundamentally equivalent to the reduction of Shannon entropy over a structured task space. This reward design enables us to avoid reliance on costly human preference annotations or external reward models. To validate our framework, we develop an automated simulation pipeline to generate a large-scale, preference-based dataset for the challenging task of scientific diagram generation. Comprehensive experiments, including ablations, subjective and objective evaluations, and tests across user expertise levels, demonstrate the effectiveness of our proposed framework. Nous achieves leading efficiency and output quality, while remaining robust to varying user expertise. Moreover, its design is domain-agnostic, and we show evidence of generalization beyond diagram generation. Experimental results prove that our work offers a principled, scalable, and adaptive paradigm for resolving uncertainty about user intent in complex human-AI collaboration.",
    "pdf_url": "https://arxiv.org/pdf/2510.27410v1",
    "github_url": null,
    "published": "2025-10-31T12:00:21+00:00",
    "updated": "2025-10-31T12:00:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27398v2",
    "title": "Complete characterization of beam deflection based on double weak value amplification system",
    "authors": [
      "Wang",
      "Yang",
      "Zhang"
    ],
    "summary": "The precise measurement of spatial attitude parameters is critical for applications in inertial navigation, industrial monitoring, instrument calibration, quantum metrology, etc. In this work, we theoretically investigate and experimentally realize the simultaneous measurement of the yaw and pitch angles using a Hermite-Gaussian-postselected double weak value system integrated with two sets of high-order-mode balanced homodyne detections, thereby achieving a complete characterization of the beam deflection. Signals of the yaw and pitch angles that are involved in TEM$_{10}$ and TEM$_{01}$ modes output from two dark ports of the system can be measured independently. As a result, the obtained minimum measurable yaw and pitch angles of beam deflection are 83 prad and 89 prad, respectively. Meanwhile, the corresponding displacements are 0.79 pm and 0.85 pm, respectively. This work expands the beam deflection measurement to two dimensions, which provides a new insight for future high-precision multi-parameter spatial precise detection.",
    "pdf_url": "https://arxiv.org/pdf/2510.27398v2",
    "github_url": null,
    "published": "2025-10-31T11:35:17+00:00",
    "updated": "2025-12-04T03:57:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27363v1",
    "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use",
    "authors": [
      "Deng",
      "Dong",
      "Dou"
    ],
    "summary": "Recently, large language models (LLMs) have demonstrated remarkable problem-solving capabilities by autonomously integrating with external tools for collaborative reasoning. However, due to the inherently complex and diverse nature of multimodal information, enabling multimodal large language models (MLLMs) to flexibly and efficiently utilize external tools during reasoning remains an underexplored challenge. In this work, we introduce ToolScope, an agentic framework designed to unify global planning with local multimodal perception, adopting a specialized Perceive tool to mitigates visual context degradation in long-horizon VQA task. ToolScope comprises three primary components: the Global Navigator, the Agentic Executor, and the Response Synthesizer. The Global Navigator functions as a \"telescope\", offering high-level strategic guidance. The Agentic Executor operates iteratively to augment MLLM with local perception through the integration of external tools-Search, Code, and Perceive. Finally, the Response Synthesizer consolidates and organizes the reasoning process into a coherent, user-friendly output. We evaluate ToolScope on four VQA benchmarks across diverse domains, including VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong generalization capabilities, achieving an average performance improvement of up to +6.69% across all datasets.",
    "pdf_url": "https://arxiv.org/pdf/2510.27363v1",
    "github_url": null,
    "published": "2025-10-31T10:51:27+00:00",
    "updated": "2025-10-31T10:51:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27324v1",
    "title": "Generative Semantic Coding for Ultra-Low Bitrate Visual Communication and Analysis",
    "authors": [
      "Chen",
      "Wang",
      "Zhu"
    ],
    "summary": "We consider the problem of ultra-low bit rate visual communication for remote vision analysis, human interactions and control in challenging scenarios with very low communication bandwidth, such as deep space exploration, battlefield intelligence, and robot navigation in complex environments. In this paper, we ask the following important question: can we accurately reconstruct the visual scene using only a very small portion of the bit rate in existing coding methods while not sacrificing the accuracy of vision analysis and performance of human interactions? Existing text-to-image generation models offer a new approach for ultra-low bitrate image description. However, they can only achieve a semantic-level approximation of the visual scene, which is far insufficient for the purpose of visual communication and remote vision analysis and human interactions. To address this important issue, we propose to seamlessly integrate image generation with deep image compression, using joint text and coding latent to guide the rectified flow models for precise generation of the visual scene. The semantic text description and coding latent are both encoded and transmitted to the decoder at a very small bit rate. Experimental results demonstrate that our method can achieve the same image reconstruction quality and vision analysis accuracy as existing methods while using much less bandwidth. The code will be released upon paper acceptance.",
    "pdf_url": "https://arxiv.org/pdf/2510.27324v1",
    "github_url": null,
    "published": "2025-10-31T09:49:42+00:00",
    "updated": "2025-10-31T09:49:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27223v1",
    "title": "A Next-Generation Exoplanet Atmospheric Retrieval Framework NEXOTRANS for Emission Spectroscopy: New Constraints and Atmospheric Characterization of WASP-69b Using JWST NIRCam and MIRI Observations",
    "authors": [
      "Deka",
      "Majumdar",
      "Khan"
    ],
    "summary": "Thermal emission spectra provide key insights into the atmospheric composition and especially the temperature structure of an exoplanet. With broader wavelength coverage, sensitivity and higher resolution, JWST has enabled robust constraints on these properties, including detections of photochemical products. This advances the need for retrieval frameworks capable of navigating complex parameter spaces for accurate data interpretation. In this work, we introduce the emission retrieval module of NEXOTRANS, which employs both one- and two-stream radiative transfer approximations and leverages Bayesian and machine learning techniques for retrievals. It also incorporates approximate disequilibrium chemistry models to infer photochemical species like SO2. We applied NEXOTRANS to the JWST NIRCam and MIRI emission observations of WASP-69b, covering the 2-12 microns range. The retrievals place robust constraints on the volume mixing ratios (VMR) of H2O, CO2, CO, CH4, and potential SO2. The best-fit model, i.e, free chemistry combined with non-uniform aerosol coverage, yields a log(VMR) = -3.78 (+0.15/-0.17) for H2O and -5.77 (+0.09/-0.10) for CO2 which has a sharp absorption at 4.3 micron. The second best-fit model, the hybrid equilibrium chemistry (utilizing equilibrium chemistry-grids) combined with non-uniform aerosol yields a C/O of 0.42 (+0.17/-0.13) and a metallicity of log[M/H] = 1.24 (+0.17/-0.14), corresponding to approximately 17.38 times the solar value. This hybrid chemistry retrieval also constrain SO2 with a log(VMR) = -4.85 (+0.28/-0.29), indicating possible absorption features in the 7-8 microns range. These results highlight NEXOTRANS's capability to significantly advance JWST emission spectra interpretation, offering broader insights into exoplanetary atmospheres.",
    "pdf_url": "https://arxiv.org/pdf/2510.27223v1",
    "github_url": null,
    "published": "2025-10-31T06:36:49+00:00",
    "updated": "2025-10-31T06:36:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27210v1",
    "title": "GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation",
    "authors": [
      "Liu",
      "Wang",
      "Li"
    ],
    "summary": "While Multimodal Large Language Models (MLLMs) have advanced GUI navigation agents, current approaches face limitations in cross-domain generalization and effective history utilization. We present a reasoning-enhanced framework that systematically integrates structured reasoning, action prediction, and history summarization. The structured reasoning component generates coherent Chain-of-Thought analyses combining progress estimation and decision reasoning, which inform both immediate action predictions and compact history summaries for future steps. Based on this framework, we train a GUI agent, \\textbf{GUI-Rise}, through supervised fine-tuning on pseudo-labeled trajectories and reinforcement learning with Group Relative Policy Optimization (GRPO). This framework employs specialized rewards, including a history-aware objective, directly linking summary quality to subsequent action performance. Comprehensive evaluations on standard benchmarks demonstrate state-of-the-art results under identical training data conditions, with particularly strong performance in out-of-domain scenarios. These findings validate our framework's ability to maintain robust reasoning and generalization across diverse GUI navigation tasks. Code is available at https://leon022.github.io/GUI-Rise.",
    "pdf_url": "https://arxiv.org/pdf/2510.27210v1",
    "github_url": null,
    "published": "2025-10-31T06:10:57+00:00",
    "updated": "2025-10-31T06:10:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27157v1",
    "title": "A Survey on Generative Recommendation: Data, Model, and Tasks",
    "authors": [
      "Hou",
      "Wu",
      "Liao"
    ],
    "summary": "Recommender systems serve as foundational infrastructure in modern information ecosystems, helping users navigate digital content and discover items aligned with their preferences. At their core, recommender systems address a fundamental problem: matching users with items. Over the past decades, the field has experienced successive paradigm shifts, from collaborative filtering and matrix factorization in the machine learning era to neural architectures in the deep learning era. Recently, the emergence of generative models, especially large language models (LLMs) and diffusion models, have sparked a new paradigm: generative recommendation, which reconceptualizes recommendation as a generation task rather than discriminative scoring. This survey provides a comprehensive examination through a unified tripartite framework spanning data, model, and task dimensions. Rather than simply categorizing works, we systematically decompose approaches into operational stages-data augmentation and unification, model alignment and training, task formulation and execution. At the data level, generative models enable knowledge-infused augmentation and agent-based simulation while unifying heterogeneous signals. At the model level, we taxonomize LLM-based methods, large recommendation models, and diffusion approaches, analyzing their alignment mechanisms and innovations. At the task level, we illuminate new capabilities including conversational interaction, explainable reasoning, and personalized content generation. We identify five key advantages: world knowledge integration, natural language understanding, reasoning capabilities, scaling laws, and creative generation. We critically examine challenges in benchmark design, model robustness, and deployment efficiency, while charting a roadmap toward intelligent recommendation assistants that fundamentally reshape human-information interaction.",
    "pdf_url": "https://arxiv.org/pdf/2510.27157v1",
    "github_url": null,
    "published": "2025-10-31T04:02:58+00:00",
    "updated": "2025-10-31T04:02:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.02097v2",
    "title": "A Step Toward World Models: A Survey on Robotic Manipulation",
    "authors": [
      "Zhang",
      "Cheng",
      "Sun"
    ],
    "summary": "Autonomous agents are increasingly expected to operate in complex, dynamic, and uncertain environments, performing tasks such as manipulation, navigation, and decision-making. Achieving these capabilities requires agents to understand the underlying mechanisms and dynamics of the world, moving beyond reactive control or simple replication of observed states. This motivates the development of world models as internal representations that encode environmental states, capture dynamics, and support prediction, planning, and reasoning. Despite growing interest, the definition, scope, architectures, and essential capabilities of world models remain ambiguous. In this survey, we go beyond prescribing a fixed definition and limiting our scope to methods explicitly labeled as world models. Instead, we examine approaches that exhibit the core capabilities of world models through a review of methods in robotic manipulation. We analyze their roles across perception, prediction, and control, identify key challenges and solutions, and distill the core components, capabilities, and functions that a fully realized world model should possess. Building on this analysis, we aim to motivate further development toward generalizable and practical world models for robotics.",
    "pdf_url": "https://arxiv.org/pdf/2511.02097v2",
    "github_url": null,
    "published": "2025-10-31T00:57:24+00:00",
    "updated": "2025-11-10T03:45:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27051v1",
    "title": "Adaptive Data Flywheel: Applying MAPE Control Loops to AI Agent Improvement",
    "authors": [
      "Shukla",
      "Knowles",
      "Madugula"
    ],
    "summary": "Enterprise AI agents must continuously adapt to maintain accuracy, reduce latency, and remain aligned with user needs. We present a practical implementation of a data flywheel in NVInfo AI, NVIDIA's Mixture-of-Experts (MoE) Knowledge Assistant serving over 30,000 employees. By operationalizing a MAPE-driven data flywheel, we built a closed-loop system that systematically addresses failures in retrieval-augmented generation (RAG) pipelines and enables continuous learning. Over a 3-month post-deployment period, we monitored feedback and collected 495 negative samples. Analysis revealed two major failure modes: routing errors (5.25\\%) and query rephrasal errors (3.2\\%). Using NVIDIA NeMo microservices, we implemented targeted improvements through fine-tuning. For routing, we replaced a Llama 3.1 70B model with a fine-tuned 8B variant, achieving 96\\% accuracy, a 10x reduction in model size, and 70\\% latency improvement. For query rephrasal, fine-tuning yielded a 3.7\\% gain in accuracy and a 40\\% latency reduction. Our approach demonstrates how human-in-the-loop (HITL) feedback, when structured within a data flywheel, transforms enterprise AI agents into self-improving systems. Key learnings include approaches to ensure agent robustness despite limited user feedback, navigating privacy constraints, and executing staged rollouts in production. This work offers a repeatable blueprint for building robust, adaptive enterprise AI agents capable of learning from real-world usage at scale.",
    "pdf_url": "https://arxiv.org/pdf/2510.27051v1",
    "github_url": null,
    "published": "2025-10-30T23:41:06+00:00",
    "updated": "2025-10-30T23:41:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.27010v1",
    "title": "A Hermetic, Transparent Soft Growing Vine Robot System for Pipe Inspection",
    "authors": [
      "Heap",
      "Qin",
      "Hammond"
    ],
    "summary": "Rehabilitation of aging pipes requires accurate condition assessment and mapping far into the pipe interiors. Soft growing vine robot systems are particularly promising for navigating confined, sinuous paths such as in pipes, but are currently limited by complex subsystems and a lack of validation in real-world industrial settings. In this paper, we introduce the concept and implementation of a hermetic and transparent vine robot system for visual condition assessment and mapping within non-branching pipes. This design encloses all mechanical and electrical components within the vine robot's soft, airtight, and transparent body, protecting them from environmental interference while enabling visual sensing. Because this approach requires an enclosed mechanism for transporting sensors, we developed, modeled, and tested a passively adapting enclosed tip mount. Finally, we validated the hermetic and transparent vine robot system concept through a real-world condition assessment and mapping task in a wastewater pipe. This work advances the use of soft-growing vine robots in pipe inspection by developing and demonstrating a robust, streamlined, field-validated system suitable for continued development and deployment.",
    "pdf_url": "https://arxiv.org/pdf/2510.27010v1",
    "github_url": null,
    "published": "2025-10-30T21:23:12+00:00",
    "updated": "2025-10-30T21:23:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26997v1",
    "title": "Gradient Descent as Loss Landscape Navigation: a Normative Framework for Deriving Learning Rules",
    "authors": [
      "Vastola",
      "Gershman",
      "Rajan"
    ],
    "summary": "Learning rules -- prescriptions for updating model parameters to improve performance -- are typically assumed rather than derived. Why do some learning rules work better than others, and under what assumptions can a given rule be considered optimal? We propose a theoretical framework that casts learning rules as policies for navigating (partially observable) loss landscapes, and identifies optimal rules as solutions to an associated optimal control problem. A range of well-known rules emerge naturally within this framework under different assumptions: gradient descent from short-horizon optimization, momentum from longer-horizon planning, natural gradients from accounting for parameter space geometry, non-gradient rules from partial controllability, and adaptive optimizers like Adam from online Bayesian inference of loss landscape shape. We further show that continual learning strategies like weight resetting can be understood as optimal responses to task uncertainty. By unifying these phenomena under a single objective, our framework clarifies the computational structure of learning and offers a principled foundation for designing adaptive algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2510.26997v1",
    "github_url": null,
    "published": "2025-10-30T20:56:35+00:00",
    "updated": "2025-10-30T20:56:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26988v1",
    "title": "Inferring the Chemotaxis Distortion Function from Cellular Decision Strategies",
    "authors": [
      "Vakilipoor",
      "Konrad",
      "Schäfer"
    ],
    "summary": "Cellular intelligence enables cells to process environmental signals and make context-dependent decisions, as exemplified by chemotaxis, where cells navigate chemical gradients despite noisy signaling pathways. To investigate how cells deal with uncertainty, we apply an information-theoretic framework based on rate distortion theory (RDT). The Blahut-Arimoto algorithm (BAA) computes optimal decision strategies that minimize mutual information while satisfying distortion constraints, balancing sensing accuracy with distortion constraint equivalent to resource cost. We propose the inverse Blahut-Arimoto algorithm (IBAA) to compute the distortion function, which quantifies the system's decision-making criteria for realizing a decision strategy to map input signals to outputs. This general framework extends beyond chemotaxis to biological and engineered systems requiring efficient information processing under uncertainty. We validate the proposed IBAA by accurately estimating theoretical distortion functions in a cellular apoptosis scenario. Additionally, using the local excitation global inhibition (LEGI) model to simulate chemotactic responses, we compute the distortion functions from the cell's perspective. Our finding reveals a state-dependent decision criteria by the cell.",
    "pdf_url": "https://arxiv.org/pdf/2510.26988v1",
    "github_url": null,
    "published": "2025-10-30T20:31:20+00:00",
    "updated": "2025-10-30T20:31:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26948v1",
    "title": "Cooperative Integrated Estimation-Guidance for Simultaneous Interception of Moving Targets",
    "authors": [
      "Gopikannan",
      "Kumar",
      "Sinha"
    ],
    "summary": "This paper proposes a cooperative integrated estimation-guidance framework for simultaneous interception of a non-maneuvering target using a team of unmanned autonomous vehicles, assuming only a subset of vehicles are equipped with dedicated sensors to measure the target's states. Unlike earlier approaches that focus solely on either estimation or guidance design, the proposed framework unifies both within a cooperative architecture. To circumvent the limitation posed by heterogeneity in target observability, sensorless vehicles estimate the target's state by leveraging information exchanged with neighboring agents over a directed communication topology through a prescribed-time observer. The proposed approach employs true proportional navigation guidance (TPNG), which uses an exact time-to-go formulation and is applicable across a wide spectrum of target motions. Furthermore, prescribed-time observer and controller are employed to achieve convergence to true target's state and consensus in time-to-go within set predefined times, respectively. Simulations demonstrate the effectiveness of the proposed framework under various engagement scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2510.26948v1",
    "github_url": null,
    "published": "2025-10-30T19:03:00+00:00",
    "updated": "2025-10-30T19:03:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26909v2",
    "title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models",
    "authors": [
      "Windecker",
      "Patel",
      "Reuss"
    ],
    "summary": "Vision-language models demonstrate unprecedented performance and generalization across a wide range of tasks and scenarios. Integrating these foundation models into robotic navigation systems opens pathways toward building general-purpose robots. Yet, evaluating these models' navigation capabilities remains constrained by costly real-world trials, overly simplified simulations, and limited benchmarks. We introduce NaviTrace, a high-quality Visual Question Answering benchmark where a model receives an instruction and embodiment type (human, legged robot, wheeled robot, bicycle) and must output a 2D navigation trace in image space. Across 1000 scenarios and more than 3000 expert traces, we systematically evaluate eight state-of-the-art VLMs using a newly introduced semantic-aware trace score. This metric combines Dynamic Time Warping distance, goal endpoint error, and embodiment-conditioned penalties derived from per-pixel semantics and correlates with human preferences. Our evaluation reveals consistent gap to human performance caused by poor spatial grounding and goal localization. NaviTrace establishes a scalable and reproducible benchmark for real-world robotic navigation. The benchmark and leaderboard can be found at https://leggedrobotics.github.io/navitrace_webpage/.",
    "pdf_url": "https://arxiv.org/pdf/2510.26909v2",
    "github_url": null,
    "published": "2025-10-30T18:16:32+00:00",
    "updated": "2025-11-04T21:17:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26900v1",
    "title": "Design for One, Deploy for Many: Navigating Tree Mazes with Multiple Agents",
    "authors": [
      "Argote-Gerald",
      "Miyauchi",
      "Rau"
    ],
    "summary": "Maze-like environments, such as cave and pipe networks, pose unique challenges for multiple robots to coordinate, including communication constraints and congestion. To address these challenges, we propose a distributed multi-agent maze traversal algorithm for environments that can be represented by acyclic graphs. It uses a leader-switching mechanism where one agent, assuming a head role, employs any single-agent maze solver while the other agents each choose an agent to follow. The head role gets transferred to neighboring agents where necessary, ensuring it follows the same path as a single agent would. The multi-agent maze traversal algorithm is evaluated in simulations with groups of up to 300 agents, various maze sizes, and multiple single-agent maze solvers. It is compared against strategies that are naïve, or assume either global communication or full knowledge of the environment. The algorithm outperforms the naïve strategy in terms of makespan and sum-of-fuel. It is superior to the global-communication strategy in terms of makespan but is inferior to it in terms of sum-of-fuel. The findings suggest it is asymptotically equivalent to the full-knowledge strategy with respect to either metric. Moreover, real-world experiments with up to 20 Pi-puck robots confirm the feasibility of the approach.",
    "pdf_url": "https://arxiv.org/pdf/2510.26900v1",
    "github_url": null,
    "published": "2025-10-30T18:06:35+00:00",
    "updated": "2025-10-30T18:06:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26787v1",
    "title": "Remote Labor Index: Measuring AI Automation of Remote Work",
    "authors": [
      "Mazeika",
      "Gatti",
      "Menghini"
    ],
    "summary": "AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.",
    "pdf_url": "https://arxiv.org/pdf/2510.26787v1",
    "github_url": null,
    "published": "2025-10-30T17:58:04+00:00",
    "updated": "2025-10-30T17:58:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26782v2",
    "title": "Clone Deterministic 3D Worlds",
    "authors": [
      "Xia",
      "Lu",
      "Li"
    ],
    "summary": "A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future physical state of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. However, existing world models often focus on random generation of open worlds, but neglect the need for high-fidelity modeling of deterministic scenarios (such as fixed-map mazes and static space robot navigation). In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone a deterministic 3D world. 1) Through diagnostic experiment, we quantitatively demonstrate that high-fidelity cloning is feasible and the primary bottleneck for long-horizon fidelity is the geometric structure of the latent representation, not the dynamics model itself. 2) Building on this insight, we show that applying temporal contrastive learning principle as a geometric regularization can effectively curate a latent space that better reflects the underlying physical state manifold, demonstrating that contrastive constraints can serve as a powerful inductive bias for stable world modeling; we call this approach Geometrically-Regularized World Models (GRWM). At its core is a lightweight geometric regularization module that can be seamlessly integrated into standard autoencoders, reshaping their latent space to provide a stable foundation for effective dynamics modeling. By focusing on representation quality, GRWM offers a simple yet powerful pipeline for improving world model fidelity.",
    "pdf_url": "https://arxiv.org/pdf/2510.26782v2",
    "github_url": null,
    "published": "2025-10-30T17:56:43+00:00",
    "updated": "2025-11-18T04:52:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26653v1",
    "title": "Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2",
    "authors": [
      "Martin",
      "Gallego"
    ],
    "summary": "Accurate estimation of sea ice drift is critical for Arctic navigation, climate research, and operational forecasting. While optical flow, a computer vision technique for estimating pixel wise motion between consecutive images, has advanced rapidly in computer vision, its applicability to geophysical problems and to satellite SAR imagery remains underexplored. Classical optical flow methods rely on mathematical models and strong assumptions about motion, which limit their accuracy in complex scenarios. Recent deep learning based approaches have substantially improved performance and are now the standard in computer vision, motivating their application to sea ice drift estimation. We present the first large scale benchmark of 48 deep learning optical flow models on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the spatial scales of sea ice motion and typical navigation requirements in the Arctic. Our results demonstrate that the models are capable of capturing consistent regional drift patterns and that recent deep learning based optical flow methods, which have substantially improved motion estimation accuracy compared to classical methods, can be effectively transferred to polar remote sensing. Optical flow produces spatially continuous drift fields, providing motion estimates for every image pixel rather than at sparse buoy locations, offering new opportunities for navigation and climate modeling.",
    "pdf_url": "https://arxiv.org/pdf/2510.26653v1",
    "github_url": null,
    "published": "2025-10-30T16:20:28+00:00",
    "updated": "2025-10-30T16:20:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26646v1",
    "title": "Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments",
    "authors": [
      "He",
      "Chen",
      "Zhang"
    ],
    "summary": "This paper presents a hierarchical path-planning and control framework that combines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with a low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller for continuous actuation. The high-level module selects behaviors and sub-goals; the low-level module executes smooth velocity commands. We design a practical reward shaping scheme (direction, distance, obstacle avoidance, action smoothness, collision penalty, time penalty, and progress), together with a LiDAR-based safety gate that prevents unsafe motions. The system is implemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics, including success rate, collision rate, path efficiency, and re-planning efficiency, in dynamic and partially observable environments. Experiments show improved success rate and sample efficiency over single-algorithm baselines (DQN or TD3 alone) and rule-based planners, with better generalization to unseen obstacle configurations and reduced abrupt control changes. Code and evaluation scripts are available at the project repository.",
    "pdf_url": "https://arxiv.org/pdf/2510.26646v1",
    "github_url": null,
    "published": "2025-10-30T16:12:01+00:00",
    "updated": "2025-10-30T16:12:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26588v1",
    "title": "FLYINGTRUST: A Benchmark for Quadrotor Navigation Across Scenarios and Vehicles",
    "authors": [
      "Li",
      "Zhai",
      "Wang"
    ],
    "summary": "Visual navigation algorithms for quadrotors often exhibit a large variation in performance when transferred across different vehicle platforms and scene geometries, which increases the cost and risk of field deployment. To support systematic early-stage evaluation, we introduce FLYINGTRUST, a high-fidelity, configurable benchmarking framework that measures how platform kinodynamics and scenario structure jointly affect navigation robustness. FLYINGTRUST models vehicle capability with two compact, physically interpretable indicators: maximum thrust-to-weight ratio and axis-wise maximum angular acceleration. The benchmark pairs a diverse scenario library with a heterogeneous set of real and virtual platforms and prescribes a standardized evaluation protocol together with a composite scoring method that balances scenario importance, platform importance and performance stability. We use FLYINGTRUST to compare representative optimization-based and learning-based navigation approaches under identical conditions, performing repeated trials per platform-scenario combination and reporting uncertainty-aware metrics. The results reveal systematic patterns: navigation success depends predictably on platform capability and scene geometry, and different algorithms exhibit distinct preferences and failure modes across the evaluated conditions. These observations highlight the practical necessity of incorporating both platform capability and scenario structure into algorithm design, evaluation, and selection, and they motivate future work on methods that remain robust across diverse platforms and scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2510.26588v1",
    "github_url": null,
    "published": "2025-10-30T15:14:18+00:00",
    "updated": "2025-10-30T15:14:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26463v1",
    "title": "MIREDO: MIP-Driven Resource-Efficient Dataflow Optimization for Computing-in-Memory Accelerator",
    "authors": [
      "He",
      "Duan",
      "Qi"
    ],
    "summary": "Computing-in-Memory (CIM) architectures have emerged as a promising solution for accelerating Deep Neural Networks (DNNs) by mitigating data movement bottlenecks. However, realizing the potential of CIM requires specialized dataflow optimizations, which are challenged by an expansive design space and strict architectural constraints. Existing optimization approaches often fail to fully exploit CIM accelerators, leading to noticeable gaps between theoretical and actual system-level efficiency. To address these limitations, we propose the MIREDO framework, which formulates dataflow optimization as a Mixed-Integer Programming (MIP) problem. MIREDO introduces a hierarchical hardware abstraction coupled with an analytical latency model designed to accurately reflect the complex data transfer behaviors within CIM systems. By jointly modeling workload characteristics, dataflow strategies, and CIM-specific constraints, MIREDO systematically navigates the vast design space to determine the optimal dataflow configurations. Evaluation results demonstrate that MIREDO significantly enhances performance, achieving up to $3.2\\times$ improvement across various DNN models and hardware setups.",
    "pdf_url": "https://arxiv.org/pdf/2510.26463v1",
    "github_url": null,
    "published": "2025-10-30T13:09:00+00:00",
    "updated": "2025-10-30T13:09:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26407v1",
    "title": "Barlow Twins for Sequential Recommendation",
    "authors": [
      "Razvorotnev",
      "Munkhoeva",
      "Frolov"
    ],
    "summary": "Sequential recommendation models must navigate sparse interaction data popularity bias and conflicting objectives like accuracy versus diversity While recent contrastive selfsupervised learning SSL methods offer improved accuracy they come with tradeoffs large batch requirements reliance on handcrafted augmentations and negative sampling that can reinforce popularity bias In this paper we introduce BT-SR a novel noncontrastive SSL framework that integrates the Barlow Twins redundancyreduction principle into a Transformerbased nextitem recommender BTSR learns embeddings that align users with similar shortterm behaviors while preserving longterm distinctionswithout requiring negative sampling or artificial perturbations This structuresensitive alignment allows BT-SR to more effectively recognize emerging user intent and mitigate the influence of noisy historical context Our experiments on five public benchmarks demonstrate that BTSR consistently improves nextitem prediction accuracy and significantly enhances longtail item coverage and recommendation calibration Crucially we show that a single hyperparameter can control the accuracydiversity tradeoff enabling practitioners to adapt recommendations to specific application needs",
    "pdf_url": "https://arxiv.org/pdf/2510.26407v1",
    "github_url": null,
    "published": "2025-10-30T11:56:02+00:00",
    "updated": "2025-10-30T11:56:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26396v1",
    "title": "A Pragmatic View of AI Personhood",
    "authors": [
      "Leibo",
      "Vezhnevets",
      "Cunningham"
    ],
    "summary": "The emergence of agentic Artificial Intelligence (AI) is set to trigger a \"Cambrian explosion\" of new kinds of personhood. This paper proposes a pragmatic framework for navigating this diversification by treating personhood not as a metaphysical property to be discovered, but as a flexible bundle of obligations (rights and responsibilities) that societies confer upon entities for a variety of reasons, especially to solve concrete governance problems. We argue that this traditional bundle can be unbundled, creating bespoke solutions for different contexts. This will allow for the creation of practical tools -- such as facilitating AI contracting by creating a target \"individual\" that can be sanctioned -- without needing to resolve intractable debates about an AI's consciousness or rationality. We explore how individuals fit in to social roles and discuss the use of decentralized digital identity technology, examining both \"personhood as a problem\", where design choices can create \"dark patterns\" that exploit human social heuristics, and \"personhood as a solution\", where conferring a bundle of obligations is necessary to ensure accountability or prevent conflict. By rejecting foundationalist quests for a single, essential definition of personhood, this paper offers a more pragmatic and flexible way to think about integrating AI agents into our society.",
    "pdf_url": "https://arxiv.org/pdf/2510.26396v1",
    "github_url": null,
    "published": "2025-10-30T11:36:34+00:00",
    "updated": "2025-10-30T11:36:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26347v1",
    "title": "Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle",
    "authors": [
      "Zieglmeier",
      "Erdmann",
      "Warakagoda"
    ],
    "summary": "Reinforcement learning (RL) algorithms are designed to optimize problem-solving by learning actions that maximize rewards, a task that becomes particularly challenging in random and nonstationary environments. Even advanced RL algorithms are often limited in their ability to solve problems in these conditions. In applications such as searching for underwater pollution clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate reward-sparse environments, where actions frequently result in a zero reward. This paper aims to address these challenges by revisiting and modifying classical RL approaches to efficiently operate in sparse, randomized, and nonstationary environments. We systematically study a large number of modifications, including hierarchical algorithm changes, multigoal learning, and the integration of a location memory as an external output filter to prevent state revisits. Our results demonstrate that a modified Monte Carlo-based approach significantly outperforms traditional Q-learning and two exhaustive search patterns, illustrating its potential in adapting RL to complex environments. These findings suggest that reinforcement learning approaches can be effectively adapted for use in random, nonstationary, and reward-sparse environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.26347v1",
    "github_url": null,
    "published": "2025-10-30T10:55:05+00:00",
    "updated": "2025-10-30T10:55:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26287v1",
    "title": "Empowering RepoQA-Agent based on Reinforcement Learning Driven by Monte-carlo Tree Search",
    "authors": [
      "Li",
      "Liu",
      "Qin"
    ],
    "summary": "Repository-level software engineering tasks require large language models (LLMs) to efficiently navigate and extract information from complex codebases through multi-turn tool interactions. Existing approaches face significant limitations: training-free, in-context learning methods struggle to guide agents effectively in tool utilization and decision-making based on environmental feedback, while training-based approaches typically rely on costly distillation from larger LLMs, introducing data compliance concerns in enterprise environments. To address these challenges, we introduce RepoSearch-R1, a novel agentic reinforcement learning framework driven by Monte-carlo Tree Search (MCTS). This approach allows agents to generate diverse, high-quality reasoning trajectories via self-training without requiring model distillation or external supervision. Based on RepoSearch-R1, we construct a RepoQA-Agent specifically designed for repository question-answering tasks. Comprehensive evaluation on repository question-answering tasks demonstrates that RepoSearch-R1 achieves substantial improvements of answer completeness: 16.0% enhancement over no-retrieval methods, 19.5% improvement over iterative retrieval methods, and 33% increase in training efficiency compared to general agentic reinforcement learning approaches. Our cold-start training methodology eliminates data compliance concerns while maintaining robust exploration diversity and answer completeness across repository-level reasoning tasks.",
    "pdf_url": "https://arxiv.org/pdf/2510.26287v1",
    "github_url": null,
    "published": "2025-10-30T09:10:36+00:00",
    "updated": "2025-10-30T09:10:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26265v1",
    "title": "Look at That Distractor: Dynamic Translation Gain under Low Perceptual Load in Virtual Reality",
    "authors": [
      "Zou",
      "Tong",
      "Luo"
    ],
    "summary": "Redirected walking utilizes gain adjustments within perceptual thresholds to allow natural navigation in large scale virtual environments within confined physical environments. Previous research has found that when users are distracted by some scene elements, they are less sensitive to gain values. However, the effects on detection thresholds have not been quantitatively measured. In this paper, we present a novel method that dynamically adjusts translation gain by leveraging visual distractors. We place distractors within the user's field of view and apply a larger translation gain when their attention is drawn to them. Because the magnitude of gain adjustment depends on the user's level of engagement with the distractors, the redirection process remains smooth and unobtrusive. To evaluate our method, we developed a task oriented virtual environment for a user study. Results show that introducing distractors in the virtual environment significantly raises users' translation gain thresholds. Furthermore, assessments using the Simulator Sickness Questionnaire and Igroup Presence Questionnaire indicate that the method maintains user comfort and acceptance, supporting its effectiveness for RDW systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.26265v1",
    "github_url": null,
    "published": "2025-10-30T08:45:02+00:00",
    "updated": "2025-10-30T08:45:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26132v1",
    "title": "Embodied Intelligence for Advanced Bioinspired Microrobotics: Examples and Insights",
    "authors": [
      "Perez-Arancibia"
    ],
    "summary": "The term embodied intelligence (EI) conveys the notion that body morphology, material properties, interaction with the environment, and control strategies can be purposefully integrated into the process of robotic design to generate intelligent behavior; in particular, locomotion and navigation. In this paper, we discuss EI as a design principle for advanced microrobotics, with a particular focus on co-design -- the simultaneous and interdependent development of physical structure and behavioral function. To illustrate the contrast between EI-inspired systems and traditional architectures that decouple sensing, computation, and actuation, we present and discuss a collection of robots developed by the author and his team at the Autonomous Microrobotic Systems Laboratory (AMSL). These robots exhibit intelligent behavior that emerges from their structural dynamics and the physical interaction between their components and with the environment. Platforms such as the Bee++, RoBeetle, SMALLBug, SMARTI, WaterStrider, VLEIBot+, and FRISSHBot exemplify how feedback loops, decision logics, sensing mechanisms, and smart actuation strategies can be embedded into the physical properties of the robotic system itself. Along these lines, we contend that co-design is not only a method for empirical optimization under constraints, but also an enabler of EI, offering a scalable and robust alternative to classical control for robotics at the mm-to-cm-scale.",
    "pdf_url": "https://arxiv.org/pdf/2510.26132v1",
    "github_url": null,
    "published": "2025-10-30T04:40:58+00:00",
    "updated": "2025-10-30T04:40:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26089v1",
    "title": "Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing",
    "authors": [
      "Arasteh",
      "Haghparast",
      "Papagelis"
    ],
    "summary": "Traffic congestion in urban road networks leads to longer trip times and higher emissions, especially during peak periods. While the Shortest Path First (SPF) algorithm is optimal for a single vehicle in a static network, it performs poorly in dynamic, multi-vehicle settings, often worsening congestion by routing all vehicles along identical paths. We address dynamic vehicle routing through a multi-agent reinforcement learning (MARL) framework for coordinated, network-aware fleet navigation. We first propose Adaptive Navigation (AN), a decentralized MARL model where each intersection agent provides routing guidance based on (i) local traffic and (ii) neighborhood state modeled using Graph Attention Networks (GAT). To improve scalability in large networks, we further propose Hierarchical Hub-based Adaptive Navigation (HHAN), an extension of AN that assigns agents only to key intersections (hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles micro-routing within each hub region. For hub coordination, HHAN adopts centralized training with decentralized execution (CTDE) under the Attentive Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions via attention. Hub agents use flow-aware state features that combine local congestion and predictive dynamics for proactive routing. Experiments on synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces average travel time versus SPF and learning baselines, maintaining 100% routing success. HHAN scales to networks with hundreds of intersections, achieving up to 15.9% improvement under heavy traffic. These findings highlight the potential of network-constrained MARL for scalable, coordinated, and congestion-aware routing in intelligent transportation systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.26089v1",
    "github_url": null,
    "published": "2025-10-30T02:49:46+00:00",
    "updated": "2025-10-30T02:49:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26040v1",
    "title": "Accelerating Real-World Overtaking in F1TENTH Racing Employing Reinforcement Learning Methods",
    "authors": [
      "Steiner",
      "Spuy",
      "Zhou"
    ],
    "summary": "While autonomous racing performance in Time-Trial scenarios has seen significant progress and development, autonomous wheel-to-wheel racing and overtaking are still severely limited. These limitations are particularly apparent in real-life driving scenarios where state-of-the-art algorithms struggle to safely or reliably complete overtaking manoeuvres. This is important, as reliable navigation around other vehicles is vital for safe autonomous wheel-to-wheel racing. The F1Tenth Competition provides a useful opportunity for developing wheel-to-wheel racing algorithms on a standardised physical platform. The competition format makes it possible to evaluate overtaking and wheel-to-wheel racing algorithms against the state-of-the-art. This research presents a novel racing and overtaking agent capable of learning to reliably navigate a track and overtake opponents in both simulation and reality. The agent was deployed on an F1Tenth vehicle and competed against opponents running varying competitive algorithms in the real world. The results demonstrate that the agent's training against opponents enables deliberate overtaking behaviours with an overtaking rate of 87% compared 56% for an agent trained just to race.",
    "pdf_url": "https://arxiv.org/pdf/2510.26040v1",
    "github_url": null,
    "published": "2025-10-30T00:38:18+00:00",
    "updated": "2025-10-30T00:38:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.26023v2",
    "title": "Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization",
    "authors": [
      "Bao",
      "Li"
    ],
    "summary": "Despite significant advancements in recent decades, autonomous vehicles (AVs) continue to face challenges in navigating certain traffic scenarios where human drivers excel. In such situations, AVs often become immobilized, disrupting overall traffic flow. Current recovery solutions, such as remote intervention (which is costly and inefficient) and manual takeover (which excludes non-drivers and limits AV accessibility), are inadequate. This paper introduces StuckSolver, a novel Large Language Model (LLM) driven recovery framework that enables AVs to resolve immobilization scenarios through self-reasoning and/or passenger-guided decision-making. StuckSolver is designed as a plug-in add-on module that operates on top of the AV's existing perception-planning-control stack, requiring no modification to its internal architecture. Instead, it interfaces with standard sensor data streams to detect immobilization states, interpret environmental context, and generate high-level recovery commands that can be executed by the AV's native planner. We evaluate StuckSolver on the Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results show that StuckSolver achieves near-state-of-the-art performance through autonomous self-reasoning alone and exhibits further improvements when passenger guidance is incorporated.",
    "pdf_url": "https://arxiv.org/pdf/2510.26023v2",
    "github_url": null,
    "published": "2025-10-29T23:33:31+00:00",
    "updated": "2025-11-14T15:46:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25978v1",
    "title": "On the Go with AR: Attention to Virtual and Physical Targets while Varying Augmentation Density",
    "authors": [
      "Kim",
      "Kumaran",
      "Luo"
    ],
    "summary": "Augmented reality is projected to be a primary mode of information consumption on the go, seamlessly integrating virtual content into the physical world. However, the potential perceptual demands of viewing virtual annotations while navigating a physical environment could impact user efficacy and safety, and the implications of these demands are not well understood. Here, we investigate the impact of virtual path guidance and augmentation density (visual clutter) on search performance and memory. Participants walked along a predefined path, searching for physical or virtual items. They experienced two levels of augmentation density, and either walked freely or with enforced speed and path guidance. Augmentation density impacted behavior and reduced awareness of uncommon objects in the environment. Analysis of search task performance and post-experiment item recall revealed differing attention to physical and virtual objects. On the basis of these findings we outline considerations for AR apps designed for use on the go.",
    "pdf_url": "https://arxiv.org/pdf/2510.25978v1",
    "github_url": null,
    "published": "2025-10-29T21:22:47+00:00",
    "updated": "2025-10-29T21:22:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25957v1",
    "title": "The Impact of Navigation Aids on Search Performance and Object Recall in Wide-Area Augmented Reality",
    "authors": [
      "Kumaran",
      "Kim",
      "Milner"
    ],
    "summary": "Head-worn augmented reality (AR) is a hotly pursued and increasingly feasible contender paradigm for replacing or complementing smartphones and watches for continual information consumption. Here, we compare three different AR navigation aids (on-screen compass, on-screen radar and in-world vertical arrows) in a wide-area outdoor user study (n=24) where participants search for hidden virtual target items amongst physical and virtual objects. We analyzed participants' search task performance, movements, eye-gaze, survey responses and object recall. There were two key findings. First, all navigational aids enhanced search performance relative to a control condition, with some benefit and strongest user preference for in-world arrows. Second, users recalled fewer physical objects than virtual objects in the environment, suggesting reduced awareness of the physical environment. Together, these findings suggest that while navigational aids presented in AR can enhance search task performance, users may pay less attention to the physical environment, which could have undesirable side-effects.",
    "pdf_url": "https://arxiv.org/pdf/2510.25957v1",
    "github_url": null,
    "published": "2025-10-29T20:55:30+00:00",
    "updated": "2025-10-29T20:55:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25913v1",
    "title": "Risk-Aware Safety Filters with Poisson Safety Functions and Laplace Guidance Fields",
    "authors": [
      "Bahati",
      "Bena",
      "Wilkinson"
    ],
    "summary": "Robotic systems navigating in real-world settings require a semantic understanding of their environment to properly determine safe actions. This work aims to develop the mathematical underpinnings of such a representation -- specifically, the goal is to develop safety filters that are risk-aware. To this end, we take a two step approach: encoding an understanding of the environment via Poisson's equation, and associated risk via Laplace guidance fields. That is, we first solve a Dirichlet problem for Poisson's equation to generate a safety function that encodes system safety as its 0-superlevel set. We then separately solve a Dirichlet problem for Laplace's equation to synthesize a safe \\textit{guidance field} that encodes variable levels of caution around obstacles -- by enforcing a tunable flux boundary condition. The safety function and guidance fields are then combined to define a safety constraint and used to synthesize a risk-aware safety filter which, given a semantic understanding of an environment with associated risk levels of environmental features, guarantees safety while prioritizing avoidance of higher risk obstacles. We demonstrate this method in simulation and discuss how \\textit{a priori} understandings of obstacle risk can be directly incorporated into the safety filter to generate safe behaviors that are risk-aware.",
    "pdf_url": "https://arxiv.org/pdf/2510.25913v1",
    "github_url": null,
    "published": "2025-10-29T19:33:26+00:00",
    "updated": "2025-10-29T19:33:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25760v2",
    "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks",
    "authors": [
      "Zheng",
      "Dongfang",
      "Jiang"
    ],
    "summary": "Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.",
    "pdf_url": "https://arxiv.org/pdf/2510.25760v2",
    "github_url": "https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning",
    "published": "2025-10-29T17:55:43+00:00",
    "updated": "2025-11-02T09:49:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25727v1",
    "title": "Modeling Collapse of Steered Vine Robots Under Their Own Weight",
    "authors": [
      "McFarland",
      "McGuinness"
    ],
    "summary": "Soft, vine-inspired growing robots that move by eversion are highly mobile in confined environments, but, when faced with gaps in the environment, they may collapse under their own weight while navigating a desired path. In this work, we present a comprehensive collapse model that can predict the collapse length of steered robots in any shape using true shape information and tail tension. We validate this model by collapsing several unsteered robots without true shape information. The model accurately predicts the trends of those experiments. We then attempt to collapse a robot steered with a single actuator at different orientations. Our models accurately predict collapse when it occurs. Finally, we demonstrate how this could be used in the field by having a robot attempt a gap-crossing task with and without inflating its actuators. The robot needs its actuators inflated to cross the gap without collapsing, which our model supports. Our model has been specifically tested on straight and series pouch motor-actuated robots made of non-stretchable material, but it could be applied to other robot variations. This work enables us to model the robot's collapse behavior in any open environment and understand the parameters it needs to succeed in 3D navigation tasks.",
    "pdf_url": "https://arxiv.org/pdf/2510.25727v1",
    "github_url": null,
    "published": "2025-10-29T17:33:16+00:00",
    "updated": "2025-10-29T17:33:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25718v1",
    "title": "Retrieval-Augmented Search for Large-Scale Map Collections with ColPali",
    "authors": [
      "Mahowald",
      "Lee"
    ],
    "summary": "Multimodal approaches have shown great promise for searching and navigating digital collections held by libraries, archives, and museums. In this paper, we introduce map-RAS: a retrieval-augmented search system for historic maps. In addition to introducing our framework, we detail our publicly-hosted demo for searching 101,233 map images held by the Library of Congress. With our system, users can multimodally query the map collection via ColPali, summarize search results using Llama 3.2, and upload their own collections to perform inter-collection search. We articulate potential use cases for archivists, curators, and end-users, as well as future work with our system in both machine learning and the digital humanities. Our demo can be viewed at: http://www.mapras.com.",
    "pdf_url": "https://arxiv.org/pdf/2510.25718v1",
    "github_url": null,
    "published": "2025-10-29T17:27:21+00:00",
    "updated": "2025-10-29T17:27:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25679v1",
    "title": "Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning",
    "authors": [
      "Tonti",
      "Vinuesa"
    ],
    "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for delivery and surveillance purposes. In this work, we develop an optimal navigation strategy based on Deep Reinforcement Learning. The environment is represented by a three-dimensional high-fidelity simulation of an urban flow, characterized by turbulence and recirculation zones. The algorithm presented here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated Transformer eXtra Large (GTrXL) architecture, giving the agent richer information about the turbulent flow field in which it navigates. The results are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO combined with Long Short Term Memory (LSTM) cells and a traditional navigation algorithm. The obtained results show a significant increase in the success rate (SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the classical Zermelo's navigation algorithm, paving the way to a completely reimagined UAV landscape in complex urban environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.25679v1",
    "github_url": null,
    "published": "2025-10-29T16:46:00+00:00",
    "updated": "2025-10-29T16:46:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25668v1",
    "title": "ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents",
    "authors": [
      "Yang",
      "Ruas",
      "Tian"
    ],
    "summary": "Vision-language models (VLMs) excel at interpreting text-rich images but struggle with long, visually complex documents that demand analysis and integration of information spread across multiple pages. Existing approaches typically rely on fixed reasoning templates or rigid pipelines, which force VLMs into a passive role and hinder both efficiency and generalization. We present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement learning framework that fine-tunes VLMs as interactive agents capable of actively navigating long, visually rich documents. ALDEN introduces a novel fetch action that directly accesses the page by index, complementing the classic search action and better exploiting document structure. For dense process supervision and efficient training, we propose a rule-based cross-level reward that provides both turn- and token-level signals. To address the empirically observed training instability caused by numerous visual tokens from long documents, we further propose a visual-semantic anchoring mechanism that applies a dual-path KL-divergence constraint to stabilize visual and textual representations separately during training. Trained on a corpus constructed from three open-source datasets, ALDEN achieves state-of-the-art performance on five long-document benchmarks. Overall, ALDEN marks a step beyond passive document reading toward agents that autonomously navigate and reason across long, visually rich documents, offering a robust path to more accurate and efficient long-document understanding.",
    "pdf_url": "https://arxiv.org/pdf/2510.25668v1",
    "github_url": null,
    "published": "2025-10-29T16:32:26+00:00",
    "updated": "2025-10-29T16:32:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25656v1",
    "title": "ggtime: A Grammar of Temporal Graphics",
    "authors": [
      "Huang",
      "O'Hara-Wild",
      "Hyndman"
    ],
    "summary": "Visualizing changes over time is fundamental to learning from the past and anticipating the future. However, temporal semantics can be complicated, and existing visualization tools often struggle to accurately represent these complexities. It is common to use bespoke plot helper functions designed to produce specific graphics, due to the absence of flexible general tools that respect temporal semantics. We address this problem by proposing a grammar of temporal graphics, and an associated software implementation, 'ggtime', that encodes temporal semantics into a declarative grammar for visualizing temporal data. The grammar introduces new composable elements that support visualization across linear, cyclical, quasi-cyclical, and other granularities; standardization of irregular durations; and alignment of time points across different granularities and time zones. It is designed for interoperability with other semantic variables, allowing navigation across the space of visualizations while preserving temporal semantics.",
    "pdf_url": "https://arxiv.org/pdf/2510.25656v1",
    "github_url": null,
    "published": "2025-10-29T16:21:54+00:00",
    "updated": "2025-10-29T16:21:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.09932v1",
    "title": "Suzume-chan: Your Personal Navigator as an Embodied Information Hub",
    "authors": [
      "Torii",
      "Murakami",
      "Koseki"
    ],
    "summary": "Access to expert knowledge often requires real-time human communication. Digital tools improve access to information but rarely create the sense of connection needed for deep understanding. This study addresses this issue using Social Presence Theory, which explains how a feeling of \"being together\" enhances communication. An \"Embodied Information Hub\" is proposed as a new way to share knowledge through physical and conversational interaction. The prototype, Suzume-chan, is a small, soft AI agent running locally with a language model and retrieval-augmented generation (RAG). It learns from spoken explanations and responds through dialogue, reducing psychological distance and making knowledge sharing warmer and more human-centered.",
    "pdf_url": "https://arxiv.org/pdf/2512.09932v1",
    "github_url": null,
    "published": "2025-10-29T13:29:58+00:00",
    "updated": "2025-10-29T13:29:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25495v1",
    "title": "Reinforcement Learning techniques for the flavor problem in particle physics",
    "authors": [
      "Giarnetti",
      "Meloni"
    ],
    "summary": "This short review discusses recent applications of Reinforcement Learning (RL) techniques to the flavor problem in particle physics. Traditional approaches to fermion masses and mixing often rely on extensions of the Standard Model based on horizontal symmetries, but the vast landscape of possible models makes systematic exploration infeasible. Recent works have shown that RL can efficiently navigate this landscape by constructing models that reproduce observed quark and lepton observables. These approaches demonstrate that RL not only rediscovers models already proposed in the literature but also uncovers new, phenomenologically acceptable solutions.",
    "pdf_url": "https://arxiv.org/pdf/2510.25495v1",
    "github_url": null,
    "published": "2025-10-29T13:19:31+00:00",
    "updated": "2025-10-29T13:19:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25406v2",
    "title": "Dissect-and-Restore: AI-based Code Verification with Transient Refactoring",
    "authors": [
      "Wang",
      "Scazzariello",
      "Alshnakat"
    ],
    "summary": "Formal verification is increasingly recognized as a critical foundation for building reliable software systems. However, the need for specialized expertise to write precise specifications, navigate complex proof obligations, and learn annotations often makes verification an order of magnitude more expensive than implementation. While modern AI systems can recognize patterns in mathematical proofs and interpret natural language, effectively integrating them into the formal verification process remains an open challenge. We present Prometheus, a novel AI-assisted system that facilitates automated code verification with current AI capabilities in conjunction with modular software engineering principles (e.g., modular refactoring). Our approach begins by decomposing complex program logic, such as nested loops, into smaller, verifiable components. Once verified, these components are recomposed to construct a proof of the original program. This decomposition-recomposition workflow is non-trivial. Prometheus addresses this by guiding the proof search through structured decomposition of complex lemmas into smaller, verifiable sub-lemmas. When automated tools are insufficient, users can provide lightweight natural language guidance to steer the proof process effectively. Our evaluation demonstrates that transiently applying modular restructuring to the code substantially improves the AI's effectiveness in verifying individual components. This approach successfully verifies 86% of tasks in our curated dataset, compared to 68% for the baseline. Gains are more pronounced with increasing specification complexity, improving from 30% to 69%, and when integrating proof outlines for complex programs, from 25% to 87%.",
    "pdf_url": "https://arxiv.org/pdf/2510.25406v2",
    "github_url": null,
    "published": "2025-10-29T11:23:50+00:00",
    "updated": "2025-10-30T10:03:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25280v1",
    "title": "Development of Implicit-Explicit Control Based Amphibious Centipede-Type Robot and Evaluation of its Mobile Performance",
    "authors": [
      "Tsunoda",
      "Yamamoto",
      "Ito"
    ],
    "summary": "Multi-legged mobile robots possess high mobility performance in rough terrain environments, stemming from their high postural stability, joint flexibility, and the redundancy provided by multiple legs. In prior research on navigating between different environments such as land and water, the primary strategy employed involves switching to a controller that generates an appropriate gait for the new environment upon entering it. However, designing appropriate gaits for each complex and diverse environment and accurately determining controller switching for each environment is challenging. Therefore, this research develops a centipede-type mobile robot that navigates both aquatic and terrestrial environments with a simple, unified control scheme, based on the implicit-explicit control philosophy and by ingeniously designing the robot's body structure. In this research, we developed the robot featuring flexible joints and left and right legs on each body segment and focused on the leg structure which has extensive contact with the environment. This paper evaluates the locomotion performance on land and water using the three developed leg structures, using the robot's leg slip rate and actuator energy consumption as evaluation metrics. The experimental results confirmed the existence of an appropriate leg structure capable of navigating both aquatic and terrestrial environments under identical control.",
    "pdf_url": "https://arxiv.org/pdf/2510.25280v1",
    "github_url": null,
    "published": "2025-10-29T08:38:05+00:00",
    "updated": "2025-10-29T08:38:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25224v1",
    "title": "ProMediate: A Socio-cognitive framework for evaluating proactive agents in multi-party negotiation",
    "authors": [
      "Liu",
      "Sarrafzadeh",
      "Zhou"
    ],
    "summary": "While Large Language Models (LLMs) are increasingly used in agentic frameworks to assist individual users, there is a growing need for agents that can proactively manage complex, multi-party collaboration. Systematic evaluation methods for such proactive agents remain scarce, limiting progress in developing AI that can effectively support multiple people together. Negotiation offers a demanding testbed for this challenge, requiring socio-cognitive intelligence to navigate conflicting interests between multiple participants and multiple topics and build consensus. Here, we present ProMediate, the first framework for evaluating proactive AI mediator agents in complex, multi-topic, multi-party negotiations. ProMediate consists of two core components: (i) a simulation testbed based on realistic negotiation cases and theory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and ProMediate-Hard), with a plug-and-play proactive AI mediator grounded in socio-cognitive mediation theories, capable of flexibly deciding when and how to intervene; and (ii) a socio-cognitive evaluation framework with a new suite of metrics to measure consensus changes, intervention latency, mediator effectiveness, and intelligence. Together, these components establish a systematic framework for assessing the socio-cognitive intelligence of proactive AI agents in multi-party settings. Our results show that a socially intelligent mediator agent outperforms a generic baseline, via faster, better-targeted interventions. In the ProMediate-Hard setting, our social mediator increases consensus change by 3.6 percentage points compared to the generic baseline (10.65\\% vs 7.01\\%) while being 77\\% faster in response (15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous, theory-grounded testbed to advance the development of proactive, socially intelligent agents.",
    "pdf_url": "https://arxiv.org/pdf/2510.25224v1",
    "github_url": null,
    "published": "2025-10-29T07:00:11+00:00",
    "updated": "2025-10-29T07:00:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25191v1",
    "title": "SoraNav: Adaptive UAV Task-Centric Navigation via Zeroshot VLM Reasoning",
    "authors": [
      "Song",
      "Yadav",
      "Guo"
    ],
    "summary": "Interpreting visual observations and natural language instructions for complex task execution remains a key challenge in robotics and AI. Despite recent advances, language-driven navigation is still difficult, particularly for UAVs in small-scale 3D environments. Existing Vision-Language Navigation (VLN) approaches are mostly designed for ground robots and struggle to generalize to aerial tasks that require full 3D spatial reasoning. The emergence of large Vision-Language Models (VLMs), such as GPT and Claude, enables zero-shot semantic reasoning from visual and textual inputs. However, these models lack spatial grounding and are not directly applicable to navigation. To address these limitations, SoraNav is introduced, an adaptive UAV navigation framework that integrates zero-shot VLM reasoning with geometry-aware decision-making. Geometric priors are incorporated into image annotations to constrain the VLM action space and improve decision quality. A hybrid switching strategy leverages navigation history to alternate between VLM reasoning and geometry-based exploration, mitigating dead-ends and redundant revisits. A PX4-based hardware-software platform, comprising both a digital twin and a physical micro-UAV, enables reproducible evaluation. Experimental results show that in 2.5D scenarios, our method improves Success Rate (SR) by 25.7% and Success weighted by Path Length (SPL) by 17%. In 3D scenarios, it improves SR by 29.5% and SPL by 18.5% relative to the baseline.",
    "pdf_url": "https://arxiv.org/pdf/2510.25191v1",
    "github_url": null,
    "published": "2025-10-29T05:46:29+00:00",
    "updated": "2025-10-29T05:46:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25161v1",
    "title": "Ephemeris and Almanac Design for Lunar Navigation Satellites",
    "authors": [
      "Iiyama",
      "Gao"
    ],
    "summary": "This paper presents almanac and ephemeris message representation for lunar navigation satellites supporting the Lunar Augmented Navigation System (LANS). The proposed method combines osculating orbital elements, Chebyshev polynomials, and Fourier series to efficiently represent lunar satellite trajectories subject to complex perturbations from lunar gravity and third-body effects. For the ephemeris, a hybrid Chebyshev--Fourier formulation improves fitting accuracy over long arcs while maintaining message compactness under the data-size constraint of the LunaNet Interoperability Specification. For the almanac, a compact low-order polynomial and Fourier model is introduced to capture mid-term orbital variations over a 15-day fitting arc. The approach is validated for multiple orbit regimes, including 30-hour, 24-hour, and 12-hour elliptical lunar frozen orbits (ELFOs) and a 6-hour polar orbit. Results show that the proposed framework achieves sub-meter position and sub-millimeter-per-second velocity fitting errors within the 900-bit limit for 6-hour ephemeris arcs, and almanac fitting accuracy sufficient for reliable satellite-visibility identification in warm-start operations.",
    "pdf_url": "https://arxiv.org/pdf/2510.25161v1",
    "github_url": null,
    "published": "2025-10-29T04:33:53+00:00",
    "updated": "2025-10-29T04:33:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.25042v1",
    "title": "Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training",
    "authors": [
      "Wang",
      "Li",
      "Zeng"
    ],
    "summary": "Within the current sphere of deep learning research, despite the extensive application of optimization algorithms such as Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced inadequacy in their capability to address fluctuations in learning efficiency, meet the demands of complex models, and tackle non-convex optimization issues. These challenges primarily arise from the algorithms' limitations in handling complex data structures and models, for instance, difficulties in selecting an appropriate learning rate, avoiding local optima, and navigating through high-dimensional spaces. To address these issues, this paper introduces a novel optimization algorithm named DWMGrad. This algorithm, building on the foundations of traditional methods, incorporates a dynamic guidance mechanism reliant on historical data to dynamically update momentum and learning rates. This allows the optimizer to flexibly adjust its reliance on historical information, adapting to various training scenarios. This strategy not only enables the optimizer to better adapt to changing environments and task complexities but also, as validated through extensive experimentation, demonstrates DWMGrad's ability to achieve faster convergence rates and higher accuracies under a multitude of scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2510.25042v1",
    "github_url": null,
    "published": "2025-10-29T00:03:03+00:00",
    "updated": "2025-10-29T00:03:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24974v2",
    "title": "Conformational Rank Conditioned Committees for Machine Learning-Assisted Directed Evolution",
    "authors": [
      "Adler",
      "Liang",
      "Peng"
    ],
    "summary": "Machine Learning-assisted directed evolution (MLDE) is a powerful tool for efficiently navigating antibody fitness landscapes. Many structure-aware MLDE pipelines rely on a single conformation or a single committee across all conformations, limiting their ability to separate conformational uncertainty from epistemic uncertainty. Here, we introduce a rank -conditioned committee (RCC) framework that leverages ranked conformations to assign a deep neural network committee per rank. This design enables a principled separation between epistemic uncertainty and conformational uncertainty. We validate our RCC-MLDE approach on SARS-CoV-2 antibody docking, demonstrating significant improvements over baseline strategies. Our results offer a scalable route for therapeutic antibody discovery while directly addressing the challenge of modeling conformational uncertainty.",
    "pdf_url": "https://arxiv.org/pdf/2510.24974v2",
    "github_url": null,
    "published": "2025-10-28T21:13:37+00:00",
    "updated": "2025-12-02T14:51:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24972v1",
    "title": "Smooth path planning with safety margins using Piece-Wise Bezier curves",
    "authors": [
      "Andrei",
      "Kloetzer",
      "Mahulea"
    ],
    "summary": "In this paper, we propose a computationally efficient quadratic programming (QP) approach for generating smooth, $C^1$ continuous paths for mobile robots using piece-wise quadratic Bezier (PWB) curves. Our method explicitly incorporates safety margins within a structured optimization framework, balancing trajectory smoothness and robustness with manageable numerical complexity suitable for real-time and embedded applications. Comparative simulations demonstrate clear advantages over traditional piece-wise linear (PWL) path planning methods, showing reduced trajectory deviations, enhanced robustness, and improved overall path quality. These benefits are validated through simulations using a Pure-Pursuit controller in representative scenarios, highlighting the practical effectiveness and scalability of our approach for safe navigation.",
    "pdf_url": "https://arxiv.org/pdf/2510.24972v1",
    "github_url": null,
    "published": "2025-10-28T21:12:54+00:00",
    "updated": "2025-10-28T21:12:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24949v1",
    "title": "SCOUT: A Lightweight Framework for Scenario Coverage Assessment in Autonomous Driving",
    "authors": [
      "Yildiz",
      "Thornton",
      "Hildebrandt"
    ],
    "summary": "Assessing scenario coverage is crucial for evaluating the robustness of autonomous agents, yet existing methods rely on expensive human annotations or computationally intensive Large Vision-Language Models (LVLMs). These approaches are impractical for large-scale deployment due to cost and efficiency constraints. To address these shortcomings, we propose SCOUT (Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate model designed to predict scenario coverage labels directly from an agent's latent sensor representations. SCOUT is trained through a distillation process, learning to approximate LVLM-generated coverage labels while eliminating the need for continuous LVLM inference or human annotation. By leveraging precomputed perception features, SCOUT avoids redundant computations and enables fast, scalable scenario coverage estimation. We evaluate our method across a large dataset of real-life autonomous navigation scenarios, demonstrating that it maintains high accuracy while significantly reducing computational cost. Our results show that SCOUT provides an effective and practical alternative for large-scale coverage analysis. While its performance depends on the quality of LVLM-generated training labels, SCOUT represents a major step toward efficient scenario coverage oversight in autonomous systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.24949v1",
    "github_url": null,
    "published": "2025-10-28T20:31:19+00:00",
    "updated": "2025-10-28T20:31:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05525v1",
    "title": "Real time synchronisation of a free-running atomic clock time base with UTC using GNSS signals for application in experimental physics",
    "authors": [
      "Dalmazzone",
      "Guigue",
      "Popov"
    ],
    "summary": "We present the results obtained by applying, in real-time, a correction method to precisely synchronize a time base generated from a free-running atomic clock with the Coordinated Universal Time (UTC). The method uses the Global Navigation Satellite System (GNSS) signals to have regular time comparisons between the atomic clock generated time base and the GPS Time, perform linear fits of the measurements and extrapolate a correction to apply to the free-running signal. In this work, we apply for the first time this method in real-time. Two atomic clocks were tested, a low-cost Rubidium clock and a more expensive magnetic Caesium clock. We demonstrate that we can obtain a residual difference between the clock time base and the French official realization of UTC (UTC(OP)) in the range of $\\pm 15$ ns with no apparent residual drift.",
    "pdf_url": "https://arxiv.org/pdf/2511.05525v1",
    "github_url": null,
    "published": "2025-10-28T18:46:24+00:00",
    "updated": "2025-10-28T18:46:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24680v1",
    "title": "Fare: Failure Resilience in Learned Visual Navigation Control",
    "authors": [
      "Wang",
      "Loo",
      "Hsu"
    ],
    "summary": "While imitation learning (IL) enables effective visual navigation, IL policies are prone to unpredictable failures in out-of-distribution (OOD) scenarios. We advance the notion of failure-resilient policies, which not only detect failures but also recover from them automatically. Failure recognition that identifies the factors causing failure is key to informing recovery: e.g. pinpointing image regions triggering failure detections can provide cues to guide recovery. We present Fare, a framework to construct failure-resilient IL policies, embedding OOD-detection and recognition in them without using explicit failure data, and pairing them with recovery heuristics. Real-world experiments show that Fare enables failure recovery across two different policy architectures, enabling robust long-range navigation in complex environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.24680v1",
    "github_url": null,
    "published": "2025-10-28T17:45:26+00:00",
    "updated": "2025-10-28T17:45:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24676v1",
    "title": "Feature Matching-Based Gait Phase Prediction for Obstacle Crossing Control of Powered Transfemoral Prosthesis",
    "authors": [
      "Zhang",
      "Leng",
      "Guo"
    ],
    "summary": "For amputees with powered transfemoral prosthetics, navigating obstacles or complex terrain remains challenging. This study addresses this issue by using an inertial sensor on the sound ankle to guide obstacle-crossing movements. A genetic algorithm computes the optimal neural network structure to predict the required angles of the thigh and knee joints. A gait progression prediction algorithm determines the actuation angle index for the prosthetic knee motor, ultimately defining the necessary thigh and knee angles and gait progression. Results show that when the standard deviation of Gaussian noise added to the thigh angle data is less than 1, the method can effectively eliminate noise interference, achieving 100\\% accuracy in gait phase estimation under 150 Hz, with thigh angle prediction error being 8.71\\% and knee angle prediction error being 6.78\\%. These findings demonstrate the method's ability to accurately predict gait progression and joint angles, offering significant practical value for obstacle negotiation in powered transfemoral prosthetics.",
    "pdf_url": "https://arxiv.org/pdf/2510.24676v1",
    "github_url": null,
    "published": "2025-10-28T17:40:52+00:00",
    "updated": "2025-10-28T17:40:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24653v1",
    "title": "Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology",
    "authors": [
      "Thai",
      "Li",
      "Ling"
    ],
    "summary": "Interpretation of giga-pixel whole-slide images (WSIs) is an important but difficult task for pathologists. Their diagnostic accuracy is estimated to average around 70%. Adding a second pathologist does not substantially improve decision consistency. The field lacks adequate behavioral data to explain diagnostic errors and inconsistencies. To fill in this gap, we present PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual search and decision-making processes of the full diagnostic workflow during cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse interaction, stimulus tracking, viewport navigation, and diagnostic decision data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data collection process emphasizes ecological validity through an application-grounded testbed, called PTAH. In total, we recorded 171,909 fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In addition, such data could also be used to improve the training of both pathologists and AI systems that might support human experts. All experiments were preregistered at https://osf.io/hj9a7, and the complete dataset along with analysis code is available at https://go.osu.edu/pathogaze.",
    "pdf_url": "https://arxiv.org/pdf/2510.24653v1",
    "github_url": null,
    "published": "2025-10-28T17:18:43+00:00",
    "updated": "2025-10-28T17:18:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24594v2",
    "title": "Detecting the Use of Generative AI in Crowdsourced Surveys: Implications for Data Integrity",
    "authors": [
      "Zhang",
      "Katoh",
      "Pei"
    ],
    "summary": "The widespread adoption of generative AI (GenAI) has introduced new challenges in crowdsourced data collection, particularly in survey-based research. While GenAI offers powerful capabilities, its unintended use in crowdsourcing, such as generating automated survey responses, threatens the integrity of empirical research and complicates efforts to understand public opinion and behavior. In this study, we investigate and evaluate two approaches for detecting AI-generated responses in online surveys: LLM-based detection and signature-based detection. We conducted experiments across seven survey studies, comparing responses collected before 2022 with those collected after the release of ChatGPT. Our findings reveal a significant increase in AI-generated responses in the post-2022 studies, highlighting how GenAI may silently distort crowdsourced data. This work raises broader concerns about evolving landscape of data integrity, where GenAI can compromise data quality, mislead researchers, and influence downstream findings in fields such as health, politics, and social behavior. By surfacing detection strategies and empirical evidence of GenAI's impact, we aim to contribute to ongoing conversation about safeguarding research integrity and supporting scholars navigating these methodological and ethical challenges.",
    "pdf_url": "https://arxiv.org/pdf/2510.24594v2",
    "github_url": null,
    "published": "2025-10-28T16:23:27+00:00",
    "updated": "2025-10-29T20:17:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.05523v1",
    "title": "The Evolution of Probabilistic Price Forecasting Techniques: A Review of the Day-Ahead, Intra-Day, and Balancing Markets",
    "authors": [
      "O'Connor",
      "Bahloul",
      "Prestwich"
    ],
    "summary": "Electricity price forecasting has become a critical tool for decision-making in energy markets, particularly as the increasing penetration of renewable energy introduces greater volatility and uncertainty. Historically, research in this field has been dominated by point forecasting methods, which provide single-value predictions but fail to quantify uncertainty. However, as power markets evolve due to renewable integration, smart grids, and regulatory changes, the need for probabilistic forecasting has become more pronounced, offering a more comprehensive approach to risk assessment and market participation. This paper presents a review of probabilistic forecasting methods, tracing their evolution from Bayesian and distribution based approaches, through quantile regression techniques, to recent developments in conformal prediction. Particular emphasis is placed on advancements in probabilistic forecasting, including validity-focused methods which address key limitations in uncertainty estimation. Additionally, this review extends beyond the Day-Ahead Market to include the Intra-Day and Balancing Markets, where forecasting challenges are intensified by higher temporal granularity and real-time operational constraints. We examine state of the art methodologies, key evaluation metrics, and ongoing challenges, such as forecast validity, model selection, and the absence of standardised benchmarks, providing researchers and practitioners with a comprehensive and timely resource for navigating the complexities of modern electricity markets.",
    "pdf_url": "https://arxiv.org/pdf/2511.05523v1",
    "github_url": null,
    "published": "2025-10-28T15:25:23+00:00",
    "updated": "2025-10-28T15:25:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24427v2",
    "title": "SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge in Language Models",
    "authors": [
      "Gu",
      "Bhat",
      "Merrill"
    ],
    "summary": "Evaluating the reasoning ability of language models (LMs) is complicated by their extensive parametric world knowledge, where benchmark performance often reflects factual recall rather than genuine reasoning. Existing datasets and approaches (e.g., temporal filtering, paraphrasing, adversarial substitution) cannot cleanly separate the two. We present SynthWorlds, a framework that disentangles task reasoning complexity from factual knowledge. In SynthWorlds, we construct parallel corpora representing two worlds with identical interconnected structure: a real-mapped world, where models may exploit parametric knowledge, and a synthetic-mapped world, where such knowledge is meaningless. On top of these corpora, we design two mirrored tasks as case studies: multi-hop question answering and page navigation, which maintain equal reasoning difficulty across worlds. Experiments in parametric-only (e.g., closed-book QA) and knowledge-augmented (e.g., retrieval-augmented) LM settings reveal a persistent knowledge advantage gap, defined as the performance boost models gain from memorized parametric world knowledge. Knowledge acquisition and integration mechanisms reduce but do not eliminate this gap, highlighting opportunities for system improvements. Fully automatic and scalable, SynthWorlds provides a controlled environment for evaluating LMs in ways that were previously challenging, enabling precise and testable comparisons of reasoning and memorization.",
    "pdf_url": "https://arxiv.org/pdf/2510.24427v2",
    "github_url": null,
    "published": "2025-10-28T13:47:23+00:00",
    "updated": "2025-10-30T23:27:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.11595v1",
    "title": "Decision-Making Amid Information-Based Threats in Sociotechnical Systems: A Review",
    "authors": [
      "Allred",
      "Richardson",
      "Bostrom"
    ],
    "summary": "Technological systems increasingly mediate human information exchange, spanning interactions among humans as well as between humans and artificial agents. The unprecedented scale and reliance on information disseminated through these systems substantially expand the scope of information-based influence that can both enable and undermine sound decision-making. Consequently, understanding and protecting decision-making today faces growing challenges, as individuals and organizations must navigate evolving opportunities and information-based threats across varied domains and information environments. While these risks are widely recognized, research remains fragmented: work evaluating information-based threat phenomena has progressed largely in isolation from foundational studies of human information processing. In this review, we synthesize insights from both domains to identify shared cognitive mechanisms that mediate vulnerability to information-based threats and shape behavioral outcomes. Finally, we outline directions for future research aimed at integrating these perspectives, emphasizing the importance of such integration for mitigating human vulnerabilities and aligning human-machine representations.",
    "pdf_url": "https://arxiv.org/pdf/2511.11595v1",
    "github_url": null,
    "published": "2025-10-28T13:26:41+00:00",
    "updated": "2025-10-28T13:26:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24337v1",
    "title": "Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research",
    "authors": [
      "Kravets-Meinke",
      "Schmid-Petri",
      "Niemann"
    ],
    "summary": "Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly being used in communication research for content analysis. Studies show that gLLMs can outperform both crowd workers and trained coders, such as research assistants, on various coding tasks relevant to communication science, often at a fraction of the time and cost. Additionally, gLLMs can decode implicit meanings and contextual information, be instructed using natural language, deployed with only basic programming skills, and require little to no annotated data beyond a validation dataset - constituting a paradigm shift in automated content analysis. Despite their potential, the integration of gLLMs into the methodological toolkit of communication research remains underdeveloped. In gLLM-assisted quantitative content analysis, researchers must address at least seven critical challenges that impact result quality: (1) codebook development, (2) prompt engineering, (3) model selection, (4) parameter tuning, (5) iterative refinement, (6) validation of the model's reliability, and optionally, (7) performance enhancement. This paper synthesizes emerging research on gLLM-assisted quantitative content analysis and proposes a comprehensive best-practice guide to navigate these challenges. Our goal is to make gLLM-based content analysis more accessible to a broader range of communication researchers and ensure adherence to established disciplinary quality standards of validity, reliability, reproducibility, and research ethics.",
    "pdf_url": "https://arxiv.org/pdf/2510.24337v1",
    "github_url": null,
    "published": "2025-10-28T12:01:43+00:00",
    "updated": "2025-10-28T12:01:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24335v1",
    "title": "NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation",
    "authors": [
      "Jeong",
      "Kim",
      "Park"
    ],
    "summary": "We present NVSim, a framework that automatically constructs large-scale, navigable indoor simulators from only common image sequences, overcoming the cost and scalability limitations of traditional 3D scanning. Our approach adapts 3D Gaussian Splatting to address visual artifacts on sparsely observed floors a common issue in robotic traversal data. We introduce Floor-Aware Gaussian Splatting to ensure a clean, navigable ground plane, and a novel mesh-free traversability checking algorithm that constructs a topological graph by directly analyzing rendered views. We demonstrate our system's ability to generate valid, large-scale navigation graphs from real-world data. A video demonstration is avilable at https://youtu.be/tTiIQt6nXC8",
    "pdf_url": "https://arxiv.org/pdf/2510.24335v1",
    "github_url": null,
    "published": "2025-10-28T11:57:33+00:00",
    "updated": "2025-10-28T11:57:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24168v1",
    "title": "MGA: Memory-Driven GUI Agent for Observation-Centric Interaction",
    "authors": [
      "Cheng",
      "Ni",
      "Wang"
    ],
    "summary": "The rapid progress of Large Language Models (LLMs) and their multimodal extensions (MLLMs) has enabled agentic systems capable of perceiving and acting across diverse environments. A challenging yet impactful frontier is the development of GUI agents, which must navigate complex desktop and web interfaces while maintaining robustness and generalization. Existing paradigms typically model tasks as long-chain executions, concatenating historical trajectories into the context. While approaches such as Mirage and GTA1 refine planning or introduce multi-branch action selection, they remain constrained by two persistent issues: Dependence on historical trajectories, which amplifies error propagation. And Local exploration bias, where \"decision-first, observation-later\" mechanisms overlook critical interface cues. We introduce the Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the principle of observe first, then decide. MGA models each step as an independent, context-rich environment state represented by a triad: current screenshot, task-agnostic spatial information, and a dynamically updated structured memory. Experiments on OSworld benchmarks, real desktop applications (Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves substantial gains in robustness, generalization, and efficiency compared to state-of-the-art baselines. The code is publicly available at: {https://anonymous.4open.science/r/MGA-3571}.",
    "pdf_url": "https://arxiv.org/pdf/2510.24168v1",
    "github_url": null,
    "published": "2025-10-28T08:19:58+00:00",
    "updated": "2025-10-28T08:19:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24118v1",
    "title": "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation",
    "authors": [
      "Zhou",
      "Wang",
      "Li"
    ],
    "summary": "Navigating to a designated goal using visual information is a fundamental capability for intelligent robots. Most classical visual navigation methods are restricted to single-goal, single-modality, and closed set goal settings. To address the practical demands of multi-modal, open-vocabulary goal queries and multi-goal visual navigation, we propose LagMemo, a navigation system that leverages a language 3D Gaussian Splatting memory. During exploration, LagMemo constructs a unified 3D language memory. With incoming task goals, the system queries the memory, predicts candidate goal locations, and integrates a local perception-based verification mechanism to dynamically match and validate goals during navigation. For fair and rigorous evaluation, we curate GOAT-Core, a high-quality core split distilled from GOAT-Bench tailored to multi-modal open-vocabulary multi-goal visual navigation. Experimental results show that LagMemo's memory module enables effective multi-modal open-vocabulary goal localization, and that LagMemo outperforms state-of-the-art methods in multi-goal visual navigation. Project page: https://weekgoodday.github.io/lagmemo",
    "pdf_url": "https://arxiv.org/pdf/2510.24118v1",
    "github_url": null,
    "published": "2025-10-28T06:42:21+00:00",
    "updated": "2025-10-28T06:42:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24084v1",
    "title": "Lunar Navigation System Optimization for Targeted Coverage with Semi-Analytical Station Keeping Model and Earth-GPS Integration",
    "authors": [
      "Malik",
      "Yang"
    ],
    "summary": "The design of an indigenous Lunar Navigation Satellite System (LNSS) is receiving growing attention due to the surge in planned lunar missions and the limited accessibility of Earth-based Global Navigation Satellite Systems (GNSS) in the cislunar environment. Several studies have explored LNSS architecture using geometric analysis in both near and distant lunar orbits. The existing LNSS optimization efforts have primarily focused on global lunar coverage using analytical station-keeping models with low accuracy. Furthermore, current south pole-focused research is restricted to Elliptical Lunar Frozen Orbits (ELFOs) and lacks comprehensive optimization approach. Additionally, integration with Earth GNSS systems for ephemeris computation and time synchronization has not been adequately addressed in prior studies. In this work, we present a comprehensive LNSS mission design framework based on evolutionary multi-objective optimization integrated with a high-fidelity numerical lunar orbit propagation model. The optimization simultaneously considers navigation performance in the lunar south pole region, semi-analytical continuous station-keeping maneuver model for realistic dV estimate, and GPS-LNSS integration analysis parameters. The resulting Pareto front offers a diverse set of LNSS configurations that balance coverage, accuracy, and dV requirements. Notably, the optimization identifies diverse non-frozen elliptical orbit solutions that achieve over 90% south pole coverage with acceptable navigation accuracy using as few as six satellites and dV of less than 0.4 km/s per satellite per year. This represents a significant reduction in constellation size compared to previous studies, offering a cost-effective yet operationally efficient solution for future LNSS missions.",
    "pdf_url": "https://arxiv.org/pdf/2510.24084v1",
    "github_url": null,
    "published": "2025-10-28T05:54:23+00:00",
    "updated": "2025-10-28T05:54:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24070v1",
    "title": "Building AI Literacy at Home: How Families Navigate Children's Self-Directed Learning with AI",
    "authors": [
      "Xie",
      "Wu",
      "Wang"
    ],
    "summary": "As generative AI becomes embedded in children's learning spaces, families face new challenges in guiding its use. Middle childhood (ages 7-13) is a critical stage where children seek autonomy even as parental influence remains strong. Using self-directed learning (SDL) as a lens, we examine how parents perceive and support children's developing AI literacy through focus groups with 13 parent-child pairs. Parents described evolving phases of engagement driven by screen time, self-motivation, and growing knowledge. While many framed AI primarily as a study tool, few considered its non-educational roles or risks, such as privacy and infrastructural embedding. Parents also noted gaps in their own AI understanding, often turning to joint exploration and engagement as a form of co-learning. Our findings reveal how families co-construct children's AI literacy, exposing tensions between practical expectations and critical literacies, and provide design implications that foster SDL while balancing autonomy and oversight.",
    "pdf_url": "https://arxiv.org/pdf/2510.24070v1",
    "github_url": null,
    "published": "2025-10-28T05:06:03+00:00",
    "updated": "2025-10-28T05:06:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.24029v1",
    "title": "Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model",
    "authors": [
      "Gerstenslager",
      "Dukenbaev",
      "Minai"
    ],
    "summary": "Boundary Vector Cells (BVCs) are a class of neurons in the brains of vertebrates that encode environmental boundaries at specific distances and allocentric directions, playing a central role in forming place fields in the hippocampus. Most computational BVC models are restricted to two-dimensional (2D) environments, making them prone to spatial ambiguities in the presence of horizontal symmetries in the environment. To address this limitation, we incorporate vertical angular sensitivity into the BVC framework, thereby enabling robust boundary detection in three dimensions, and leading to significantly more accurate spatial localization in a biologically-inspired robot model.   The proposed model processes LiDAR data to capture vertical contours, thereby disambiguating locations that would be indistinguishable under a purely 2D representation. Experimental results show that in environments with minimal vertical variation, the proposed 3D model matches the performance of a 2D baseline; yet, as 3D complexity increases, it yields substantially more distinct place fields and markedly reduces spatial aliasing. These findings show that adding a vertical dimension to BVC-based localization can significantly enhance navigation and mapping in real-world 3D spaces while retaining performance parity in simpler, near-planar scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2510.24029v1",
    "github_url": null,
    "published": "2025-10-28T03:24:02+00:00",
    "updated": "2025-10-28T03:24:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23902v1",
    "title": "Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped",
    "authors": [
      "Solano",
      "Quiroz"
    ],
    "summary": "Wheeled-legged robots combine the efficiency of wheels with the obstacle negotiation of legs, yet many state-of-the-art systems rely on costly actuators and sensors, and fall-recovery is seldom integrated, especially for wheeled-legged morphologies. This work presents a recovery-aware visual-inertial navigation system on a low-cost wheeled quadruped. The proposed system leverages vision-based perception from a depth camera and deep reinforcement learning policies for robust locomotion and autonomous recovery from falls across diverse terrains. Simulation experiments show agile mobility with low-torque actuators over irregular terrain and reliably recover from external perturbations and self-induced failures. We further show goal directed navigation in structured indoor spaces with low-cost perception. Overall, this approach lowers the barrier to deploying autonomous navigation and robust locomotion policies in budget-constrained robotic platforms.",
    "pdf_url": "https://arxiv.org/pdf/2510.23902v1",
    "github_url": null,
    "published": "2025-10-27T22:19:35+00:00",
    "updated": "2025-10-27T22:19:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23798v1",
    "title": "A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras",
    "authors": [
      "Grimmer",
      "Wenger",
      "Flint"
    ],
    "summary": "The proliferation of floating anthropogenic debris in rivers has emerged as a pressing environmental concern, exerting a detrimental influence on biodiversity, water quality, and human activities such as navigation and recreation. The present study proposes a novel methodological framework for the monitoring the aforementioned waste, utilising fixed, in-situ cameras. This study provides two key contributions: (i) the continuous quantification and monitoring of floating debris using deep learning and (ii) the identification of the most suitable deep learning model in terms of accuracy and inference speed under complex environmental conditions. These models are tested in a range of environmental conditions and learning configurations, including experiments on biases related to data leakage. Furthermore, a geometric model is implemented to estimate the actual size of detected objects from a 2D image. This model takes advantage of both intrinsic and extrinsic characteristics of the camera. The findings of this study underscore the significance of the dataset constitution protocol, particularly with respect to the integration of negative images and the consideration of temporal leakage. In conclusion, the feasibility of metric object estimation using projective geometry coupled with regression corrections is demonstrated. This approach paves the way for the development of robust, low-cost, automated monitoring systems for urban aquatic environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.23798v1",
    "github_url": null,
    "published": "2025-10-27T19:29:14+00:00",
    "updated": "2025-10-27T19:29:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23576v1",
    "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
    "authors": [
      "Li",
      "Wang",
      "Zhang"
    ],
    "summary": "Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties.",
    "pdf_url": "https://arxiv.org/pdf/2510.23576v1",
    "github_url": null,
    "published": "2025-10-27T17:46:43+00:00",
    "updated": "2025-10-27T17:46:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23509v1",
    "title": "Deductive Chain-of-Thought Augmented Socially-aware Robot Navigation World Model",
    "authors": [
      "Wang",
      "Ike",
      "Choi"
    ],
    "summary": "Social robot navigation increasingly relies on large language models for reasoning, path planning, and enabling movement in dynamic human spaces. However, relying solely on LLMs for planning often leads to unpredictable and unsafe behaviors, especially in dynamic human spaces, due to limited physical grounding and weak logical consistency. In this work, we introduce NaviWM, a socially-aware robot Navigation World Model that augments LLM reasoning with a structured world model and a logic-driven chain-of-thought process. NaviWM consists of two main components: (1) a spatial-temporal world model that captures the positions, velocities, and activities of agents in the environment, and (2) a deductive reasoning module that guides LLMs through a multi-step, logic-based inference process. This integration enables the robot to generate navigation decisions that are both socially compliant and physically safe, under well-defined constraints such as personal space, collision avoidance, and timing. Unlike previous methods based on prompting or fine-tuning, NaviWM encodes social norms as first-order logic, enabling interpretable and verifiable reasoning. Experiments show that NaviWM improves success rates and reduces social violations, particularly in crowded environments. These results demonstrate the benefit of combining formal reasoning with LLMs for robust social navigation. Additional experimental details and demo videos for this work can be found at: https://sites.google.com/view/NaviWM.",
    "pdf_url": "https://arxiv.org/pdf/2510.23509v1",
    "github_url": null,
    "published": "2025-10-27T16:47:15+00:00",
    "updated": "2025-10-27T16:47:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23473v1",
    "title": "Video-Thinker: Sparking \"Thinking with Videos\" via Reinforcement Learning",
    "authors": [
      "Wang",
      "Jin",
      "Wang"
    ],
    "summary": "Recent advances in image reasoning methods, particularly \"Thinking with Images\", have demonstrated remarkable success in Multimodal Large Language Models (MLLMs); however, this dynamic reasoning paradigm has not yet been extended to video reasoning tasks. In this paper, we propose Video-Thinker, which empowers MLLMs to think with videos by autonomously leveraging their intrinsic \"grounding\" and \"captioning\" capabilities to generate reasoning clues throughout the inference process. To spark this capability, we construct Video-Thinker-10K, a curated dataset featuring autonomous tool usage within chain-of-thought reasoning sequences. Our training strategy begins with Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group Relative Policy Optimization (GRPO) to strengthen this reasoning capability. Through this approach, Video-Thinker enables MLLMs to autonomously navigate grounding and captioning tasks for video reasoning, eliminating the need for constructing and calling external tools. Extensive experiments demonstrate that Video-Thinker achieves significant performance gains on both in-domain tasks and challenging out-of-domain video reasoning benchmarks, including Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B substantially outperforms existing baselines such as Video-R1 and establishes state-of-the-art performance among 7B-sized MLLMs.",
    "pdf_url": "https://arxiv.org/pdf/2510.23473v1",
    "github_url": null,
    "published": "2025-10-27T16:10:45+00:00",
    "updated": "2025-10-27T16:10:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23443v1",
    "title": "A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration",
    "authors": [
      "Bonfanti",
      "Druetto",
      "Basile"
    ],
    "summary": "The growing intersection of cybersecurity and law creates a complex information space where traditional legal research tools struggle to deal with nuanced connections between cases, statutes, and technical vulnerabilities. This knowledge divide hinders collaboration between legal experts and cybersecurity professionals. To address this important gap, this work provides a first step towards intelligent systems capable of navigating the increasingly intricate cyber-legal domain. We demonstrate promising initial results on multilingual tasks.",
    "pdf_url": "https://arxiv.org/pdf/2510.23443v1",
    "github_url": null,
    "published": "2025-10-27T15:46:02+00:00",
    "updated": "2025-10-27T15:46:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23359v1",
    "title": "T-ESKF: Transformed Error-State Kalman Filter for Consistent Visual-Inertial Navigation",
    "authors": [
      "Tian",
      "Hao",
      "He"
    ],
    "summary": "This paper presents a novel approach to address the inconsistency problem caused by observability mismatch in visual-inertial navigation systems (VINS). The key idea involves applying a linear time-varying transformation to the error-state within the Error-State Kalman Filter (ESKF). This transformation ensures that \\textrr{the unobservable subspace of the transformed error-state system} becomes independent of the state, thereby preserving the correct observability of the transformed system against variations in linearization points. We introduce the Transformed ESKF (T-ESKF), a consistent VINS estimator that performs state estimation using the transformed error-state system. Furthermore, we develop an efficient propagation technique to accelerate the covariance propagation based on the transformation relationship between the transition and accumulated matrices of T-ESKF and ESKF. We validate the proposed method through extensive simulations and experiments, demonstrating better (or competitive at least) performance compared to state-of-the-art methods. The code is available at github.com/HITCSC/T-ESKF.",
    "pdf_url": "https://arxiv.org/pdf/2510.23359v1",
    "github_url": null,
    "published": "2025-10-27T14:08:32+00:00",
    "updated": "2025-10-27T14:08:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23329v1",
    "title": "Transferable Deep Reinforcement Learning for Cross-Domain Navigation: from Farmland to the Moon",
    "authors": [
      "Santra",
      "Robbins",
      "Yoshida"
    ],
    "summary": "Autonomous navigation in unstructured environments is essential for field and planetary robotics, where robots must efficiently reach goals while avoiding obstacles under uncertain conditions. Conventional algorithmic approaches often require extensive environment-specific tuning, limiting scalability to new domains. Deep Reinforcement Learning (DRL) provides a data-driven alternative, allowing robots to acquire navigation strategies through direct interactions with their environment. This work investigates the feasibility of DRL policy generalization across visually and topographically distinct simulated domains, where policies are trained in terrestrial settings and validated in a zero-shot manner in extraterrestrial environments. A 3D simulation of an agricultural rover is developed and trained using Proximal Policy Optimization (PPO) to achieve goal-directed navigation and obstacle avoidance in farmland settings. The learned policy is then evaluated in a lunar-like simulated environment to assess transfer performance. The results indicate that policies trained under terrestrial conditions retain a high level of effectiveness, achieving close to 50\\% success in lunar simulations without the need for additional training and fine-tuning. This underscores the potential of cross-domain DRL-based policy transfer as a promising approach to developing adaptable and efficient autonomous navigation for future planetary exploration missions, with the added benefit of minimizing retraining costs.",
    "pdf_url": "https://arxiv.org/pdf/2510.23329v1",
    "github_url": null,
    "published": "2025-10-27T13:45:50+00:00",
    "updated": "2025-10-27T13:45:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23286v1",
    "title": "Precise Time Delay Measurement and Compensation for Tightly Coupled Underwater SINS/piUSBL Navigation",
    "authors": [
      "Huang",
      "Wang",
      "Li"
    ],
    "summary": "In multi-sensor systems, time synchronization between sensors is a significant challenge, and this issue is particularly pronounced in underwater integrated navigation systems incorporating acoustic positioning. Such systems are highly susceptible to time delay, which can significantly degrade accuracy when measurement and fusion moments are misaligned. To address this challenge, this paper introduces a tightly coupled navigation framework that integrates a passive inverted ultra-short baseline (piUSBL) acoustic positioning system, a strapdown inertial navigation system (SINS), and a depth gauge under precise time synchronization. The framework fuses azimuth and slant range from the piUSBL with depth data, thereby avoiding poor vertical-angle observability in planar arrays. A novel delay measurement strategy is introduced, combining synchronized timing with acoustic signal processing, which redefines delay-traditionally an unobservable error-into a quantifiable parameter, enabling explicit estimation of both acoustic propagation and system processing delays. Simulations and field experiments confirm the feasibility of the proposed method, with delay-compensated navigation reducing RMSE by 40.45% and maximum error by 32.55%. These findings show that precise delay measurement and compensation not only enhance underwater navigation accuracy but also establish a generalizable framework for acoustic positioning integration, offering valuable insights into time alignment and data fusion in latency-sensitive multi-sensor systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.23286v1",
    "github_url": null,
    "published": "2025-10-27T12:54:58+00:00",
    "updated": "2025-10-27T12:54:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23258v1",
    "title": "Deep Active Inference with Diffusion Policy and Multiple Timescale World Model for Real-World Exploration and Navigation",
    "authors": [
      "Yokozawa",
      "Fujii",
      "Nomura"
    ],
    "summary": "Autonomous robotic navigation in real-world environments requires exploration to acquire environmental information as well as goal-directed navigation in order to reach specified targets. Active inference (AIF) based on the free-energy principle provides a unified framework for these behaviors by minimizing the expected free energy (EFE), thereby combining epistemic and extrinsic values. To realize this practically, we propose a deep AIF framework that integrates a diffusion policy as the policy model and a multiple timescale recurrent state-space model (MTRSSM) as the world model. The diffusion policy generates diverse candidate actions while the MTRSSM predicts their long-horizon consequences through latent imagination, enabling action selection that minimizes EFE. Real-world navigation experiments demonstrated that our framework achieved higher success rates and fewer collisions compared with the baselines, particularly in exploration-demanding scenarios. These results highlight how AIF based on EFE minimization can unify exploration and goal-directed navigation in real-world robotic settings.",
    "pdf_url": "https://arxiv.org/pdf/2510.23258v1",
    "github_url": null,
    "published": "2025-10-27T12:21:33+00:00",
    "updated": "2025-10-27T12:21:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00034v1",
    "title": "On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning",
    "authors": [
      "Akella"
    ],
    "summary": "Recent advances in learnable reward shaping have shown promise in single-agent reinforcement learning by automatically discovering effective feedback signals. However, the effectiveness of decentralized learnable reward shaping in cooperative multi-agent settings remains poorly understood. We propose DMARL-RSA, a fully decentralized system where each agent learns individual reward shaping, and evaluate it on cooperative navigation tasks in the simple_spread_v3 environment. Despite sophisticated reward learning, DMARL-RSA achieves only -24.20 +/- 0.09 average reward, compared to MAPPO with centralized training at 1.92 +/- 0.87 -- a 26.12-point gap. DMARL-RSA performs similarly to simple independent learning (IPPO: -23.19 +/- 0.96), indicating that advanced reward shaping cannot overcome fundamental decentralized coordination limitations. Interestingly, decentralized methods achieve higher landmark coverage (0.888 +/- 0.029 for DMARL-RSA, 0.960 +/- 0.045 for IPPO out of 3 total) but worse overall performance than centralized MAPPO (0.273 +/- 0.008 landmark coverage) -- revealing a coordination paradox between local optimization and global performance. Analysis identifies three critical barriers: (1) non-stationarity from concurrent policy updates, (2) exponential credit assignment complexity, and (3) misalignment between individual reward optimization and global objectives. These results establish empirical limits for decentralized reward learning and underscore the necessity of centralized coordination for effective multi-agent cooperation.",
    "pdf_url": "https://arxiv.org/pdf/2511.00034v1",
    "github_url": null,
    "published": "2025-10-27T07:01:35+00:00",
    "updated": "2025-10-27T07:01:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23057v1",
    "title": "Seq-DeepIPC: Sequential Sensing for End-to-End Control in Legged Robot Navigation",
    "authors": [
      "Natan",
      "Miura"
    ],
    "summary": "We present Seq-DeepIPC, a sequential end-to-end perception-to-control model for legged robot navigation in realworld environments. Seq-DeepIPC advances intelligent sensing for autonomous legged navigation by tightly integrating multi-modal perception (RGB-D + GNSS) with temporal fusion and control. The model jointly predicts semantic segmentation and depth estimation, giving richer spatial features for planning and control. For efficient deployment on edge devices, we use EfficientNet-B0 as the encoder, reducing computation while maintaining accuracy. Heading estimation is simplified by removing the noisy IMU and instead computing the bearing angle directly from consecutive GNSS positions. We collected a larger and more diverse dataset that includes both road and grass terrains, and validated Seq-DeepIPC on a robot dog. Comparative and ablation studies show that sequential inputs improve perception and control in our models, while other baselines do not benefit. Seq-DeepIPC achieves competitive or better results with reasonable model size; although GNSS-only heading is less reliable near tall buildings, it is robust in open areas. Overall, Seq-DeepIPC extends end-to-end navigation beyond wheeled robots to more versatile and temporally-aware systems. To support future research, we will release the codes to our GitHub repository at https://github.com/oskarnatan/Seq-DeepIPC.",
    "pdf_url": "https://arxiv.org/pdf/2510.23057v1",
    "github_url": "https://github.com/oskarnatan/Seq-DeepIPC",
    "published": "2025-10-27T06:39:57+00:00",
    "updated": "2025-10-27T06:39:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23021v1",
    "title": "Planning Oriented Integrated Sensing and Communication",
    "authors": [
      "Jin",
      "Li",
      "Wang"
    ],
    "summary": "Integrated sensing and communication (ISAC) enables simultaneous localization, environment perception, and data exchange for connected autonomous vehicles. However, most existing ISAC designs prioritize sensing accuracy and communication throughput, treating all targets uniformly and overlooking the impact of critical obstacles on motion efficiency. To overcome this limitation, we propose a planning-oriented ISAC (PISAC) framework that reduces the sensing uncertainty of planning-bottleneck obstacles and expands the safe navigable path for the ego-vehicle, thereby bridging the gap between physical-layer optimization and motion-level planning. The core of PISAC lies in deriving a closed-form safety bound that explicitly links ISAC transmit power to sensing uncertainty, based on the Cramér-Rao Bound and occupancy inflation principles. Using this model, we formulate a bilevel power allocation and motion planning (PAMP) problem, where the inner layer optimizes the ISAC beam power distribution and the outer layer computes a collision-free trajectory under uncertainty-aware safety constraints. Comprehensive simulations in high-fidelity urban driving environments demonstrate that PISAC achieves up to 40% higher success rates and over 5% shorter traversal times than existing ISAC-based and communication-oriented benchmarks, validating its effectiveness in enhancing both safety and efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2510.23021v1",
    "github_url": null,
    "published": "2025-10-27T05:33:00+00:00",
    "updated": "2025-10-27T05:33:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.23017v1",
    "title": "Mastering energy landscapes via liquid liquid phase separation to program active supramolecular coassembly from the nano to macro scale",
    "authors": [
      "Wu",
      "Teijlingen",
      "Watts"
    ],
    "summary": "The energy landscape dictates pathways and outcomes in supramolecular selfassembly, yet harnessing it from the nano to the macro scales remains a major challenge. Here, we demonstrate liquid liquid phase separation (LLPS) as a powerful tool to navigate and engineer the energy landscapes of coassembly systems comprising disordered proteins and peptides. We quantitatively map the energy barriers and transition states governing structural transitions, enabling predictive on off control of assembly and hierarchical order from nano to macro scales. By integrating supramolecular biofabrication, we achieve spatially organized architectures with life like non equilibrium behaviour. Crucially, assembly stability and scalable selfsorting are shown to depend on accessing minimum energy states, regardless of whether the co assembled structures are disordered or ordered. This work establishes energy landscape mediation via LLPS as a general framework for designing lifelike, hierarchically structured materials.",
    "pdf_url": "https://arxiv.org/pdf/2510.23017v1",
    "github_url": null,
    "published": "2025-10-27T05:25:22+00:00",
    "updated": "2025-10-27T05:25:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.00033v1",
    "title": "STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization",
    "authors": [
      "He",
      "Gao",
      "Li"
    ],
    "summary": "The Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) task requires agents to navigate previously unseen 3D environments using natural language instructions, without any scene-specific training. A critical challenge in this setting lies in ensuring agents' actions align with both spatial structure and task intent over long-horizon execution. Existing methods often fail to achieve robust navigation due to a lack of structured decision-making and insufficient integration of feedback from previous actions. To address these challenges, we propose STRIDER (Instruction-Aligned Structural Decision Space Optimization), a novel framework that systematically optimizes the agent's decision space by integrating spatial layout priors and dynamic task feedback. Our approach introduces two key innovations: 1) a Structured Waypoint Generator that constrains the action space through spatial structure, and 2) a Task-Alignment Regulator that adjusts behavior based on task progress, ensuring semantic alignment throughout navigation. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms strong SOTA across key metrics; in particular, it improves Success Rate (SR) from 29% to 35%, a relative gain of 20.7%. Such results highlight the importance of spatially constrained decision-making and feedback-guided execution in improving navigation fidelity for zero-shot VLN-CE.",
    "pdf_url": "https://arxiv.org/pdf/2511.00033v1",
    "github_url": null,
    "published": "2025-10-27T04:37:21+00:00",
    "updated": "2025-10-27T04:37:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22947v1",
    "title": "Intelligent Multimodal Multi-Sensor Fusion-Based UAV Identification, Localization, and Countermeasures for Safeguarding Low-Altitude Economy",
    "authors": [
      "Tao",
      "Gao",
      "Ye"
    ],
    "summary": "The development of the low-altitude economy has led to a growing prominence of uncrewed aerial vehicle (UAV) safety management issues. Therefore, accurate identification, real-time localization, and effective countermeasures have become core challenges in airspace security assurance. This paper introduces an integrated UAV management and control system based on deep learning, which integrates multimodal multi-sensor fusion perception, precise positioning, and collaborative countermeasures. By incorporating deep learning methods, the system combines radio frequency (RF) spectral feature analysis, radar detection, electro-optical identification, and other methods at the detection level to achieve the identification and classification of UAVs. At the localization level, the system relies on multi-sensor data fusion and the air-space-ground integrated communication network to conduct real-time tracking and prediction of UAV flight status, providing support for early warning and decision-making. At the countermeasure level, it adopts comprehensive measures that integrate ``soft kill'' and ``hard kill'', including technologies such as electromagnetic signal jamming, navigation spoofing, and physical interception, to form a closed-loop management and control process from early warning to final disposal, which significantly enhances the response efficiency and disposal accuracy of low-altitude UAV management.",
    "pdf_url": "https://arxiv.org/pdf/2510.22947v1",
    "github_url": null,
    "published": "2025-10-27T03:01:25+00:00",
    "updated": "2025-10-27T03:01:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22940v4",
    "title": "Generating Auxiliary Tasks with Reinforcement Learning",
    "authors": [
      "Goldfeder",
      "So",
      "Lipson"
    ],
    "summary": "Auxiliary Learning (AL) is a form of multi-task learning in which a model trains on auxiliary tasks to boost performance on a primary objective. While AL has improved generalization across domains such as navigation, image classification, and NLP, it often depends on human-labeled auxiliary tasks that are costly to design and require domain expertise. Meta-learning approaches mitigate this by learning to generate auxiliary tasks, but typically rely on gradient based bi-level optimization, adding substantial computational and implementation overhead. We propose RL-AUX, a reinforcement-learning (RL) framework that dynamically creates auxiliary tasks by assigning auxiliary labels to each training example, rewarding the agent whenever its selections improve the performance on the primary task. We also explore learning per-example weights for the auxiliary loss. On CIFAR-100 grouped into 20 superclasses, our RL method outperforms human-labeled auxiliary tasks and matches the performance of a prominent bi-level optimization baseline. We present similarly strong results on other classification datasets. These results suggest RL is a viable path to generating effective auxiliary tasks.",
    "pdf_url": "https://arxiv.org/pdf/2510.22940v4",
    "github_url": null,
    "published": "2025-10-27T02:51:51+00:00",
    "updated": "2025-11-03T23:55:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22917v2",
    "title": "HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment",
    "authors": [
      "Yin",
      "Zhao",
      "Li"
    ],
    "summary": "Objective-oriented navigation(ObjNav) enables robot to navigate to target object directly and autonomously in an unknown environment. Effective perception in navigation in unknown environment is critical for autonomous robots. While egocentric observations from RGB-D sensors provide abundant local information, real-time top-down maps offer valuable global context for ObjNav. Nevertheless, the majority of existing studies focus on a single source, seldom integrating these two complementary perceptual modalities, despite the fact that humans naturally attend to both. With the rapid advancement of Vision-Language Models(VLMs), we propose Hybrid Perception Navigation (HyPerNav), leveraging VLMs' strong reasoning and vision-language understanding capabilities to jointly perceive both local and global information to enhance the effectiveness and intelligence of navigation in unknown environments. In both massive simulation evaluation and real-world validation, our methods achieved state-of-the-art performance against popular baselines. Benefiting from hybrid perception approach, our method captures richer cues and finds the objects more effectively, by simultaneously leveraging information understanding from egocentric observations and the top-down map. Our ablation study further proved that either of the hybrid perception contributes to the navigation performance.",
    "pdf_url": "https://arxiv.org/pdf/2510.22917v2",
    "github_url": null,
    "published": "2025-10-27T01:43:56+00:00",
    "updated": "2025-10-28T02:49:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22832v1",
    "title": "HRM-Agent: Training a recurrent reasoning model in dynamic environments using reinforcement learning",
    "authors": [
      "Dang",
      "Rawlinson"
    ],
    "summary": "The Hierarchical Reasoning Model (HRM) has impressive reasoning abilities given its small size, but has only been applied to supervised, static, fully-observable problems. One of HRM's strengths is its ability to adapt its computational effort to the difficulty of the problem. However, in its current form it cannot integrate and reuse computation from previous time-steps if the problem is dynamic, uncertain or partially observable, or be applied where the correct action is undefined, characteristics of many real-world problems.   This paper presents HRM-Agent, a variant of HRM trained using only reinforcement learning. We show that HRM can learn to navigate to goals in dynamic and uncertain maze environments. Recent work suggests that HRM's reasoning abilities stem from its recurrent inference process. We explore the dynamics of the recurrent inference process and find evidence that it is successfully reusing computation from earlier environment time-steps.",
    "pdf_url": "https://arxiv.org/pdf/2510.22832v1",
    "github_url": null,
    "published": "2025-10-26T21:01:04+00:00",
    "updated": "2025-10-26T21:01:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22789v1",
    "title": "Learning Neural Observer-Predictor Models for Limb-level Sampling-based Locomotion Planning",
    "authors": [
      "Kulkarni",
      "Poulakakis",
      "Huang"
    ],
    "summary": "Accurate full-body motion prediction is essential for the safe, autonomous navigation of legged robots, enabling critical capabilities like limb-level collision checking in cluttered environments. Simplified kinematic models often fail to capture the complex, closed-loop dynamics of the robot and its low-level controller, limiting their predictions to simple planar motion. To address this, we present a learning-based observer-predictor framework that accurately predicts this motion. Our method features a neural observer with provable UUB guarantees that provides a reliable latent state estimate from a history of proprioceptive measurements. This stable estimate initializes a computationally efficient predictor, designed for the rapid, parallel evaluation of thousands of potential trajectories required by modern sampling-based planners. We validated the system by integrating our neural predictor into an MPPI-based planner on a Vision 60 quadruped. Hardware experiments successfully demonstrated effective, limb-aware motion planning in a challenging, narrow passage and over small objects, highlighting our system's ability to provide a robust foundation for high-performance, collision-aware planning on dynamic robotic platforms.",
    "pdf_url": "https://arxiv.org/pdf/2510.22789v1",
    "github_url": null,
    "published": "2025-10-26T18:46:46+00:00",
    "updated": "2025-10-26T18:46:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22782v1",
    "title": "Exploiting Electrolyzer Flexibility via Multiscale Model Predictive Control Cross Heterogeneous Energy Markets",
    "authors": [
      "Chen",
      "Sheng",
      "Wang"
    ],
    "summary": "Green hydrogen production via electrolysis is crucial for decarbonization but faces significant economic hurdles primarily due to the high cost of the electricity. However, current electrolyzer-based hydrogen production processes predominantly rely on the single-scale Day-Ahead Market (DAM) for electricity procurement, failing to fully exploit the economic benefits offered by multi-scale electricity market that integrates both the DAM and the Real-Time Market (RTM), thereby eliminating the opportunity to reduce the overall cost. To mitigate this technical gap, this research investigates a dynamic operational strategy enabling electrolyzers to strategically navigate between the DAM and RTM to minimize net operation costs. Using a rolling horizon optimization framework to coordinate bidding and operation, we demonstrate a strategy where electrolyzers secure primary energy via exclusive DAM purchases, then actively engage the RTM to buy supplemental energy cheaply or, critically, sell procured DAM energy back at a profit during high RTM price periods. Our analysis reveals that this coordinated multi-scale electricity market participation strategy can dramatically reduce net electricity expenditures, achieving near-zero or even negative effective electricity costs for green hydrogen production under realistic market scenarios, effectively meaning the operation can profit from its electricity market interactions. By transforming electrolyzers from simple price-takers into active participants capable of arbitrage between market timescales, this approach unlocks a financially compelling pathway for green hydrogen, accelerating its deployment while simultaneously enhancing power grid flexibility.",
    "pdf_url": "https://arxiv.org/pdf/2510.22782v1",
    "github_url": null,
    "published": "2025-10-26T18:20:26+00:00",
    "updated": "2025-10-26T18:20:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.08938v1",
    "title": "The Impact of Artificial Intelligence on Strategic Technology Management: A Mixed-Methods Analysis of Resources, Capabilities, and Human-AI Collaboration",
    "authors": [
      "Fascinari",
      "English"
    ],
    "summary": "This paper investigates how artificial intelligence (AI) can be effectively integrated into Strategic Technology Management (STM) practices to enhance the strategic alignment and effectiveness of technology investments. Through a mixed-methods approach combining quantitative survey data (n=230) and qualitative expert interviews (n=14), this study addresses three critical research questions: what success factors AI innovates for STM roadmap formulation under uncertainty; what resources and capabilities organizations require for AI-enhanced STM; and how human-AI interaction should be designed for complex STM tasks. The findings reveal that AI fundamentally transforms STM through data-driven strategic alignment and continuous adaptation, while success depends on cultivating proprietary data ecosystems, specialized human talent, and robust governance capabilities. The study introduces the AI-based Strategic Technology Management (AIbSTM) conceptual framework, which synthesizes technical capabilities with human and organizational dimensions across three layers: strategic alignment, resource-based view, and human-AI interaction. Contrary to visions of autonomous AI leadership, the research demonstrates that the most viable trajectory is human-centric augmentation, where AI serves as a collaborative partner rather than a replacement for human judgment. This work contributes to theory by extending the Resource-Based View to AI contexts and addressing cognitive and socio-technical chasms in AI adoption, while offering practitioners a prescriptive framework for navigating AI integration in strategic technology management.",
    "pdf_url": "https://arxiv.org/pdf/2512.08938v1",
    "github_url": null,
    "published": "2025-10-26T17:34:08+00:00",
    "updated": "2025-10-26T17:34:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22736v2",
    "title": "Cross-view Localization and Synthesis -- Datasets, Challenges and Opportunities",
    "authors": [
      "Xu",
      "Qin"
    ],
    "summary": "Cross-view localization and synthesis are two fundamental tasks in cross-view visual understanding, which deals with cross-view datasets: overhead (satellite or aerial) and ground-level imagery. These tasks have gained increasing attention due to their broad applications in autonomous navigation, urban planning, and augmented reality. Cross-view localization aims to estimate the geographic position of ground-level images based on information provided by overhead imagery while cross-view synthesis seeks to generate ground-level images based on information from the overhead imagery. Both tasks remain challenging due to significant differences in viewing perspective, resolution, and occlusion, which are widely embedded in cross-view datasets. Recent years have witnessed rapid progress driven by the availability of large-scale datasets and novel approaches. Typically, cross-view localization is formulated as an image retrieval problem where ground-level features are matched with tiled overhead images feature, extracted by convolutional neural networks (CNNs) or vision transformers (ViTs) for cross-view feature embedding. Cross-view synthesis, on the other hand, seeks to generate ground-level views based on information from overhead imagery, generally using generative adversarial networks (GANs) or diffusion models. This paper presents a comprehensive survey of advances in cross-view localization and synthesis, reviewing widely used datasets, highlighting key challenges, and providing an organized overview of state-of-the-art techniques. Furthermore, it discusses current limitations, offers comparative analyses, and outlines promising directions for future research. We also include the project page via https://github.com/GDAOSU/Awesome-Cross-View-Methods.",
    "pdf_url": "https://arxiv.org/pdf/2510.22736v2",
    "github_url": "https://github.com/GDAOSU/Awesome-Cross-View-Methods",
    "published": "2025-10-26T16:09:53+00:00",
    "updated": "2025-10-30T18:07:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22632v1",
    "title": "Environment-aware Motion Matching",
    "authors": [
      "Ponton",
      "Andrews",
      "Andujar"
    ],
    "summary": "Interactive applications demand believable characters that respond naturally to dynamic environments. Traditional character animation techniques often struggle to handle arbitrary situations, leading to a growing trend of dynamically selecting motion-captured animations based on predefined features. While Motion Matching has proven effective for locomotion by aligning to target trajectories, animating environment interactions and crowd behaviors remains challenging due to the need to consider surrounding elements. Existing approaches often involve manual setup or lack the naturalism of motion capture. Furthermore, in crowd animation, body animation is frequently treated as a separate process from trajectory planning, leading to inconsistencies between body pose and root motion. To address these limitations, we present Environment-aware Motion Matching, a novel real-time system for full-body character animation that dynamically adapts to obstacles and other agents, emphasizing the bidirectional relationship between pose and trajectory. In a preprocessing step, we extract shape, pose, and trajectory features from a motion capture database. At runtime, we perform an efficient search that matches user input and current pose while penalizing collisions with a dynamic environment. Our method allows characters to naturally adjust their pose and trajectory to navigate crowded scenes.",
    "pdf_url": "https://arxiv.org/pdf/2510.22632v1",
    "github_url": null,
    "published": "2025-10-26T11:28:50+00:00",
    "updated": "2025-10-26T11:28:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22602v1",
    "title": "Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance",
    "authors": [
      "Abbasian",
      "Jain"
    ],
    "summary": "Building on decades of success in digital infrastructure and biomedical innovation, we propose the Personal Care Utility (PCU) - a cybernetic system for lifelong health guidance. PCU is conceived as a global, AI-powered utility that continuously orchestrates multimodal data, knowledge, and services to assist individuals and populations alike. Drawing on multimodal agents, event-centric modeling, and contextual inference, it offers three essential capabilities: (1) trusted health information tailored to the individual, (2) proactive health navigation and behavior guidance, and (3) ongoing interpretation of recovery and treatment response after medical events. Unlike conventional episodic care, PCU functions as an ambient, adaptive companion - observing, interpreting, and guiding health in real time across daily life. By integrating personal sensing, experiential computing, and population-level analytics, PCU promises not only improved outcomes for individuals but also a new substrate for public health and scientific discovery. We describe the architecture, design principles, and implementation challenges of this emerging paradigm.",
    "pdf_url": "https://arxiv.org/pdf/2510.22602v1",
    "github_url": null,
    "published": "2025-10-26T09:43:33+00:00",
    "updated": "2025-10-26T09:43:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22582v2",
    "title": "MobileGeo: Exploring Hierarchical Knowledge Distillation for Resource-Efficient Cross-view Drone Geo-Localization",
    "authors": [
      "Sun",
      "Liu",
      "Zhang"
    ],
    "summary": "Cross-view geo-localization (CVGL) enables drone localization by matching aerial images to geo-tagged satellite databases, which is critical for autonomous navigation in GNSS-denied environments. However, existing methods rely on resource-intensive feature alignment and multi-branch architectures, incurring high inference costs that limit their deployment on mobile edge devices. We propose MobileGeo, a mobile-friendly framework designed for efficient on-device CVGL. MobileGeo achieves its efficiency through two key components: 1) During training, a Hierarchical Distillation (HD-CVGL) paradigm, coupled with Uncertainty-Aware Prediction Alignment (UAPA), distills essential information into a compact model without incurring inference overhead. 2) During inference, an efficient Multi-view Selection Refinement Module (MSRM) leverages mutual information to filter redundant views and reduce computational load. Extensive experiments demonstrate that MobileGeo outperforms previous state-of-the-art methods, achieving a 4.19\\% improvement in AP on University-1652 dataset while being over 5$\\times$ more efficient in FLOPs and 3$\\times$ faster. Crucially, MobileGeo runs at 251.5 FPS on an NVIDIA AGX Orin edge device, demonstrating its practical viability for real-time on-device drone geo-localization.",
    "pdf_url": "https://arxiv.org/pdf/2510.22582v2",
    "github_url": null,
    "published": "2025-10-26T08:47:20+00:00",
    "updated": "2025-11-05T02:55:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22504v1",
    "title": "On Steerability Factors for Growing Vine Robots",
    "authors": [
      "McFarland",
      "Alvarez",
      "Taher"
    ],
    "summary": "Vine robots extend their tubular bodies by everting material from the tip, enabling navigation in complex environments with a minimalist soft body. Despite their promise for field applications, especially in the urban search and rescue domain, performance is constrained by the weight of attached sensors or tools, as well as other design and control choices. This work investigates how tip load, pressure, length, diameter, and fabrication method shape vine robot steerability--the ability to maneuver with controlled curvature--for robots that steer with series pouch motor-style pneumatic actuators. We conduct two groups of experiments: (1) studying tip load, chamber pressure, length, and diameter in a robot supporting itself against gravity, and (2) studying fabrication method and ratio of actuator to chamber pressure in a robot supported on the ground. Results show that steerability decreases with increasing tip load, is best at moderate chamber pressure, increases with length, and is largely unaffected by diameter. Robots with actuators attached on their exterior begin curving at low pressure ratios, but curvature saturates at high pressure ratios; those with actuators integrated into the robot body require higher pressure ratios to begin curving but achieve higher curvature overall. We demonstrate that robots optimized with these principles outperform those with ad hoc parameters in a mobility task that involves maximizing upward and horizontal curvatures.",
    "pdf_url": "https://arxiv.org/pdf/2510.22504v1",
    "github_url": null,
    "published": "2025-10-26T03:00:02+00:00",
    "updated": "2025-10-26T03:00:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22503v1",
    "title": "Accelerating Materials Design via LLM-Guided Evolutionary Search",
    "authors": [
      "Abhyankar",
      "Kabra",
      "Desai"
    ],
    "summary": "Materials discovery requires navigating vast chemical and structural spaces while satisfying multiple, often conflicting, objectives. We present LLM-guided Evolution for MAterials design (LLEMA), a unified framework that couples the scientific knowledge embedded in large language models with chemistry-informed evolutionary rules and memory-based refinement. At each iteration, an LLM proposes crystallographically specified candidates under explicit property constraints; a surrogate-augmented oracle estimates physicochemical properties; and a multi-objective scorer updates success/failure memories to guide subsequent generations. Evaluated on 14 realistic tasks spanning electronics, energy, coatings, optics, and aerospace, LLEMA discovers candidates that are chemically plausible, thermodynamically stable, and property-aligned, achieving higher hit-rates and stronger Pareto fronts than generative and LLM-only baselines. Ablation studies confirm the importance of rule-guided generation, memory-based refinement, and surrogate prediction. By enforcing synthesizability and multi-objective trade-offs, LLEMA delivers a principled pathway to accelerate practical materials discovery.   Code: https://github.com/scientific-discovery/LLEMA",
    "pdf_url": "https://arxiv.org/pdf/2510.22503v1",
    "github_url": "https://github.com/scientific-discovery/LLEMA",
    "published": "2025-10-26T02:47:15+00:00",
    "updated": "2025-10-26T02:47:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22448v1",
    "title": "A short methodological review on social robot navigation benchmarking",
    "authors": [
      "Chhetri",
      "Torrejon",
      "Eslava"
    ],
    "summary": "Social Robot Navigation is the skill that allows robots to move efficiently in human-populated environments while ensuring safety, comfort, and trust. Unlike other areas of research, the scientific community has not yet achieved an agreement on how Social Robot Navigation should be benchmarked. This is notably important, as the lack of a de facto standard to benchmark Social Robot Navigation can hinder the progress of the field and may lead to contradicting conclusions. Motivated by this gap, we contribute with a short review focused exclusively on benchmarking trends in the period from January 2020 to July 2025. Of the 130 papers identified by our search using IEEE Xplore, we analysed the 85 papers that met the criteria of the review. This review addresses the metrics used in the literature for benchmarking purposes, the algorithms employed in such benchmarks, the use of human surveys for benchmarking, and how conclusions are drawn from the benchmarking results, when applicable.",
    "pdf_url": "https://arxiv.org/pdf/2510.22448v1",
    "github_url": null,
    "published": "2025-10-25T22:31:06+00:00",
    "updated": "2025-10-25T22:31:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22417v1",
    "title": "Genetic Optimization of a Software-Defined GNSS Receiver",
    "authors": [
      "Train",
      "Castellanos",
      "Gómez-López"
    ],
    "summary": "Commercial off-the-shelf (COTS) Global Navigation Satellite System (GNSS) receivers face significant limitations under high-dynamic conditions, particularly in high-acceleration environments such as those experienced by launch vehicles. These performance degradations, often observed as discontinuities in the navigation solution, arise from the inability of traditional tracking loop bandwidths to cope with rapid variations in synchronization parameters. Software-Defined Radio (SDR) receivers overcome these constraints by enabling flexible reconfiguration of tracking loops; however, manual tuning involves a complex, multidimensional search and seldom ensures optimal performance. This work introduces a genetic algorithm-based optimization framework that autonomously explores the receiver configuration space to determine optimal loop parameters for phase, frequency, and delay tracking. The approach is validated within an SDR environment using realistically simulated GPS L1 signals for three representative dynamic regimes -guided rocket flight, Low Earth Orbit (LEO) satellite, and static receiver-processed with the open-source GNSS-SDR architecture. Results demonstrate that evolutionary optimization enables SDR receivers to maintain robust and accurate Position, Velocity, and Time (PVT) solutions across diverse dynamic conditions. The optimized configurations yielded maximum position and velocity errors of approximately 6 m and 0.08 m/s for the static case, 12 m and 2.5 m/s for the rocket case, and 5 m and 0.2 m/s for the LEO case.",
    "pdf_url": "https://arxiv.org/pdf/2510.22417v1",
    "github_url": null,
    "published": "2025-10-25T19:32:32+00:00",
    "updated": "2025-10-25T19:32:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22316v1",
    "title": "Dynamically Detect and Fix Hardness for Efficient Approximate Nearest Neighbor Search",
    "authors": [
      "Hua",
      "Mo",
      "Yao"
    ],
    "summary": "Approximate Nearest Neighbor Search (ANNS) has become a fundamental component in many real-world applications. Among various ANNS algorithms, graph-based methods are state-of-the-art. However, ANNS often suffers from a significant drop in accuracy for certain queries, especially in Out-of-Distribution (OOD) scenarios. To address this issue, a recent approach named RoarGraph constructs a bipartite graph between the base data and historical queries to bridge the gap between two different distributions. However, it suffers from some limitations: (1) Building a bipartite graph between two distributions lacks theoretical support, resulting in the query distribution not being effectively utilized by the graph index. (2) Requires a sufficient number of historical queries before graph construction and suffers from high construction times. (3) When the query workload changes, it requires reconstruction to maintain high search accuracy.   In this paper, we first propose Escape Hardness, a metric to evaluate the quality of the graph structure around the query. Then we divide the graph search into two stages and dynamically identify and fix defective graph regions in each stage based on Escape Hardness. (1) From the entry point to the vicinity of the query. We propose Reachability Fixing (RFix), which enhances the navigability of some key nodes. (2) Searching within the vicinity of the query. We propose Neighboring Graph Defects Fixing (NGFix) to improve graph connectivity in regions where queries are densely distributed. The results of extensive experiments show that our method outperforms other state-of-the-art methods on real-world datasets, achieving up to 2.25x faster search speed for OOD queries at 99% recall compared with RoarGraph and 6.88x faster speed compared with HNSW. It also accelerates index construction by 2.35-9.02x compared to RoarGraph.",
    "pdf_url": "https://arxiv.org/pdf/2510.22316v1",
    "github_url": null,
    "published": "2025-10-25T14:32:58+00:00",
    "updated": "2025-10-25T14:32:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22209v1",
    "title": "Visual Model Selection using Feature Importance Clusters in Fairness-Performance Similarity Optimized Space",
    "authors": [
      "Kitharidis",
      "Veenman",
      "Bäck"
    ],
    "summary": "In the context of algorithmic decision-making, fair machine learning methods often yield multiple models that balance predictive fairness and performance in varying degrees. This diversity introduces a challenge for stakeholders who must select a model that aligns with their specific requirements and values. To address this, we propose an interactive framework that assists in navigating and interpreting the trade-offs across a portfolio of models. Our approach leverages weakly supervised metric learning to learn a Mahalanobis distance that reflects similarity in fairness and performance outcomes, effectively structuring the feature importance space of the models according to stakeholder-relevant criteria. We then apply clustering technique (k-means) to group models based on their transformed representations of feature importances, allowing users to explore clusters of models with similar predictive behaviors and fairness characteristics. This facilitates informed decision-making by helping users understand how models differ not only in their fairness-performance balance but also in the features that drive their predictions.",
    "pdf_url": "https://arxiv.org/pdf/2510.22209v1",
    "github_url": null,
    "published": "2025-10-25T08:18:41+00:00",
    "updated": "2025-10-25T08:18:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22164v1",
    "title": "LT-Exosense: A Vision-centric Multi-session Mapping System for Lifelong Safe Navigation of Exoskeletons",
    "authors": [
      "Wang",
      "Mattamala",
      "Kassab"
    ],
    "summary": "Self-balancing exoskeletons offer a promising mobility solution for individuals with lower-limb disabilities. For reliable long-term operation, these exoskeletons require a perception system that is effective in changing environments. In this work, we introduce LT-Exosense, a vision-centric, multi-session mapping system designed to support long-term (semi)-autonomous navigation for exoskeleton users. LT-Exosense extends single-session mapping capabilities by incrementally fusing spatial knowledge across multiple sessions, detecting environmental changes, and updating a persistent global map. This representation enables intelligent path planning, which can adapt to newly observed obstacles and can recover previous routes when obstructions are removed. We validate LT-Exosense through several real-world experiments, demonstrating a scalable multi-session map that achieves an average point-to-point error below 5 cm when compared to ground-truth laser scans. We also illustrate the potential application of adaptive path planning in dynamically changing indoor environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.22164v1",
    "github_url": null,
    "published": "2025-10-25T05:23:50+00:00",
    "updated": "2025-10-25T05:23:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.22039v1",
    "title": "Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability",
    "authors": [
      "Kuo",
      "Hou",
      "Dabney"
    ],
    "summary": "Learning a compact representation of history is critical for planning and generalization in partially observable environments. While meta-reinforcement learning (RL) agents can attain near Bayes-optimal policies, they often fail to learn the compact, interpretable Bayes-optimal belief states. This representational inefficiency potentially limits the agent's adaptability and generalization capacity. Inspired by predictive coding in neuroscience--which suggests that the brain predicts sensory inputs as a neural implementation of Bayesian inference--and by auxiliary predictive objectives in deep RL, we investigate whether integrating self-supervised predictive coding modules into meta-RL can facilitate learning of Bayes-optimal representations. Through state machine simulation, we show that meta-RL with predictive modules consistently generates more interpretable representations that better approximate Bayes-optimal belief states compared to conventional meta-RL across a wide variety of tasks, even when both achieve optimal policies. In challenging tasks requiring active information seeking, only meta-RL with predictive modules successfully learns optimal representations and policies, whereas conventional meta-RL struggles with inadequate representation learning. Finally, we demonstrate that better representation learning leads to improved generalization. Our results strongly suggest the role of predictive learning as a guiding principle for effective representation learning in agents navigating partial observability.",
    "pdf_url": "https://arxiv.org/pdf/2510.22039v1",
    "github_url": null,
    "published": "2025-10-24T21:45:56+00:00",
    "updated": "2025-10-24T21:45:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21932v1",
    "title": "Emergent Microrobotic Behavior of Active Flexicles in Complex Environments",
    "authors": [
      "Lee",
      "Schönhöfer",
      "Glotzer"
    ],
    "summary": "Collections of simple, self-propelled colloidal particles exhibit complex, emergent dynamical behavior, with promising applications in microrobotics. When confined within a deformable vesicle, self-propelled rods cluster and align, propelling the vesicle and inducing changes in the vesicle shape. We explore potential microrobotic capabilities of such vesicle-encapsulated particles, which form a composite particle system termed a `flexicle'. Using molecular dynamics simulations, we demonstrate that the alignment of rods enables flexicles to locomote and respond adaptively to their physical environment. When encountering solid boundaries or obstacles, the rods reorient at the interface, triggering novel emergent behaviors such as crawling, corner-preferencing, wall climbing, and object-latching. These interactions and accompanying internal rod re-arrangement lead to spontaneous, temporary differentiation of the rods into `latchers' and `navigators'. This division of labor among the rods enables coordinated locomotion and environmental response. Our findings establish flexicles as a versatile platform for programmable, geometry-sensitive microrobotic behavior, offering a step toward autonomous colloidal robotics.",
    "pdf_url": "https://arxiv.org/pdf/2510.21932v1",
    "github_url": null,
    "published": "2025-10-24T18:03:51+00:00",
    "updated": "2025-10-24T18:03:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21648v1",
    "title": "Design and Structural Validation of a Micro-UAV with On-Board Dynamic Route Planning",
    "authors": [
      "Ravikumar",
      "Sundhar",
      "Vijayakumar"
    ],
    "summary": "Micro aerial vehicles are becoming increasingly important in search and rescue operations due to their agility, speed, and ability to access confined spaces or hazardous areas. However, designing lightweight aerial systems presents significant structural, aerodynamic, and computational challenges. This work addresses two key limitations in many low-cost aerial systems under two kilograms: their lack of structural durability during flight through rough terrains and inability to replan paths dynamically when new victims or obstacles are detected. We present a fully customised drone built from scratch using only commonly available components and materials, emphasising modularity, low cost, and ease of assembly. The structural frame is reinforced with lightweight yet durable materials to withstand impact, while the onboard control system is powered entirely by free, open-source software solutions. The proposed system demonstrates real-time perception and adaptive navigation capabilities without relying on expensive hardware accelerators, offering an affordable and practical solution for real-world search and rescue missions.",
    "pdf_url": "https://arxiv.org/pdf/2510.21648v1",
    "github_url": null,
    "published": "2025-10-24T17:05:19+00:00",
    "updated": "2025-10-24T17:05:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21548v1",
    "title": "Swimming patterns of a multi-mode bacterial swimmer in fluid shear flow",
    "authors": [
      "Muraveva",
      "Datta",
      "Park"
    ],
    "summary": "Bacterial swimming is well characterized in uniform liquids at rest. The natural habitat of bacterial swimmers, however, is often dominated by moving fluids and interfaces, resulting in shear flows that may strongly alter bacterial navigation strategies. Here, we study how fluid shear flow affects the swimming motility of the soil bacterium Pseudomonas putida, a bacterial swimmer that moves in a versatile pattern composed of three different swimming modes, where the flagella may push, pull, or wrap around the cell body (multi-mode swimmer). We introduce a computer automated cell tracking and swimming mode detection tool to show that shear induced alignment depends on the swimming mode, while motility and proximity to surfaces counteract the alignment effect. Moreover, filament wrapping becomes less efficient with increasing shear stress. Numerical simulations of realistic swimmer geometries complement our experimental results, providing more detailed mechanistic insights into movement patterns of bacterial swimmers in a shear flow.",
    "pdf_url": "https://arxiv.org/pdf/2510.21548v1",
    "github_url": null,
    "published": "2025-10-24T15:07:46+00:00",
    "updated": "2025-10-24T15:07:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21536v1",
    "title": "AURASeg: Attention Guided Upsampling with Residual Boundary-Assistive Refinement for Drivable-Area Segmentation",
    "authors": [
      "Vijayakumar",
      "M"
    ],
    "summary": "Free space ground segmentation is essential to navigate robots and autonomous vehicles, recognize drivable zones, and traverse efficiently. Fine-grained features remain challenging for existing segmentation models, particularly for robots in indoor and structured environments. These difficulties arise from ineffective multi-scale processing, suboptimal boundary refinement, and limited feature representation. In order to overcome these limitations, we propose Attention-Guided Upsampling with Residual Boundary-Assistive Refinement (AURASeg), a ground-plane semantic segmentation model that maintains high segmentation accuracy while improving border precision. Our method uses CSP-Darknet backbone by adding a Residual Border Refinement Module (RBRM) for accurate edge delineation and an Attention Progressive Upsampling Decoder (APUD) for strong feature integration. We also incorporate a lightweight Atrous Spatial Pyramid Pooling (ASPP-Lite) module to ensure multi-scale context extraction without compromising real-time performance. The proposed model beats benchmark segmentation architectures in mIoU and F1 metrics when tested on the Ground Mobile Robot Perception (GMRP) Dataset and a custom Gazebo indoor dataset. Our approach achieves an improvement in mean Intersection-over-Union (mIoU) of +1.26% and segmentation precision of +1.65% compared to state-of-the-art models. These results show that our technique is feasible for autonomous perception in both indoor and outdoor environments, enabling precise border refinement with minimal effect on inference speed.",
    "pdf_url": "https://arxiv.org/pdf/2510.21536v1",
    "github_url": null,
    "published": "2025-10-24T15:01:18+00:00",
    "updated": "2025-10-24T15:01:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21526v1",
    "title": "Recommended Practices for NPOV Research on Wikipedia",
    "authors": [
      "Johnson",
      "Liou",
      "Rogers"
    ],
    "summary": "Writing Wikipedia with a neutral point of view is one of the five pillars of Wikipedia. Although the topic is core to Wikipedia, it is relatively understudied considering hundreds of research studies are published annually about the project. We hypothesize that part of the reason for the low research activity on the topic is that Wikipedia's definition of neutrality and its importance are not well understood within the research community. Neutrality is also an inherently challenging and contested concept. Our aim with this paper is to accelerate high quality research in this space that can help Wikipedia communities continue to improve their work in writing the encyclopedia. We do this by helping researchers to learn what Neutral Point of View means in the context of Wikipedia, identifying some common challenges with studying NPOV and how to navigate them, and offering guidance on how researchers can communicate the results of their work for increased impact on the ground for the benefit of Wikipedia.",
    "pdf_url": "https://arxiv.org/pdf/2510.21526v1",
    "github_url": null,
    "published": "2025-10-24T14:50:31+00:00",
    "updated": "2025-10-24T14:50:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21467v1",
    "title": "Co-Designing with Multiple Stakeholders and Datasets: A Community-Centered Process to Understand Youth Deviance in the Italian City of Turin",
    "authors": [
      "Annapureddy",
      "Fornaroli",
      "Fattori"
    ],
    "summary": "This paper presents the co-design and design evaluation of Sbocciamo Torino civic tool, which helps understand and act upon the issues of youth deviance in the Italian city of Turin through multi-stakeholder collaboration and collaborative data analysis. Rooted in research through design and participatory design methodologies, the civic tool integrates a data dashboard, stakeholder committee, and structured co-design sessions to facilitate collaborative analysis and intervention planning. The civic tool was developed in partnership with municipal authorities, law enforcement, NGOs, and social services, and reflects their institutional priorities while centering community knowledge. We describe the iterative co-design process, including stakeholder workshops for design, validation, training, and evaluation. The civic tool's impact on stakeholder trust, collaboration, and decision-making was assessed through surveys and open-ended questionnaires. Our findings show that stakeholders valued the inclusive design approach and data-driven collaboration while revealing barriers in communication, data literacy, and operational coordination. Furthermore, political and institutional support was identified as critical to the civic tool's success. This paper contributes to research on community technologies by demonstrating how civic tools can be collaboratively developed to navigate wicked social problems through participatory design.",
    "pdf_url": "https://arxiv.org/pdf/2510.21467v1",
    "github_url": null,
    "published": "2025-10-24T13:53:06+00:00",
    "updated": "2025-10-24T13:53:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21438v1",
    "title": "PREVENT: Proactive Risk Evaluation and Vigilant Execution of Tasks for Mobile Robotic Chemists using Multi-Modal Behavior Trees",
    "authors": [
      "Veeramani",
      "Zhou",
      "Munguia-Galeano"
    ],
    "summary": "Mobile robotic chemists are a fast growing trend in the field of chemistry and materials research. However, so far these mobile robots lack workflow awareness skills. This poses the risk that even a small anomaly, such as an improperly capped sample vial could disrupt the entire workflow. This wastes time, and resources, and could pose risks to human researchers, such as exposure to toxic materials. Existing perception mechanisms can be used to predict anomalies but they often generate excessive false positives. This may halt workflow execution unnecessarily, requiring researchers to intervene and to resume the workflow when no problem actually exists, negating the benefits of autonomous operation. To address this problem, we propose PREVENT a system comprising navigation and manipulation skills based on a multimodal Behavior Tree (BT) approach that can be integrated into existing software architectures with minimal modifications. Our approach involves a hierarchical perception mechanism that exploits AI techniques and sensory feedback through Dexterous Vision and Navigational Vision cameras and an IoT gas sensor module for execution-related decision-making. Experimental evaluations show that the proposed approach is comparatively efficient and completely avoids both false negatives and false positives when tested in simulated risk scenarios within our robotic chemistry workflow. The results also show that the proposed multi-modal perception skills achieved deployment accuracies that were higher than the average of the corresponding uni-modal skills, both for navigation and for manipulation.",
    "pdf_url": "https://arxiv.org/pdf/2510.21438v1",
    "github_url": null,
    "published": "2025-10-24T13:16:01+00:00",
    "updated": "2025-10-24T13:16:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21369v1",
    "title": "Load-bearing Assessment for Safe Locomotion of Quadruped Robots on Collapsing Terrain",
    "authors": [
      "Medeiros",
      "Dessy",
      "Boaventura"
    ],
    "summary": "Collapsing terrains, often present in search and rescue missions or planetary exploration, pose significant challenges for quadruped robots. This paper introduces a robust locomotion framework for safe navigation over unstable surfaces by integrating terrain probing, load-bearing analysis, motion planning, and control strategies. Unlike traditional methods that rely on specialized sensors or external terrain mapping alone, our approach leverages joint measurements to assess terrain stability without hardware modifications. A Model Predictive Control (MPC) system optimizes robot motion, balancing stability and probing constraints, while a state machine coordinates terrain probing actions, enabling the robot to detect collapsible regions and dynamically adjust its footholds. Experimental results on custom-made collapsing platforms and rocky terrains demonstrate the framework's ability to traverse collapsing terrain while maintaining stability and prioritizing safety.",
    "pdf_url": "https://arxiv.org/pdf/2510.21369v1",
    "github_url": null,
    "published": "2025-10-24T11:52:15+00:00",
    "updated": "2025-10-24T11:52:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21307v2",
    "title": "Towards Physically Executable 3D Gaussian for Embodied Navigation",
    "authors": [
      "Miao",
      "Wei",
      "Ge"
    ],
    "summary": "3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic real-time rendering capabilities, is regarded as an effective tool for narrowing the sim-to-real gap. However, it lacks fine-grained semantics and physical executability for Visual-Language Navigation (VLN). To address this, we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments for 3D Navigation), a new paradigm that upgrades 3DGS into an executable, semantically and physically aligned environment. It comprises two components: (1) Object-Centric Semantic Grounding, which adds object-level fine-grained annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds collision objects into 3DGS and constructs rich physical interfaces. We release InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data. Experiments show that 3DGS scene data is more difficult to converge, while exhibiting strong generalizability, improving baseline performance by 31% on the VLN-CE Unseen task. Our data and code are available at: https://sage-3d.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2510.21307v2",
    "github_url": null,
    "published": "2025-10-24T10:05:00+00:00",
    "updated": "2025-12-15T10:05:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21219v1",
    "title": "World Models Should Prioritize the Unification of Physical and Social Dynamics",
    "authors": [
      "Zhang",
      "Ma",
      "Huang"
    ],
    "summary": "World models, which explicitly learn environmental dynamics to lay the foundation for planning, reasoning, and decision-making, are rapidly advancing in predicting both physical dynamics and aspects of social behavior, yet predominantly in separate silos. This division results in a systemic failure to model the crucial interplay between physical environments and social constructs, rendering current models fundamentally incapable of adequately addressing the true complexity of real-world systems where physical and social realities are inextricably intertwined. This position paper argues that the systematic, bidirectional unification of physical and social predictive capabilities is the next crucial frontier for world model development. We contend that comprehensive world models must holistically integrate objective physical laws with the subjective, evolving, and context-dependent nature of social dynamics. Such unification is paramount for AI to robustly navigate complex real-world challenges and achieve more generalizable intelligence. This paper substantiates this imperative by analyzing core impediments to integration, proposing foundational guiding principles (ACE Principles), and outlining a conceptual framework alongside a research roadmap towards truly holistic world models.",
    "pdf_url": "https://arxiv.org/pdf/2510.21219v1",
    "github_url": null,
    "published": "2025-10-24T07:42:37+00:00",
    "updated": "2025-10-24T07:42:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21123v1",
    "title": "Navigating Through Turbulence: Blueprint for the Next Generation of Weather-Climate Scientists",
    "authors": [
      "Zhang",
      "Wang",
      "Reed"
    ],
    "summary": "The field of weather and climate science is at a pivotal moment, defined by the dual forces of unprecedented technological advancement. While a shifting research and employment landscape has created career uncertainty, leading to a significant migration of talent toward the private sector, it has simultaneously spurred an expansion of the ecosystem through the emergence of new computational tools and the growing role of industry innovators and stakeholders. This perspective paper argues that this new, expanded ecosystem presents extraordinary opportunities for students and early-career professionals. We outline the emerging scientific frontiers powered by high-resolution simulations and artificial intelligence, suggest a practical path for navigating a more fluid career landscape, and propose how education and training must evolve to equip the next generation for success.",
    "pdf_url": "https://arxiv.org/pdf/2510.21123v1",
    "github_url": null,
    "published": "2025-10-24T03:26:50+00:00",
    "updated": "2025-10-24T03:26:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21082v1",
    "title": "Soppia: A Structured Prompting Framework for the Proportional Assessment of Non-Pecuniary Damages in Personal Injury Cases",
    "authors": [
      "Araujo"
    ],
    "summary": "Applying complex legal rules characterized by multiple, heterogeneously weighted criteria presents a fundamental challenge in judicial decision-making, often hindering the consistent realization of legislative intent. This challenge is particularly evident in the quantification of non-pecuniary damages in personal injury cases. This paper introduces Soppia, a structured prompting framework designed to assist legal professionals in navigating this complexity. By leveraging advanced AI, the system ensures a comprehensive and balanced analysis of all stipulated criteria, fulfilling the legislator's intent that compensation be determined through a holistic assessment of each case. Using the twelve criteria for non-pecuniary damages established in the Brazilian CLT (Art. 223-G) as a case study, we demonstrate how Soppia (System for Ordered Proportional and Pondered Intelligent Assessment) operationalizes nuanced legal commands into a practical, replicable, and transparent methodology. The framework enhances consistency and predictability while providing a versatile and explainable tool adaptable across multi-criteria legal contexts, bridging normative interpretation and computational reasoning toward auditable legal AI.",
    "pdf_url": "https://arxiv.org/pdf/2510.21082v1",
    "github_url": null,
    "published": "2025-10-24T01:42:38+00:00",
    "updated": "2025-10-24T01:42:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21074v1",
    "title": "Revisiting Replanning from Scratch: Real-Time Incremental Planning with Fast Almost-Surely Asymptotically Optimal Planners",
    "authors": [
      "Sabbadini",
      "Liu",
      "Ruan"
    ],
    "summary": "Robots operating in changing environments either predict obstacle changes and/or plan quickly enough to react to them. Predictive approaches require a strong prior about the position and motion of obstacles. Reactive approaches require no assumptions about their environment but must replan quickly and find high-quality paths to navigate effectively.   Reactive approaches often reuse information between queries to reduce planning cost. These techniques are conceptually sound but updating dense planning graphs when information changes can be computationally prohibitive. It can also require significant effort to detect the changes in some applications.   This paper revisits the long-held assumption that reactive replanning requires updating existing plans. It shows that the incremental planning problem can alternatively be solved more efficiently as a series of independent problems using fast almost-surely asymptotically optimal (ASAO) planning algorithms. These ASAO algorithms quickly find an initial solution and converge towards an optimal solution which allows them to find consistent global plans in the presence of changing obstacles without requiring explicit plan reuse. This is demonstrated with simulated experiments where Effort Informed Trees (EIT*) finds shorter median solution paths than the tested reactive planning algorithms and is further validated using Asymptotically Optimal RRT-Connect (AORRTC) on a real-world planning problem on a robot arm.",
    "pdf_url": "https://arxiv.org/pdf/2510.21074v1",
    "github_url": null,
    "published": "2025-10-24T01:06:36+00:00",
    "updated": "2025-10-24T01:06:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20976v1",
    "title": "L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks",
    "authors": [
      "Cui",
      "Wu",
      "Zhao"
    ],
    "summary": "Large language models have demonstrated remarkable reasoning capabilities across diverse natural language tasks. However, comparable breakthroughs in scientific discovery are more limited, because understanding complex physical phenomena demands multifaceted representations far beyond language alone. A compelling example is the design of functional materials such as MOFs-critical for a range of impactful applications like carbon capture and hydrogen storage. Navigating their vast and intricate design space in language-based representations interpretable by LLMs is challenging due to the numerous possible three-dimensional atomic arrangements and strict reticular rules of coordination geometry and topology. Despite promising early results in LLM-assisted discovery for simpler materials systems, MOF design remains heavily reliant on tacit human expertise rarely codified in textual information alone. To overcome this barrier, we introduce L2M3OF, the first multimodal LLM for MOFs. L2M3OF integrates crystal representation learning with language understanding to process structural, textual, and knowledge modalities jointly. L2M3OF employs a pre-trained crystal encoder with a lightweight projection layer to compress structural information into a token space, enabling efficient alignment with language instructions. To facilitate training and evaluation, we curate a structure-property-knowledge database of crystalline materials and benchmark L2M3OF against state-of-the-art closed-source LLMs such as GPT-5, Gemini-2.5-Pro and DeepSeek-R1. Experiments show that L2M3OF outperforms leading text-based closed-source LLMs in property prediction and knowledge generation tasks, despite using far fewer parameters. These results highlight the importance of multimodal approaches for porous material understanding and establish L2M3OF as a foundation for next-generation AI systems in materials discovery.",
    "pdf_url": "https://arxiv.org/pdf/2510.20976v1",
    "github_url": null,
    "published": "2025-10-23T20:12:46+00:00",
    "updated": "2025-10-23T20:12:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20932v1",
    "title": "An Experimental Study of Trojan Vulnerabilities in UAV Autonomous Landing",
    "authors": [
      "Ahmari",
      "Mohammadi",
      "Hemmati"
    ],
    "summary": "This study investigates the vulnerabilities of autonomous navigation and landing systems in Urban Air Mobility (UAM) vehicles. Specifically, it focuses on Trojan attacks that target deep learning models, such as Convolutional Neural Networks (CNNs). Trojan attacks work by embedding covert triggers within a model's training data. These triggers cause specific failures under certain conditions, while the model continues to perform normally in other situations. We assessed the vulnerability of Urban Autonomous Aerial Vehicles (UAAVs) using the DroNet framework. Our experiments showed a significant drop in accuracy, from 96.4% on clean data to 73.3% on data triggered by Trojan attacks. To conduct this study, we collected a custom dataset and trained models to simulate real-world conditions. We also developed an evaluation framework designed to identify Trojan-infected models. This work demonstrates the potential security risks posed by Trojan attacks and lays the groundwork for future research on enhancing the resilience of UAM systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.20932v1",
    "github_url": null,
    "published": "2025-10-23T18:47:40+00:00",
    "updated": "2025-10-23T18:47:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20818v1",
    "title": "VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation",
    "authors": [
      "Castro",
      "Rajagopal",
      "Gorbatov"
    ],
    "summary": "A fundamental challenge in robot navigation lies in learning policies that generalize across diverse environments while conforming to the unique physical constraints and capabilities of a specific embodiment (e.g., quadrupeds can walk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that decouples semantic planning from embodiment grounding: a generalist planner learns from diverse, open-world data, while a specialist affordance model learns the robot's physical constraints and capabilities in safe, low-cost simulation. We enabled this separation by carefully designing an interface that lets a high-level planner propose candidate paths directly in image space that the affordance model then evaluates and re-ranks. Our real-world experiments show that VAMOS achieves higher success rates in both indoor and complex outdoor navigation than state-of-the-art model-based and end-to-end learning methods. We also show that our hierarchical design enables cross-embodied navigation across legged and wheeled robots and is easily steerable using natural language. Real-world ablations confirm that the specialist model is key to embodiment grounding, enabling a single high-level planner to be deployed across physically distinct wheeled and legged robots. Finally, this model significantly enhances single-robot reliability, achieving 3X higher success rates by rejecting physically infeasible plans. Website: https://vamos-vla.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.20818v1",
    "github_url": null,
    "published": "2025-10-23T17:59:45+00:00",
    "updated": "2025-10-23T17:59:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20808v1",
    "title": "The Reality Gap in Robotics: Challenges, Solutions, and Best Practices",
    "authors": [
      "Aljalbout",
      "Xing",
      "Romero"
    ],
    "summary": "Machine learning has facilitated significant advancements across various robotics domains, including navigation, locomotion, and manipulation. Many such achievements have been driven by the extensive use of simulation as a critical tool for training and testing robotic systems prior to their deployment in real-world environments. However, simulations consist of abstractions and approximations that inevitably introduce discrepancies between simulated and real environments, known as the reality gap. These discrepancies significantly hinder the successful transfer of systems from simulation to the real world. Closing this gap remains one of the most pressing challenges in robotics. Recent advances in sim-to-real transfer have demonstrated promising results across various platforms, including locomotion, navigation, and manipulation. By leveraging techniques such as domain randomization, real-to-sim transfer, state and action abstractions, and sim-real co-training, many works have overcome the reality gap. However, challenges persist, and a deeper understanding of the reality gap's root causes and solutions is necessary. In this survey, we present a comprehensive overview of the sim-to-real landscape, highlighting the causes, solutions, and evaluation metrics for the reality gap and sim-to-real transfer.",
    "pdf_url": "https://arxiv.org/pdf/2510.20808v1",
    "github_url": null,
    "published": "2025-10-23T17:58:53+00:00",
    "updated": "2025-10-23T17:58:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20708v1",
    "title": "ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata",
    "authors": [
      "Soutullo",
      "Yermo",
      "Vilariño"
    ],
    "summary": "3D LiDAR sensors are essential for autonomous navigation, environmental monitoring, and precision mapping in remote sensing applications. To efficiently process the massive point clouds generated by these sensors, LiDAR data is often projected into 2D range images that organize points by their angular positions and distances. While these range image representations enable efficient processing, conventional projection methods suffer from fundamental geometric inconsistencies that cause irreversible information loss, compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), the first general, sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds without requiring manufacturer metadata or calibration files. Our algorithm automatically reverse-engineers the intrinsic geometry of any spinning LiDAR sensor by inferring critical parameters including laser beam configuration, angular distributions, and per-beam calibration corrections, enabling lossless projection and complete point cloud reconstruction with zero point loss. Comprehensive evaluation across the complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect point preservation, with zero points lost across all point clouds. Geometric accuracy is maintained well within sensor precision limits, establishing geometric losslessness with real-time performance. We also present a compression case study that validates substantial downstream benefits, demonstrating significant quality improvements in practical applications. This paradigm shift from approximate to lossless LiDAR projections opens new possibilities for high-precision remote sensing applications requiring complete geometric preservation.",
    "pdf_url": "https://arxiv.org/pdf/2510.20708v1",
    "github_url": null,
    "published": "2025-10-23T16:22:58+00:00",
    "updated": "2025-10-23T16:22:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20685v2",
    "title": "C-NAV: Towards Self-Evolving Continual Object Navigation in Open World",
    "authors": [
      "Yu",
      "Zhu",
      "Liu"
    ],
    "summary": "Embodied agents are expected to perform object navigation in dynamic, open-world environments. However, existing approaches typically rely on static trajectories and a fixed set of object categories during training, overlooking the real-world requirement for continual adaptation to evolving scenarios. To facilitate related studies, we introduce the continual object navigation benchmark, which requires agents to acquire navigation skills for new object categories while avoiding catastrophic forgetting of previously learned knowledge. To tackle this challenge, we propose C-Nav, a continual visual navigation framework that integrates two key innovations: (1) A dual-path anti-forgetting mechanism, which comprises feature distillation that aligns multi-modal inputs into a consistent representation space to ensure representation consistency, and feature replay that retains temporal features within the action decoder to ensure policy consistency. (2) An adaptive sampling strategy that selects diverse and informative experiences, thereby reducing redundancy and minimizing memory overhead. Extensive experiments across multiple model architectures demonstrate that C-Nav consistently outperforms existing approaches, achieving superior performance even compared to baselines with full trajectory retention, while significantly lowering memory requirements. The code will be publicly available at https://bigtree765.github.io/C-Nav-project.",
    "pdf_url": "https://arxiv.org/pdf/2510.20685v2",
    "github_url": null,
    "published": "2025-10-23T15:57:43+00:00",
    "updated": "2025-10-30T08:58:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20666v1",
    "title": "Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts",
    "authors": [
      "Jaramillo-Civill",
      "González-Gudiño",
      "Imbiriba"
    ],
    "summary": "Global Navigation Satellite System (GNSS) signals are vulnerable to jamming, particularly in urban areas where multipath and shadowing distort received power. Previous data-driven approaches achieved reasonable localization but poorly reconstructed the received signal strength (RSS) field due to limited spatial context. We propose a hybrid Bayesian mixture-of-experts framework that fuses a physical path-loss (PL) model and a convolutional neural network (CNN) through log-linear pooling. The PL expert ensures physical consistency, while the CNN leverages building-height maps to capture urban propagation effects. Bayesian inference with Laplace approximation provides posterior uncertainty over both the jammer position and RSS field. Experiments on urban ray-tracing data show that localization accuracy improves and uncertainty decreases with more training points, while uncertainty concentrates near the jammer and along urban canyons where propagation is most sensitive.",
    "pdf_url": "https://arxiv.org/pdf/2510.20666v1",
    "github_url": null,
    "published": "2025-10-23T15:45:45+00:00",
    "updated": "2025-10-23T15:45:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20620v1",
    "title": "A Gateway to Quantum Computing for Industrial Engineering",
    "authors": [
      "Tucker",
      "Mohammadisiahroudi"
    ],
    "summary": "Quantum computing is rapidly emerging as a new computing paradigm with the potential to improve decision-making, optimization, and simulation across industries. For industrial engineering (IE) and operations research (OR), this shift introduces both unprecedented opportunities and substantial challenges. The learning curve is high, and to help researchers navigate the emerging field of quantum operations research, we provide a road map of the current field of quantum operations research. We introduce the foundational principles of quantum computing, outline the current hardware and software landscape, and survey major algorithmic advances relevant to IE/OR, including quantum approaches to linear algebra, optimization, machine learning, and stochastic simulation. We then highlight applied research directions, including the importance of problem domains for driving long-term value of quantum computers and how existing classical OR models can be reformulated for quantum hardware. Recognizing the steep learning curve, we propose pathways for IE/OR researchers to develop technical fluency and engage in this interdisciplinary domain. By bridging theory with application, and emphasizing the interplay between hardware and research development, we argue that industrial engineers are uniquely positioned to shape the trajectory of quantum computing for practical problem-solving. Ultimately, we aim to lower the barrier to entry into quantum computing, motivate new collaborations, and chart future directions where quantum technologies may deliver tangible impact for industry and academia.",
    "pdf_url": "https://arxiv.org/pdf/2510.20620v1",
    "github_url": null,
    "published": "2025-10-23T14:54:11+00:00",
    "updated": "2025-10-23T14:54:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20549v1",
    "title": "Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation",
    "authors": [
      "Bamdad",
      "Hutter",
      "Darvishy"
    ],
    "summary": "Despite advancements in SLAM technologies, robust operation under challenging conditions such as low-texture, motion-blur, or challenging lighting remains an open challenge. Such conditions are common in applications such as assistive navigation for the visually impaired. These challenges undermine localization accuracy and tracking stability, reducing navigation reliability and safety. To overcome these limitations, we present SELM-SLAM3, a deep learning-enhanced visual SLAM framework that integrates SuperPoint and LightGlue for robust feature extraction and matching. We evaluated our framework using TUM RGB-D, ICL-NUIM, and TartanAir datasets, which feature diverse and challenging scenarios. SELM-SLAM3 outperforms conventional ORB-SLAM3 by an average of 87.84% and exceeds state-of-the-art RGB-D SLAM systems by 36.77%. Our framework demonstrates enhanced performance under challenging conditions, such as low-texture scenes and fast motion, providing a reliable platform for developing navigation aids for the visually impaired.",
    "pdf_url": "https://arxiv.org/pdf/2510.20549v1",
    "github_url": null,
    "published": "2025-10-23T13:35:12+00:00",
    "updated": "2025-10-23T13:35:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20480v1",
    "title": "Degradation-Aware Cooperative Multi-Modal GNSS-Denied Localization Leveraging LiDAR-Based Robot Detections",
    "authors": [
      "Pritzl",
      "Yu",
      "Westerlund"
    ],
    "summary": "Accurate long-term localization using onboard sensors is crucial for robots operating in Global Navigation Satellite System (GNSS)-denied environments. While complementary sensors mitigate individual degradations, carrying all the available sensor types on a single robot significantly increases the size, weight, and power demands. Distributing sensors across multiple robots enhances the deployability but introduces challenges in fusing asynchronous, multi-modal data from independently moving platforms. We propose a novel adaptive multi-modal multi-robot cooperative localization approach using a factor-graph formulation to fuse asynchronous Visual-Inertial Odometry (VIO), LiDAR-Inertial Odometry (LIO), and 3D inter-robot detections from distinct robots in a loosely-coupled fashion. The approach adapts to changing conditions, leveraging reliable data to assist robots affected by sensory degradations. A novel interpolation-based factor enables fusion of the unsynchronized measurements. LIO degradations are evaluated based on the approximate scan-matching Hessian. A novel approach of weighting odometry data proportionally to the Wasserstein distance between the consecutive VIO outputs is proposed. A theoretical analysis is provided, investigating the cooperative localization problem under various conditions, mainly in the presence of sensory degradations. The proposed method has been extensively evaluated on real-world data gathered with heterogeneous teams of an Unmanned Ground Vehicle (UGV) and Unmanned Aerial Vehicles (UAVs), showing that the approach provides significant improvements in localization accuracy in the presence of various sensory degradations.",
    "pdf_url": "https://arxiv.org/pdf/2510.20480v1",
    "github_url": null,
    "published": "2025-10-23T12:20:09+00:00",
    "updated": "2025-10-23T12:20:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21865v1",
    "title": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular Framework and Conceptual Analysis",
    "authors": [
      "Qowy"
    ],
    "summary": "Caching and prefetching techniques are fundamental to modern computing, serving to bridge the growing performance gap between processors and memory. Traditional prefetching strategies are often limited by their reliance on predefined heuristics or simplified statistical models, which fail to capture the complex, non-linear dependencies in modern data access patterns. This paper introduces a modular framework leveraging Graph Neural Networks (GNNs) to model and predict access patterns within graph-structured data, focusing on web navigation and hierarchical file systems. The toolchain consists of: a route mapper for extracting structural information, a graph constructor for creating graph representations, a walk session generator for simulating user behaviors, and a gnn prefetch module for training and inference. We provide a detailed conceptual analysis showing how GNN-based approaches can outperform conventional methods by learning intricate dependencies. This work offers both theoretical foundations and a practical, replicable pipeline for future research in graph-driven systems optimization.",
    "pdf_url": "https://arxiv.org/pdf/2510.21865v1",
    "github_url": null,
    "published": "2025-10-23T10:35:35+00:00",
    "updated": "2025-10-23T10:35:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20349v1",
    "title": "Synthetic Data for Robust Runway Detection",
    "authors": [
      "Chigot",
      "Wilson",
      "Ghrib"
    ],
    "summary": "Deep vision models are now mature enough to be integrated in industrial and possibly critical applications such as autonomous navigation. Yet, data collection and labeling to train such models requires too much efforts and costs for a single company or product. This drawback is more significant in critical applications, where training data must include all possible conditions including rare scenarios. In this perspective, generating synthetic images is an appealing solution, since it allows a cheap yet reliable covering of all the conditions and environments, if the impact of the synthetic-to-real distribution shift is mitigated. In this article, we consider the case of runway detection that is a critical part in autonomous landing systems developed by aircraft manufacturers. We propose an image generation approach based on a commercial flight simulator that complements a few annotated real images. By controlling the image generation and the integration of real and synthetic data, we show that standard object detection models can achieve accurate prediction. We also evaluate their robustness with respect to adverse conditions, in our case nighttime images, that were not represented in the real data, and show the interest of using a customized domain adaptation strategy.",
    "pdf_url": "https://arxiv.org/pdf/2510.20349v1",
    "github_url": null,
    "published": "2025-10-23T08:48:37+00:00",
    "updated": "2025-10-23T08:48:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20333v2",
    "title": "GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?",
    "authors": [
      "Chen",
      "Song",
      "Chai"
    ],
    "summary": "Vision-Language Models (VLMs) are increasingly deployed as autonomous agents to navigate mobile graphical user interfaces (GUIs). Operating in dynamic on-device ecosystems, which include notifications, pop-ups, and inter-app interactions, exposes them to a unique and underexplored threat vector: environmental injection. Unlike prompt-based attacks that manipulate textual instructions, environmental injection corrupts an agent's visual perception by inserting adversarial UI elements (for example, deceptive overlays or spoofed notifications) directly into the GUI. This bypasses textual safeguards and can derail execution, causing privacy leakage, financial loss, or irreversible device compromise. To systematically evaluate this threat, we introduce GhostEI-Bench, the first benchmark for assessing mobile agents under environmental injection attacks within dynamic, executable environments. Moving beyond static image-based assessments, GhostEI-Bench injects adversarial events into realistic application workflows inside fully operational Android emulators and evaluates performance across critical risk scenarios. We further propose a judge-LLM protocol that conducts fine-grained failure analysis by reviewing the agent's action trajectory alongside the corresponding screenshot sequence, pinpointing failure in perception, recognition, or reasoning. Comprehensive experiments on state-of-the-art agents reveal pronounced vulnerability to deceptive environmental cues: current models systematically fail to perceive and reason about manipulated UIs. GhostEI-Bench provides a framework for quantifying and mitigating this emerging threat, paving the way toward more robust and secure embodied agents.",
    "pdf_url": "https://arxiv.org/pdf/2510.20333v2",
    "github_url": null,
    "published": "2025-10-23T08:33:24+00:00",
    "updated": "2025-11-21T07:38:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20300v1",
    "title": "Privacy Protection of Automotive Location Data Based on Format-Preserving Encryption of Geographical Coordinates",
    "authors": [
      "Ji",
      "Jin",
      "Li"
    ],
    "summary": "There are increasing risks of privacy disclosure when sharing the automotive location data in particular functions such as route navigation, driving monitoring and vehicle scheduling. These risks could lead to the attacks including user behavior recognition, sensitive location inference and trajectory reconstruction. In order to mitigate the data security risk caused by the automotive location sharing, this paper proposes a high-precision privacy protection mechanism based on format-preserving encryption (FPE) of geographical coordinates. The automotive coordinate data key mapping mechanism is designed to reduce to the accuracy loss of the geographical location data caused by the repeated encryption and decryption. The experimental results demonstrate that the average relative distance retention rate (RDR) reached 0.0844, and the number of hotspots in the critical area decreased by 98.9% after encryption. To evaluate the accuracy loss of the proposed encryption algorithm on automotive geographical location data, this paper presents the experimental analysis of decryption accuracy, and the result indicates that the decrypted coordinate data achieves a restoration accuracy of 100%. This work presents a high-precision privacy protection method for automotive location data, thereby providing an efficient data security solution for the sensitive data sharing in autonomous driving.",
    "pdf_url": "https://arxiv.org/pdf/2510.20300v1",
    "github_url": null,
    "published": "2025-10-23T07:39:59+00:00",
    "updated": "2025-10-23T07:39:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21860v1",
    "title": "Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence",
    "authors": [
      "Sharrock",
      "Petersson",
      "Petersson"
    ],
    "summary": "We present Butter-Bench, a benchmark evaluating large language model (LLM) controlled robots for practical intelligence, defined as the ability to navigate the messiness of the physical world. Current state-of-the-art robotic systems use a hierarchical architecture with LLMs in charge of high-level reasoning, and a Vision Language Action (VLA) model for low-level control. Butter-Bench evaluates the LLM part in isolation from the VLA. Although LLMs have repeatedly surpassed humans in evaluations requiring analytical intelligence, we find humans still outperform LLMs on Butter-Bench. The best LLMs score 40% on Butter-Bench, while the mean human score is 95%. LLMs struggled the most with multi-step spatial planning and social understanding. We also evaluate LLMs that are fine-tuned for embodied reasoning and conclude that this training does not improve their score on Butter-Bench.",
    "pdf_url": "https://arxiv.org/pdf/2510.21860v1",
    "github_url": null,
    "published": "2025-10-23T07:28:28+00:00",
    "updated": "2025-10-23T07:28:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20291v1",
    "title": "A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization",
    "authors": [
      "Li",
      "Zhao",
      "Yang"
    ],
    "summary": "We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone Navigation. The task retrieves the most relevant geo-referenced image from a large multi-platform corpus (satellite/drone/ground) given a natural-language query. Two obstacles are severe inter-platform heterogeneity and a domain gap between generic training descriptions and platform-specific test queries. We mitigate these with a domain-aligned preprocessing pipeline and a Mixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite augmentation, and removal of orientation words; (ii) an LLM-based caption refinement pipeline to align textual semantics with the distinct visual characteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we train three platform experts using a progressive two-stage, hard-negative mining strategy to enhance discriminative power, and fuse their scores at inference. The system tops the official leaderboard, demonstrating robust cross-modal geo-localization under heterogeneous viewpoints.",
    "pdf_url": "https://arxiv.org/pdf/2510.20291v1",
    "github_url": null,
    "published": "2025-10-23T07:23:47+00:00",
    "updated": "2025-10-23T07:23:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20261v1",
    "title": "Kinaema: a recurrent sequence model for memory and pose in motion",
    "authors": [
      "Sariyildiz",
      "Weinzaepfel",
      "Bono"
    ],
    "summary": "One key aspect of spatially aware robots is the ability to \"find their bearings\", ie. to correctly situate themselves in previously seen spaces. In this work, we focus on this particular scenario of continuous robotics operations, where information observed before an actual episode start is exploited to optimize efficiency. We introduce a new model, Kinaema, and agent, capable of integrating a stream of visual observations while moving in a potentially large scene, and upon request, processing a query image and predicting the relative position of the shown space with respect to its current position. Our model does not explicitly store an observation history, therefore does not have hard constraints on context length. It maintains an implicit latent memory, which is updated by a transformer in a recurrent way, compressing the history of sensor readings into a compact representation. We evaluate the impact of this model in a new downstream task we call \"Mem-Nav\". We show that our large-capacity recurrent model maintains a useful representation of the scene, navigates to goals observed before the actual episode start, and is computationally efficient, in particular compared to classical transformers with attention over an observation history.",
    "pdf_url": "https://arxiv.org/pdf/2510.20261v1",
    "github_url": null,
    "published": "2025-10-23T06:34:53+00:00",
    "updated": "2025-10-23T06:34:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20177v1",
    "title": "A Contact-Driven Framework for Manipulating in the Blind",
    "authors": [
      "Saleem",
      "Yuan",
      "Likhachev"
    ],
    "summary": "Robots often face manipulation tasks in environments where vision is inadequate due to clutter, occlusions, or poor lighting--for example, reaching a shutoff valve at the back of a sink cabinet or locating a light switch above a crowded shelf. In such settings, robots, much like humans, must rely on contact feedback to distinguish free from occupied space and navigate around obstacles. Many of these environments often exhibit strong structural priors--for instance, pipes often span across sink cabinets--that can be exploited to anticipate unseen structure and avoid unnecessary collisions. We present a theoretically complete and empirically efficient framework for manipulation in the blind that integrates contact feedback with structural priors to enable robust operation in unknown environments. The framework comprises three tightly coupled components: (i) a contact detection and localization module that utilizes joint torque sensing with a contact particle filter to detect and localize contacts, (ii) an occupancy estimation module that uses the history of contact observations to build a partial occupancy map of the workspace and extrapolate it into unexplored regions with learned predictors, and (iii) a planning module that accounts for the fact that contact localization estimates and occupancy predictions can be noisy, computing paths that avoid collisions and complete tasks efficiently without eliminating feasible solutions. We evaluate the system in simulation and in the real world on a UR10e manipulator across two domestic tasks--(i) manipulating a valve under a kitchen sink surrounded by pipes and (ii) retrieving a target object from a cluttered shelf. Results show that the framework reliably solves these tasks, achieving up to a 2x reduction in task completion time compared to baselines, with ablations confirming the contribution of each module.",
    "pdf_url": "https://arxiv.org/pdf/2510.20177v1",
    "github_url": null,
    "published": "2025-10-23T03:52:08+00:00",
    "updated": "2025-10-23T03:52:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20169v2",
    "title": "Empowering Targeted Neighborhood Search via Hyper Tour for Large-Scale TSP",
    "authors": [
      "Lu",
      "Ma",
      "Tao"
    ],
    "summary": "Traveling Salesman Problem (TSP) is a classic NP-hard problem that has garnered significant attention from both academia and industry. While neural-based methods have shown promise for solving TSPs, they still face challenges in scaling to larger instances, particularly in memory constraints associated with global heatmaps, edge weights, or access matrices, as well as in generating high-quality initial solutions and insufficient global guidance for efficiently navigating vast search spaces. To address these challenges, we propose a Hyper Tour Guided Neighborhood Search (HyperNS) method for large-scale TSP instances. Inspired by the ``clustering first, route second\" strategy, our approach initially divides the TSP instance into clusters using a sparse heatmap graph and abstracts them as supernodes, followed by the generation of a hyper tour to guide both the initialization and optimization processes. This method reduces the search space by focusing on edges relevant to the hyper tour, leading to more efficient and effective optimization. Experimental results on both synthetic and real-world datasets demonstrate that our approach outperforms existing neural-based methods, particularly in handling larger-scale instances, offering a significant reduction in the gap to the optimal solution.",
    "pdf_url": "https://arxiv.org/pdf/2510.20169v2",
    "github_url": null,
    "published": "2025-10-23T03:30:18+00:00",
    "updated": "2025-11-26T07:43:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20027v1",
    "title": "Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses",
    "authors": [
      "Bowness",
      "Poullis"
    ],
    "summary": "When viewing a 3D Gaussian Splatting (3DGS) model from camera positions significantly outside the training data distribution, substantial visual noise commonly occurs. These artifacts result from the lack of training data in these extrapolated regions, leading to uncertain density, color, and geometry predictions from the model.   To address this issue, we propose a novel real-time render-aware filtering method. Our approach leverages sensitivity scores derived from intermediate gradients, explicitly targeting instabilities caused by anisotropic orientations rather than isotropic variance. This filtering method directly addresses the core issue of generative uncertainty, allowing 3D reconstruction systems to maintain high visual fidelity even when users freely navigate outside the original training viewpoints.   Experimental evaluation demonstrates that our method substantially improves visual quality, realism, and consistency compared to existing Neural Radiance Field (NeRF)-based approaches such as BayesRays. Critically, our filter seamlessly integrates into existing 3DGS rendering pipelines in real-time, unlike methods that require extensive post-hoc retraining or fine-tuning.   Code and results at https://damian-bowness.github.io/EV3DGS",
    "pdf_url": "https://arxiv.org/pdf/2510.20027v1",
    "github_url": null,
    "published": "2025-10-22T21:09:16+00:00",
    "updated": "2025-10-22T21:09:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.20008v1",
    "title": "Simultaneous learning of state-to-state minimum-time planning and control",
    "authors": [
      "Dantu",
      "Pěnička",
      "Saska"
    ],
    "summary": "This paper tackles the challenge of learning a generalizable minimum-time flight policy for UAVs, capable of navigating between arbitrary start and goal states while balancing agile flight and stable hovering. Traditional approaches, particularly in autonomous drone racing, achieve impressive speeds and agility but are constrained to predefined track layouts, limiting real-world applicability. To address this, we propose a reinforcement learning-based framework that simultaneously learns state-to-state minimum-time planning and control and generalizes to arbitrary state-to-state flights. Our approach leverages Point Mass Model (PMM) trajectories as proxy rewards to approximate the true optimal flight objective and employs curriculum learning to scale the training process efficiently and to achieve generalization. We validate our method through simulation experiments, comparing it against Nonlinear Model Predictive Control (NMPC) tracking PMM-generated trajectories and conducting ablation studies to assess the impact of curriculum learning. Finally, real-world experiments confirm the robustness of our learned policy in outdoor environments, demonstrating its ability to generalize and operate on a small ARM-based single-board computer.",
    "pdf_url": "https://arxiv.org/pdf/2510.20008v1",
    "github_url": null,
    "published": "2025-10-22T20:20:42+00:00",
    "updated": "2025-10-22T20:20:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21850v1",
    "title": "SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models",
    "authors": [
      "Lim",
      "Koo",
      "Madisetti"
    ],
    "summary": "Understanding long-context visual information remains a fundamental challenge for vision-language models, particularly in agentic tasks such as GUI control and web navigation. While web pages and GUI environments are inherently structured documents, current VLMs typically neglect decision-oriented document understanding in their training objectives. Existing approaches primarily extend visual embeddings to process long, high-resolution inputs, but these methods are memory-intensive and impractical for locally deployable solutions. To address these issues, we propose SCoPE VLM, a document navigation expert that leverages a novel Chain of Scroll mechanism to selectively and recursively navigate documents, focusing exclusively on relevant segments. We introduce a dedicated data generation pipeline to construct informative Chain of Scroll trajectories and Episodic Group Relative Policy Optimization, a tailored reinforcement learning method to reduce the gap between training and inference. Our method substantially reduces memory usage and effectively models human-like reading behaviors. To the best of our knowledge, SCoPE VLM is the first framework to explicitly model agentic reading patterns in multi-page document question answering, advancing the capabilities of multimodal agents.",
    "pdf_url": "https://arxiv.org/pdf/2510.21850v1",
    "github_url": null,
    "published": "2025-10-22T17:47:12+00:00",
    "updated": "2025-10-22T17:47:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19732v2",
    "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning",
    "authors": [
      "Gupta",
      "Yadav",
      "Kira"
    ],
    "summary": "To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo's effectiveness on a gridworld meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints. Our code is available at: https://github.com/gunshi/memo.",
    "pdf_url": "https://arxiv.org/pdf/2510.19732v2",
    "github_url": "https://github.com/gunshi/memo",
    "published": "2025-10-22T16:24:47+00:00",
    "updated": "2025-11-27T02:24:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19723v1",
    "title": "From Answers to Guidance: A Proactive Dialogue System for Legal Documents",
    "authors": [
      "Chouhan",
      "Gertz"
    ],
    "summary": "The accessibility of legal information remains a constant challenge, particularly for laypersons seeking to understand and apply complex institutional texts. While the European Union provides open access to legislation, parliamentary responses, and regulatory documents, these resources can be challenging for laypeople to explore. In this paper, we introduce EUDial, a proactive multi-turn dialogue dataset constructed from 204 blogs curated by the Citizens' Enquiries Unit (AskEP) of the European Parliamentary Research Service. EUDial contains 880 dialogue turns (averaging 4.3 turns per dialogue), where each dialogue includes initial questions, structured answers, and follow-up questions. Beyond dataset construction, we propose the LexGuide framework that leverages retrieval-augmented generation with hierarchical topic organization to structure dialogue progression, ensuring both comprehensive coverage of legal aspects and coherence across conversational turns. The results demonstrate that proactive, structured navigation closes the gap between the availability of legal information and citizen comprehension, establishing EUDial and LexGuide as practical resources for advancing proactive legal dialogue systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.19723v1",
    "github_url": null,
    "published": "2025-10-22T16:08:05+00:00",
    "updated": "2025-10-22T16:08:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19687v1",
    "title": "Are Large Language Models Sensitive to the Motives Behind Communication?",
    "authors": [
      "Wu",
      "Liu",
      "Oktar"
    ],
    "summary": "Human communication is motivated: people speak, write, and create content with a particular communicative intent in mind. As a result, information that large language models (LLMs) and AI agents process is inherently framed by humans' intentions and incentives. People are adept at navigating such nuanced information: we routinely identify benevolent or self-serving motives in order to decide what statements to trust. For LLMs to be effective in the real world, they too must critically evaluate content by factoring in the motivations of the source -- for instance, weighing the credibility of claims made in a sales pitch. In this paper, we undertake a comprehensive study of whether LLMs have this capacity for motivational vigilance. We first employ controlled experiments from cognitive science to verify that LLMs' behavior is consistent with rational models of learning from motivated testimony, and find they successfully discount information from biased sources in a human-like manner. We then extend our evaluation to sponsored online adverts, a more naturalistic reflection of LLM agents' information ecosystems. In these settings, we find that LLMs' inferences do not track the rational models' predictions nearly as closely -- partly due to additional information that distracts them from vigilance-relevant considerations. However, a simple steering intervention that boosts the salience of intentions and incentives substantially increases the correspondence between LLMs and the rational model. These results suggest that LLMs possess a basic sensitivity to the motivations of others, but generalizing to novel real-world settings will require further improvements to these models.",
    "pdf_url": "https://arxiv.org/pdf/2510.19687v1",
    "github_url": null,
    "published": "2025-10-22T15:35:00+00:00",
    "updated": "2025-10-22T15:35:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19655v1",
    "title": "LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments",
    "authors": [
      "Ding",
      "Xu",
      "Fang"
    ],
    "summary": "Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires an agent to navigate unseen environments based on natural language instructions without any prior training. Current methods face a critical trade-off: either rely on environment-specific waypoint predictors that limit scene generalization, or underutilize the reasoning capabilities of large models during navigation. We introduce LaViRA, a simple yet effective zero-shot framework that addresses this dilemma by decomposing action into a coarse-to-fine hierarchy: Language Action for high-level planning, Vision Action for perceptual grounding, and Robot Action for robust navigation. This modular decomposition allows us to leverage the distinct strengths of different scales of Multimodal Large Language Models (MLLMs) at each stage, creating a system that is powerful in its reasoning, grounding and practical control. LaViRA significantly outperforms existing state-of-the-art methods on the VLN-CE benchmark, demonstrating superior generalization capabilities in unseen environments, while maintaining transparency and efficiency for real-world deployment.",
    "pdf_url": "https://arxiv.org/pdf/2510.19655v1",
    "github_url": null,
    "published": "2025-10-22T14:58:16+00:00",
    "updated": "2025-10-22T14:58:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19415v1",
    "title": "Risk Assessment of an Autonomous Underwater Snake Robot in Confined Operations",
    "authors": [
      "Sayed"
    ],
    "summary": "The growing interest in ocean discovery imposes a need for inspection and intervention in confined and demanding environments. Eely's slender shape, in addition to its ability to change its body configurations, makes articulated underwater robots an adequate option for such environments. However, operation of Eely in such environments imposes demanding requirements on the system, as it must deal with uncertain and unstructured environments, extreme environmental conditions, and reduced navigational capabilities. This paper proposes a Bayesian approach to assess the risks of losing Eely during two mission scenarios. The goal of this work is to improve Eely's performance and the likelihood of mission success. Sensitivity analysis results are presented in order to demonstrate the causes having the highest impact on losing Eely.",
    "pdf_url": "https://arxiv.org/pdf/2510.19415v1",
    "github_url": null,
    "published": "2025-10-22T09:37:04+00:00",
    "updated": "2025-10-22T09:37:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19364v1",
    "title": "ProTerrain: Probabilistic Physics-Informed Rough Terrain World Modeling",
    "authors": [
      "Raja",
      "Agishev",
      "Prágr"
    ],
    "summary": "Uncertainty-aware robot motion prediction is crucial for downstream traversability estimation and safe autonomous navigation in unstructured, off-road environments, where terrain is heterogeneous and perceptual uncertainty is high. Most existing methods assume deterministic or spatially independent terrain uncertainties, ignoring the inherent local correlations of 3D spatial data and often producing unreliable predictions. In this work, we introduce an efficient probabilistic framework that explicitly models spatially correlated aleatoric uncertainty over terrain parameters as a probabilistic world model and propagates this uncertainty through a differentiable physics engine for probabilistic trajectory forecasting. By leveraging structured convolutional operators, our approach provides high-resolution multivariate predictions at manageable computational cost. Experimental evaluation on a publicly available dataset shows significantly improved uncertainty estimation and trajectory prediction accuracy over aleatoric uncertainty estimation baselines.",
    "pdf_url": "https://arxiv.org/pdf/2510.19364v1",
    "github_url": null,
    "published": "2025-10-22T08:36:46+00:00",
    "updated": "2025-10-22T08:36:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19352v1",
    "title": "ConvXformer: Differentially Private Hybrid ConvNeXt-Transformer for Inertial Navigation",
    "authors": [
      "Tariq",
      "Bilal",
      "Hassan"
    ],
    "summary": "Data-driven inertial sequence learning has revolutionized navigation in GPS-denied environments, offering superior odometric resolution compared to traditional Bayesian methods. However, deep learning-based inertial tracking systems remain vulnerable to privacy breaches that can expose sensitive training data. \\hl{Existing differential privacy solutions often compromise model performance by introducing excessive noise, particularly in high-frequency inertial measurements.} In this article, we propose ConvXformer, a hybrid architecture that fuses ConvNeXt blocks with Transformer encoders in a hierarchical structure for robust inertial navigation. We propose an efficient differential privacy mechanism incorporating adaptive gradient clipping and gradient-aligned noise injection (GANI) to protect sensitive information while ensuring model performance. Our framework leverages truncated singular value decomposition for gradient processing, enabling precise control over the privacy-utility trade-off. Comprehensive performance evaluations on benchmark datasets (OxIOD, RIDI, RoNIN) demonstrate that ConvXformer surpasses state-of-the-art methods, achieving more than 40% improvement in positioning accuracy while ensuring $(ε,δ)$-differential privacy guarantees. To validate real-world performance, we introduce the Mech-IO dataset, collected from the mechanical engineering building at KAIST, where intense magnetic fields from industrial equipment induce significant sensor perturbations. This demonstrated robustness under severe environmental distortions makes our framework well-suited for secure and intelligent navigation in cyber-physical systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.19352v1",
    "github_url": null,
    "published": "2025-10-22T08:20:31+00:00",
    "updated": "2025-10-22T08:20:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19286v2",
    "title": "TheMCPCompany: Creating General-purpose Agents with Task-specific Tools",
    "authors": [
      "Esfandiarpoor",
      "Suryanarayanan",
      "Bach"
    ],
    "summary": "Since the introduction of the Model Context Protocol (MCP), the number of available tools for Large Language Models (LLMs) has increased significantly. These task-specific tool sets offer an alternative to general-purpose tools such as web browsers, while being easier to develop and maintain than GUIs. However, current general-purpose agents predominantly rely on web browsers for interacting with the environment. Here, we introduce TheMCPCompany, a benchmark for evaluating tool-calling agents on tasks that involve interacting with various real-world services. We use the REST APIs of these services to create MCP servers, which include over 18,000 tools. We also provide manually annotated ground-truth tools for each task. In our experiments, we use the ground truth tools to show the potential of tool-calling agents for both improving performance and reducing costs assuming perfect tool retrieval. Next, we explore agent performance using tool retrieval to study the real-world practicality of tool-based agents. While all models with tool retrieval perform similarly or better than browser-based agents, smaller models cannot take full advantage of the available tools through retrieval. On the other hand, GPT-5's performance with tool retrieval is very close to its performance with ground-truth tools. Overall, our work shows that the most advanced reasoning models are effective at discovering tools in simpler environments, but seriously struggle with navigating complex enterprise environments. TheMCPCompany reveals that navigating tens of thousands of tools and combining them in non-trivial ways to solve complex problems is still a challenging task for current models and requires both better reasoning and better retrieval models.",
    "pdf_url": "https://arxiv.org/pdf/2510.19286v2",
    "github_url": null,
    "published": "2025-10-22T06:42:01+00:00",
    "updated": "2025-12-11T03:43:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19226v1",
    "title": "Controllable Machine Unlearning via Gradient Pivoting",
    "authors": [
      "Hwang",
      "Lim"
    ],
    "summary": "Machine unlearning (MU) aims to remove the influence of specific data from a trained model. However, approximate unlearning methods, often formulated as a single-objective optimization (SOO) problem, face a critical trade-off between unlearning efficacy and model fidelity. This leads to three primary challenges: the risk of over-forgetting, a lack of fine-grained control over the unlearning process, and the absence of metrics to holistically evaluate the trade-off. To address these issues, we reframe MU as a multi-objective optimization (MOO) problem. We then introduce a novel algorithm, Controllable Unlearning by Pivoting Gradient (CUP), which features a unique pivoting mechanism. Unlike traditional MOO methods that converge to a single solution, CUP's mechanism is designed to controllably navigate the entire Pareto frontier. This navigation is governed by a single intuitive hyperparameter, the `unlearning intensity', which allows for precise selection of a desired trade-off. To evaluate this capability, we adopt the hypervolume indicator, a metric that captures both the quality and diversity of the entire set of solutions an algorithm can generate. Our experimental results demonstrate that CUP produces a superior set of Pareto-optimal solutions, consistently outperforming existing methods across various vision tasks.",
    "pdf_url": "https://arxiv.org/pdf/2510.19226v1",
    "github_url": null,
    "published": "2025-10-22T04:20:24+00:00",
    "updated": "2025-10-22T04:20:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19127v1",
    "title": "Steering Autoregressive Music Generation with Recursive Feature Machines",
    "authors": [
      "Zhao",
      "Beaglehole",
      "Berg-Kirkpatrick"
    ],
    "summary": "Controllable music generation remains a significant challenge, with existing methods often requiring model retraining or introducing audible artifacts. We introduce MusicRFM, a framework that adapts Recursive Feature Machines (RFMs) to enable fine-grained, interpretable control over frozen, pre-trained music models by directly steering their internal activations. RFMs analyze a model's internal gradients to produce interpretable \"concept directions\", or specific axes in the activation space that correspond to musical attributes like notes or chords. We first train lightweight RFM probes to discover these directions within MusicGen's hidden states; then, during inference, we inject them back into the model to guide the generation process in real-time without per-step optimization. We present advanced mechanisms for this control, including dynamic, time-varying schedules and methods for the simultaneous enforcement of multiple musical properties. Our method successfully navigates the trade-off between control and generation quality: we can increase the accuracy of generating a target musical note from 0.23 to 0.82, while text prompt adherence remains within approximately 0.02 of the unsteered baseline, demonstrating effective control with minimal impact on prompt fidelity. We release code to encourage further exploration on RFMs in the music domain.",
    "pdf_url": "https://arxiv.org/pdf/2510.19127v1",
    "github_url": null,
    "published": "2025-10-21T23:23:14+00:00",
    "updated": "2025-10-21T23:23:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19101v1",
    "title": "Safe Active Navigation and Exploration for Planetary Environments Using Proprioceptive Measurements",
    "authors": [
      "Jiang",
      "Liu",
      "Qian"
    ],
    "summary": "Legged robots can sense terrain through force interactions during locomotion, offering more reliable traversability estimates than remote sensing and serving as scouts for guiding wheeled rovers in challenging environments. However, even legged scouts face challenges when traversing highly deformable or unstable terrain. We present Safe Active Exploration for Granular Terrain (SAEGT), a navigation framework that enables legged robots to safely explore unknown granular environments using proprioceptive sensing, particularly where visual input fails to capture terrain deformability. SAEGT estimates the safe region and frontier region online from leg-terrain interactions using Gaussian Process regression for traversability assessment, with a reactive controller for real-time safe exploration and navigation. SAEGT demonstrated its ability to safely explore and navigate toward a specified goal using only proprioceptively estimated traversability in simulation.",
    "pdf_url": "https://arxiv.org/pdf/2510.19101v1",
    "github_url": null,
    "published": "2025-10-21T21:45:14+00:00",
    "updated": "2025-10-21T21:45:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.19054v1",
    "title": "Motion Planning and Control of an Overactuated 4-Wheel Drive with Constrained Independent Steering",
    "authors": [
      "Liu",
      "Hadzic",
      "Gupta"
    ],
    "summary": "This paper addresses motion planning and con- trol of an overactuated 4-wheel drive train with independent steering (4WIS) where mechanical constraints prevent the wheels from executing full 360-degree rotations (swerve). The configuration space of such a robot is constrained and contains discontinuities that affect the smoothness of the robot motion. We introduce a mathematical formulation of the steering constraints and derive discontinuity planes that partition the velocity space into regions of smooth and efficient motion. We further design the motion planner for path tracking and ob- stacle avoidance that explicitly accounts for swerve constraints and the velocity transition smoothness. The motion controller uses local feedback to generate actuation from the desired velocity, while properly handling the discontinuity crossing by temporarily stopping the motion and repositioning the wheels. We implement the proposed motion planner as an extension to ROS Navigation package and evaluate the system in simulation and on a physical robot.",
    "pdf_url": "https://arxiv.org/pdf/2510.19054v1",
    "github_url": null,
    "published": "2025-10-21T20:12:42+00:00",
    "updated": "2025-10-21T20:12:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18996v1",
    "title": "SHRUMS: Sensor Hallucination for Real-time Underwater Motion Planning with a Compact 3D Sonar",
    "authors": [
      "Vadakkekuruppath",
      "Amundsen",
      "O'Kane"
    ],
    "summary": "Autonomous navigation in 3D is a fundamental problem for autonomy. Despite major advancements in terrestrial and aerial settings due to improved range sensors including LiDAR, compact sensors with similar capabilities for underwater robots have only recently become available, in the form of 3D sonars. This paper introduces a novel underwater 3D navigation pipeline, called SHRUMS (Sensor Hallucination for Robust Underwater Motion planning with 3D Sonar). To the best of the authors' knowledge, SHRUMS is the first underwater autonomous navigation stack to integrate a 3D sonar. The proposed pipeline exhibits strong robustness while operating in complex 3D environments in spite of extremely poor visibility conditions. To accommodate the intricacies of the novel sensor data stream while achieving real-time locally optimal performance, SHRUMS introduces the concept of hallucinating sensor measurements from non-existent sensors with convenient arbitrary parameters, tailored to application specific requirements. The proposed concepts are validated with real 3D sonar sensor data, utilizing real inputs in challenging settings and local maps constructed in real-time. Field deployments validating the proposed approach in full are planned in the very near future.",
    "pdf_url": "https://arxiv.org/pdf/2510.18996v1",
    "github_url": null,
    "published": "2025-10-21T18:21:03+00:00",
    "updated": "2025-10-21T18:21:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18678v1",
    "title": "Towards An Adaptive Locomotion Strategy For Quadruped Rovers: Quantifying When To Slide Or Walk On Planetary Slopes",
    "authors": [
      "Sanchez-Delgado",
      "Soares",
      "Tawil"
    ],
    "summary": "Legged rovers provide enhanced mobility compared to wheeled platforms, enabling navigation on steep and irregular planetary terrains. However, traditional legged locomotion might be energetically inefficient and potentially dangerous to the rover on loose and inclined surfaces, such as crater walls and cave slopes. This paper introduces a preliminary study that compares the Cost of Transport (CoT) of walking and torso-based sliding locomotion for quadruped robots across different slopes, friction conditions and speed levels. By identifying intersections between walking and sliding CoT curves, we aim to define threshold conditions that may trigger transitions between the two strategies. The methodology combines physics-based simulations in Isaac Sim with particle interaction validation in ANSYS-Rocky. Our results represent an initial step towards adaptive locomotion strategies for planetary legged rovers.",
    "pdf_url": "https://arxiv.org/pdf/2510.18678v1",
    "github_url": null,
    "published": "2025-10-21T14:34:53+00:00",
    "updated": "2025-10-21T14:34:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18931v1",
    "title": "A Justice Lens on Fairness and Ethics Courses in Computing Education: LLM-Assisted Multi-Perspective and Thematic Evaluation",
    "authors": [
      "Andrews",
      "Kanubala",
      "Aruleba"
    ],
    "summary": "Course syllabi set the tone and expectations for courses, shaping the learning experience for both students and instructors. In computing courses, especially those addressing fairness and ethics in artificial intelligence (AI), machine learning (ML), and algorithmic design, it is imperative that we understand how approaches to navigating barriers to fair outcomes are being addressed.These expectations should be inclusive, transparent, and grounded in promoting critical thinking. Syllabus analysis offers a way to evaluate the coverage, depth, practices, and expectations within a course. Manual syllabus evaluation, however, is time-consuming and prone to inconsistency. To address this, we developed a justice-oriented scoring rubric and asked a large language model (LLM) to review syllabi through a multi-perspective role simulation. Using this rubric, we evaluated 24 syllabi from four perspectives: instructor, departmental chair, institutional reviewer, and external evaluator. We also prompted the LLM to identify thematic trends across the courses. Findings show that multiperspective evaluation aids us in noting nuanced, role-specific priorities, leveraging them to fill hidden gaps in curricula design of AI/ML and related computing courses focused on fairness and ethics. These insights offer concrete directions for improving the design and delivery of fairness, ethics, and justice content in such courses.",
    "pdf_url": "https://arxiv.org/pdf/2510.18931v1",
    "github_url": null,
    "published": "2025-10-21T13:30:45+00:00",
    "updated": "2025-10-21T13:30:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18623v2",
    "title": "Optimal quantum learning in proximity to universality",
    "authors": [
      "Ivaki",
      "Karjula",
      "Ala-Nissila"
    ],
    "summary": "The boundary between classically simulable and computationally superior quantum systems is fundamental to identifying true quantum advantage. We investigate this within the framework of quantum reservoir computing by introducing a tunable $N$-qubit random circuit model, where a fraction $p$ of Clifford gates are probabilistically substituted with nonstabilizing conditional-$\\hat{T}$ gates. We establish a direct correspondence between the reservoir's performance on temporal processing tasks and its entanglement spectrum statistics and long-range nonstabilizer resource content. To assess scalability, we study the scaling of the anti-flatness of states in the large-$N$ limit at a fixed circuit depth ratio $d/N \\sim \\mathcal{O}(1)$. This is taken as a witness to concentration of measures, a known impediment to learning in thermalizing systems. We demonstrate that the learnability and scalability of the reservoir can be continuously controlled by the parameter $p$, allowing us to navigate from classically tractable to maximally expressive quantum dynamics. These architecture-agnostic results offer a general strategy for designing powerful and trainable quantum machine learning systems and clarify the physical resources underpinning quantum computational advantage.",
    "pdf_url": "https://arxiv.org/pdf/2510.18623v2",
    "github_url": null,
    "published": "2025-10-21T13:27:41+00:00",
    "updated": "2025-10-22T13:53:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18546v2",
    "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval",
    "authors": [
      "Yang",
      "Zheng",
      "Xie"
    ],
    "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the location of a specific object in an unseen environment. Embodied agents equipped with large language models (LLMs) and online constructed navigation maps can perform ObjNav in a zero-shot manner. However, existing agents heavily rely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small LLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to limited model capacity for understanding complex navigation maps, which prevents deploying ObjNav on local devices. At the same time, the long prompt introduced by the navigation map description will cause high planning latency on local devices. In this paper, we propose EfficientNav to enable on-device efficient LLM-based zero-shot ObjNav. To help the smaller LLMs better understand the environment, we propose semantics-aware memory retrieval to prune redundant information in navigation maps. To reduce planning latency, we propose discrete memory caching and attention-based memory clustering to efficiently save and re-use the KV cache. Extensive experimental results demonstrate that EfficientNav achieves 11.1% improvement in success rate on HM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time latency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our code is available on https://github.com/PKU-SEC-Lab/EfficientNav.",
    "pdf_url": "https://arxiv.org/pdf/2510.18546v2",
    "github_url": "https://github.com/PKU-SEC-Lab/EfficientNav",
    "published": "2025-10-21T11:52:44+00:00",
    "updated": "2025-11-27T13:54:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18544v3",
    "title": "SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices",
    "authors": [
      "Chow"
    ],
    "summary": "Large Language Models (LLMs), as the foundational architecture for next-generation interactive AI applications, not only power intelligent dialogue systems but also drive the evolution of embodied intelligence on edge devices, including humanoid robots, smart vehicles, and other scenarios. The applications running on these edge devices impose differentiated Service Level Objectives (SLO) requirements on LLM services, specifically manifested as distinct constraints on Time to First Token (TTFT) and Time Per Output Token (TPOT) as well as end-to-end latency. Notably, edge devices typically handle real-time tasks that are extremely sensitive to latency, such as machine control and navigation planning. However, existing scheduling service systems still prioritize maximizing output token throughput as the sole optimization objective, failing to adequately address the diversity of SLO requirements. This ultimately results in persistently high violation rates for end-to-end latency or TPOT related SLOs.   This paper proposes SLICE, an innovative scheduling solution designed for edge computing scenarios with differentiated SLO requirements. By combining a utility-maximizing request scheduling algorithm with a dynamic iterative control mechanism for generation rates, SLICE significantly improves LLM inference service SLO attainment. Experimental results demonstrate that compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to 35x higher SLO attainment and 3.4x advantage in task completion time than the other two solutions. This version is temporarily hosted anonymously for double-blind review.",
    "pdf_url": "https://arxiv.org/pdf/2510.18544v3",
    "github_url": null,
    "published": "2025-10-21T11:47:42+00:00",
    "updated": "2025-11-18T16:40:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18485v1",
    "title": "Learning to Navigate Under Imperfect Perception: Conformalised Segmentation for Safe Reinforcement Learning",
    "authors": [
      "Bethell",
      "Gerasimou",
      "Calinescu"
    ],
    "summary": "Reliable navigation in safety-critical environments requires both accurate hazard perception and principled uncertainty handling to strengthen downstream safety handling. Despite the effectiveness of existing approaches, they assume perfect hazard detection capabilities, while uncertainty-aware perception approaches lack finite-sample guarantees. We present COPPOL, a conformal-driven perception-to-policy learning approach that integrates distribution-free, finite-sample safety guarantees into semantic segmentation, yielding calibrated hazard maps with rigorous bounds for missed detections. These maps induce risk-aware cost fields for downstream RL planning. Across two satellite-derived benchmarks, COPPOL increases hazard coverage (up to 6x) compared to comparative baselines, achieving near-complete detection of unsafe regions while reducing hazardous violations during navigation (up to approx 50%). More importantly, our approach remains robust to distributional shift, preserving both safety and efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2510.18485v1",
    "github_url": null,
    "published": "2025-10-21T10:07:04+00:00",
    "updated": "2025-10-21T10:07:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18437v1",
    "title": "Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection",
    "authors": [
      "Du",
      "Wang",
      "Hao"
    ],
    "summary": "At the core of Camouflaged Object Detection (COD) lies segmenting objects from their highly similar surroundings. Previous efforts navigate this challenge primarily through image-level modeling or annotation-based optimization. Despite advancing considerably, this commonplace practice hardly taps valuable dataset-level contextual information or relies on laborious annotations. In this paper, we propose RISE, a RetrIeval SElf-augmented paradigm that exploits the entire training dataset to generate pseudo-labels for single images, which could be used to train COD models. RISE begins by constructing prototype libraries for environments and camouflaged objects using training images (without ground truth), followed by K-Nearest Neighbor (KNN) retrieval to generate pseudo-masks for each image based on these libraries. It is important to recognize that using only training images without annotations exerts a pronounced challenge in crafting high-quality prototype libraries. In this light, we introduce a Clustering-then-Retrieval (CR) strategy, where coarse masks are first generated through clustering, facilitating subsequent histogram-based image filtering and cross-category retrieval to produce high-confidence prototypes. In the KNN retrieval stage, to alleviate the effect of artifacts in feature maps, we propose Multi-View KNN Retrieval (MVKR), which integrates retrieval results from diverse views to produce more robust and precise pseudo-masks. Extensive experiments demonstrate that RISE outperforms state-of-the-art unsupervised and prompt-based methods. Code is available at https://github.com/xiaohainku/RISE.",
    "pdf_url": "https://arxiv.org/pdf/2510.18437v1",
    "github_url": "https://github.com/xiaohainku/RISE",
    "published": "2025-10-21T09:12:26+00:00",
    "updated": "2025-10-21T09:12:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21809v1",
    "title": "Embodied Navigation with Auxiliary Task of Action Description Prediction",
    "authors": [
      "Kondoh",
      "Kanezaki"
    ],
    "summary": "The field of multimodal robot navigation in indoor environments has garnered significant attention in recent years. However, as tasks and methods become more advanced, the action decision systems tend to become more complex and operate as black-boxes. For a reliable system, the ability to explain or describe its decisions is crucial; however, there tends to be a trade-off in that explainable systems can not outperform non-explainable systems in terms of performance. In this paper, we propose incorporating the task of describing actions in language into the reinforcement learning of navigation as an auxiliary task. Existing studies have found it difficult to incorporate describing actions into reinforcement learning due to the absence of ground-truth data. We address this issue by leveraging knowledge distillation from pre-trained description generation models, such as vision-language models. We comprehensively evaluate our approach across various navigation tasks, demonstrating that it can describe actions while attaining high navigation performance. Furthermore, it achieves state-of-the-art performance in the particularly challenging multimodal navigation task of semantic audio-visual navigation.",
    "pdf_url": "https://arxiv.org/pdf/2510.21809v1",
    "github_url": null,
    "published": "2025-10-21T09:12:22+00:00",
    "updated": "2025-10-21T09:12:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18364v1",
    "title": "Evaluating LLM-Based Mobile App Recommendations: An Empirical Study",
    "authors": [
      "Motger",
      "Franch",
      "Gervasi"
    ],
    "summary": "Large Language Models (LLMs) are increasingly used to recommend mobile applications through natural language prompts, offering a flexible alternative to keyword-based app store search. Yet, the reasoning behind these recommendations remains opaque, raising questions about their consistency, explainability, and alignment with traditional App Store Optimization (ASO) metrics. In this paper, we present an empirical analysis of how widely-used general purpose LLMs generate, justify, and rank mobile app recommendations. Our contributions are: (i) a taxonomy of 16 generalizable ranking criteria elicited from LLM outputs; (ii) a systematic evaluation framework to analyse recommendation consistency and responsiveness to explicit ranking instructions; and (iii) a replication package to support reproducibility and future research on AI-based recommendation systems. Our findings reveal that LLMs rely on a broad yet fragmented set of ranking criteria, only partially aligned with standard ASO metrics. While top-ranked apps tend to be consistent across runs, variability increases with ranking depth and search specificity. LLMs exhibit varying sensitivity to explicit ranking instructions - ranging from substantial adaptations to near-identical outputs - highlighting their complex reasoning dynamics in conversational app discovery. Our results aim to support end-users, app developers, and recommender-systems researchers in navigating the emerging landscape of conversational app discovery.",
    "pdf_url": "https://arxiv.org/pdf/2510.18364v1",
    "github_url": null,
    "published": "2025-10-21T07:35:19+00:00",
    "updated": "2025-10-21T07:35:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18316v1",
    "title": "MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation",
    "authors": [
      "Li",
      "Xu",
      "Bahety"
    ],
    "summary": "Imitation learning from large-scale, diverse human demonstrations has proven effective for training robots, but collecting such data is costly and time-consuming. This challenge is amplified for multi-step bimanual mobile manipulation, where humans must teleoperate both a mobile base and two high-degree-of-freedom arms. Prior automated data generation frameworks have addressed static bimanual manipulation by augmenting a few human demonstrations in simulation, but they fall short for mobile settings due to two key challenges: (1) determining base placement to ensure reachability, and (2) positioning the camera to provide sufficient visibility for visuomotor policies. To address these issues, we introduce MoMaGen, which formulates data generation as a constrained optimization problem that enforces hard constraints (e.g., reachability) while balancing soft constraints (e.g., visibility during navigation). This formulation generalizes prior approaches and provides a principled foundation for future methods. We evaluate MoMaGen on four multi-step bimanual mobile manipulation tasks and show that it generates significantly more diverse datasets than existing methods. Leveraging this diversity, MoMaGen can train successful imitation learning policies from a single source demonstration, and these policies can be fine-tuned with as few as 40 real-world demonstrations to achieve deployment on physical robotic hardware. More details are available at our project page: momagen.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2510.18316v1",
    "github_url": null,
    "published": "2025-10-21T05:56:47+00:00",
    "updated": "2025-10-21T05:56:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18313v4",
    "title": "OmniNWM: Omniscient Driving Navigation World Models",
    "authors": [
      "Li",
      "Ma",
      "Du"
    ],
    "summary": "Autonomous driving world models are expected to work effectively across three core dimensions: state, action, and reward. Existing models, however, are typically restricted to limited state modalities, short video sequences, imprecise action control, and a lack of reward awareness. In this paper, we introduce OmniNWM, an omniscient panoramic navigation world model that addresses all three dimensions within a unified framework. For state, OmniNWM jointly generates panoramic videos of RGB, semantics, metric depth, and 3D occupancy. A flexible forcing strategy enables high-quality long-horizon auto-regressive generation. For action, we introduce a normalized panoramic Plucker ray-map representation that encodes input trajectories into pixel-level signals, enabling highly precise and generalizable control over panoramic video generation. Regarding reward, we move beyond learning reward functions with external image-based models: instead, we leverage the generated 3D occupancy to directly define rule-based dense rewards for driving compliance and safety. Extensive experiments demonstrate that OmniNWM achieves state-of-the-art performance in video generation, control accuracy, and long-horizon stability, while providing a reliable closed-loop evaluation framework through occupancy-grounded rewards. Project page is available at https://arlo0o.github.io/OmniNWM/.",
    "pdf_url": "https://arxiv.org/pdf/2510.18313v4",
    "github_url": null,
    "published": "2025-10-21T05:49:01+00:00",
    "updated": "2025-11-15T09:04:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18311v1",
    "title": "Reimagining Disassembly Interfaces with Visualization: Combining Instruction Tracing and Control Flow with DisViz",
    "authors": [
      "Hye",
      "LeGendre",
      "Isaacs"
    ],
    "summary": "In applications where efficiency is critical, developers may examine their compiled binaries, seeking to understand how the compiler transformed their source code and what performance implications that transformation may have. This analysis is challenging due to the vast number of disassembled binary instructions and the many-to-many mappings between them and the source code. These problems are exacerbated as source code size increases, giving the compiler more freedom to map and disperse binary instructions across the disassembly space. Interfaces for disassembly typically display instructions as an unstructured listing or sacrifice the order of execution. We design a new visual interface for disassembly code that combines execution order with control flow structure, enabling analysts to both trace through code and identify familiar aspects of the computation. Central to our approach is a novel layout of instructions grouped into basic blocks that displays a looping structure in an intuitive way. We add to this disassembly representation a unique block-based mini-map that leverages our layout and shows context across thousands of disassembly instructions. Finally, we embed our disassembly visualization in a web-based tool, DisViz, which adds dynamic linking with source code across the entire application. DizViz was developed in collaboration with program analysis experts following design study methodology and was validated through evaluation sessions with ten participants from four institutions. Participants successfully completed the evaluation tasks, hypothesized about compiler optimizations, and noted the utility of our new disassembly view. Our evaluation suggests that our new integrated view helps application developers in understanding and navigating disassembly code.",
    "pdf_url": "https://arxiv.org/pdf/2510.18311v1",
    "github_url": null,
    "published": "2025-10-21T05:43:29+00:00",
    "updated": "2025-10-21T05:43:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18296v1",
    "title": "Relief or displacement? How teachers are negotiating generative AI's role in their professional practice",
    "authors": [
      "Dangol",
      "Kotiyal",
      "Wolfe"
    ],
    "summary": "As generative AI (genAI) rapidly enters classrooms, accompanied by district-level policy rollouts and industry-led teacher trainings, it is important to rethink the canonical ``adopt and train'' playbook. Decades of educational technology research show that tools promising personalization and access often deepen inequities due to uneven resources, training, and institutional support. Against this backdrop, we conducted semi-structured interviews with 22 teachers from a large U.S. school district that was an early adopter of genAI. Our findings reveal the motivations driving adoption, the factors underlying resistance, and the boundaries teachers negotiate to align genAI use with their values. We further contribute by unpacking the sociotechnical dynamics -- including district policies, professional norms, and relational commitments -- that shape how teachers navigate the promises and risks of these tools.",
    "pdf_url": "https://arxiv.org/pdf/2510.18296v1",
    "github_url": null,
    "published": "2025-10-21T04:57:02+00:00",
    "updated": "2025-10-21T04:57:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2511.01866v1",
    "title": "EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs",
    "authors": [
      "Kubwimana",
      "Huang"
    ],
    "summary": "Edge intelligence paradigm is increasingly demanded by the emerging autonomous systems, such as robotics. Beyond ensuring privacy-preserving operation and resilience in connectivity-limited environments, edge deployment offers significant energy and cost advantages over cloud-based solutions. However, deploying large language models (LLMs) for reasoning tasks on edge GPUs faces critical challenges from strict latency constraints and limited computational resources. To navigate these constraints, developers must balance multiple design factors - choosing reasoning versus non-reasoning architectures, selecting appropriate model sizes, allocating token budgets, and applying test-time scaling strategies - to meet target latency and optimize accuracy. Yet guidance on optimal combinations of these variables remains scarce. In this work, we present EdgeReasoning, a comprehensive study characterizing the deployment of reasoning LLMs on edge GPUs. We systematically quantify latency-accuracy tradeoffs across various LLM architectures and model sizes. We systematically evaluate prompt-based and model-tuning-based techniques for reducing reasoning token length while maintaining performance quality. We further profile test-time scaling methods with varying degrees of parallelism to maximize accuracy under strict latency budgets. Through these analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency configurations, offering systematic guidance for optimal edge deployment of reasoning LLMs.",
    "pdf_url": "https://arxiv.org/pdf/2511.01866v1",
    "github_url": null,
    "published": "2025-10-21T04:18:25+00:00",
    "updated": "2025-10-21T04:18:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18211v1",
    "title": "Distributed Stochastic Search for Multi-Agent Model Predictive Control",
    "authors": [
      "Yoon",
      "Saravanos",
      "Theodorou"
    ],
    "summary": "Many real-world multi-agent systems exhibit nonlinear dynamics and complex inter-agent interactions. As these systems increase in scale, the main challenges arise from achieving scalability and handling nonconvexity. To address these challenges, this paper presents a distributed sampling-based optimization framework for multi-agent model predictive control (MPC). We first introduce stochastic search, a generalized sampling-based optimization method, as an effective approach to solving nonconvex MPC problems because of its exploration capabilities. Nevertheless, optimizing the multi-agent systems in a centralized fashion is not scalable as the computational complexity grows intractably as the number of agents increases. To achieve scalability, we formulate a distributed MPC problem and employ the alternating direction method of multipliers (ADMM) to leverage the distributed approach. In multi-robot navigation simulations, the proposed method shows a remarkable capability to navigate through nonconvex environments, outperforming a distributed optimization baseline using the interior point optimizer (IPOPT). In a 64-agent multi-car formation task with a challenging configuration, our method achieves 100% task completion with zero collisions, whereas distributed IPOPT fails to find a feasible solution.",
    "pdf_url": "https://arxiv.org/pdf/2510.18211v1",
    "github_url": null,
    "published": "2025-10-21T01:28:04+00:00",
    "updated": "2025-10-21T01:28:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18185v1",
    "title": "Enhancing Urban Data Exploration: Layer Toggling and Visibility-Preserving Lenses for Multi-Attribute Spatial Analysis",
    "authors": [
      "Salinas",
      "Nonato",
      "Fekete"
    ],
    "summary": "We propose two novel interaction techniques for visualization-assisted exploration of urban data: Layer Toggling and Visibility-Preserving Lenses. Layer Toggling mitigates visual overload by organizing information into separate layers while enabling comparisons through controlled overlays. This technique supports focused analysis without losing spatial context and allows users to switch layers using a dedicated button. Visibility-Preserving Lenses adapt their size and transparency dynamically, enabling detailed inspection of dense spatial regions and temporal attributes. These techniques facilitate urban data exploration and improve prediction. Understanding complex phenomena related to crime, mobility, and residents' behavior is crucial for informed urban planning. Yet navigating such data often causes cognitive overload and visual clutter due to overlapping layers. We validate our visualization tool through a user study measuring performance, cognitive load, and interaction efficiency. Using real-world data from Sao Paulo, we demonstrate how our approach enhances exploratory and analytical tasks and provides guidelines for future interactive systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.18185v1",
    "github_url": null,
    "published": "2025-10-21T00:24:21+00:00",
    "updated": "2025-10-21T00:24:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18063v1",
    "title": "MOFM-Nav: On-Manifold Ordering-Flexible Multi-Robot Navigation",
    "authors": [
      "Hu",
      "Yao",
      "Cao"
    ],
    "summary": "This paper addresses the problem of multi-robot navigation where robots maneuver on a desired \\(m\\)-dimensional (i.e., \\(m\\)-D) manifold in the $n$-dimensional Euclidean space, and maintain a {\\it flexible spatial ordering}. We consider $ m\\geq 2$, and the multi-robot coordination is achieved via non-Euclidean metrics. However, since the $m$-D manifold can be characterized by the zero-level sets of $n$ implicit functions, the last $m$ entries of the GVF propagation term become {\\it strongly coupled} with the partial derivatives of these functions if the auxiliary vectors are not appropriately chosen. These couplings not only influence the on-manifold maneuvering of robots, but also pose significant challenges to the further design of the ordering-flexible coordination via non-Euclidean metrics.   To tackle this issue, we first identify a feasible solution of auxiliary vectors such that the last $m$ entries of the propagation term are effectively decoupled to be the same constant. Then, we redesign the coordinated GVF (CGVF) algorithm to {\\it boost} the advantages of singularities elimination and global convergence by treating $m$ manifold parameters as additional $m$ virtual coordinates. Furthermore, we enable the on-manifold ordering-flexible motion coordination by allowing each robot to share $m$ virtual coordinates with its time-varying neighbors and a virtual target robot, which {\\it circumvents} the possible complex calculation if Euclidean metrics were used instead. Finally, we showcase the proposed algorithm's flexibility, adaptability, and robustness through extensive simulations with different initial positions, higher-dimensional manifolds, and robot breakdown, respectively.",
    "pdf_url": "https://arxiv.org/pdf/2510.18063v1",
    "github_url": null,
    "published": "2025-10-20T19:56:02+00:00",
    "updated": "2025-10-20T19:56:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18053v1",
    "title": "Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models",
    "authors": [
      "Fan",
      "Wei",
      "Cheng"
    ],
    "summary": "Balancing exploration and exploitation during reinforcement learning fine-tuning of generative models presents a critical challenge, as existing approaches rely on fixed divergence regularization that creates an inherent dilemma: strong regularization preserves model capabilities but limits reward optimization, while weak regularization enables greater alignment but risks instability or reward hacking. We introduce Adaptive Divergence Regularized Policy Optimization (ADRPO), which automatically adjusts regularization strength based on advantage estimates-reducing regularization for high-value samples while applying stronger regularization to poor samples, enabling policies to navigate between exploration and aggressive exploitation according to data quality. Our implementation with Wasserstein-2 regularization for flow matching generative models achieves remarkable results on text-to-image generation, achieving better semantic alignment and diversity than offline methods like DPO and online methods with fixed regularization like ORW-CFM-W2. ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B and 12B parameters in attribute binding, semantic consistency, artistic style transfer, and compositional control while maintaining generation diversity. ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and multi-modal reasoning models, enhancing existing online RL methods like GRPO. In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local optima through active exploration, while in multi-modal audio reasoning, it outperforms GRPO through superior step-by-step reasoning, enabling a 7B model to outperform substantially larger commercial models including Gemini 2.5 Pro and GPT-4o Audio, offering an effective plug-and-play solution to the exploration-exploitation challenge across diverse generative architectures and modalities.",
    "pdf_url": "https://arxiv.org/pdf/2510.18053v1",
    "github_url": null,
    "published": "2025-10-20T19:46:02+00:00",
    "updated": "2025-10-20T19:46:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18900v1",
    "title": "Foundation Models for Discovery and Exploration in Chemical Space",
    "authors": [
      "Wadell",
      "Bhutani",
      "Azumah"
    ],
    "summary": "Accurate prediction of atomistic, thermodynamic, and kinetic properties from molecular structures underpins materials innovation. Existing computational and experimental approaches lack the scalability required to efficiently navigate chemical space. Scientific foundation models trained on large unlabeled datasets offer a path toward exploring chemical space across diverse application domains. Here we develop MIST, a family of molecular foundation models with up to an order of magnitude more parameters and data than prior works. Trained using a novel tokenization scheme that comprehensively captures nuclear, electronic, and geometric information, MIST learns from a diverse range of molecules. MIST models have been fine-tuned to predict more than 400 structure -- property relationships and match or exceed state-of-the-art performance across benchmarks spanning physiology, electrochemistry, and quantum chemistry. We demonstrate the ability of these models to solve real-world problems across chemical space, including multiobjective electrolyte solvent screening, olfactory perception mapping, isotope half-life prediction, stereochemical reasoning for chiral organometallic compounds, and binary and multi-component mixture property prediction. Probing MIST models using mechanistic interpretability methods reveals identifiable patterns and trends not explicitly present in the training data, suggesting that the models learn generalizable scientific concepts. We formulate hyperparameter-penalized Bayesian neural scaling laws and use them to reduce the computational cost of model development by an order of magnitude. The methods and findings presented here represent a significant step toward accelerating materials discovery, design, and optimization using foundation models and provide valuable guidance for training compute-optimal scientific foundation models.",
    "pdf_url": "https://arxiv.org/pdf/2510.18900v1",
    "github_url": null,
    "published": "2025-10-20T17:56:01+00:00",
    "updated": "2025-10-20T17:56:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17747v1",
    "title": "Active polymers translocate faster in confinement",
    "authors": [
      "Prathyusha",
      "Sarkar",
      "Xu"
    ],
    "summary": "Living organisms employ diverse strategies to navigate confined environments. Inspired by translocation observations on California blackworms (\\textit{Lumbriculus variegatus}), we combine biological experiments and active-polymer simulations to examine how confinement and stiffness govern translocation. Active filaments translocate fastest when the channel width is comparable to their diameter, with escape time determined by propulsion speed, filament length, and channel geometry. In wider channels, activity and flexibility induce reorientation-dominated conformational changes that prolong escape. A single dimensionless ratio linking confinement to stiffness captures the transition from axis-aligned escape with short wall deflections for stiffer filaments, to reorientation-controlled motion with blob-like shapes for flexible filaments. These results provide a unified physical framework for active translocation in confinement and suggest design principles for flexible robotic filaments in complex environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.17747v1",
    "github_url": null,
    "published": "2025-10-20T17:02:37+00:00",
    "updated": "2025-10-20T17:02:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17604v1",
    "title": "Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm",
    "authors": [
      "Qiao",
      "Wang",
      "Yang"
    ],
    "summary": "With the rapid growth of bike sharing and the increasing diversity of cycling applications, accurate bicycle localization has become essential. traditional GNSS-based methods suffer from multipath effects, while existing inertial navigation approaches rely on precise modeling and show limited robustness. Tight Learned Inertial Odometry (TLIO) achieves low position drift by combining raw IMU data with predicted displacements by neural networks, but its high computational cost restricts deployment on mobile devices. To overcome this, we extend TLIO to bicycle localization and introduce an improved Mixture-of Experts (MoE) model that reduces both training and inference costs. Experiments show that, compared to the state-of-the-art LLIO framework, our method achieves comparable accuracy while reducing parameters by 64.7% and computational cost by 81.8%.",
    "pdf_url": "https://arxiv.org/pdf/2510.17604v1",
    "github_url": null,
    "published": "2025-10-20T14:52:50+00:00",
    "updated": "2025-10-20T14:52:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17564v1",
    "title": "An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning",
    "authors": [
      "Spoor",
      "Serra-Gómez",
      "Plaat"
    ],
    "summary": "In safety-critical domains such as robotics, navigation and power systems, constrained optimization problems arise where maximizing performance must be carefully balanced with associated constraints. Safe reinforcement learning provides a framework to address these challenges, with Lagrangian methods being a popular choice. However, the effectiveness of Lagrangian methods crucially depends on the choice of the Lagrange multiplier $λ$, which governs the trade-off between return and constraint cost. A common approach is to update the multiplier automatically during training. Although this is standard in practice, there remains limited empirical evidence on the robustness of an automated update and its influence on overall performance. Therefore, we analyze (i) optimality and (ii) stability of Lagrange multipliers in safe reinforcement learning across a range of tasks. We provide $λ$-profiles that give a complete visualization of the trade-off between return and constraint cost of the optimization problem. These profiles show the highly sensitive nature of $λ$ and moreover confirm the lack of general intuition for choosing the optimal value $λ^*$. Our findings additionally show that automated multiplier updates are able to recover and sometimes even exceed the optimal performance found at $λ^*$ due to the vast difference in their learning trajectories. Furthermore, we show that automated multiplier updates exhibit oscillatory behavior during training, which can be mitigated through PID-controlled updates. However, this method requires careful tuning to achieve consistently better performance across tasks. This highlights the need for further research on stabilizing Lagrangian methods in safe reinforcement learning. The code used to reproduce our results can be found at https://github.com/lindsayspoor/Lagrangian_SafeRL.",
    "pdf_url": "https://arxiv.org/pdf/2510.17564v1",
    "github_url": "https://github.com/lindsayspoor/Lagrangian_SafeRL",
    "published": "2025-10-20T14:13:17+00:00",
    "updated": "2025-10-20T14:13:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17929v1",
    "title": "Optimal Control from a Fluid Dynamics Perspective",
    "authors": [
      "Pratt",
      "Schneider",
      "Perloff"
    ],
    "summary": "An optimal control problem described by the Hamilton-Jacobi-Bellman equation can be developed into a problem that can be solved by general computational fluid dynamics packages. We describe how this formulation would allow a classical problem in optimal control, Zermelo's problem, to be treated as a multi-fluid problem. This approach has the advantage of allowing optimal navigation problems to be conducted over large areas, as well as to include moderately larger numbers of ships. We draw comparisons between this approach and the field of fluid control for fluid animations in movies.",
    "pdf_url": "https://arxiv.org/pdf/2510.17929v1",
    "github_url": null,
    "published": "2025-10-20T13:58:29+00:00",
    "updated": "2025-10-20T13:58:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17530v1",
    "title": "Navigate in Demanding Missions: Integrating Human Intelligence and Brain-Inspired Intelligence",
    "authors": [
      "He",
      "Meng",
      "Zhang"
    ],
    "summary": "This perspective analyzes the intricate interplay among neuroscience, Brain-Inspired Intelligence (BII), and Brain-Inspired Navigation (BIN), revealing a current lack of cooperative relationship between Brain-Computer Interfaces (BCIs) and BIN fields. We advocate for the integration of neuromorphic-empowered BCI into BIN, thereby bolstering the unmanned systems' reliable navigation in demanding missions, such as deep space exploration, etc. We highlight that machine intelligence, reinforced by brain-inspired artificial consciousness, can extend human intelligence, with human intelligence mediated by neuromorphic-enabled BCI acting as a safeguard in case machine intelligence failures. This study also discusses the potentials of the proposed approach to enhance unmanned systems' capabilities and facilitate the diagnostics of spatial cognition disorders, while considering associated ethical and security concerns.",
    "pdf_url": "https://arxiv.org/pdf/2510.17530v1",
    "github_url": null,
    "published": "2025-10-20T13:33:08+00:00",
    "updated": "2025-10-20T13:33:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17525v1",
    "title": "HumanMPC - Safe and Efficient MAV Navigation among Humans",
    "authors": [
      "Schaefer",
      "Oleynikova",
      "Hirche"
    ],
    "summary": "Safe and efficient robotic navigation among humans is essential for integrating robots into everyday environments. Most existing approaches focus on simplified 2D crowd navigation and fail to account for the full complexity of human body dynamics beyond root motion. We present HumanMPC, a Model Predictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation among humans that combines theoretical safety guarantees with data-driven models for realistic human motion forecasting. Our approach introduces a novel twist to reachability-based safety formulation that constrains only the initial control input for safety while modeling its effects over the entire planning horizon, enabling safe yet efficient navigation. We validate HumanMPC in both simulated experiments using real human trajectories and in the real-world, demonstrating its effectiveness across tasks ranging from goal-directed navigation to visual servoing for human tracking. While we apply our method to MAVs in this work, it is generic and can be adapted by other platforms. Our results show that the method ensures safety without excessive conservatism and outperforms baseline approaches in both efficiency and reliability.",
    "pdf_url": "https://arxiv.org/pdf/2510.17525v1",
    "github_url": null,
    "published": "2025-10-20T13:24:36+00:00",
    "updated": "2025-10-20T13:24:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17426v2",
    "title": "Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging",
    "authors": [
      "Hu",
      "Minixhofer",
      "Collier"
    ],
    "summary": "The \"alignment tax\" of post-training is typically framed as a drop in task accuracy. We show it also involves a severe loss of calibration, making models overconfident, less reliable, and model outputs less diverse. We show that this trade-off can be navigated effectively via a simple post-hoc intervention: interpolating between a model's weights before and after alignment. Crucially, this is not a strict trade-off. We find that the process consistently reveals Pareto-optimal interpolations - models that improve accuracy beyond both parents while substantially recovering the calibration lost during alignment. Our work demonstrates that simple model merging provides a computationally efficient method for mitigating the full scope of the alignment tax, yielding models that are more capable and more reliable.",
    "pdf_url": "https://arxiv.org/pdf/2510.17426v2",
    "github_url": null,
    "published": "2025-10-20T11:12:41+00:00",
    "updated": "2025-10-30T22:41:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17355v2",
    "title": "SmartSustain Recommender System: Navigating Sustainability Trade-offs in Personalized City Trip Planning",
    "authors": [
      "Banerjee",
      "Aksoy",
      "Wörndl"
    ],
    "summary": "Tourism is a major contributor to global carbon emissions and over-tourism, creating an urgent need for recommender systems that not only inform but also gently steer users toward more sustainable travel decisions. Such choices, however, often require balancing complex trade-offs between environmental impact, cost, convenience, and personal interests. To address this, we present the SmartSustain Recommender, a web application designed to nudge users toward eco-friendlier options through an interactive, user-centric interface. The system visualizes the broader consequences of travel decisions by combining CO2e emissions, destination popularity, and seasonality with personalized interest matching. It employs mechanisms such as interactive city cards for quick comparisons, dynamic banners that surface sustainable alternatives in specific trade-off scenarios, and real-time impact feedback using animated environmental indicators. A preliminary user study with 21 participants indicated strong usability and perceived effectiveness. The system is accessible at https://smartsustainrecommender.web.app.",
    "pdf_url": "https://arxiv.org/pdf/2510.17355v2",
    "github_url": null,
    "published": "2025-10-20T09:56:47+00:00",
    "updated": "2025-10-30T14:14:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17324v1",
    "title": "When 5G NTN Meets GNSS: Tracking GNSS Signals under Overlaid 5G Waveforms",
    "authors": [
      "Edjekouane",
      "Garrido",
      "Querol"
    ],
    "summary": "Global Navigation Satellite Systems (GNSS) provide the backbone of Positioning, Navigation, and Timing (PNT) but remain vulnerable to interference. Low Earth Orbit (LEO) constellations within Fifth-Generation (5G) Non-Terrestrial Networks (NTN) can enhance resilience by jointly supporting communication and navigation. This paper presents the first quantitative analysis of GNSS tracking and navigation message demodulation under a hybrid waveform where a low-power Direct-Sequence Spread Spectrum (DSSS) component is overlaid on an Orthogonal Frequency-Division Multiplexing (OFDM) 5G downlink. We evaluate a minimally modified GNSS receiver that tracks a legacy Global Positioning System (GPS) L1 Coarse/Acquisition (C/A) overlay aligned with 5G frames while treating the 5G waveform as structured interference. Using Monte Carlo simulations under realistic LEO Doppler dynamics, we analyze the Bit Error Rate (BER) of GPS L1 C/A navigation bits and the subframe decoding probability versus Signalto- Interference-plus-Noise Ratio (SINR) for multiple Signalto- Interference Ratios (SIR) and dynamic classes. Results show reliable demodulation across wide SINR ranges for low and medium dynamics, whereas high dynamics impose strict lock limits. These findings confirm the feasibility of Joint Communication and Positioning (JCAP) using a near-legacy GNSS chipset with minimal receiver modifications.",
    "pdf_url": "https://arxiv.org/pdf/2510.17324v1",
    "github_url": null,
    "published": "2025-10-20T09:17:40+00:00",
    "updated": "2025-10-20T09:17:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17315v1",
    "title": "Implicit State Estimation via Video Replanning",
    "authors": [
      "Ko",
      "Mao",
      "Fu"
    ],
    "summary": "Video-based representations have gained prominence in planning and decision-making due to their ability to encode rich spatiotemporal dynamics and geometric relationships. These representations enable flexible and generalizable solutions for complex tasks such as object manipulation and navigation. However, existing video planning frameworks often struggle to adapt to failures at interaction time due to their inability to reason about uncertainties in partially observed environments. To overcome these limitations, we introduce a novel framework that integrates interaction-time data into the planning process. Our approach updates model parameters online and filters out previously failed plans during generation. This enables implicit state estimation, allowing the system to adapt dynamically without explicitly modeling unknown state variables. We evaluate our framework through extensive experiments on a new simulated manipulation benchmark, demonstrating its ability to improve replanning performance and advance the field of video-based decision-making.",
    "pdf_url": "https://arxiv.org/pdf/2510.17315v1",
    "github_url": null,
    "published": "2025-10-20T09:02:25+00:00",
    "updated": "2025-10-20T09:02:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17263v1",
    "title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
    "authors": [
      "Lahiri",
      "Hou",
      "Sanyal"
    ],
    "summary": "Taxonomies play a crucial role in helping researchers structure and navigate knowledge in a hierarchical manner. They also form an important part in the creation of comprehensive literature surveys. The existing approaches to automatic survey generation do not compare the structure of the generated surveys with those written by human experts. To address this gap, we present our own method for automated taxonomy creation that can bridge the gap between human-generated and automatically-created taxonomies. For this purpose, we create the CS-TaxoBench benchmark which consists of 460 taxonomies that have been extracted from human-written survey papers. We also include an additional test set of 80 taxonomies curated from conference survey papers. We propose TaxoAlign, a three-phase topic-based instruction-guided method for scholarly taxonomy generation. Additionally, we propose a stringent automated evaluation framework that measures the structural alignment and semantic coherence of automatically generated taxonomies in comparison to those created by human experts. We evaluate our method and various baselines on CS-TaxoBench, using both automated evaluation metrics and human evaluation studies. The results show that TaxoAlign consistently surpasses the baselines on nearly all metrics. The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.",
    "pdf_url": "https://arxiv.org/pdf/2510.17263v1",
    "github_url": "https://github.com/AvishekLahiri/TaxoAlign",
    "published": "2025-10-20T07:49:51+00:00",
    "updated": "2025-10-20T07:49:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17249v1",
    "title": "An adaptive hierarchical control framework for quadrupedal robots in planetary exploration",
    "authors": [
      "Stark",
      "Kumar",
      "Vyas"
    ],
    "summary": "Planetary exploration missions require robots capable of navigating extreme and unknown environments. While wheeled rovers have dominated past missions, their mobility is limited to traversable surfaces. Legged robots, especially quadrupeds, can overcome these limitations by handling uneven, obstacle-rich, and deformable terrains. However, deploying such robots in unknown conditions is challenging due to the need for environment-specific control, which is infeasible when terrain and robot parameters are uncertain. This work presents a modular control framework that combines model-based dynamic control with online model adaptation and adaptive footstep planning to address uncertainties in both robot and terrain properties. The framework includes state estimation for quadrupeds with and without contact sensing, supports runtime reconfiguration, and is integrated into ROS 2 with open-source availability. Its performance was validated on two quadruped platforms, multiple hardware architectures, and in a volcano field test, where the robot walked over 700 m.",
    "pdf_url": "https://arxiv.org/pdf/2510.17249v1",
    "github_url": null,
    "published": "2025-10-20T07:37:49+00:00",
    "updated": "2025-10-20T07:37:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17073v1",
    "title": "Planar or Spatial: Exploring Design Aspects and Challenges for Presentations in Virtual Reality with No-coding Interface",
    "authors": [
      "Wu",
      "Zhang",
      "Leung"
    ],
    "summary": "The proliferation of virtual reality (VR) has led to its increasing adoption as an immersive medium for delivering presentations, distinct from other VR experiences like games and 360-degree videos by sharing information in richly interactive environments. However, creating engaging VR presentations remains a challenging and time-consuming task for users, hindering the full realization of VR presentation's capabilities. This research aims to explore the potential of VR presentation, analyze users' opinions, and investigate these via providing a user-friendly no-coding authoring tool. Through an examination of popular presentation software and interviews with seven professionals, we identified five design aspects and four design challenges for VR presentations. Based on the findings, we developed VRStory, a prototype for presentation authoring without coding to explore the design aspects and strategies for addressing the challenges. VRStory offers a variety of predefined and customizable VR elements, as well as modules for layout design, navigation control, and asset generation. A user study was then conducted with 12 participants to investigate their opinions and authoring experience with VRStory. Our results demonstrated that, while acknowledging the advantages of immersive and spatial features in VR, users often have a consistent mental model for traditional 2D presentations and may still prefer planar and static formats in VR for better accessibility and efficient communication. We finally shared our learned design considerations for future development of VR presentation tools, emphasizing the importance of balancing of promoting immersive features and ensuring accessibility.",
    "pdf_url": "https://arxiv.org/pdf/2510.17073v1",
    "github_url": null,
    "published": "2025-10-20T01:00:15+00:00",
    "updated": "2025-10-20T01:00:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17038v1",
    "title": "DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation",
    "authors": [
      "Fekri",
      "Roshanfar",
      "Barbeau"
    ],
    "summary": "Cardiac catheterization remains a cornerstone of minimally invasive interventions, yet it continues to rely heavily on manual operation. Despite advances in robotic platforms, existing systems are predominantly follow-leader in nature, requiring continuous physician input and lacking intelligent autonomy. This dependency contributes to operator fatigue, more radiation exposure, and variability in procedural outcomes. This work moves towards autonomous catheter navigation by introducing DINO-CVA, a multimodal goal-conditioned behavior cloning framework. The proposed model fuses visual observations and joystick kinematics into a joint embedding space, enabling policies that are both vision-aware and kinematic-aware. Actions are predicted autoregressively from expert demonstrations, with goal conditioning guiding navigation toward specified destinations. A robotic experimental setup with a synthetic vascular phantom was designed to collect multimodal datasets and evaluate performance. Results show that DINO-CVA achieves high accuracy in predicting actions, matching the performance of a kinematics-only baseline while additionally grounding predictions in the anatomical environment. These findings establish the feasibility of multimodal, goal-conditioned architectures for catheter navigation, representing an important step toward reducing operator dependency and improving the reliability of catheterbased therapies.",
    "pdf_url": "https://arxiv.org/pdf/2510.17038v1",
    "github_url": null,
    "published": "2025-10-19T22:59:32+00:00",
    "updated": "2025-10-19T22:59:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16996v1",
    "title": "STARK: Strategic Team of Agents for Refining Kernels",
    "authors": [
      "Dong",
      "Yang",
      "Liu"
    ],
    "summary": "The efficiency of GPU kernels is central to the progress of modern AI, yet optimizing them remains a difficult and labor-intensive task due to complex interactions between memory hierarchies, thread scheduling, and hardware-specific characteristics. While recent advances in large language models (LLMs) provide new opportunities for automated code generation, existing approaches largely treat LLMs as single-shot generators or naive refinement tools, limiting their effectiveness in navigating the irregular kernel optimization landscape. We introduce an LLM agentic framework for GPU kernel optimization that systematically explores the design space through multi-agent collaboration, grounded instruction, dynamic context management, and strategic search. This framework mimics the workflow of expert engineers, enabling LLMs to reason about hardware trade-offs, incorporate profiling feedback, and refine kernels iteratively. We evaluate our approach on KernelBench, a benchmark for LLM-based kernel optimization, and demonstrate substantial improvements over baseline agents: our system produces correct solutions where baselines often fail, and achieves kernels with up to 16x faster runtime performance. These results highlight the potential of agentic LLM frameworks to advance fully automated, scalable GPU kernel optimization.",
    "pdf_url": "https://arxiv.org/pdf/2510.16996v1",
    "github_url": null,
    "published": "2025-10-19T20:41:46+00:00",
    "updated": "2025-10-19T20:41:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16905v1",
    "title": "C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control",
    "authors": [
      "Cao",
      "Moorthy",
      "Poyrazoglu"
    ],
    "summary": "Trajectory sampling is a key component of sampling-based control mechanisms. Trajectory samplers rely on control input samplers, which generate control inputs u from a distribution p(u | x) where x is the current state. We introduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for short) which has two key features: (i) it generates a control input distribution so as to uniformly sample the free configuration space, and (ii) in contrast to previously introduced trajectory sampling mechanisms where the distribution p(u | x) is independent of the environment, C-Free-Uniform is explicitly conditioned on the current local map. Next, we integrate this sampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI. Experiments show that CFU-MPPI outperforms existing methods in terms of success rate in challenging navigation tasks in cluttered polygonal environments while requiring a much smaller sampling budget.",
    "pdf_url": "https://arxiv.org/pdf/2510.16905v1",
    "github_url": null,
    "published": "2025-10-19T15:58:31+00:00",
    "updated": "2025-10-19T15:58:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16853v2",
    "title": "Agentic Inequality",
    "authors": [
      "Sharp",
      "Bilgin",
      "Gabriel"
    ],
    "summary": "Autonomous AI agents, capable of complex planning and action, represent a significant technological evolution beyond current generative tools. As these systems become integrated into political and economic life, their distribution and capabilities will be highly consequential. This paper introduces and explores \"agentic inequality\" - the potential disparities in power, opportunity, and outcomes stemming from differential access to, and capabilities of, AI agents. We analyse the dual potential of this technology, exploring how agents could both exacerbate existing divides and, under the right conditions, serve as a powerful equalising force. To this end, the paper makes three primary contributions. First, it establishes an analytical framework by delineating the three core dimensions through which this inequality can manifest: disparities in the availability, quality, and quantity of agents. Second, it argues that agentic inequality is distinct from prior technological divides. Unlike tools that primarily augment human abilities, agents act as autonomous delegates, creating novel power asymmetries through scalable goal delegation and direct agent-to-agent competition that are poised to reshape outcomes across economic and socio-political spheres. Finally, it provides a systematic analysis of the technical and socioeconomic drivers - from model release strategies to market incentives - that will shape the distribution of agentic power, concluding with a research agenda for navigating the complex governance challenges ahead.",
    "pdf_url": "https://arxiv.org/pdf/2510.16853v2",
    "github_url": null,
    "published": "2025-10-19T14:32:46+00:00",
    "updated": "2025-10-22T15:29:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16771v1",
    "title": "A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT",
    "authors": [
      "He",
      "Meng",
      "Yin"
    ],
    "summary": "Developing universal Positioning, Navigation, and Timing (PNT) is our enduring goal. Today's complex environments demand PNT that is more resilient, energy-efficient and cognitively capable. This paper asks how we can endow unmanned systems with brain-inspired spatial cognition navigation while exploiting the high precision of machine PNT to advance universal PNT. We provide a new perspective and roadmap for shifting PNT from \"tool-oriented\" to \"cognition-driven\". Contributions: (1) multi-level dissection of differences among traditional PNT, biological brain PNT and brain-inspired PNT; (2) a four-layer (observation-capability-decision-hardware) fusion framework that unites numerical precision and brain-inspired intelligence; (3) forward-looking recommendations for future development of brain-inspired PNT.",
    "pdf_url": "https://arxiv.org/pdf/2510.16771v1",
    "github_url": null,
    "published": "2025-10-19T09:32:35+00:00",
    "updated": "2025-10-19T09:32:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.18894v1",
    "title": "Refugees of the Digital Space: Platform Migration from TikTok to RedNote",
    "authors": [
      "Feng",
      "Dong",
      "Lei"
    ],
    "summary": "In January 2025, the U.S. government enacted a nationwide ban on TikTok, prompting a wave of American users -- self-identified as ``TikTok Refugees'' -- to migrate to alternative platforms, particularly the Chinese social media app RedNote (Xiaohongshu). This paper examines how these digital migrants navigate cross-cultural platform environments and develop adaptive communicative strategies under algorithmic governance. Drawing on a multi-method framework, the study analyzes temporal posting patterns, influence dynamics, thematic preferences, and sentiment-weighted topic expressions across three distinct migration phases: Pre-Ban, Refugee Surge, and Stabilization.   An entropy-weighted influence score was used to classify users into high- and low-influence groups, enabling comparative analysis of content strategies. Findings reveal that while dominant topics remained relatively stable over time (e.g., self-expression, lifestyle, and creativity), high-influence users were more likely to engage in culturally resonant or commercially strategic content. Additionally, political discourse was not avoided, but selectively activated as a point of transnational engagement.   Emotionally, high-influence users tended to express more positive affect in culturally connective topics, while low-influence users showed stronger emotional intensity in personal narratives. These findings suggest that cross-cultural platform migration is shaped not only by structural affordances but also by users' differential capacities to adapt, perform, and maintain visibility. The study contributes to literature on platform society, affective publics, and user agency in transnational digital environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.18894v1",
    "github_url": null,
    "published": "2025-10-19T05:00:13+00:00",
    "updated": "2025-10-19T05:00:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16624v1",
    "title": "Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs",
    "authors": [
      "Mocanu",
      "Slusanschi",
      "Leordeanu"
    ],
    "summary": "This paper presents a vision-only autonomous flight system for small UAVs operating in controlled indoor environments. The system combines semantic segmentation with monocular depth estimation to enable obstacle avoidance, scene exploration, and autonomous safe landing operations without requiring GPS or expensive sensors such as LiDAR. A key innovation is an adaptive scale factor algorithm that converts non-metric monocular depth predictions into accurate metric distance measurements by leveraging semantic ground plane detection and camera intrinsic parameters, achieving a mean distance error of 14.4 cm. The approach uses a knowledge distillation framework where a color-based Support Vector Machine (SVM) teacher generates training data for a lightweight U-Net student network (1.6M parameters) capable of real-time semantic segmentation. For more complex environments, the SVM teacher can be replaced with a state-of-the-art segmentation model. Testing was conducted in a controlled 5x4 meter laboratory environment with eight cardboard obstacles simulating urban structures. Extensive validation across 30 flight tests in a real-world environment and 100 flight tests in a digital-twin environment demonstrates that the combined segmentation and depth approach increases the distance traveled during surveillance and reduces mission time while maintaining 100% success rates. The system is further optimized through end-to-end learning, where a compact student neural network learns complete flight policies from demonstration data generated by our best-performing method, achieving an 87.5% autonomous mission success rate. This work advances practical vision-based drone navigation in structured environments, demonstrating solutions for metric depth estimation and computational efficiency challenges that enable deployment on resource-constrained platforms.",
    "pdf_url": "https://arxiv.org/pdf/2510.16624v1",
    "github_url": null,
    "published": "2025-10-18T19:35:17+00:00",
    "updated": "2025-10-18T19:35:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16518v1",
    "title": "DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation",
    "authors": [
      "Ortega-Peimbert",
      "Busch",
      "Homberger"
    ],
    "summary": "Advances in open-vocabulary semantic mapping and object navigation have enabled robots to perform an informed search of their environment for an arbitrary object. However, such zero-shot object navigation is typically designed for simple queries with an object name like \"television\" or \"blue rug\". Here, we consider more complex free-text queries with spatial relationships, such as \"find the remote on the table\" while still leveraging robustness of a semantic map. We present DIV-Nav, a real-time navigation system that efficiently addresses this problem through a series of relaxations: i) Decomposing natural language instructions with complex spatial constraints into simpler object-level queries on a semantic map, ii) computing the Intersection of individual semantic belief maps to identify regions where all objects co-exist, and iii) Validating the discovered objects against the original, complex spatial constrains via a LVLM. We further investigate how to adapt the frontier exploration objectives of online semantic mapping to such spatial search queries to more effectively guide the search process. We validate our system through extensive experiments on the MultiON benchmark and real-world deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More details and videos are available at https://anonsub42.github.io/reponame/",
    "pdf_url": "https://arxiv.org/pdf/2510.16518v1",
    "github_url": null,
    "published": "2025-10-18T14:22:32+00:00",
    "updated": "2025-10-18T14:22:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16457v1",
    "title": "NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation",
    "authors": [
      "Xu",
      "Gong",
      "MU"
    ],
    "summary": "In this work we concentrate on the task of goal-oriented Vision-and-Language Navigation (VLN). Existing methods often make decisions based on historical information, overlooking the future implications and long-term outcomes of the actions. In contrast, we aim to develop a foresighted agent. Specifically, we draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory data, in order to learn the general knowledge regarding the layout and object relations within indoor scenes. This model can generate a Q-feature, analogous to the Q-value in traditional Q-network, for each candidate action, which describes the potential future information that may be observed after taking the specific action. Subsequently, a cross-modal future encoder integrates the task-agnostic Q-feature with navigation instructions to produce a set of action scores reflecting future prospects. These scores, when combined with the original scores based on history, facilitate an A*-style searching strategy to effectively explore the regions that are more likely to lead to the destination. Extensive experiments conducted on widely used goal-oriented VLN datasets validate the effectiveness of the proposed method.",
    "pdf_url": "https://arxiv.org/pdf/2510.16457v1",
    "github_url": null,
    "published": "2025-10-18T11:29:33+00:00",
    "updated": "2025-10-18T11:29:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16409v1",
    "title": "Mastering Uncertainty: From Understanding to Prediction",
    "authors": [
      "Sornette"
    ],
    "summary": "Uncertainty defines our age: it shapes climate, finance, technology, and society, yet remains profoundly misunderstood. We oscillate between the illusion of control and the paralysis of fatalism. This paper reframes uncertainty not as randomness but as ignorance: a product of poor models, institutional blindness, and cognitive bias. Drawing on insights from physics, complex systems, and decades of empirical research, I show that much of what appears unpredictable reveals structure near transitions, where feedbacks, critical thresholds, and early-warning signals emerge. Across domains from financial crises to industrial disasters, uncertainty is amplified less by nature than by human behavior and organizational failure. To master it, prediction must shift from prophecy to diagnosis, identifying precursors of instability rather than forecasting exact outcomes. I propose a framework of dynamic foresight grounded in adaptive leadership, transparent communication, and systemic learning. Mastering uncertainty thus means transforming fear into foresight and building institutions that navigate, rather than deny, the complexity of change.",
    "pdf_url": "https://arxiv.org/pdf/2510.16409v1",
    "github_url": null,
    "published": "2025-10-18T08:48:10+00:00",
    "updated": "2025-10-18T08:48:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16373v1",
    "title": "Navigating through the hidden embedding space: steering LLMs to improve mental health assessment",
    "authors": [
      "Ravenda",
      "Bahrainian",
      "Raballo"
    ],
    "summary": "The rapid evolution of Large Language Models (LLMs) is transforming AI, opening new opportunities in sensitive and high-impact areas such as Mental Health (MH). Yet, despite these advancements, recent evidence reveals that smaller-scale models still struggle to deliver optimal performance in domain-specific applications. In this study, we present a cost-efficient yet powerful approach to improve MH assessment capabilities of an LLM, without relying on any computationally intensive techniques. Our lightweight method consists of a linear transformation applied to a specific layer's activations, leveraging steering vectors to guide the model's output. Remarkably, this intervention enables the model to achieve improved results across two distinct tasks: (1) identifying whether a Reddit post is useful for detecting the presence or absence of depressive symptoms (relevance prediction task), and (2) completing a standardized psychological screening questionnaire for depression based on users' Reddit post history (questionnaire completion task). Results highlight the untapped potential of steering mechanisms as computationally efficient tools for LLMs' MH domain adaptation.",
    "pdf_url": "https://arxiv.org/pdf/2510.16373v1",
    "github_url": null,
    "published": "2025-10-18T06:51:39+00:00",
    "updated": "2025-10-18T06:51:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16347v2",
    "title": "Computer Navigated Spinal Surgery Using Magnetic Resonance Imaging and Augmented Reality",
    "authors": [
      "Lu",
      "Hui",
      "Weeks"
    ],
    "summary": "Current spinal pain management procedures, such as radiofrequency ablation (RFA) and epidural steroid injection (ESI), rely on fluoroscopy for needle placement which exposes patients and physicians to ionizing radiation. In this paper, we investigate a radiation-free surgical navigation system for spinal pain management procedures that combines magnetic resonance imaging (MRI) with fiducial ArUco marker-based augmented reality (AR). High-resolution MRI scans of a lumbar spinal phantom were obtained and assembled as a surface mesh. Laplacian smoothing algorithms were then applied to smoothen the surface and improve the model fidelity. A commercially available stereo camera (ZED2) was used to track single or dual fiducial ArUco markers on the patient to determine the patient's real-time pose. Custom AR software was applied to overlay the MRI image onto the patient, allowing the physician to see not only the outer surface of the patient but also the complete anatomy of the patient below the surface. Needle-insertion trials on a 3D-printed 3-vertebra phantom showed that dual-ArUco marker tracking increased the accuracy of needle insertions and reduced the average needle misplacement distance compared to single-ArUco marker procedures. The average needle misplacement is comparable to the average deviation of 2 mm for conventional epidural techniques using fluoroscopy. Our radiation-free system demonstrates promise to serve as an alternative to fluoroscopy by improving image-guided spinal navigation.",
    "pdf_url": "https://arxiv.org/pdf/2510.16347v2",
    "github_url": null,
    "published": "2025-10-18T04:38:29+00:00",
    "updated": "2025-10-21T03:59:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16308v1",
    "title": "SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling",
    "authors": [
      "Zhang",
      "Huang",
      "Dong"
    ],
    "summary": "UAVs equipped with a single depth camera encounter significant challenges in dynamic obstacle avoidance due to limited field of view and inevitable blind spots. While active vision strategies that steer onboard cameras have been proposed to expand sensing coverage, most existing methods separate motion planning from sensing considerations, resulting in less effective and delayed obstacle response. To address this limitation, we introduce SPOT (Sensing-augmented Planning via Obstacle Threat modeling), a unified planning framework for observation-aware trajectory planning that explicitly incorporates sensing objectives into motion optimization. At the core of our method is a Gaussian Process-based obstacle belief map, which establishes a unified probabilistic representation of both recognized (previously observed) and potential obstacles. This belief is further processed through a collision-aware inference mechanism that transforms spatial uncertainty and trajectory proximity into a time-varying observation urgency map. By integrating urgency values within the current field of view, we define differentiable objectives that enable real-time, observation-aware trajectory planning with computation times under 10 ms. Simulation and real-world experiments in dynamic, cluttered, and occluded environments show that our method detects potential dynamic obstacles 2.8 seconds earlier than baseline approaches, increasing dynamic obstacle visibility by over 500\\%, and enabling safe navigation through cluttered, occluded environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.16308v1",
    "github_url": null,
    "published": "2025-10-18T02:37:29+00:00",
    "updated": "2025-10-18T02:37:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16229v1",
    "title": "C/N0 Analysis-Based GPS Spoofing Detection with Variable Antenna Orientations",
    "authors": [
      "Li",
      "Villa",
      "Diessner"
    ],
    "summary": "GPS spoofing poses a growing threat to aviation by falsifying satellite signals and misleading aircraft navigation systems. This paper demonstrates a proof-of-concept spoofing detection strategy based on analyzing satellite Carrier-to-Noise Density Ratio (C/N$_0$) variation during controlled static antenna orientations. Using a u-blox EVK-M8U receiver and a GPSG-1000 satellite simulator, C/N$_0$ data is collected under three antenna orientations flat, banked right, and banked left) in both real-sky (non-spoofed) and spoofed environments. Our findings reveal that under non-spoofed signals, C/N$_0$ values fluctuate naturally with orientation, reflecting true geometric dependencies. However, spoofed signals demonstrate a distinct pattern: the flat orientation, which directly faces the spoofing antenna, consistently yielded the highest C/N$_0$ values, while both banked orientations showed reduced C/N$_0$ due to misalignment with the spoofing source. These findings suggest that simple maneuvers such as brief banking to induce C/N$_0$ variations can provide early cues of GPS spoofing for general aviation and UAV systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.16229v1",
    "github_url": null,
    "published": "2025-10-17T21:40:58+00:00",
    "updated": "2025-10-17T21:40:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16223v1",
    "title": "Case Study of GAI for Generating Novel Images for Real-World Embroidery",
    "authors": [
      "Glazko",
      "Arugunta",
      "Chan"
    ],
    "summary": "In this paper, we present a case study exploring the potential use of Generative Artificial Intelligence (GAI) to address the real-world need of making the design of embroiderable art patterns more accessible. Through an auto-ethnographic case study by a disabled-led team, we examine the application of GAI as an assistive technology in generating embroidery patterns, addressing the complexity involved in designing culturally-relevant patterns as well as those that meet specific needs regarding detail and color. We detail the iterative process of prompt engineering custom GPTs tailored for producing specific visual outputs, emphasizing the nuances of achieving desirable results that align with real-world embroidery requirements. Our findings underscore the mixed outcomes of employing GAI for producing embroiderable images, from facilitating creativity and inclusion to navigating the unpredictability of AI-generated designs. Future work aims to refine GAI tools we explored for generating embroiderable images to make them more performant and accessible, with the goal of fostering more inclusion in the domains of creativity and making.",
    "pdf_url": "https://arxiv.org/pdf/2510.16223v1",
    "github_url": null,
    "published": "2025-10-17T21:16:23+00:00",
    "updated": "2025-10-17T21:16:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16216v1",
    "title": "Topological decoding of grid cell activity via path lifting to covering spaces",
    "authors": [
      "Yao",
      "Yoon"
    ],
    "summary": "High-dimensional neural activity often reside in a low-dimensional subspace, referred to as neural manifolds. Grid cells in the medial entorhinal cortex provide a periodic spatial code that are organized near a toroidal manifold, independent of the spatial environment. Due to the periodic nature of its code, it is unclear how the brain utilizes the toroidal manifold to understand its state in a spatial environment. We introduce a novel framework that decodes spatial information from grid cell activity using topology. Our approach uses topological data analysis to extract toroidal coordinates from grid cell population activity and employs path-lifting to reconstruct trajectories in physical space. The reconstructed paths differ from the original by an affine transformation. We validated the method on both continuous attractor network simulations and experimental recordings of grid cells, demonstrating that local trajectories can be reliably reconstructed from a single grid cell module without external position information or training data. These results suggest that co-modular grid cells contain sufficient information for path integration and suggest a potential computational mechanism for spatial navigation.",
    "pdf_url": "https://arxiv.org/pdf/2510.16216v1",
    "github_url": null,
    "published": "2025-10-17T21:02:28+00:00",
    "updated": "2025-10-17T21:02:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16160v1",
    "title": "Automated C-Arm Positioning via Conformal Landmark Localization",
    "authors": [
      "Arrabi",
      "Jung",
      "Luo"
    ],
    "summary": "Accurate and reliable C-arm positioning is essential for fluoroscopy-guided interventions. However, clinical workflows rely on manual alignment that increases radiation exposure and procedural delays. In this work, we present a pipeline that autonomously navigates the C-arm to predefined anatomical landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary starting location on the operating table, the model predicts a 3D displacement vector toward each target landmark along the body. To ensure reliable deployment, we capture both aleatoric and epistemic uncertainties in the model's predictions and further calibrate them using conformal prediction. The derived prediction regions are interpreted as 3D confidence regions around the predicted landmark locations. The training framework combines a probabilistic loss with skeletal pose regularization to encourage anatomically plausible outputs. We validate our approach on a synthetic X-ray dataset generated from DeepDRR. Results show not only strong localization accuracy across multiple architectures but also well-calibrated prediction bounds. These findings highlight the pipeline's potential as a component in safe and reliable autonomous C-arm systems. Code is available at https://github.com/AhmadArrabi/C_arm_guidance_APAH",
    "pdf_url": "https://arxiv.org/pdf/2510.16160v1",
    "github_url": "https://github.com/AhmadArrabi/C_arm_guidance_APAH",
    "published": "2025-10-17T19:04:08+00:00",
    "updated": "2025-10-17T19:04:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15863v1",
    "title": "PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction",
    "authors": [
      "Yu",
      "Li",
      "Shi"
    ],
    "summary": "Large language models (LLMs) are moving beyond static uses and are now powering agents that learn continually during their interaction with external environments. For example, agents can learn reusable skills while navigating web pages or toggling new tools. However, existing methods for skill learning often create skills that are over-specialized to a single website and fail to generalize. We introduce PolySkill, a new framework that enables agents to learn generalizable and compositional skills. The core idea, inspired by polymorphism in software engineering, is to decouple a skill's abstract goal (what it accomplishes) and its concrete implementation (how it is executed). Experiments show that our method (1) improves skill reuse by 1.7x on seen websites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on unseen websites, while reducing steps by over 20%. (3) In self-exploration settings without specified tasks, our framework improves the quality of proposed tasks and enables agents to learn generalizable skills that work across different sites. By enabling the agent to identify and refine its own goals, the PolySkill enhances the agent's ability to learn a better curriculum, leading to the acquisition of more generalizable skills compared to baseline methods. This work provides a practical path toward building agents capable of continual learning in adaptive environments. Our findings show that separating a skill's goal from its execution is a crucial step toward developing autonomous agents that can learn and generalize across the open web continuously.",
    "pdf_url": "https://arxiv.org/pdf/2510.15863v1",
    "github_url": null,
    "published": "2025-10-17T17:56:00+00:00",
    "updated": "2025-10-17T17:56:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15842v1",
    "title": "Paper2Web: Let's Make Your Paper Alive!",
    "authors": [
      "Chen",
      "Lv",
      "Zhang"
    ],
    "summary": "Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. In this paper, we introduce Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. We further present PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Our experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation.",
    "pdf_url": "https://arxiv.org/pdf/2510.15842v1",
    "github_url": null,
    "published": "2025-10-17T17:35:58+00:00",
    "updated": "2025-10-17T17:35:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15803v1",
    "title": "Dynamic Recalibration in LiDAR SLAM: Integrating AI and Geometric Methods with Real-Time Feedback Using INAF Fusion",
    "authors": [
      "Arjmandi",
      "Sohn"
    ],
    "summary": "This paper presents a novel fusion technique for LiDAR Simultaneous Localization and Mapping (SLAM), aimed at improving localization and 3D mapping using LiDAR sensor. Our approach centers on the Inferred Attention Fusion (INAF) module, which integrates AI with geometric odometry. Utilizing the KITTI dataset's LiDAR data, INAF dynamically adjusts attention weights based on environmental feedback, enhancing the system's adaptability and measurement accuracy. This method advances the precision of both localization and 3D mapping, demonstrating the potential of our fusion technique to enhance autonomous navigation systems in complex scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2510.15803v1",
    "github_url": null,
    "published": "2025-10-17T16:29:02+00:00",
    "updated": "2025-10-17T16:29:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15707v1",
    "title": "Mitigating Underwater Noise from Offshore Wind Turbines via Individual Pitch Control",
    "authors": [
      "Frutos",
      "Botero-Bolívar",
      "Ferrer"
    ],
    "summary": "This paper proposes a pitch control strategy to mitigate the underwater acoustic footprint of offshore wind turbines, a measure that will soon become necessary to minimize impacts on marine life, which rely on sound for communication, navigation, and survival. First, we quantify the underwater acoustic signature of blade-generated aerodynamic noise from three reference turbines, the NREL 5 MW, DTU 10 MW, and IEA 22 MW, using coupling blade element momentum and coupled air-water acoustic propagation modeling. Second, we propose and implement an open-loop individual pitch control (IPC) strategy that modulates the pitch of the blade at the blade passing frequency to attenuate the overall sound pressure level (OSPL) and the amplitude modulation (AM) of the transmitted noise. Third, we benchmark IPC performance against conventional pitch schemes. The results indicate that up to 5 dB reductions in OSPL and a decrease in AM depth 20% can be achieved with a pitch variation of $Δθ\\approx 5^\\circ$, with small losses (5-10%) in energy capture. These findings highlight a previously underappreciated noise pathway and demonstrate that targeted blade-pitch modulation can mitigate its impact.",
    "pdf_url": "https://arxiv.org/pdf/2510.15707v1",
    "github_url": null,
    "published": "2025-10-17T14:49:18+00:00",
    "updated": "2025-10-17T14:49:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15562v1",
    "title": "Airway Mucus Rheology: Physical Insights for Navigating through Health to Pathology and Clinical Applications",
    "authors": [
      "Liu",
      "Che",
      "Zhang"
    ],
    "summary": "Airway mucus is a complex gel with an anisotropic three-dimensional network structure. As a crucial component of the respiratory defense barrier, it plays a vital role in maintaining airway hydration and supporting the function of airway epithelial cells. Through linear and nonlinear rheological mechanisms such as ciliary motion and coughing, airway mucus expels foreign pathogens and toxic nano- and microparticles while selectively allowing the passage of specific nutrients and proteins. These protective and clearance functions depend on the proper rheological properties of mucus under normal physiological conditions. However, in respiratory disease such as CF, COPD, asthma, and COVID-19, excessive mucus secretion is often accompanied by abnormal rheological behaviors. This leads to impaired mucus flow, airway obstruction, and potentially life-threatening conditions. Therefore, this review examines the rheological behaviors of airway mucus in relation to health and disease, focusing on both macrorheology and microrheology. The review highlights those changes in the chemical composition and microstructure of airway mucus, especially under pathological conditions, that can significantly alter its rheological behavior. Rheological parameters can also serve as biological indicators to study the role of mucus in clearance functions and aid in developing pulmonary drug delivery systems. By integrating findings from both macro- and microrheological studies, this review aims to enhance our understanding of the complex behavior of airway mucus, supporting better diagnosis, treatment, and management of chronic respiratory diseases.",
    "pdf_url": "https://arxiv.org/pdf/2510.15562v1",
    "github_url": null,
    "published": "2025-10-17T11:47:30+00:00",
    "updated": "2025-10-17T11:47:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15484v1",
    "title": "Definition and Realization of the International Lunar Reference Frame",
    "authors": [
      "Sośnica",
      "Fienga",
      "Pavlov"
    ],
    "summary": "All future lunar missions require a definition of the lunar reference system and a realization in the form of the lunar reference frame to ensure consistent products for positioning, navigation, cartography, and timing. This paper defines the origin, orientation, and scale of the Lunar Reference System (LRS), as well as provides numerical solutions for the first realization of the International Lunar Reference Frame (ILRF). ILRF is defined as the Principal Axis (PA) system, attached to the surface and co-rotating with the Moon, with its origin in the lunar center of mass (lunocenter). The ILRF realization is based on variance component estimation of the three lunar ephemeris solutions: INPOP21a, DE430, and EPM2021 for the series of the position of the lunar center of mass and rotation Euler angles -- precession, nutation, and proper rotation. The solution is valid starting with the period covered by Lunar Laser Ranging (LLR) data in 1970 and ending with extrapolated ILRF realizations in 2052 for future lunar missions. Results. The combined ILRF is characterized by the mean error of 17.6 cm for 2010-2030, where 15.3 cm comes from the origin and 8.6 cm from the orientation realization. The error in the realization of the origin is mainly caused by a poor geometry of the retroreflector network, resulting in a high correlation between the scale and the X component of the lunocenter in PA. The LLR post-fit residuals in ILRF are at the level of 2-3 cm in terms of the standard deviations of one-way ranges for best-performing LLR stations. The mean errors of the transformation between ILRF and other reference frame realizations in PA are at the level of 3 cm, whereas the mean transformation error to the DE421 Mean Earth frame equals 5 cm.",
    "pdf_url": "https://arxiv.org/pdf/2510.15484v1",
    "github_url": null,
    "published": "2025-10-17T09:54:38+00:00",
    "updated": "2025-10-17T09:54:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15400v1",
    "title": "Robust High-Resolution Multi-Organ Diffusion MRI Using Synthetic-Data-Tuned Prompt Learning",
    "authors": [
      "Qian",
      "Zhang",
      "Ma"
    ],
    "summary": "Clinical adoption of multi-shot diffusion-weighted magnetic resonance imaging (multi-shot DWI) for body-wide tumor diagnostics is limited by severe motion-induced phase artifacts from respiration, peristalsis, and so on, compounded by multi-organ, multi-slice, multi-direction and multi-b-value complexities. Here, we introduce a reconstruction framework, LoSP-Prompt, that overcomes these challenges through physics-informed modeling and synthetic-data-driven prompt learning. We model inter-shot phase variations as a high-order Locally Smooth Phase (LoSP), integrated into a low-rank Hankel matrix reconstruction. Crucially, the algorithm's rank parameter is automatically set via prompt learning trained exclusively on synthetic abdominal DWI data emulating physiological motion. Validated across 10,000+ clinical images (43 subjects, 4 scanner models, 5 centers), LoSP-Prompt: (1) Achieved twice the spatial resolution of clinical single-shot DWI, enhancing liver lesion conspicuity; (2) Generalized to seven diverse anatomical regions (liver, kidney, sacroiliac, pelvis, knee, spinal cord, brain) with a single model; (3) Outperformed state-of-the-art methods in image quality, artifact suppression, and noise reduction (11 radiologists' evaluations on a 5-point scale, $p<0.05$), achieving 4-5 points (excellent) on kidney DWI, 4 points (good to excellent) on liver, sacroiliac and spinal cord DWI, and 3-4 points (good) on knee and tumor brain. The approach eliminates navigator signals and realistic data supervision, providing an interpretable, robust solution for high-resolution multi-organ multi-shot DWI. Its scanner-agnostic performance signifies transformative potential for precision oncology.",
    "pdf_url": "https://arxiv.org/pdf/2510.15400v1",
    "github_url": null,
    "published": "2025-10-17T07:51:35+00:00",
    "updated": "2025-10-17T07:51:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15376v1",
    "title": "Towards Automated Chicken Deboning via Learning-based Dynamically-Adaptive 6-DoF Multi-Material Cutting",
    "authors": [
      "Yang",
      "Hu",
      "Ravichandar"
    ],
    "summary": "Automating chicken shoulder deboning requires precise 6-DoF cutting through a partially occluded, deformable, multi-material joint, since contact with the bones presents serious health and safety risks. Our work makes both systems-level and algorithmic contributions to train and deploy a reactive force-feedback cutting policy that dynamically adapts a nominal trajectory and enables full 6-DoF knife control to traverse the narrow joint gap while avoiding contact with the bones. First, we introduce an open-source custom-built simulator for multi-material cutting that models coupling, fracture, and cutting forces, and supports reinforcement learning, enabling efficient training and rapid prototyping. Second, we design a reusable physical testbed to emulate the chicken shoulder: two rigid \"bone\" spheres with controllable pose embedded in a softer block, enabling rigorous and repeatable evaluation while preserving essential multi-material characteristics of the target problem. Third, we train and deploy a residual RL policy, with discretized force observations and domain randomization, enabling robust zero-shot sim-to-real transfer and the first demonstration of a learned policy that debones a real chicken shoulder. Our experiments in our simulator, on our physical testbed, and on real chicken shoulders show that our learned policy reliably navigates the joint gap and reduces undesired bone/cartilage contact, resulting in up to a 4x improvement over existing open-loop cutting baselines in terms of success rate and bone avoidance. Our results also illustrate the necessity of force feedback for safe and effective multi-material cutting. The project website is at https://sites.google.com/view/chickendeboning-2026.",
    "pdf_url": "https://arxiv.org/pdf/2510.15376v1",
    "github_url": null,
    "published": "2025-10-17T07:27:19+00:00",
    "updated": "2025-10-17T07:27:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15352v1",
    "title": "GaussGym: An open-source real-to-sim framework for learning locomotion from pixels",
    "authors": [
      "Escontrela",
      "Kerr",
      "Allshire"
    ],
    "summary": "We present a novel approach for photorealistic robot simulation that integrates 3D Gaussian Splatting as a drop-in renderer within vectorized physics simulators such as IsaacGym. This enables unprecedented speed -- exceeding 100,000 steps per second on consumer GPUs -- while maintaining high visual fidelity, which we showcase across diverse tasks. We additionally demonstrate its applicability in a sim-to-real robotics setting. Beyond depth-based sensing, our results highlight how rich visual semantics improve navigation and decision-making, such as avoiding undesirable regions. We further showcase the ease of incorporating thousands of environments from iPhone scans, large-scale scene datasets (e.g., GrandTour, ARKit), and outputs from generative video models like Veo, enabling rapid creation of realistic training worlds. This work bridges high-throughput simulation and high-fidelity perception, advancing scalable and generalizable robot learning. All code and data will be open-sourced for the community to build upon. Videos, code, and data available at https://escontrela.me/gauss_gym/.",
    "pdf_url": "https://arxiv.org/pdf/2510.15352v1",
    "github_url": null,
    "published": "2025-10-17T06:34:52+00:00",
    "updated": "2025-10-17T06:34:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15336v1",
    "title": "Adaptive Cost-Map-based Path Planning in Partially Unknown Environments with Movable Obstacles",
    "authors": [
      "Stan",
      "Bezerra",
      "Kojima"
    ],
    "summary": "Reliable navigation in disaster-response and other unstructured indoor settings requires robots not only to avoid obstacles but also to recognise when those obstacles can be pushed aside. We present an adaptive, LiDAR and odometry-based path-planning framework that embeds this capability into the ROS2 Nav2 stack. A new Movable Obstacles Layer labels all LiDAR returns missing from a prior static map as tentatively movable and assigns a reduced traversal cost. A companion Slow-Pose Progress Checker monitors the ratio of commanded to actual velocity; when the robot slows appreciably, the local cost is raised from light to heavy, and on a stall to lethal, prompting the global planner to back out and re-route. Gazebo evaluations on a Scout Mini, spanning isolated objects and cluttered corridors, show higher goal-reach rates and fewer deadlocks than a no-layer baseline, with traversal times broadly comparable. Because the method relies only on planar scans and CPU-level computation, it suits resource-constrained search and rescue robots and integrates into heterogeneous platforms with minimal engineering. Overall, the results indicate that interaction-aware cost maps are a lightweight, ROS2-native extension for navigating among potentially movable obstacles in unstructured settings. The full implementation will be released as open source athttps://costmap-namo.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2510.15336v1",
    "github_url": null,
    "published": "2025-10-17T05:57:20+00:00",
    "updated": "2025-10-17T05:57:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15271v1",
    "title": "CuSfM: CUDA-Accelerated Structure-from-Motion",
    "authors": [
      "Yu",
      "Liu",
      "Ren"
    ],
    "summary": "Efficient and accurate camera pose estimation forms the foundational requirement for dense reconstruction in autonomous navigation, robotic perception, and virtual simulation systems. This paper addresses the challenge via cuSfM, a CUDA-accelerated offline Structure-from-Motion system that leverages GPU parallelization to efficiently employ computationally intensive yet highly accurate feature extractors, generating comprehensive and non-redundant data associations for precise camera pose estimation and globally consistent mapping. The system supports pose optimization, mapping, prior-map localization, and extrinsic refinement. It is designed for offline processing, where computational resources can be fully utilized to maximize accuracy. Experimental results demonstrate that cuSfM achieves significantly improved accuracy and processing speed compared to the widely used COLMAP method across various testing scenarios, while maintaining the high precision and global consistency essential for offline SfM applications. The system is released as an open-source Python wrapper implementation, PyCuSfM, available at https://github.com/nvidia-isaac/pyCuSFM, to facilitate research and applications in computer vision and robotics.",
    "pdf_url": "https://arxiv.org/pdf/2510.15271v1",
    "github_url": "https://github.com/nvidia-isaac/pyCuSFM",
    "published": "2025-10-17T03:29:11+00:00",
    "updated": "2025-10-17T03:29:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15220v1",
    "title": "LVI-Q: Robust LiDAR-Visual-Inertial-Kinematic Odometry for Quadruped Robots Using Tightly-Coupled and Efficient Alternating Optimization",
    "authors": [
      "Marsim",
      "Oh",
      "Yu"
    ],
    "summary": "Autonomous navigation for legged robots in complex and dynamic environments relies on robust simultaneous localization and mapping (SLAM) systems to accurately map surroundings and localize the robot, ensuring safe and efficient operation. While prior sensor fusion-based SLAM approaches have integrated various sensor modalities to improve their robustness, these algorithms are still susceptible to estimation drift in challenging environments due to their reliance on unsuitable fusion strategies. Therefore, we propose a robust LiDAR-visual-inertial-kinematic odometry system that integrates information from multiple sensors, such as a camera, LiDAR, inertial measurement unit (IMU), and joint encoders, for visual and LiDAR-based odometry estimation. Our system employs a fusion-based pose estimation approach that runs optimization-based visual-inertial-kinematic odometry (VIKO) and filter-based LiDAR-inertial-kinematic odometry (LIKO) based on measurement availability. In VIKO, we utilize the footpreintegration technique and robust LiDAR-visual depth consistency using superpixel clusters in a sliding window optimization. In LIKO, we incorporate foot kinematics and employ a point-toplane residual in an error-state iterative Kalman filter (ESIKF). Compared with other sensor fusion-based SLAM algorithms, our approach shows robust performance across public and longterm datasets.",
    "pdf_url": "https://arxiv.org/pdf/2510.15220v1",
    "github_url": null,
    "published": "2025-10-17T00:58:11+00:00",
    "updated": "2025-10-17T00:58:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15120v1",
    "title": "Procedural Game Level Design with Deep Reinforcement Learning",
    "authors": [
      "Özkan"
    ],
    "summary": "Procedural content generation (PCG) has become an increasingly popular technique in game development, allowing developers to generate dynamic, replayable, and scalable environments with reduced manual effort. In this study, a novel method for procedural level design using Deep Reinforcement Learning (DRL) within a Unity-based 3D environment is proposed. The system comprises two agents: a hummingbird agent, acting as a solver, and a floating island agent, responsible for generating and placing collectible objects (flowers) on the terrain in a realistic and context-aware manner. The hummingbird is trained using the Proximal Policy Optimization (PPO) algorithm from the Unity ML-Agents toolkit. It learns to navigate through the terrain efficiently, locate flowers, and collect them while adapting to the ever-changing procedural layout of the island. The island agent is also trained using the Proximal Policy Optimization (PPO) algorithm. It learns to generate flower layouts based on observed obstacle positions, the hummingbird's initial state, and performance feedback from previous episodes. The interaction between these agents leads to emergent behavior and robust generalization across various environmental configurations. The results demonstrate that the approach not only produces effective and efficient agent behavior but also opens up new opportunities for autonomous game level design driven by machine learning. This work highlights the potential of DRL in enabling intelligent agents to both generate and solve content in virtual environments, pushing the boundaries of what AI can contribute to creative game development processes.",
    "pdf_url": "https://arxiv.org/pdf/2510.15120v1",
    "github_url": null,
    "published": "2025-10-16T20:26:14+00:00",
    "updated": "2025-10-16T20:26:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15113v1",
    "title": "Extending Temporal Disturbance Estimations For Magnetic Anomaly Navigation and Mapping",
    "authors": [
      "Srinivasan",
      "Nielsen"
    ],
    "summary": "Slow-moving vehicles relying on crustal magnetic anomaly navigation (MagNav) or vehicles revisiting the same location in a short time - such as those used for surveys in magnetic anomaly mapping - require fixed ground stations within 100 km of the vehicle's trajectory to measure and remove the geomagnetic disturbance field from magnetic readings. This approach is impractical due to the limited network of fixed-ground magnetometer stations, making long-range (several hundred kilometers long) aeromagnetic surveys for anomaly map-making infeasible. To address these challenges, we developed the Extended Reference Station Model (ERSM). ERSM applies a longitudinal correction and regression model to an extended reference ground magnetometer station (ERS) to produce an estimate of the local temporal disturbance field. ERSM is regression model-agnostic, so we implemented a linear regression, a k-nearest neighbors (kNN) regression, and a neural-network regression model to assess performance benefits. Our results show typical performance below 10nT root mean square error and median performance below 5nT for typical use with the kNN and neural-net model for farther distances and below 5nT performance using the linear regression model on stations with proximity. We also consider how space-weather events, water-body separation, and proximity to polar regions affect the model performance based on ERS selection.",
    "pdf_url": "https://arxiv.org/pdf/2510.15113v1",
    "github_url": null,
    "published": "2025-10-16T20:14:40+00:00",
    "updated": "2025-10-16T20:14:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16051v1",
    "title": "GUIrilla: A Scalable Framework for Automated Desktop UI Exploration",
    "authors": [
      "Garkot",
      "Shamrai",
      "Synytsia"
    ],
    "summary": "Autonomous agents capable of operating complex graphical user interfaces (GUIs) have the potential to transform desktop automation. While recent advances in large language models (LLMs) have significantly improved UI understanding, navigating full-window, multi-application desktop environments remains a major challenge. Data availability is limited by costly manual annotation, closed-source datasets and surface-level synthetic pipelines. We introduce GUIrilla, an automated scalable framework that systematically explores applications via native accessibility APIs to address the critical data collection challenge in GUI automation. Our framework focuses on macOS - an ecosystem with limited representation in current UI datasets - though many of its components are designed for broader cross-platform applicability. GUIrilla organizes discovered interface elements and crawler actions into hierarchical GUI graphs and employs specialized interaction handlers to achieve comprehensive application coverage. Using the application graphs from GUIrilla crawler, we construct and release GUIrilla-Task, a large-scale dataset of 27,171 functionally grounded tasks across 1,108 macOS applications, each annotated with full-desktop and window-level screenshots, accessibility metadata, and semantic action traces. Empirical results show that tuning LLM-based agents on GUIrilla-Task significantly improves performance on downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro benchmark while using 97% less data. We also release macapptree, an open-source library for reproducible collection of structured accessibility metadata, along with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold benchmark, and the framework code to support open research in desktop autonomy.",
    "pdf_url": "https://arxiv.org/pdf/2510.16051v1",
    "github_url": null,
    "published": "2025-10-16T19:03:45+00:00",
    "updated": "2025-10-16T19:03:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15022v2",
    "title": "LoRAverse: A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models",
    "authors": [
      "Sonmezer",
      "Zheng",
      "Yanardag"
    ],
    "summary": "Low-rank Adaptation (LoRA) models have revolutionized the personalization of pre-trained diffusion models by enabling fine-tuning through low-rank, factorized weight matrices specifically optimized for attention layers. These models facilitate the generation of highly customized content across a variety of objects, individuals, and artistic styles without the need for extensive retraining. Despite the availability of over 100K LoRA adapters on platforms like Civit.ai, users often face challenges in navigating, selecting, and effectively utilizing the most suitable adapters due to their sheer volume, diversity, and lack of structured organization. This paper addresses the problem of selecting the most relevant and diverse LoRA models from this vast database by framing the task as a combinatorial optimization problem and proposing a novel submodular framework. Our quantitative and qualitative experiments demonstrate that our method generates diverse outputs across a wide range of domains.",
    "pdf_url": "https://arxiv.org/pdf/2510.15022v2",
    "github_url": null,
    "published": "2025-10-16T17:59:45+00:00",
    "updated": "2025-12-17T13:03:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14959v2",
    "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions",
    "authors": [
      "Yang",
      "Werner",
      "Sa"
    ],
    "summary": "Reinforcement learning (RL), while powerful and expressive, can often prioritize performance at the expense of safety. Yet safety violations can lead to catastrophic outcomes in real-world deployments. Control Barrier Functions (CBFs) offer a principled method to enforce dynamic safety -- traditionally deployed online via safety filters. While the result is safe behavior, the fact that the RL policy does not have knowledge of the CBF can lead to conservative behaviors. This paper proposes CBF-RL, a framework for generating safe behaviors with RL by enforcing CBFs in training. CBF-RL has two key attributes: (1) minimally modifying a nominal RL policy to encode safety constraints via a CBF term, (2) and safety filtering of the policy rollouts in training. Theoretically, we prove that continuous-time safety filters can be deployed via closed-form expressions on discrete-time roll-outs. Practically, we demonstrate that CBF-RL internalizes the safety constraints in the learned policy -- both enforcing safer actions and biasing towards safer rewards -- enabling safe deployment without the need for an online safety filter. We validate our framework through ablation studies on navigation tasks and on the Unitree G1 humanoid robot, where CBF-RL enables safer exploration, faster convergence, and robust performance under uncertainty, enabling the humanoid robot to avoid obstacles and climb stairs safely in real-world settings without a runtime safety filter.",
    "pdf_url": "https://arxiv.org/pdf/2510.14959v2",
    "github_url": null,
    "published": "2025-10-16T17:58:58+00:00",
    "updated": "2025-10-19T01:26:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14946v1",
    "title": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices",
    "authors": [
      "Aalishah",
      "Navardi",
      "Mohsenin"
    ],
    "summary": "Deployment of efficient and accurate Deep Learning models has long been a challenge in autonomous navigation, particularly for real-time applications on resource-constrained edge devices. Edge devices are limited in computing power and memory, making model efficiency and compression essential. In this work, we propose EdgeNavMamba, a reinforcement learning-based framework for goal-directed navigation using an efficient Mamba object detection model. To train and evaluate the detector, we introduce a custom shape detection dataset collected in diverse indoor settings, reflecting visual cues common in real-world navigation. The object detector serves as a pre-processing module, extracting bounding boxes (BBOX) from visual input, which are then passed to an RL policy to control goal-oriented navigation. Experimental results show that the student model achieved a reduction of 67% in size, and up to 73% in energy per inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5, while keeping the same performance as the teacher model. EdgeNavMamba also maintains high detection accuracy in MiniWorld and IsaacLab simulators while reducing parameters by 31% compared to the baseline. In the MiniWorld simulator, the navigation policy achieves over 90% success across environments of varying complexity.",
    "pdf_url": "https://arxiv.org/pdf/2510.14946v1",
    "github_url": null,
    "published": "2025-10-16T17:55:56+00:00",
    "updated": "2025-10-16T17:55:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15018v1",
    "title": "UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos",
    "authors": [
      "Liu",
      "He",
      "Ricci"
    ],
    "summary": "Urban embodied AI agents, ranging from delivery robots to quadrupeds, are increasingly populating our cities, navigating chaotic streets to provide last-mile connectivity. Training such agents requires diverse, high-fidelity urban environments to scale, yet existing human-crafted or procedurally generated simulation scenes either lack scalability or fail to capture real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim system that converts crowd-sourced city-tour videos into physics-aware, interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a repository of 100k+ annotated urban 3D assets with semantic and physical attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene layouts from video and instantiates metric-scale 3D simulations using retrieved assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed scenes from 24 countries, along with a curated benchmark of 10 artist-designed test scenes. Experiments show that UrbanVerse scenes preserve real-world semantics and layouts, achieving human-evaluated realism comparable to manually crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit scaling power laws and strong generalization, improving success by +6.3% in simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior methods, accomplishing a 300 m real-world mission with only two interventions.",
    "pdf_url": "https://arxiv.org/pdf/2510.15018v1",
    "github_url": null,
    "published": "2025-10-16T17:42:34+00:00",
    "updated": "2025-10-16T17:42:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14911v1",
    "title": "Dude, Where's My (Autonomous) Car? Defining an Accessible Description Logic for Blind and Low Vision Travelers Using Autonomous Vehicles",
    "authors": [
      "Fink",
      "Brown",
      "Coombs"
    ],
    "summary": "Purpose: Autonomous vehicles (AVs) are becoming a promising transportation solution for blind and low-vision (BLV) travelers, offering the potential for greater independent mobility. This paper explores the information needs of BLV users across multiple steps of the transportation journey, including finding and navigating to, entering, and exiting vehicles independently.   Methods: A survey with 202 BLV respondents and interviews with 12 BLV individuals revealed the perspectives of BLV end-users and informed the sequencing of natural language information required for successful travel. Whereas the survey identified key information needs across the three trip segments, the interviews helped prioritize how that information should be presented in a sequence of accessible descriptions to travelers.   Results: Taken together, the survey and interviews reveal that BLV users prioritize knowing the vehicle's make and model and how to find the correct vehicle during the navigation phase. They also emphasize the importance of confirmations about the vehicle's destination and onboard safety features upon entering the vehicle. While exiting, BLV users value information about hazards and obstacles, as well as knowing which side of the vehicle to exit. Furthermore, results highlight that BLV travelers desire using their own smartphone devices when receiving information from AVs and prefer audio-based interaction.   Conclusion: The findings from this research contribute a structured framework for delivering trip-related information to BLV users, useful for designers incorporating natural language descriptions tailored to each travel segment. This work offers important contributions for sequencing transportation-related descriptions throughout the AV journey, ultimately enhancing the mobility and independence of BLV individuals.",
    "pdf_url": "https://arxiv.org/pdf/2510.14911v1",
    "github_url": null,
    "published": "2025-10-16T17:28:58+00:00",
    "updated": "2025-10-16T17:28:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14904v2",
    "title": "MaskCaptioner: Learning to Jointly Segment and Caption Object Trajectories in Videos",
    "authors": [
      "Fiastre",
      "Yang",
      "Schmid"
    ],
    "summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting, tracking, and captioning object trajectories in a video, requiring the ability to understand spatio-temporal details and describe them in natural language. Due to the complexity of the task and the high cost associated with manual annotation, previous approaches resort to disjoint training strategies, potentially leading to suboptimal performance. To circumvent this issue, we propose to generate captions about spatio-temporally localized entities leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an end-to-end model capable of jointly detecting, segmenting, tracking and captioning object trajectories. Moreover, with pretraining on LVISCap and LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are available at https://www.gabriel.fiastre.fr/maskcaptioner/.",
    "pdf_url": "https://arxiv.org/pdf/2510.14904v2",
    "github_url": null,
    "published": "2025-10-16T17:20:22+00:00",
    "updated": "2025-10-30T15:39:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14893v3",
    "title": "STITCHER: Constrained Trajectory Planning in Complex Environments with Real-Time Motion Primitive Search",
    "authors": [
      "Levy",
      "Lopez"
    ],
    "summary": "Autonomous high-speed navigation through large, complex environments requires real-time generation of agile trajectories that are dynamically feasible, collision-free, and satisfy state or actuator constraints. Modern trajectory planning techniques primarily use numerical optimization, as they enable the systematic computation of high-quality, expressive trajectories that satisfy various constraints. However, stringent requirements on computation time and the risk of numerical instability can limit the use of optimization-based planners in safety-critical scenarios. This work presents an optimization-free planning framework called STITCHER that stitches short trajectory segments together with graph search to compute long-range, expressive, and near-optimal trajectories in real-time. STITCHER outperforms modern optimization-based planners through our innovative planning architecture and several algorithmic developments that make real-time planning possible. Extensive simulation testing is performed to analyze the algorithmic components that make up STITCHER, along with a thorough comparison with two state-of-the-art optimization planners. Simulation tests show that safe trajectories can be created within a few milliseconds for paths that span the entirety of two 50 m x 50 m environments. Hardware tests with a custom quadrotor verify that STITCHER can produce trackable paths in real-time while respecting nonconvex constraints, such as limits on tilt angle and motor forces, which are otherwise hard to include in optimization-based planners.",
    "pdf_url": "https://arxiv.org/pdf/2510.14893v3",
    "github_url": null,
    "published": "2025-10-16T17:12:06+00:00",
    "updated": "2025-12-12T20:21:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14819v2",
    "title": "Capturing Context-Aware Route Choice Semantics for Trajectory Representation Learning",
    "authors": [
      "Cao",
      "Wang",
      "Zheng"
    ],
    "summary": "Trajectory representation learning (TRL) aims to encode raw trajectory data into low-dimensional embeddings for downstream tasks such as travel time estimation, mobility prediction, and trajectory similarity analysis. From a behavioral perspective, a trajectory reflects a sequence of route choices within an urban environment. However, most existing TRL methods ignore this underlying decision-making process and instead treat trajectories as static, passive spatiotemporal sequences, thereby limiting the semantic richness of the learned representations. To bridge this gap, we propose CORE, a TRL framework that integrates context-aware route choice semantics into trajectory embeddings. CORE first incorporates a multi-granular Environment Perception Module, which leverages large language models (LLMs) to distill environmental semantics from point of interest (POI) distributions, thereby constructing a context-enriched road network. Building upon this backbone, CORE employs a Route Choice Encoder with a mixture-of-experts (MoE) architecture, which captures route choice patterns by jointly leveraging the context-enriched road network and navigational factors. Finally, a Transformer encoder aggregates the route-choice-aware representations into a global trajectory embedding. Extensive experiments on 4 real-world datasets across 6 downstream tasks demonstrate that CORE consistently outperforms 12 state-of-the-art TRL methods, achieving an average improvement of 9.79% over the best-performing baseline. Our code is available at https://github.com/caoji2001/CORE.",
    "pdf_url": "https://arxiv.org/pdf/2510.14819v2",
    "github_url": "https://github.com/caoji2001/CORE",
    "published": "2025-10-16T15:55:28+00:00",
    "updated": "2025-12-01T11:42:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14670v1",
    "title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence",
    "authors": [
      "Simoni",
      "Fontana",
      "Saracino"
    ],
    "summary": "TITAN (Threat Intelligence Through Automated Navigation) is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph. It integrates a path planner model, which predicts logical relation chains from text, and a graph executor that traverses the TITAN Ontology to retrieve factual answers and supporting evidence. Unlike traditional retrieval systems, TITAN operates on a typed, bidirectional graph derived from MITRE, allowing reasoning to move clearly and reversibly between threats, behaviors, and defenses. To support training and evaluation, we introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test: 13951) pairing natural language questions with executable reasoning paths and step by step Chain of Thought explanations. Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.",
    "pdf_url": "https://arxiv.org/pdf/2510.14670v1",
    "github_url": null,
    "published": "2025-10-16T13:27:05+00:00",
    "updated": "2025-10-16T13:27:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14598v1",
    "title": "Two Explorative Studies on Tangible Augmented Reality for Neurodevelopmental Disorders",
    "authors": [
      "Vona",
      "Valcamonica",
      "Garzotto"
    ],
    "summary": "Tangible Augmented Reality (TAR) is an interaction paradigm that integrates physical and digital worlds to create immersive, interactive experiences. This paper explores two TAR applications, Holomarket and Along the Oceanic Flow (ATOF), and presents insights from two exploratory studies evaluating their usability and likeability among individuals with neurodevelopmental disorders (NDD). Holomarket is designed to simulate a supermarket shopping experience, helping users develop essential life skills such as item selection, basic arithmetic, and money handling. Participants interacted with augmented food items and a smart cash register, navigating a virtual supermarket environment. While participants enjoyed the realistic setting and tangible interactions, some usability challenges, such as difficulty manipulating virtual objects and discomfort with prolonged headset use, were noted. ATOF transforms the user environment into an oceanic world, where participants use a dolphin-shaped smart object to complete tasks like collecting items and solving puzzles. This application aims to improve motor coordination and cognitive skills. Participants appreciated the immersive experience, the customizable tasks, and the tangible dolphin interface. However, some faced difficulties interacting with specific virtual elements. Overall, both applications demonstrated potential as therapeutic tools for NDD, offering engaging and immersive experiences. Despite some usability challenges and hardware limitations, the positive feedback suggests that TAR could play a crucial role in future therapeutic interventions. Further research is needed to refine these applications and enhance user interaction and comfort.",
    "pdf_url": "https://arxiv.org/pdf/2510.14598v1",
    "github_url": null,
    "published": "2025-10-16T11:59:36+00:00",
    "updated": "2025-10-16T11:59:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14357v1",
    "title": "SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation",
    "authors": [
      "Zhao",
      "Lyu",
      "Li"
    ],
    "summary": "Agricultural robots are emerging as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily rely on manual operation or fixed rail systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling robots to navigate to the target positions following the natural language instructions. In practical agricultural scenarios, navigation instructions often repeatedly occur, yet AgriVLN treat each instruction as an independent episode, overlooking the potential of past experiences to provide spatial context for subsequent ones. To bridge this gap, we propose the method of Spatial Understanding Memory for Agricultural Vision-and-Language Navigation (SUM-AgriVLN), in which the SUM module employs spatial understanding and save spatial memory through 3D reconstruction and representation. When evaluated on the A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47 to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m, demonstrating the state-of-the-art performance in the agricultural domain. Code: https://github.com/AlexTraveling/SUM-AgriVLN.",
    "pdf_url": "https://arxiv.org/pdf/2510.14357v1",
    "github_url": "https://github.com/AlexTraveling/SUM-AgriVLN",
    "published": "2025-10-16T06:53:32+00:00",
    "updated": "2025-10-16T06:53:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14267v2",
    "title": "TapNav: Adaptive Spatiotactile Screen Readers for Tactually Guided Touchscreen Interactions for Blind and Low Vision People",
    "authors": [
      "Gonzalez",
      "Liu",
      "MacIntyre"
    ],
    "summary": "Screen readers are audio-based software that Blind and Low Vision (BLV) people use to interact with computing devices, such as tablets and smartphones. Although this technology has significantly improved the accessibility of touchscreen devices, the sequential nature of audio limits the bandwidth of information users can receive and process. We introduce TapNav, an adaptive spatiotactile screen reader prototype developed to interact with touchscreen interfaces spatially. TapNav's screen reader provides adaptive auditory feedback that, in combination with a tactile overlay, conveys spatial information and location of interface elements on-screen. We evaluated TapNav with 12 BLV users who interacted with TapNav to explore a data visualization and interact with a bank transactions application. Our qualitative findings show that touch points and spatially constrained navigation helped users anticipate outcomes for faster exploration, and offload cognitive load to touch. We provide design guidelines for creating tactile overlays for adaptive spatiotactile screen readers and discuss their generalizability beyond our exploratory data analysis and everyday application navigation scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2510.14267v2",
    "github_url": null,
    "published": "2025-10-16T03:36:35+00:00",
    "updated": "2025-10-17T13:34:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14229v1",
    "title": "Demonstrating Exoplanet Transit Photometry from Space with a 15-mm Aperture Optical Navigation Camera on Hayabusa2",
    "authors": [
      "Yumoto",
      "Kouyama",
      "Yamada"
    ],
    "summary": "Observations of exoplanet transits by small satellites have gained increasing attention for reducing detection biases. However, no unambiguous detection of an exoplanet has yet been demonstrated using optics with apertures smaller than 60 mm. Here, we investigated the detectability of exoplanet transits using the telescopic Optical Navigation Camera (ONC-T) onboard the Hayabusa2 spacecraft, which has an effective aperture of only 15 mm. We conducted transit observations of the hot Jupiters WASP-189 b and MASCARA-1 b, collecting data for ten and four events, respectively. The transit signal was detected with a signal-to-noise ratio (SNR) of 13 for WASP-189 b and 8 for MASCARA-1 b for each event. Stacking all events improved the SNR to 40 and 16, respectively. The transit mid-times of each event were measured with a precision of 6 minutes and were consistent with Transiting Exoplanet Survey Satellite (TESS) data to within 2 minutes. The planet-to-star radius ratio was determined with an absolute precision of 0.004 (6% relative) and agreed with TESS results to within 0.002 (3% relative). The recent ONC-T and TESS data enabled an update to the planetary ephemerides. We report a 4 sigma discrepancy between the updated orbital period of MASCARA-1 b and previously reported values. ONC-T sets a new record for the smallest-aperture instrument to detect an exoplanet transit from space, advancing the frontier of exoplanet science with miniature instrumentation. Our results suggest that optics as small as ONC-T may be capable of detecting transiting long-period Jupiters: a population that remains underrepresented in current surveys.",
    "pdf_url": "https://arxiv.org/pdf/2510.14229v1",
    "github_url": null,
    "published": "2025-10-16T02:11:50+00:00",
    "updated": "2025-10-16T02:11:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14206v1",
    "title": "Simulation-Based Optimization over Discrete Spaces using Projection to Continuous Latent Spaces",
    "authors": [
      "Hernández-Morales",
      "Cansino-Loeza",
      "Jiménez-Gutiérrez"
    ],
    "summary": "Simulation-based optimization of complex systems over discrete decision spaces is a challenging computational problem. Specifically, discrete decision spaces lead to a combinatorial explosion of possible alternatives, making it computationally prohibitive to perform simulations for all possible combinations. In this work, we present a new approach to handle these issues by transforming/projecting the discrete decision space into a continuous latent space using a probabilistic model know as Variational AutoEncoders. The transformation of the decision space facilitates the implementation of Bayesian optimization (BO), which is an efficient approach that strategically navigates the space to reduce the number of expensive simulations. Here, the key observation is that points in the latent space correspond to decisions in the original mixed-discrete space, but the latent space is much easier to navigate using BO. We illustrate the benefits of our approach through a couple of case studies that aim to design complex distillation systems: the recovery of caprylic acid from water by liquid-liquid extraction and the separation of an azeotropic mixture using a thermally couple column know as extractive dividing wall column.",
    "pdf_url": "https://arxiv.org/pdf/2510.14206v1",
    "github_url": null,
    "published": "2025-10-16T01:27:37+00:00",
    "updated": "2025-10-16T01:27:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.14141v1",
    "title": "Understanding Data Usage when Making High-Stakes Frontline Decisions in Homelessness Services",
    "authors": [
      "Masrani",
      "Messier",
      "Voida"
    ],
    "summary": "Frontline staff of emergency shelters face challenges such as vicarious trauma, compassion fatigue, and burnout. The technology they use is often not designed for their unique needs, and can feel burdensome on top of their already cognitively and emotionally taxing work. While existing literature focuses on data-driven technologies that automate or streamline frontline decision-making about vulnerable individuals, we discuss scenarios in which staff may resist such automation. We then suggest how data-driven technologies can better align with their human-centred decision-making processes. This paper presents findings from a qualitative fieldwork study conducted from 2022 to 2024 at a large emergency shelter in Canada. The goal of this fieldwork was to co-design, develop, and deploy an interactive data-navigation interface that supports frontline staff when making collaborative, high-stakes decisions about individuals experiencing homelessness. By reflecting on this fieldwork, we contribute insight into the role that administrative shelter data play during decision-making, and unpack staff members' apparent reluctance to outsource decisions about vulnerable individuals to data systems. Our findings suggest a data-outsourcing continuum, which we discuss in terms of how designers may create technologies to support compassionate, data-driven decision-making in nonprofit domains.",
    "pdf_url": "https://arxiv.org/pdf/2510.14141v1",
    "github_url": null,
    "published": "2025-10-15T22:15:42+00:00",
    "updated": "2025-10-15T22:15:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.16030v2",
    "title": "Trade-off Analysis for Lunar Augmented Navigation Service (LANS) Constellation Design",
    "authors": [
      "Iiyama",
      "Gao"
    ],
    "summary": "The establishment of a sustainable human presence on the Moon demands robust positioning, navigation, and timing (PNT) services capable of supporting both surface and orbital operations. This paper presents a comprehensive trade-off analysis of lunar frozen-orbit constellations for the Lunar Augmented Navigation Service (LANS), focusing on how the number of satellites and orbital parameters influence coverage, position dilution of precision (PDOP), orbit determination accuracy, receiver noise, and orbit insertion cost. Three Walker-constellation families based on frozen elliptical and circular orbits are examined to characterize their relative advantages across different semi-major axes and inclinations. Results show that larger semi-major axes enhance both polar and global coverage, though the optimal inclination depends on the constellation type and target service region. The south elliptical lunar frozen orbit (ELFO) Walker constellation provides superior performance for polar coverage and PDOP, whereas the circular lunar frozen orbit (CLFO) Walker configuration achieves the best global uniformity. Orbit determination errors and receiver noise both increase with larger semi-major axes and higher inclinations, reflecting weaker geometric observability and reduced received signal power at apolune for eccentric orbits. Orbit insertion analysis reveals clear trade-offs among transfer duration, characteristic energy ($C_3$) at trans-lunar injection, and insertion $ΔV$: shorter transfers require higher insertion $ΔV$, while low-energy transfers achieve smaller $ΔV$ at the cost of months-long durations and higher $C_3$. These findings provide a systematic framework for designing LANS constellations for both regional and global coverage.",
    "pdf_url": "https://arxiv.org/pdf/2510.16030v2",
    "github_url": null,
    "published": "2025-10-15T21:59:42+00:00",
    "updated": "2025-11-21T19:31:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13644v1",
    "title": "On Your Own: Pro-level Autonomous Drone Racing in Uninstrumented Arenas",
    "authors": [
      "Bosello",
      "Pinzarrone",
      "Kiade"
    ],
    "summary": "Drone technology is proliferating in many industries, including agriculture, logistics, defense, infrastructure, and environmental monitoring. Vision-based autonomy is one of its key enablers, particularly for real-world applications. This is essential for operating in novel, unstructured environments where traditional navigation methods may be unavailable. Autonomous drone racing has become the de facto benchmark for such systems. State-of-the-art research has shown that autonomous systems can surpass human-level performance in racing arenas. However, direct applicability to commercial and field operations is still limited as current systems are often trained and evaluated in highly controlled environments. In our contribution, the system's capabilities are analyzed within a controlled environment -- where external tracking is available for ground-truth comparison -- but also demonstrated in a challenging, uninstrumented environment -- where ground-truth measurements were never available. We show that our approach can match the performance of professional human pilots in both scenarios. We also publicly release the data from the flights carried out by our approach and a world-class human pilot.",
    "pdf_url": "https://arxiv.org/pdf/2510.13644v1",
    "github_url": null,
    "published": "2025-10-15T15:06:47+00:00",
    "updated": "2025-10-15T15:06:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13619v1",
    "title": "Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization",
    "authors": [
      "Choate",
      "Rife"
    ],
    "summary": "In this paper we introduce a visualization methodology to aid a human analyst in classifying adversity modes that impact lidar scan matching. Our methodology is intended for offline rather than real-time analysis. The method generates a vector-field plot that characterizes local discrepancies between a pair of registered point clouds. The vector field plot reveals patterns that would be difficult for the analyst to extract from raw point-cloud data. After introducing our methodology, we apply the process to two proof-of-concept examples: one a simulation study and the other a field experiment. For both data sets, a human analyst was able to reason about a series of adversity mechanisms and iteratively remove those mechanisms from the raw data, to help focus attention on progressively smaller discrepancies.",
    "pdf_url": "https://arxiv.org/pdf/2510.13619v1",
    "github_url": null,
    "published": "2025-10-15T14:49:27+00:00",
    "updated": "2025-10-15T14:49:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13488v1",
    "title": "Bridge the Gap: Enhancing Quadruped Locomotion with Vertical Ground Perturbations",
    "authors": [
      "Stasica",
      "Bick",
      "Bohlinger"
    ],
    "summary": "Legged robots, particularly quadrupeds, excel at navigating rough terrains, yet their performance under vertical ground perturbations, such as those from oscillating surfaces, remains underexplored. This study introduces a novel approach to enhance quadruped locomotion robustness by training the Unitree Go2 robot on an oscillating bridge - a 13.24-meter steel-and-concrete structure with a 2.0 Hz eigenfrequency designed to perturb locomotion. Using Reinforcement Learning (RL) with the Proximal Policy Optimization (PPO) algorithm in a MuJoCo simulation, we trained 15 distinct locomotion policies, combining five gaits (trot, pace, bound, free, default) with three training conditions: rigid bridge and two oscillating bridge setups with differing height regulation strategies (relative to bridge surface or ground). Domain randomization ensured zero-shot transfer to the real-world bridge. Our results demonstrate that policies trained on the oscillating bridge exhibit superior stability and adaptability compared to those trained on rigid surfaces. Our framework enables robust gait patterns even without prior bridge exposure. These findings highlight the potential of simulation-based RL to improve quadruped locomotion during dynamic ground perturbations, offering insights for designing robots capable of traversing vibrating environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.13488v1",
    "github_url": null,
    "published": "2025-10-15T12:37:55+00:00",
    "updated": "2025-10-15T12:37:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13458v1",
    "title": "A non-parametric Zermelo navigation equation for strictly convex control sets",
    "authors": [
      "Rossa",
      "Freddi",
      "Pinatto"
    ],
    "summary": "We study a generalized version of Zermelo's navigation problem in which the admissible set of control velocities is a strictly convex compact set, rather than the classical spherical or ball-shaped one. After establishing existence results under the natural assumption of weak currents, we derive necessary optimality conditions via Pontryagin's maximum principle and convex analysis. In particular, we prove that strictly convex control sets ensure smoothness of optimal controls. In dimension two, this regularity allows us to eliminate the adjoint variables and obtain a second-order differential equation for the optimal control, which extends the classical Zermelo navigation equation to strictly convex control sets in a non-parametric setting. We also develop the case of an affine current, with a particular emphasis on the constant one where optimal trajectories reduce to straight lines. The results are illustrated with examples relevant to ship routing with asymmetric or sail-assisted propulsion.",
    "pdf_url": "https://arxiv.org/pdf/2510.13458v1",
    "github_url": null,
    "published": "2025-10-15T11:59:43+00:00",
    "updated": "2025-10-15T11:59:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13404v2",
    "title": "SWIR-LightFusion: Multi-spectral Semantic Fusion of Synthetic SWIR with Thermal IR (LWIR/MWIR) and RGB",
    "authors": [
      "Hussain",
      "Linh",
      "Naz"
    ],
    "summary": "Enhancing scene understanding in adverse visibility conditions remains a critical challenge for surveillance and autonomous navigation systems. Conventional imaging modalities, such as RGB and thermal infrared (MWIR / LWIR), when fused, often struggle to deliver comprehensive scene information, particularly under conditions of atmospheric interference or inadequate illumination. To address these limitations, Short-Wave Infrared (SWIR) imaging has emerged as a promising modality due to its ability to penetrate atmospheric disturbances and differentiate materials with improved clarity. However, the advancement and widespread implementation of SWIR-based systems face significant hurdles, primarily due to the scarcity of publicly accessible SWIR datasets. In response to this challenge, our research introduces an approach to synthetically generate SWIR-like structural/contrast cues (without claiming spectral reproduction) images from existing LWIR data using advanced contrast enhancement techniques. We then propose a multimodal fusion framework integrating synthetic SWIR, LWIR, and RGB modalities, employing an optimized encoder-decoder neural network architecture with modality-specific encoders and a softmax-gated fusion head. Comprehensive experiments on public RGB-LWIR benchmarks (M3FD, TNO, CAMEL, MSRS, RoadScene) and an additional private real RGB-MWIR-SWIR dataset demonstrate that our synthetic-SWIR-enhanced fusion framework improves fused-image quality (contrast, edge definition, structural fidelity) while maintaining real-time performance. We also add fair trimodal baselines (LP, LatLRR, GFF) and cascaded trimodal variants of U2Fusion/SwinFusion under a unified protocol. The outcomes highlight substantial potential for real-world applications in surveillance and autonomous systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.13404v2",
    "github_url": null,
    "published": "2025-10-15T11:00:41+00:00",
    "updated": "2025-10-20T00:09:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13394v2",
    "title": "Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models",
    "authors": [
      "Huang",
      "He",
      "Huang"
    ],
    "summary": "Spatial reasoning ability is crucial for Vision Language Models (VLMs) to support real-world applications in diverse domains including robotics, augmented reality, and autonomous navigation. Unfortunately, existing benchmarks are inadequate in assessing spatial reasoning ability, especially the \\emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of human spatial cognition. In this paper, we propose a unified benchmark, \\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that categorizes tasks into four fundamental quadrants: \\textbf{I}ntrinsic-\\textbf{S}tatic, Intrinsic-\\textbf{D}ynamic, \\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover, to address the issue of data scarcity, we develop a scalable and automated pipeline to generate diverse and verifiable spatial reasoning questions, resulting in a new \\textbf{Spatial-DISE} dataset that includes Spatial-DISE Bench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA pairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals that, current VLMs have a large and consistent gap to human competence, especially on multi-step multi-view spatial reasoning. Spatial-DISE offers a robust framework, valuable dataset, and clear direction for future research toward human-like spatial intelligence. Benchmark, dataset, and code will be publicly released.",
    "pdf_url": "https://arxiv.org/pdf/2510.13394v2",
    "github_url": null,
    "published": "2025-10-15T10:44:01+00:00",
    "updated": "2025-10-23T14:31:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13287v1",
    "title": "DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping",
    "authors": [
      "Chandna",
      "Kaushal"
    ],
    "summary": "LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential for enabling precise navigation and environmental reconstruction across various applications. Although current point-to-plane ICP algorithms perform effec- tively in structured, feature-rich environments, they struggle in scenarios with sparse features, repetitive geometric structures, and high-frequency motion. This leads to degeneracy in 6- DOF pose estimation. Most state-of-the-art algorithms address these challenges by incorporating additional sensing modalities, but LiDAR-only solutions continue to face limitations under such conditions. To address these issues, we propose a novel Degeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module. Our system improves mapping accuracy through point cloud classification based on surface normals and neighborhood analysis. Points are classified into ground, walls, roof, edges, and non-planar points, enabling accurate correspondences. A Degeneracy-based weighted least squares-based ICP algorithm is then applied for accurate odom- etry estimation. Additionally, a Scan Context based back-end is implemented to support robust loop closures. DAMM-LOAM demonstrates significant improvements in odometry accuracy, especially in indoor environments such as long corridors",
    "pdf_url": "https://arxiv.org/pdf/2510.13287v1",
    "github_url": null,
    "published": "2025-10-15T08:32:47+00:00",
    "updated": "2025-10-15T08:32:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13217v1",
    "title": "LLM-guided Hierarchical Retrieval",
    "authors": [
      "Gupta",
      "Chang",
      "Bui"
    ],
    "summary": "Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM-based IR has shown great promise, the prevailing retrieve-then-rerank paradigm inherits the limitations of embedding-based retrieval; parametric generative approaches are difficult to update with new information; and long-context methods that place the entire corpus in context are computationally infeasible for large document collections. To address these challenges, we introduce LATTICE, a hierarchical retrieval framework that enables an LLM to reason over and navigate large corpora with logarithmic search complexity by imposing a semantic tree structure on the corpus. Our approach consists of two stages: (1) an offline phase that organizes the corpus into a semantic hierarchy via either a bottom-up agglomerative strategy or a top-down divisive strategy using multi-level summaries and (2) an online traversal phase where a search LLM navigates this tree. A central challenge in such LLM-guided search is that the model's relevance judgments are noisy, context-dependent, and unaware of the hierarchy, making cross-branch and cross-level comparisons difficult. To overcome this, we propose a traversal algorithm that estimates calibrated latent relevance scores from local LLM outputs and aggregates them into a global path relevance metric. Our training-free framework achieves state-of-the-art zero-shot performance on the reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline. Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains comparable results on BRIGHT subsets that use a static corpus for evaluation.",
    "pdf_url": "https://arxiv.org/pdf/2510.13217v1",
    "github_url": null,
    "published": "2025-10-15T07:05:17+00:00",
    "updated": "2025-10-15T07:05:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13184v1",
    "title": "Synergy-Guided Compiler Auto-Tuning of Nested LLVM Pass Pipelines",
    "authors": [
      "Pan",
      "Dong",
      "Xing"
    ],
    "summary": "Compiler optimization relies on sequences of passes to improve program performance. Selecting and ordering these passes automatically, known as compiler auto-tuning, is challenging due to the large and complex search space. Existing approaches generally assume a linear sequence of passes, a model compatible with legacy compilers but fundamentally misaligned with the hierarchical design of the LLVM New Pass Manager. This misalignment prevents them from guaranteeing the generation of syntactically valid optimization pipelines. In this work, we present a new auto-tuning framework built from the ground up for the New Pass Manager. We introduce a formal grammar to define the space of valid nested pipelines and a forest-based data structure for their native representation. Upon this foundation, we develop a structure-aware Genetic Algorithm whose operators manipulate these forests directly, ensuring that all candidate solutions are valid by construction. The framework first mines synergistic pass relationships to guide the search. An optional refinement stage further explores subtle performance variations arising from different valid structural arrangements.   We evaluate our approach on seven benchmark datasets using LLVM 18.1.6. The discovered pipelines achieve an average of 13.62% additional instruction count reduction compared to the standard opt -Oz optimization level, showing that our framework is capable of navigating this complex, constrained search space to identify valid and effective pass pipelines.",
    "pdf_url": "https://arxiv.org/pdf/2510.13184v1",
    "github_url": null,
    "published": "2025-10-15T06:14:44+00:00",
    "updated": "2025-10-15T06:14:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13162v1",
    "title": "Searching for a Farang: Collective Security among Women in Pattaya, Thailand",
    "authors": [
      "Robinson",
      "Jensen"
    ],
    "summary": "We report on two months of ethnographic fieldwork in a women's centre in Pattaya, and interviews with 76 participants. Our findings, as they relate to digital security, show how (i) women in Pattaya, often working in the sex and massage industries, perceived relationships with farang men as their best, and sometimes only, option to achieve security; (ii) the strategies used by the women to appeal to a farang involved presenting themselves online, mirroring how they were being advertised by bar owners to attract customers; (iii) appealing to what they considered `Western ideals', the women sought out `Western technologies' and appropriated them for their benefit; (iv) the women navigated a series of online security risks, such as scams and abuse, which shaped their search for a farang; (v) the women developed collective security through knowledge-sharing to protect themselves and each other in their search for a farang. We situate our work in emerging digital security scholarship within marginalised contexts.",
    "pdf_url": "https://arxiv.org/pdf/2510.13162v1",
    "github_url": null,
    "published": "2025-10-15T05:25:55+00:00",
    "updated": "2025-10-15T05:25:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13112v1",
    "title": "Neural Triangular Transport Maps: A New Approach Towards Sampling in Lattice QCD",
    "authors": [
      "Bryutkin",
      "Marzouk"
    ],
    "summary": "Lattice field theories are fundamental testbeds for computational physics; yet, sampling their Boltzmann distributions remains challenging due to multimodality and long-range correlations. While normalizing flows offer a promising alternative, their application to large lattices is often constrained by prohibitive memory requirements and the challenge of maintaining sufficient model expressivity. We propose sparse triangular transport maps that explicitly exploit the conditional independence structure of the lattice graph under periodic boundary conditions using monotone rectified neural networks (MRNN). We introduce a comprehensive framework for triangular transport maps that navigates the fundamental trade-off between \\emph{exact sparsity} (respecting marginal conditional independence in the target distribution) and \\emph{approximate sparsity} (computational tractability without fill-ins). Restricting each triangular map component to a local past enables site-wise parallel evaluation and linear time complexity in lattice size $N$, while preserving the expressive, invertible structure. Using $φ^4$ in two dimensions as a controlled setting, we analyze how node labelings (orderings) affect the sparsity and performance of triangular maps. We compare against Hybrid Monte Carlo (HMC) and established flow approaches (RealNVP).",
    "pdf_url": "https://arxiv.org/pdf/2510.13112v1",
    "github_url": null,
    "published": "2025-10-15T03:15:10+00:00",
    "updated": "2025-10-15T03:15:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13058v1",
    "title": "From misinformation to climate crisis: Navigating vulnerabilities in the cyber-physical-social systems",
    "authors": [
      "Aamir",
      "Grobler",
      "Russello"
    ],
    "summary": "Within the cyber-physical-social-climate nexus, all systems are deeply interdependent: cyber infrastructure facilitates communication, data processing, and automation across physical systems (such as power grids and networks), while social infrastructure provides the human capital and societal norms necessary for the system's functionality. Any disruption within any of these components, whether due to human error or system mismanagement, can propagate throughout the network, amplifying vulnerabilities and creating a significantly scaled impact. This chapter explores the critical role of human vulnerabilities within the cyber-physical-social-climate nexus, focusing on the interdependencies across cyber, physical, and social systems and how these interdependencies can scale in a climate context. While cyber and physical vulnerabilities are readily apparent, social vulnerabilities (such as misinformation, resistance to policy change, and lack of public awareness) often go unaddressed despite their profound impact on resilience and climate adaptation. Social infrastructure, including human capital, societal norms, and policy frameworks, shapes community responses and underpins adaptive capacity, yet it is also a significant point of failure when overlooked. This chapter examines how human cognitive biases, risk misperception, and decision-making silos within interconnected systems can lead to resource misallocation and weakened policy effectiveness. These factors are analyzed to demonstrate how inadequate responses across cyber-physical-social layers can cascade, amplifying climate-related risks. By addressing these human factors and aligning decision-making frameworks, we aim to strengthen resilience and foster cohesive adaptation strategies that account for the intricate interrelations of cyber-physical-social-climate systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.13058v1",
    "github_url": null,
    "published": "2025-10-15T00:39:11+00:00",
    "updated": "2025-10-15T00:39:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13004v1",
    "title": "Comparison of Forced and Unforced Rendezvous, Proximity Operations, and Docking Under Model Mismatch",
    "authors": [
      "Muldrow",
      "Ludden",
      "Petersen"
    ],
    "summary": "This paper compares the required fuel usage for forced and unforced motion of a chaser satellite engaged in Rendezvous, Proximity Operations, and Docking (RPOD) maneuvers. Improved RPOD models are vital, particularly as the space industry expands and demands for improved fuel efficiency, cost effectiveness, and mission life span increase. This paper specifically examines the Clohessy- Wiltshire (CW) Equations and the extent of model mismatch by comparing pre- dicted trajectories from this model with a more computationally complex, higher fidelity RPOD model. This paper assesses several test cases of similar mission parameters, in each case comparing natural motion circumnavigation (NMC) with comparable forced motion circumnavigation. The Guidance, Navigation, and Con- trol (GNC) impulse maneuvers required to maintain the supposedly zero fuel CW trajectories is representative of the extent of CW model mismatch. This paper demonstrates that unforced motions are not inherently more fuel efficient than forced motions, thus permitting extended orbital operations given the higher fuel efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2510.13004v1",
    "github_url": null,
    "published": "2025-10-14T21:40:45+00:00",
    "updated": "2025-10-14T21:40:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2512.00009v2",
    "title": "Development and Benchmarking of a Blended Human-AI Qualitative Research Assistant",
    "authors": [
      "Matveyenko",
      "Liu",
      "Parsons"
    ],
    "summary": "Qualitative research emphasizes constructing meaning through iterative engagement with textual data. Traditionally this human-driven process requires navigating coder fatigue and interpretative drift, thus posing challenges when scaling analysis to larger, more complex datasets. Computational approaches to augment qualitative research have been met with skepticism, partly due to their inability to replicate the nuance, context-awareness, and sophistication of human analysis. Large language models, however, present new opportunities to automate aspects of qualitative analysis while upholding rigor and research quality in important ways. To assess their benefits and limitations - and build trust among qualitative researchers - these approaches must be rigorously benchmarked against human-generated datasets. In this work, we benchmark Muse, an interactive, AI-powered qualitative research system that allows researchers to identify themes and annotate datasets, finding an inter-rater reliability between Muse and humans of Cohen's $κ$ = 0.71 for well-specified codes. We also conduct robust error analysis to identify failure mode, guide future improvements, and demonstrate the capacity to correct for human bias.",
    "pdf_url": "https://arxiv.org/pdf/2512.00009v2",
    "github_url": null,
    "published": "2025-10-14T21:17:34+00:00",
    "updated": "2025-12-15T01:41:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12972v1",
    "title": "TaskAudit: Detecting Functiona11ity Errors in Mobile Apps via Agentic Task Execution",
    "authors": [
      "Zhong",
      "Chen",
      "Kyi"
    ],
    "summary": "Accessibility checkers are tools in support of accessible app development and their use is encouraged by accessibility best practices. However, most current checkers evaluate static or mechanically-generated contexts, failing to capture common accessibility errors impacting mobile app functionality. We present TaskAudit, an accessibility evaluation system that focuses on detecting functiona11ity errors through simulated interactions. TaskAudit comprises three components: a Task Generator that constructs interactive tasks from app screens, a Task Executor that uses agents with a screen reader proxy to perform these tasks, and an Accessibility Analyzer that detects and reports accessibility errors by examining interaction traces. Evaluation on real-world apps shows that our strategy detects 48 functiona11ity errors from 54 app screens, compared to between 4 and 20 with existing checkers. Our analysis demonstrates common error patterns that TaskAudit can detect in addition to prior work, including label-functionality mismatch, cluttered navigation, and inappropriate feedback.",
    "pdf_url": "https://arxiv.org/pdf/2510.12972v1",
    "github_url": null,
    "published": "2025-10-14T20:28:49+00:00",
    "updated": "2025-10-14T20:28:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12919v1",
    "title": "Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation",
    "authors": [
      "Khan",
      "Ibuki",
      "Chatterjee"
    ],
    "summary": "Level set methods underpin modern safety techniques such as control barrier functions (CBFs), while also serving as implicit surface representations for geometric shapes via distance fields. Inspired by these two paradigms, we propose a unified framework where the implicit surface itself acts as a CBF. We leverage Gaussian process (GP) implicit surface (GPIS) to represent the safety boundaries, using safety samples which are derived from sensor measurements to condition the GP. The GP posterior mean defines the implicit safety surface (safety belief), while the posterior variance provides a robust safety margin. Although GPs have favorable properties such as uncertainty estimation and analytical tractability, they scale cubically with data. To alleviate this issue, we develop a sparse solution called sparse Gaussian CBFs. To the best of our knowledge, GPIS have not been explicitly used to synthesize CBFs. We validate the approach on collision avoidance tasks in two settings: a simulated 7-DOF manipulator operating around the Stanford bunny, and a quadrotor navigating in 3D around a physical chair. In both cases, Gaussian CBFs (with and without sparsity) enable safe interaction and collision-free execution of trajectories that would otherwise intersect the objects.",
    "pdf_url": "https://arxiv.org/pdf/2510.12919v1",
    "github_url": null,
    "published": "2025-10-14T18:45:59+00:00",
    "updated": "2025-10-14T18:45:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12684v1",
    "title": "Autonomous Legged Mobile Manipulation for Lunar Surface Operations via Constrained Reinforcement Learning",
    "authors": [
      "Belmonte-Baeza",
      "Cazorla",
      "García"
    ],
    "summary": "Robotics plays a pivotal role in planetary science and exploration, where autonomous and reliable systems are crucial due to the risks and challenges inherent to space environments. The establishment of permanent lunar bases demands robotic platforms capable of navigating and manipulating in the harsh lunar terrain. While wheeled rovers have been the mainstay for planetary exploration, their limitations in unstructured and steep terrains motivate the adoption of legged robots, which offer superior mobility and adaptability. This paper introduces a constrained reinforcement learning framework designed for autonomous quadrupedal mobile manipulators operating in lunar environments. The proposed framework integrates whole-body locomotion and manipulation capabilities while explicitly addressing critical safety constraints, including collision avoidance, dynamic stability, and power efficiency, in order to ensure robust performance under lunar-specific conditions, such as reduced gravity and irregular terrain. Experimental results demonstrate the framework's effectiveness in achieving precise 6D task-space end-effector pose tracking, achieving an average positional accuracy of 4 cm and orientation accuracy of 8.1 degrees. The system consistently respects both soft and hard constraints, exhibiting adaptive behaviors optimized for lunar gravity conditions. This work effectively bridges adaptive learning with essential mission-critical safety requirements, paving the way for advanced autonomous robotic explorers for future lunar missions.",
    "pdf_url": "https://arxiv.org/pdf/2510.12684v1",
    "github_url": null,
    "published": "2025-10-14T16:21:34+00:00",
    "updated": "2025-10-14T16:21:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12566v1",
    "title": "Evaluating End-User Device Energy Models in Sustainability Reporting of Browser-Based Web Services",
    "authors": [
      "Kirkeby",
      "Lagermann"
    ],
    "summary": "Sustainability reporting in web-based services increasingly relies on simplified energy and carbon models such as the Danish Agency of Digital Government's Digst framework and the United Kingdom-based DIMPACT model. Although these models are widely adopted, their accuracy and precision remain underexplored. This paper presents an empirical study evaluating how well such models reflect actual energy consumption during realistic user interactions with common website categories. Energy use was measured across shopping, booking, navigation, and news services using predefined user flows executed on four laptop platforms. The results show that the commonly applied constant-power approximation (P * t) can diverge substantially from measured energy, depending on website category, device type, and task characteristics. The findings demonstrate that model deviations are systematic rather than random and highlight the need for category-aware and device-reflective power parameters in reproducible sustainability reporting frameworks.",
    "pdf_url": "https://arxiv.org/pdf/2510.12566v1",
    "github_url": null,
    "published": "2025-10-14T14:25:26+00:00",
    "updated": "2025-10-14T14:25:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12454v1",
    "title": "First GNSS-deployed optical clock for local time scale upgrade",
    "authors": [
      "Yuan",
      "Cao",
      "Yuan"
    ],
    "summary": "Precise time scale is the universal base for all measurements. Here we report the deployment of a compact and transportable optical clock to a timekeeping institution and steering an active hydrogen maser to generate an optical time scale, realizing the upgrade of the local time scale in the Global Navigation Satellite System. The optical clock was transported over 1200 km by express delivery and resume work as normal promptly, and its extremely high uptime of 93.6% in the half-year enabled us to precisely correct the frequency drift of hydrogen maser, ultimately achieving an unprecedented monthly instability of 4E-17. This steering experiment with a deployable optical clock marks a significant advancement, demonstrating that a timing accuracy below 100 ps per month can be achieved feasibly in various timekeeping institutions where hydrogen masers are typically employed as the primary contributor to timekeeping. In the future, mobile optical time scale based on such transportable optical clock can be deployed flexibly and rapidly, which is particularly important in scenarios lacking International Atomic Time reference.",
    "pdf_url": "https://arxiv.org/pdf/2510.12454v1",
    "github_url": null,
    "published": "2025-10-14T12:36:49+00:00",
    "updated": "2025-10-14T12:36:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12403v1",
    "title": "Robot Learning: A Tutorial",
    "authors": [
      "Capuano",
      "Pascal",
      "Zouitine"
    ],
    "summary": "Robot learning is at an inflection point, driven by rapid advancements in machine learning and the growing availability of large-scale robotics data. This shift from classical, model-based methods to data-driven, learning-based paradigms is unlocking unprecedented capabilities in autonomous systems. This tutorial navigates the landscape of modern robot learning, charting a course from the foundational principles of Reinforcement Learning and Behavioral Cloning to generalist, language-conditioned models capable of operating across diverse tasks and even robot embodiments. This work is intended as a guide for researchers and practitioners, and our goal is to equip the reader with the conceptual understanding and practical tools necessary to contribute to developments in robot learning, with ready-to-use examples implemented in $\\texttt{lerobot}$.",
    "pdf_url": "https://arxiv.org/pdf/2510.12403v1",
    "github_url": null,
    "published": "2025-10-14T11:36:46+00:00",
    "updated": "2025-10-14T11:36:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12386v1",
    "title": "Hey Dashboard!: Supporting Voice, Text, and Pointing Modalities in Dashboard Onboarding",
    "authors": [
      "Dhanoa",
      "León",
      "Hoggan"
    ],
    "summary": "Visualization dashboards are regularly used for data exploration and analysis, but their complex interactions and interlinked views often require time-consuming onboarding sessions from dashboard authors. Preparing these onboarding materials is labor-intensive and requires manual updates when dashboards change. Recent advances in multimodal interaction powered by large language models (LLMs) provide ways to support self-guided onboarding. We present DIANA (Dashboard Interactive Assistant for Navigation and Analysis), a multimodal dashboard assistant that helps users for navigation and guided analysis through chat, audio, and mouse-based interactions. Users can choose any interaction modality or a combination of them to onboard themselves on the dashboard. Each modality highlights relevant dashboard features to support user orientation. Unlike typical LLM systems that rely solely on text-based chat, DIANA combines multiple modalities to provide explanations directly in the dashboard interface. We conducted a qualitative user study to understand the use of different modalities for different types of onboarding tasks and their complexities.",
    "pdf_url": "https://arxiv.org/pdf/2510.12386v1",
    "github_url": null,
    "published": "2025-10-14T11:10:35+00:00",
    "updated": "2025-10-14T11:10:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12332v1",
    "title": "Shape-Aware Whole-Body Control for Continuum Robots with Application in Endoluminal Surgical Robotics",
    "authors": [
      "Kasaei",
      "Ghobadi",
      "Khadem"
    ],
    "summary": "This paper presents a shape-aware whole-body control framework for tendon-driven continuum robots with direct application to endoluminal surgical navigation. Endoluminal procedures, such as bronchoscopy, demand precise and safe navigation through tortuous, patient-specific anatomy where conventional tip-only control often leads to wall contact, tissue trauma, or failure to reach distal targets. To address these challenges, our approach combines a physics-informed backbone model with residual learning through an Augmented Neural ODE, enabling accurate shape estimation and efficient Jacobian computation. A sampling-based Model Predictive Path Integral (MPPI) controller leverages this representation to jointly optimize tip tracking, backbone conformance, and obstacle avoidance under actuation constraints. A task manager further enhances adaptability by allowing real-time adjustment of objectives, such as wall clearance or direct advancement, during tele-operation. Extensive simulation studies demonstrate millimeter-level accuracy across diverse scenarios, including trajectory tracking, dynamic obstacle avoidance, and shape-constrained reaching. Real-robot experiments on a bronchoscopy phantom validate the framework, showing improved lumen-following accuracy, reduced wall contacts, and enhanced adaptability compared to joystick-only navigation and existing baselines. These results highlight the potential of the proposed framework to increase safety, reliability, and operator efficiency in minimally invasive endoluminal surgery, with broader applicability to other confined and safety-critical environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.12332v1",
    "github_url": null,
    "published": "2025-10-14T09:43:47+00:00",
    "updated": "2025-10-14T09:43:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12323v1",
    "title": "RAG-Anything: All-in-One RAG Framework",
    "authors": [
      "Guo",
      "Ren",
      "Xu"
    ],
    "summary": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inherently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content, creating fundamental gaps when processing multimodal documents. We present RAG-Anything, a unified framework that enables comprehensive knowledge retrieval across all modalities. Our approach reconceptualizes multimodal content as interconnected knowledge entities rather than isolated data types. The framework introduces dual-graph construction to capture both cross-modal relationships and textual semantics within a unified representation. We develop cross-modal hybrid retrieval that combines structural knowledge navigation with semantic matching. This enables effective reasoning over heterogeneous content where relevant evidence spans multiple modalities. RAG-Anything demonstrates superior performance on challenging multimodal benchmarks, achieving significant improvements over state-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. Our framework establishes a new paradigm for multimodal knowledge access, eliminating the architectural fragmentation that constrains current systems. Our framework is open-sourced at: https://github.com/HKUDS/RAG-Anything.",
    "pdf_url": "https://arxiv.org/pdf/2510.12323v1",
    "github_url": "https://github.com/HKUDS/RAG-Anything",
    "published": "2025-10-14T09:25:35+00:00",
    "updated": "2025-10-14T09:25:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12282v1",
    "title": "PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes",
    "authors": [
      "A",
      "Sun",
      "Zeng"
    ],
    "summary": "Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet current methods face a stark trade-off between fidelity and computational cost. This inefficiency stems from their semantically agnostic design, which allocates resources uniformly, treating static backgrounds and safety-critical objects with equal importance. To address this, we introduce Priority-Adaptive Gaussian Splatting (PAGS), a framework that injects task-aware semantic priorities directly into the 3D reconstruction and rendering pipeline. PAGS introduces two core contributions: (1) Semantically-Guided Pruning and Regularization strategy, which employs a hybrid importance metric to aggressively simplify non-critical scene elements while preserving fine-grained details on objects vital for navigation. (2) Priority-Driven Rendering pipeline, which employs a priority-based depth pre-pass to aggressively cull occluded primitives and accelerate the final shading computations. Extensive experiments on the Waymo and KITTI datasets demonstrate that PAGS achieves exceptional reconstruction quality, particularly on safety-critical objects, while significantly reducing training time and boosting rendering speeds to over 350 FPS.",
    "pdf_url": "https://arxiv.org/pdf/2510.12282v1",
    "github_url": null,
    "published": "2025-10-14T08:36:09+00:00",
    "updated": "2025-10-14T08:36:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12268v1",
    "title": "How Far I'll Go: Imagining Futures of Conversational AI with People with Visual Impairments Through Design Fiction",
    "authors": [
      "Choi",
      "Choi",
      "Jeong"
    ],
    "summary": "People with visual impairments (PVI) use a variety of assistive technologies to navigate their daily lives, and conversational AI (CAI) tools are a growing part of this toolset. Much existing HCI research has focused on the technical capabilities of current CAI tools, but in this paper, we instead examine how PVI themselves envision potential futures for living with CAI. We conducted a study with 14 participants with visual impairments using an audio-based Design Fiction probe featuring speculative dialogues between participants and a future CAI. Participants imagined using CAI to expand their boundaries by exploring new opportunities or places, but also voiced concerns about balancing reliance on CAI with maintaining autonomy, the need to consider diverse levels of vision-loss, and enhancing visibility of PVI for greater inclusion. We discuss implications for designing CAI that support genuine agency for PVI based on the future lives they envisioned.",
    "pdf_url": "https://arxiv.org/pdf/2510.12268v1",
    "github_url": null,
    "published": "2025-10-14T08:21:38+00:00",
    "updated": "2025-10-14T08:21:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12215v1",
    "title": "Learning Social Navigation from Positive and Negative Demonstrations and Rule-Based Specifications",
    "authors": [
      "Kim",
      "Yoon",
      "Kim"
    ],
    "summary": "Mobile robot navigation in dynamic human environments requires policies that balance adaptability to diverse behaviors with compliance to safety constraints. We hypothesize that integrating data-driven rewards with rule-based objectives enables navigation policies to achieve a more effective balance of adaptability and safety. To this end, we develop a framework that learns a density-based reward from positive and negative demonstrations and augments it with rule-based objectives for obstacle avoidance and goal reaching. A sampling-based lookahead controller produces supervisory actions that are both safe and adaptive, which are subsequently distilled into a compact student policy suitable for real-time operation with uncertainty estimates. Experiments in synthetic and elevator co-boarding simulations show consistent gains in success rate and time efficiency over baselines, and real-world demonstrations with human participants confirm the practicality of deployment. A video illustrating this work can be found on our project page https://chanwookim971024.github.io/PioneeR/.",
    "pdf_url": "https://arxiv.org/pdf/2510.12215v1",
    "github_url": null,
    "published": "2025-10-14T07:10:15+00:00",
    "updated": "2025-10-14T07:10:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12200v1",
    "title": "HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities",
    "authors": [
      "Ren",
      "Jiang",
      "Li"
    ],
    "summary": "Web applications are prime targets for cyberattacks as gateways to critical services and sensitive data. Traditional penetration testing is costly and expertise-intensive, making it difficult to scale with the growing web ecosystem. While language model agents show promise in cybersecurity, modern web applications demand visual understanding, dynamic content handling, and multi-step interactions that only computer-use agents (CUAs) can perform. Yet, their ability to discover and exploit vulnerabilities through graphical interfaces remains largely unexplored. We present HackWorld, the first framework for systematically evaluating CUAs' capabilities to exploit web application vulnerabilities via visual interaction. Unlike sanitized benchmarks, HackWorld includes 36 real-world applications across 11 frameworks and 7 languages, featuring realistic flaws such as injection vulnerabilities, authentication bypasses, and unsafe input handling. Using a Capture-the-Flag (CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses while navigating complex web interfaces. Evaluation of state-of-the-art CUAs reveals concerning trends: exploitation rates below 12% and low cybersecurity awareness. CUAs often fail at multi-step attack planning and misuse security tools. These results expose the current limitations of CUAs in web security contexts and highlight opportunities for developing more security-aware agents capable of effective vulnerability detection and exploitation.",
    "pdf_url": "https://arxiv.org/pdf/2510.12200v1",
    "github_url": null,
    "published": "2025-10-14T06:52:15+00:00",
    "updated": "2025-10-14T06:52:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12169v2",
    "title": "Hybrid Terrain-Aware Path Planning: Integrating VD-RRT* Exploration and VD-D* Lite Repair",
    "authors": [
      "Naik",
      "Norris",
      "Nottage"
    ],
    "summary": "Autonomous ground vehicles operating off-road must plan curvature-feasible paths while accounting for spatially varying soil strength and slope hazards in real time. We present a continuous state--cost metric that combines a Bekker pressure--sinkage model with elevation-derived slope and attitude penalties. The resulting terrain cost field is analytic, bounded, and monotonic in soil modulus and slope, ensuring well-posed discretization and stable updates under sensor noise. This metric is evaluated on a lattice with exact steering primitives: Dubins and Reeds--Shepp motions for differential drive and time-parameterized bicycle arcs for Ackermann steering. Global exploration is performed using Vehicle-Dynamics RRT\\(^{*}\\), while local repair is managed by Vehicle-Dynamics D\\(^{*}\\) Lite, enabling millisecond-scale replanning without heuristic smoothing. By separating the terrain--vehicle model from the planner, the framework provides a reusable basis for deterministic, sampling-based, or learning-driven planning in deformable terrain. Hardware trials on an off-road platform demonstrate real-time navigation across soft soil and slope transitions, supporting reliable autonomy in unstructured environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.12169v2",
    "github_url": null,
    "published": "2025-10-14T05:54:46+00:00",
    "updated": "2025-10-15T06:58:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12121v1",
    "title": "Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing",
    "authors": [
      "Zhang",
      "Ye",
      "Heng"
    ],
    "summary": "Precise attribute intensity control--generating Large Language Model (LLM) outputs with specific, user-defined attribute intensities--is crucial for AI systems adaptable to diverse user expectations. Current LLM alignment methods, however, typically provide only directional or open-ended guidance, failing to reliably achieve exact attribute intensities. We address this limitation with three key designs: (1) reformulating precise attribute intensity control as a target-reaching problem, rather than simple maximization; (2) training a lightweight value function via temporal-difference learning to predict final attribute intensity scores from partial generations, thereby steering LLM outputs; and (3) employing gradient-based interventions on hidden representations to navigate the model precisely towards specific attribute intensity targets. Our method enables fine-grained, continuous control over attribute intensities, moving beyond simple directional alignment. Experiments on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text generation to user-specified attribute intensities with high accuracy. Finally, we demonstrate efficiency enhancements across three downstream tasks: preference data synthesis, Pareto frontier approximation and optimization, and distillation of aligned behaviors for intervention-free inference. Our code is available on https://github.com/Pre-Control/pre-control",
    "pdf_url": "https://arxiv.org/pdf/2510.12121v1",
    "github_url": "https://github.com/Pre-Control/pre-control",
    "published": "2025-10-14T03:50:22+00:00",
    "updated": "2025-10-14T03:50:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.12106v1",
    "title": "Optimal run-tumble navigation in disordered landscapes",
    "authors": [
      "Bai",
      "He",
      "Liu"
    ],
    "summary": "Active navigation in disordered media is governed by the interplay between self-propulsion and environmental constraints. Using the chemotaxis of E. coli in agar gels as a model system, we uncover a universal trade-off between persistence and obstacle avoidance that dictates optimal search strategies. We find that populations evolving under pressure for rapid expansion adapt by shortening their mean run time (τ_f), counter to the intuition that longer runs always favor faster migration. Controlled experiments with a tunable strain confirm a non-monotonic relationship between run time and chemotactic velocity, with a clear optimum that shifts with environmental trap density. At the single-agent level, we identify and characterize a key motility state: transient trapping in the gel's pores. A minimal theoretical model, integrating run-tumble and run-trap dynamics, explains the optimum as a consequence of the antagonistic scaling of the diffusion coefficient (increasing with τ_f) and the chemotactic bias coefficient (decreasing with τ_f). This work establishes a general principle for the optimization of active matter transport in complex and obstructed environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.12106v1",
    "github_url": null,
    "published": "2025-10-14T03:18:11+00:00",
    "updated": "2025-10-14T03:18:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11977v1",
    "title": "Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation",
    "authors": [
      "Kapoor",
      "Stroebl",
      "Kirgis"
    ],
    "summary": "AI agents have been developed for complex real-world tasks from coding to customer service. But AI agent evaluations suffer from many challenges that undermine our understanding of how well agents really work. We introduce the Holistic Agent Leaderboard (HAL) to address these challenges. We make three main contributions. First, we provide a standardized evaluation harness that orchestrates parallel evaluations across hundreds of VMs, reducing evaluation time from weeks to hours while eliminating common implementation bugs. Second, we conduct three-dimensional analysis spanning models, scaffolds, and benchmarks. We validate the harness by conducting 21,730 agent rollouts across 9 models and 9 benchmarks in coding, web navigation, science, and customer service with a total cost of about $40,000. Our analysis reveals surprising insights, such as higher reasoning effort reducing accuracy in the majority of runs. Third, we use LLM-aided log inspection to uncover previously unreported behaviors, such as searching for the benchmark on HuggingFace instead of solving a task, or misusing credit cards in flight booking tasks. We share all agent logs, comprising 2.5B tokens of language model calls, to incentivize further research into agent behavior. By standardizing how the field evaluates agents and addressing common pitfalls in agent evaluation, we hope to shift the focus from agents that ace benchmarks to agents that work reliably in the real world.",
    "pdf_url": "https://arxiv.org/pdf/2510.11977v1",
    "github_url": null,
    "published": "2025-10-13T22:22:28+00:00",
    "updated": "2025-10-13T22:22:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11938v1",
    "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters",
    "authors": [
      "Lin",
      "Peng",
      "Lu"
    ],
    "summary": "Serving Large Language Models (LLMs) in production faces significant challenges from highly variable request patterns and severe resource fragmentation in serverless clusters. Current systems rely on static pipeline configurations that struggle to adapt to dynamic workload conditions, leading to substantial inefficiencies. We present FlexPipe, a novel system that dynamically reconfigures pipeline architectures during runtime to address these fundamental limitations. FlexPipe decomposes models into fine-grained stages and intelligently adjusts pipeline granularity based on real-time request pattern analysis, implementing three key innovations: fine-grained model partitioning with preserved computational graph constraints, inflight pipeline refactoring with consistent cache transitions, and topology-aware resource allocation that navigates GPU fragmentation. Comprehensive evaluation on an 82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource efficiency while maintaining 38.3% lower latency compared to state-of-the-art systems, reducing GPU reservation requirements from 75% to 30% of peak capacity.",
    "pdf_url": "https://arxiv.org/pdf/2510.11938v1",
    "github_url": null,
    "published": "2025-10-13T21:01:40+00:00",
    "updated": "2025-10-13T21:01:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11884v1",
    "title": "Optimal and efficient inference tools for field tracking with precessing spins",
    "authors": [
      "Dilcher",
      "Bania",
      "Mendez-Avalos"
    ],
    "summary": "Precise, real-time monitoring of magnetic field evolution is important in applications including magnetic navigation and searches for physics beyond the standard model. One main field-monitoring technique, the spin-precession magnetometer (SPM), observes electron, nucleus, color center, or muon spins as they precess in response to their local magnetic field. Here, we study Bayesian signal-recovery methods for SPMs in the free-induction decay (FID) mode. In particular, we study tracking of field changes well within the coherence time of the spin system, and thus well beyond the response bandwidth, as in [Phys. Rev. Lett. 120, 040503 (2018)]. We derive the Bayesian Cramér-Rao bound that dictates the ultimate precision in estimating the Larmor frequency, which we show to be attained by the computationally-expensive prediction error method (PEM). Relative to this benchmark, we show that the extended Kalman filter (EKF) and cubature Kalman filter (CKF) offer near-optimal tracking that is also computationally efficient, with the use of the latter giving better results only for large spin number. Focusing thus on the EKF, we show that it is sufficient to accurately track fluctuating and unknown transient signals. Our methods can be easily adapted to other types of sensors undergoing non-linear dissipative dynamics and experiencing intrinsic Gaussian-like stochastic noises.",
    "pdf_url": "https://arxiv.org/pdf/2510.11884v1",
    "github_url": null,
    "published": "2025-10-13T19:44:33+00:00",
    "updated": "2025-10-13T19:44:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.15987v1",
    "title": "Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models",
    "authors": [
      "Lippl",
      "McGee",
      "Lopez"
    ],
    "summary": "How do latent and inference time computations enable large language models (LLMs) to solve multi-step reasoning? We introduce a framework for tracing and steering algorithmic primitives that underlie model reasoning. Our approach links reasoning traces to internal activation patterns and evaluates algorithmic primitives by injecting them into residual streams and measuring their effect on reasoning steps and task performance. We consider four benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph navigation. We operationalize primitives by clustering neural activations and labeling their matched reasoning traces. We then apply function vector methods to derive primitive vectors as reusable compositional building blocks of reasoning. Primitive vectors can be combined through addition, subtraction, and scalar operations, revealing a geometric logic in activation space. Cross-task and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both shared and task-specific primitives. Notably, comparing Phi-4 with its reasoning-finetuned variant highlights compositional generalization after finetuning: Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives. Injecting the associated primitive vectors in Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning. Together, these findings demonstrate that reasoning in LLMs may be supported by a compositional geometry of algorithmic primitives, that primitives transfer cross-task and cross-model, and that reasoning finetuning strengthens algorithmic generalization across domains.",
    "pdf_url": "https://arxiv.org/pdf/2510.15987v1",
    "github_url": null,
    "published": "2025-10-13T18:36:43+00:00",
    "updated": "2025-10-13T18:36:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11552v1",
    "title": "Robot Soccer Kit: Omniwheel Tracked Soccer Robots for Education",
    "authors": [
      "Passault",
      "Gaspard",
      "Ly"
    ],
    "summary": "Recent developments of low cost off-the-shelf programmable components, their modularity, and also rapid prototyping made educational robotics flourish, as it is accessible in most schools today. They allow to illustrate and embody theoretical problems in practical and tangible applications, and gather multidisciplinary skills. They also give a rich natural context for project-oriented pedagogy. However, most current robot kits all are limited to egocentric aspect of the robots perception. This makes it difficult to access more high-level problems involving e.g. coordinates or navigation. In this paper we introduce an educational holonomous robot kit that comes with an external tracking system, which lightens the constraint on embedded systems, but allows in the same time to discover high-level aspects of robotics, otherwise unreachable.",
    "pdf_url": "https://arxiv.org/pdf/2510.11552v1",
    "github_url": null,
    "published": "2025-10-13T15:53:51+00:00",
    "updated": "2025-10-13T15:53:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11542v1",
    "title": "NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning",
    "authors": [
      "Janwani",
      "Madabushi",
      "Tucker"
    ],
    "summary": "Reinforcement learning (RL) has emerged as a powerful method to learn robust control policies for bipedal locomotion. Yet, it can be difficult to tune desired robot behaviors due to unintuitive and complex reward design. In comparison, offline trajectory optimization methods, like Hybrid Zero Dynamics, offer more tuneable, interpretable, and mathematically grounded motion plans for high-dimensional legged systems. However, these methods often remain brittle to real-world disturbances like external perturbations.   In this work, we present NaviGait, a hierarchical framework that combines the structure of trajectory optimization with the adaptability of RL for robust and intuitive locomotion control. NaviGait leverages a library of offline-optimized gaits and smoothly interpolates between them to produce continuous reference motions in response to high-level commands. The policy provides both joint-level and velocity command residual corrections to modulate and stabilize the reference trajectories in the gait library. One notable advantage of NaviGait is that it dramatically simplifies reward design by encoding rich motion priors from trajectory optimization, reducing the need for finely tuned shaping terms and enabling more stable and interpretable learning. Our experimental results demonstrate that NaviGait enables faster training compared to conventional and imitation-based RL, and produces motions that remain closest to the original reference. Overall, by decoupling high-level motion generation from low-level correction, NaviGait offers a more scalable and generalizable approach for achieving dynamic and robust locomotion.",
    "pdf_url": "https://arxiv.org/pdf/2510.11542v1",
    "github_url": null,
    "published": "2025-10-13T15:41:38+00:00",
    "updated": "2025-10-13T15:41:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11520v2",
    "title": "mmWalk: Towards Multi-modal Multi-view Walking Assistance",
    "authors": [
      "Ying",
      "Liu",
      "Chen"
    ],
    "summary": "Walking assistance in extreme or complex environments remains a significant challenge for people with blindness or low vision (BLV), largely due to the lack of a holistic scene understanding. Motivated by the real-world needs of the BLV community, we build mmWalk, a simulated multi-modal dataset that integrates multi-view sensor and accessibility-oriented features for outdoor safe navigation. Our dataset comprises 120 manually controlled, scenario-categorized walking trajectories with 62k synchronized frames. It contains over 559k panoramic images across RGB, depth, and semantic modalities. Furthermore, to emphasize real-world relevance, each trajectory involves outdoor corner cases and accessibility-specific landmarks for BLV users. Additionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual question-answer triplets across 9 categories tailored for safe and informed walking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs) using zero- and few-shot settings and found they struggle with our risk assessment and navigational tasks. We validate our mmWalk-finetuned model on real-world datasets and show the effectiveness of our dataset for advancing multi-modal walking assistance.",
    "pdf_url": "https://arxiv.org/pdf/2510.11520v2",
    "github_url": null,
    "published": "2025-10-13T15:25:52+00:00",
    "updated": "2025-10-23T16:40:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11503v1",
    "title": "People use fast, flat goal-directed simulation to reason about novel problems",
    "authors": [
      "Collins",
      "Zhang",
      "Wong"
    ],
    "summary": "Games have long been a microcosm for studying planning and reasoning in both natural and artificial intelligence, especially with a focus on expert-level or even super-human play. But real life also pushes human intelligence along a different frontier, requiring people to flexibly navigate decision-making problems that they have never thought about before. Here, we use novice gameplay to study how people make decisions and form judgments in new problem settings. We show that people are systematic and adaptively rational in how they play a game for the first time, or evaluate a game (e.g., how fair or how fun it is likely to be) before they have played it even once. We explain these capacities via a computational cognitive model that we call the \"Intuitive Gamer\". The model is based on mechanisms of fast and flat (depth-limited) goal-directed probabilistic simulation--analogous to those used in Monte Carlo tree-search models of expert game-play, but scaled down to use very few stochastic samples, simple goal heuristics for evaluating actions, and no deep search. In a series of large-scale behavioral studies with over 1000 participants and 121 two-player strategic board games (almost all novel to our participants), our model quantitatively captures human judgments and decisions varying the amount and kind of experience people have with a game--from no experience at all (\"just thinking\"), to a single round of play, to indirect experience watching another person and predicting how they should play--and does so significantly better than much more compute-intensive expert-level models. More broadly, our work offers new insights into how people rapidly evaluate, act, and make suggestions when encountering novel problems, and could inform the design of more flexible and human-like AI systems that can determine not just how to solve new tasks, but whether a task is worth thinking about at all.",
    "pdf_url": "https://arxiv.org/pdf/2510.11503v1",
    "github_url": null,
    "published": "2025-10-13T15:12:08+00:00",
    "updated": "2025-10-13T15:12:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11447v1",
    "title": "Building and Evaluating a Realistic Virtual World for Large Scale Urban Exploration from 360° Videos",
    "authors": [
      "Takenawa",
      "Sugimoto",
      "Wöhler"
    ],
    "summary": "We propose to build realistic virtual worlds, called 360RVW, for large urban environments directly from 360° videos. We provide an interface for interactive exploration, where users can freely navigate via their own avatars. 360° videos record the entire environment of the shooting location simultaneously leading to highly realistic and immersive representations. Our system uses 360° videos recorded along streets and builds a 360RVW through four main operations: video segmentation by intersection detection, video completion to remove the videographer, semantic segmentation for virtual collision detection with the avatar, and projection onto a distorted sphere that moves along the camera trajectory following the avatar's movements. Our interface allows users to explore large urban environments by changing their walking direction at intersections or choosing a new location by clicking on a map. Even without a 3D model, the users can experience collision with buildings using metadata produced by semantic segmentation. Furthermore, we stream the 360° videos so users can directly access 360RVW via their web browser. We fully evaluate our system, including a perceptual experiment comparing our approach to previous exploratory interfaces. The results confirm the quality of our system, especially regarding the presence of users and the interactive exploration, making it most suitable for a virtual tour of urban environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.11447v1",
    "github_url": null,
    "published": "2025-10-13T14:17:06+00:00",
    "updated": "2025-10-13T14:17:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11327v1",
    "title": "The Ethical and Sustainable Concerns Triangle: A Framework for Navigating Discourses in Mathematics and Its Education",
    "authors": [
      "Müller",
      "Chiodo",
      "Meyer"
    ],
    "summary": "The literature on ethics and sustainability in mathematics and its education is increasingly complex and fragmented, potentially leading to communication breakdowns between different scholarly traditions. To address this, the paper introduces the \"Ethical and Sustainable Concerns Triangle,\" a framework that maps discourses based on their relative concern for three areas (represented as three vertices in the triangle): \"Mathematics\", \"Community\", and \"Society/Planet\". By integrating a systems theoretic perspective, we analyse discourses as dynamic systemic reactions to external irritations. Our analysis reveals that the field's fragmentation can be explained by the \"location effect\": the phenomenon whereby a discourse's position within the triangle shapes its perception and acceptance of other scholarship. By mapping key discourses and educator archetypes, the framework functions as a meta-heuristic tool. Ultimately, it serves not only to facilitate critical reflection but also as a call for the epistemic humility and dialogue needed to advance the field.",
    "pdf_url": "https://arxiv.org/pdf/2510.11327v1",
    "github_url": null,
    "published": "2025-10-13T12:24:03+00:00",
    "updated": "2025-10-13T12:24:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11306v1",
    "title": "Rotor-Failure-Aware Quadrotors Flight in Unknown Environments",
    "authors": [
      "Zhou",
      "Wang",
      "Li"
    ],
    "summary": "Rotor failures in quadrotors may result in high-speed rotation and vibration due to rotor imbalance, which introduces significant challenges for autonomous flight in unknown environments. The mainstream approaches against rotor failures rely on fault-tolerant control (FTC) and predefined trajectory tracking. To the best of our knowledge, online failure detection and diagnosis (FDD), trajectory planning, and FTC of the post-failure quadrotors in unknown and complex environments have not yet been achieved. This paper presents a rotor-failure-aware quadrotor navigation system designed to mitigate the impacts of rotor imbalance. First, a composite FDD-based nonlinear model predictive controller (NMPC), incorporating motor dynamics, is designed to ensure fast failure detection and flight stability. Second, a rotor-failure-aware planner is designed to leverage FDD results and spatial-temporal joint optimization, while a LiDAR-based quadrotor platform with four anti-torque plates is designed to enable reliable perception under high-speed rotation. Lastly, extensive benchmarks against state-of-the-art methods highlight the superior performance of the proposed approach in addressing rotor failures, including propeller unloading and motor stoppage. The experimental results demonstrate, for the first time, that our approach enables autonomous quadrotor flight with rotor failures in challenging environments, including cluttered rooms and unknown forests.",
    "pdf_url": "https://arxiv.org/pdf/2510.11306v1",
    "github_url": null,
    "published": "2025-10-13T11:54:49+00:00",
    "updated": "2025-10-13T11:54:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11194v1",
    "title": "Aligning Deep Implicit Preferences by Learning to Reason Defensively",
    "authors": [
      "Li",
      "Hu",
      "Tang"
    ],
    "summary": "Personalized alignment is crucial for enabling Large Language Models (LLMs) to engage effectively in user-centric interactions. However, current methods face a dual challenge: they fail to infer users' deep implicit preferences (including unstated goals, semantic context and risk tolerances), and they lack the defensive reasoning required to navigate real-world ambiguity. This cognitive gap leads to responses that are superficial, brittle and short-sighted. To address this, we propose Critique-Driven Reasoning Alignment (CDRA), which reframes alignment from a scalar reward-matching task into a structured reasoning process. First, to bridge the preference inference gap, we introduce the DeepPref benchmark. This dataset, comprising 3000 preference-query pairs across 20 topics, is curated by simulating a multi-faceted cognitive council that produces critique-annotated reasoning chains to deconstruct query semantics and reveal latent risks. Second, to instill defensive reasoning, we introduce the Personalized Generative Process Reward Model (Pers-GenPRM), which frames reward modeling as a personalized reasoning task. It generates a critique chain to evaluate a response's alignment with user preferences before outputting a final score based on this rationale. Ultimately, this interpretable, structured reward signal guides policy model through Critique-Driven Policy Alignment, a process-level online reinforcement learning algorithm integrating both numerical and natural language feedback. Experiments demonstrate that CDRA excels at discovering and aligning with users' true preferences while executing robust reasoning. Our code and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref.",
    "pdf_url": "https://arxiv.org/pdf/2510.11194v1",
    "github_url": "https://github.com/Zephyrian-Hugh/Deep-pref",
    "published": "2025-10-13T09:26:47+00:00",
    "updated": "2025-10-13T09:26:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11113v1",
    "title": "Navigating the Dual-Use Nature and Security Implications of Reconfigurable Intelligent Surfaces in Next-Generation Wireless Systems",
    "authors": [
      "Wang",
      "Lv",
      "Cao"
    ],
    "summary": "Reconfigurable intelligent surface (RIS) technology offers significant promise in enhancing wireless communication systems, but its dual-use potential also introduces substantial security risks. This survey explores the security implications of RIS in next-generation wireless networks. We first highlight the dual-use nature of RIS, demonstrating how its communication-enhancing capabilities can be exploited by adversaries to compromise legitimate users. We identify a new class of security vulnerabilities termed ``passive-active hybrid attacks,'' where RIS, despite passively handling signals, can be reconfigured to actively engage in malicious activities, enabling various RIS-assisted attacks, such as eavesdropping, man-in-the-middle (MITM), replay, reflection jamming, and side-channel attacks. Furthermore, we reveal how adversaries can exploit the openness of wireless channels to introduce adversarial perturbations in artificial intelligence-driven RIS networks, disrupting communication terminals and causing misclassifications or errors in RIS reflection predictions. Despite these risks, RIS technology also plays a critical role in enhancing security and privacy across radio frequency (RF) and visible light communication (VLC) systems. By synthesizing current insights and highlighting emerging threats, we provide actionable insights into cross-layer collaboration, advanced adversarial defenses, and the balance between security and cost. This survey provides a comprehensive overview of RIS technology's security landscape and underscores the urgent need for robust security frameworks in the development of future wireless systems.",
    "pdf_url": "https://arxiv.org/pdf/2510.11113v1",
    "github_url": null,
    "published": "2025-10-13T08:02:37+00:00",
    "updated": "2025-10-13T08:02:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11035v1",
    "title": "SusBench: An Online Benchmark for Evaluating Dark Pattern Susceptibility of Computer-Use Agents",
    "authors": [
      "Guo",
      "Yuan",
      "Zhong"
    ],
    "summary": "As LLM-based computer-use agents (CUAs) begin to autonomously interact with real-world interfaces, understanding their vulnerability to manipulative interface designs becomes increasingly critical. We introduce SusBench, an online benchmark for evaluating the susceptibility of CUAs to UI dark patterns, designs that aim to manipulate or deceive users into taking unintentional actions. Drawing nine common dark pattern types from existing taxonomies, we developed a method for constructing believable dark patterns on real-world consumer websites through code injections, and designed 313 evaluation tasks across 55 websites. Our study with 29 participants showed that humans perceived our dark pattern injections to be highly realistic, with the vast majority of participants not noticing that these had been injected by the research team. We evaluated five state-of-the-art CUAs on the benchmark. We found that both human participants and agents are particularly susceptible to the dark patterns of Preselection, Trick Wording, and Hidden Information, while being resilient to other overt dark patterns. Our findings inform the development of more trustworthy CUAs, their use as potential human proxies in evaluating deceptive designs, and the regulation of an online environment increasingly navigated by autonomous agents.",
    "pdf_url": "https://arxiv.org/pdf/2510.11035v1",
    "github_url": null,
    "published": "2025-10-13T06:12:03+00:00",
    "updated": "2025-10-13T06:12:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11014v1",
    "title": "Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces",
    "authors": [
      "Bhattacharjee",
      "Lu",
      "Campbell"
    ],
    "summary": "Priors are vital for planning under partial observability, yet difficult to obtain in practice. We present a sampling-based pipeline that leverages large-scale pretrained generative models to produce probabilistic priors capturing environmental uncertainty and spatio-semantic relationships in a zero-shot manner. Conditioned on partial observations, the pipeline recovers complete RGB-D point cloud samples with occupancy and target semantics, formulated to be directly useful in configuration-space planning. We establish a Matterport3D benchmark of rooms partially visible through doorways, where a robot must navigate to an unobserved target object. Effective priors for this setting must represent both occupancy and target-location uncertainty in unobserved regions. Experiments show that our approach recovers commonsense spatial semantics consistent with ground truth, yielding diverse, clean 3D point clouds usable in motion planning, highlight the promise of generative models as a rich source of priors for robotic planning.",
    "pdf_url": "https://arxiv.org/pdf/2510.11014v1",
    "github_url": null,
    "published": "2025-10-13T05:08:48+00:00",
    "updated": "2025-10-13T05:08:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11760v1",
    "title": "Audio-Guided Visual Perception for Audio-Visual Navigation",
    "authors": [
      "Wang",
      "Yu",
      "Sun"
    ],
    "summary": "Audio-Visual Embodied Navigation aims to enable agents to autonomously navigate to sound sources in unknown 3D environments using auditory cues. While current AVN methods excel on in-distribution sound sources, they exhibit poor cross-source generalization: navigation success rates plummet and search paths become excessively long when agents encounter unheard sounds or unseen environments. This limitation stems from the lack of explicit alignment mechanisms between auditory signals and corresponding visual regions. Policies tend to memorize spurious \\enquote{acoustic fingerprint-scenario} correlations during training, leading to blind exploration when exposed to novel sound sources. To address this, we propose the AGVP framework, which transforms sound from policy-memorable acoustic fingerprint cues into spatial guidance. The framework first extracts global auditory context via audio self-attention, then uses this context as queries to guide visual feature attention, highlighting sound-source-related regions at the feature level. Subsequent temporal modeling and policy optimization are then performed. This design, centered on interpretable cross-modal alignment and region reweighting, reduces dependency on specific acoustic fingerprints. Experimental results demonstrate that AGVP improves both navigation efficiency and robustness while achieving superior cross-scenario generalization on previously unheard sounds.",
    "pdf_url": "https://arxiv.org/pdf/2510.11760v1",
    "github_url": null,
    "published": "2025-10-13T05:06:45+00:00",
    "updated": "2025-10-13T05:06:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10977v1",
    "title": "Revisiting Model Interpolation for Efficient Reasoning",
    "authors": [
      "Wu",
      "Yang",
      "Liu"
    ],
    "summary": "Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide a principled guide for navigating the performance-cost trade-off. Empirical results demonstrate that a strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers a practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at \\href{https://github.com/wutaiqiang/MI}{Github}.",
    "pdf_url": "https://arxiv.org/pdf/2510.10977v1",
    "github_url": "https://github.com/wutaiqiang/MI",
    "published": "2025-10-13T03:30:01+00:00",
    "updated": "2025-10-13T03:30:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10912v2",
    "title": "More than A Point: Capturing Uncertainty with Adaptive Affordance Heatmaps for Spatial Grounding in Robotic Tasks",
    "authors": [
      "Shao",
      "Tang",
      "Xie"
    ],
    "summary": "Many language-guided robotic systems rely on collapsing spatial reasoning into discrete points, making them brittle to perceptual noise and semantic ambiguity. To address this challenge, we propose RoboMAP, a framework that represents spatial targets as continuous, adaptive affordance heatmaps. This dense representation captures the uncertainty in spatial grounding and provides richer information for downstream policies, thereby significantly enhancing task success and interpretability. RoboMAP surpasses the previous state-of-the-art on a majority of grounding benchmarks with up to a 50x speed improvement, and achieves an 82\\% success rate in real-world manipulation. Across extensive simulated and physical experiments, it demonstrates robust performance and shows strong zero-shot generalization to navigation. More details and videos can be found at https://robo-map.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2510.10912v2",
    "github_url": null,
    "published": "2025-10-13T02:14:30+00:00",
    "updated": "2025-10-15T13:06:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10865v1",
    "title": "GRIP: A Unified Framework for Grid-Based Relay and Co-Occurrence-Aware Planning in Dynamic Environments",
    "authors": [
      "Alanazi",
      "Ho",
      "Lee"
    ],
    "summary": "Robots navigating dynamic, cluttered, and semantically complex environments must integrate perception, symbolic reasoning, and spatial planning to generalize across diverse layouts and object categories. Existing methods often rely on static priors or limited memory, constraining adaptability under partial observability and semantic ambiguity. We present GRIP, Grid-based Relay with Intermediate Planning, a unified, modular framework with three scalable variants: GRIP-L (Lightweight), optimized for symbolic navigation via semantic occupancy grids; GRIP-F (Full), supporting multi-hop anchor chaining and LLM-based introspection; and GRIP-R (Real-World), enabling physical robot deployment under perceptual uncertainty. GRIP integrates dynamic 2D grid construction, open-vocabulary object grounding, co-occurrence-aware symbolic planning, and hybrid policy execution using behavioral cloning, D* search, and grid-conditioned control. Empirical results on AI2-THOR and RoboTHOR benchmarks show that GRIP achieves up to 9.6% higher success rates and over $2\\times$ improvement in path efficiency (SPL and SAE) on long-horizon tasks. Qualitative analyses reveal interpretable symbolic plans in ambiguous scenes. Real-world deployment on a Jetbot further validates GRIP's generalization under sensor noise and environmental variation. These results position GRIP as a robust, scalable, and explainable framework bridging simulation and real-world navigation.",
    "pdf_url": "https://arxiv.org/pdf/2510.10865v1",
    "github_url": null,
    "published": "2025-10-13T00:13:37+00:00",
    "updated": "2025-10-13T00:13:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10851v1",
    "title": "Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion",
    "authors": [
      "Leng",
      "Wang",
      "Zheng"
    ],
    "summary": "Humanoid locomotion requires not only accurate command tracking for navigation but also compliant responses to external forces during human interaction. Despite significant progress, existing RL approaches mainly emphasize robustness, yielding policies that resist external forces but lack compliance-particularly challenging for inherently unstable humanoids. In this work, we address this by formulating humanoid locomotion as a multi-objective optimization problem that balances command tracking and external force compliance. We introduce a preference-conditioned multi-objective RL (MORL) framework that integrates rigid command following and compliant behaviors within a single omnidirectional locomotion policy. External forces are modeled via velocity-resistance factor for consistent reward design, and training leverages an encoder-decoder structure that infers task-relevant privileged features from deployable observations. We validate our approach in both simulation and real-world experiments on a humanoid robot. Experimental results indicate that our framework not only improves adaptability and convergence over standard pipelines, but also realizes deployable preference-conditioned humanoid locomotion.",
    "pdf_url": "https://arxiv.org/pdf/2510.10851v1",
    "github_url": null,
    "published": "2025-10-12T23:29:03+00:00",
    "updated": "2025-10-12T23:29:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10823v1",
    "title": "The Irrational Machine: Neurosis and the Limits of Algorithmic Safety",
    "authors": [
      "Howard"
    ],
    "summary": "We present a framework for characterizing neurosis in embodied AI: behaviors that are internally coherent yet misaligned with reality, arising from interactions among planning, uncertainty handling, and aversive memory. In a grid navigation stack we catalogue recurrent modalities including flip-flop, plan churn, perseveration loops, paralysis and hypervigilance, futile search, belief incoherence, tie break thrashing, corridor thrashing, optimality compulsion, metric mismatch, policy oscillation, and limited-visibility variants. For each we give lightweight online detectors and reusable escape policies (short commitments, a margin to switch, smoothing, principled arbitration). We then show that durable phobic avoidance can persist even under full visibility when learned aversive costs dominate local choice, producing long detours despite globally safe routes. Using First/Second/Third Law as engineering shorthand for safety latency, command compliance, and resource efficiency, we argue that local fixes are insufficient; global failures can remain. To surface them, we propose genetic-programming based destructive testing that evolves worlds and perturbations to maximize law pressure and neurosis scores, yielding adversarial curricula and counterfactual traces that expose where architectural revision, not merely symptom-level patches, is required.",
    "pdf_url": "https://arxiv.org/pdf/2510.10823v1",
    "github_url": null,
    "published": "2025-10-12T22:22:17+00:00",
    "updated": "2025-10-12T22:22:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10778v1",
    "title": "Real2USD: Scene Representations in Universal Scene Description Language",
    "authors": [
      "Hsu",
      "Chaudhari"
    ],
    "summary": "Large Language Models (LLMs) can help robots reason about abstract task specifications. This requires augmenting classical representations of the environment used by robots with natural language-based priors. There are a number of existing approaches to doing so, but they are tailored to specific tasks, e.g., visual-language models for navigation, language-guided neural radiance fields for mapping, etc. This paper argues that the Universal Scene Description (USD) language is an effective and general representation of geometric, photometric and semantic information in the environment for LLM-based robotics tasks. Our argument is simple: a USD is an XML-based scene graph, readable by LLMs and humans alike, and rich enough to support essentially any task -- Pixar developed this language to store assets, scenes and even movies. We demonstrate a ``Real to USD'' system using a Unitree Go2 quadruped robot carrying LiDAR and a RGB camera that (i) builds an explicit USD representation of indoor environments with diverse objects and challenging settings with lots of glass, and (ii) parses the USD using Google's Gemini to demonstrate scene understanding, complex inferences, and planning. We also study different aspects of this system in simulated warehouse and hospital settings using Nvidia's Issac Sim. Code is available at https://github.com/grasp-lyrl/Real2USD .",
    "pdf_url": "https://arxiv.org/pdf/2510.10778v1",
    "github_url": "https://github.com/grasp-lyrl/Real2USD",
    "published": "2025-10-12T19:43:10+00:00",
    "updated": "2025-10-12T19:43:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11754v1",
    "title": "Zero-Shot Large Language Model Agents for Fully Automated Radiotherapy Treatment Planning",
    "authors": [
      "Yang",
      "Wu",
      "Xie"
    ],
    "summary": "Radiation therapy treatment planning is an iterative, expertise-dependent process, and the growing burden of cancer cases has made reliance on manual planning increasingly unsustainable, underscoring the need for automation. In this study, we propose a workflow that leverages a large language model (LLM)-based agent to navigate inverse treatment planning for intensity-modulated radiation therapy (IMRT). The LLM agent was implemented to directly interact with a clinical treatment planning system (TPS) to iteratively extract intermediate plan states and propose new constraint values to guide inverse optimization. The agent's decision-making process is informed by current observations and previous optimization attempts and evaluations, allowing for dynamic strategy refinement. The planning process was performed in a zero-shot inference setting, where the LLM operated without prior exposure to manually generated treatment plans and was utilized without any fine-tuning or task-specific training. The LLM-generated plans were evaluated on twenty head-and-neck cancer cases against clinical manual plans, with key dosimetric endpoints analyzed and reported. The LLM-generated plans achieved comparable organ-at-risk (OAR) sparing relative to clinical plans while demonstrating improved hot spot control (Dmax: 106.5% vs. 108.8%) and superior conformity (conformity index: 1.18 vs. 1.39 for boost PTV; 1.82 vs. 1.88 for primary PTV). This study demonstrates the feasibility of a zero-shot, LLM-driven workflow for automated IMRT treatment planning in a commercial TPS. The proposed approach provides a generalizable and clinically applicable solution that could reduce planning variability and support broader adoption of AI-based planning strategies.",
    "pdf_url": "https://arxiv.org/pdf/2510.11754v1",
    "github_url": null,
    "published": "2025-10-12T19:21:21+00:00",
    "updated": "2025-10-12T19:21:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10597v1",
    "title": "Fast Vision in the Dark: A Case for Single-Photon Imaging in Planetary Navigation",
    "authors": [
      "Rodríguez-Martínez",
      "Pulgar"
    ],
    "summary": "Improving robotic navigation is critical for extending exploration range and enhancing operational efficiency. Vision-based navigation relying on traditional CCD or CMOS cameras faces major challenges when complex illumination conditions are paired with motion, limiting the range and accessibility of mobile planetary robots. In this study, we propose a novel approach to planetary navigation that leverages the unique imaging capabilities of Single-Photon Avalanche Diode (SPAD) cameras. We present the first comprehensive evaluation of single-photon imaging as an alternative passive sensing technology for robotic exploration missions targeting perceptually challenging locations, with a special emphasis on high-latitude lunar regions. We detail the operating principles and performance characteristics of SPAD cameras, assess their advantages and limitations in addressing key perception challenges of upcoming exploration missions to the Moon, and benchmark their performance under representative illumination conditions.",
    "pdf_url": "https://arxiv.org/pdf/2510.10597v1",
    "github_url": null,
    "published": "2025-10-12T13:29:57+00:00",
    "updated": "2025-10-12T13:29:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10506v1",
    "title": "SuperEx: Enhancing Indoor Mapping and Exploration using Non-Line-of-Sight Perception",
    "authors": [
      "Garg",
      "Dave"
    ],
    "summary": "Efficient exploration and mapping in unknown indoor environments is a fundamental challenge, with high stakes in time-critical settings. In current systems, robot perception remains confined to line-of-sight; occluded regions remain unknown until physically traversed, leading to inefficient exploration when layouts deviate from prior assumptions. In this work, we bring non-line-of-sight (NLOS) sensing to robotic exploration. We leverage single-photon LiDARs, which capture time-of-flight histograms that encode the presence of hidden objects - allowing robots to look around blind corners. Recent single-photon LiDARs have become practical and portable, enabling deployment beyond controlled lab settings. Prior NLOS works target 3D reconstruction in static, lab-based scenarios, and initial efforts toward NLOS-aided navigation consider simplified geometries. We introduce SuperEx, a framework that integrates NLOS sensing directly into the mapping-exploration loop. SuperEx augments global map prediction with beyond-line-of-sight cues by (i) carving empty NLOS regions from timing histograms and (ii) reconstructing occupied structure via a two-step physics-based and data-driven approach that leverages structural regularities. Evaluations on complex simulated maps and the real-world KTH Floorplan dataset show a 12% gain in mapping accuracy under < 30% coverage and improved exploration efficiency compared to line-of-sight baselines, opening a path to reliable mapping beyond direct visibility.",
    "pdf_url": "https://arxiv.org/pdf/2510.10506v1",
    "github_url": null,
    "published": "2025-10-12T08:52:20+00:00",
    "updated": "2025-10-12T08:52:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10471v2",
    "title": "DAGLFNet: Deep Feature Attention Guided Global and Local Feature Fusion for Pseudo-Image Point Cloud Segmentation",
    "authors": [
      "Chen",
      "Lin",
      "Wang"
    ],
    "summary": "Environmental perception systems are crucial for high-precision mapping and autonomous navigation, with LiDAR serving as a core sensor providing accurate 3D point cloud data. Efficiently processing unstructured point clouds while extracting structured semantic information remains a significant challenge. In recent years, numerous pseudo-image-based representation methods have emerged to balance efficiency and performance by fusing 3D point clouds with 2D grids. However, the fundamental inconsistency between the pseudo-image representation and the original 3D information critically undermines 2D-3D feature fusion, posing a primary obstacle for coherent information fusion and leading to poor feature discriminability. This work proposes DAGLFNet, a pseudo-image-based semantic segmentation framework designed to extract discriminative features. It incorporates three key components: first, a Global-Local Feature Fusion Encoding (GL-FFE) module to enhance intra-set local feature correlation and capture global contextual information; second, a Multi-Branch Feature Extraction (MB-FE) network to capture richer neighborhood information and improve the discriminability of contour features; and third, a Feature Fusion via Deep Feature-guided Attention (FFDFA) mechanism to refine cross-channel feature fusion precision. Experimental evaluations demonstrate that DAGLFNet achieves mean Intersection-over-Union (mIoU) scores of 69.9% and 78.7% on the validation sets of SemanticKITTI and nuScenes, respectively. The method achieves an excellent balance between accuracy and efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2510.10471v2",
    "github_url": null,
    "published": "2025-10-12T06:35:03+00:00",
    "updated": "2025-11-24T02:38:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10468v1",
    "title": "Galilean Symmetry in Robotics",
    "authors": [
      "Mahony",
      "Kelly",
      "Weiss"
    ],
    "summary": "Galilean symmetry is the natural symmetry of inertial motion that underpins Newtonian physics. Although rigid-body symmetry is one of the most established and fundamental tools in robotics, there appears to be no comparable treatment of Galilean symmetry for a robotics audience. In this paper, we present a robotics-tailored exposition of Galilean symmetry that leverages the community's familiarity with and understanding of rigid-body transformations and pose representations. Our approach contrasts with common treatments in the physics literature that introduce Galilean symmetry as a stepping stone to Einstein's relativity. A key insight is that the Galilean matrix Lie group can be used to describe two different pose representations, Galilean frames, that use inertial velocity in the state definition, and extended poses, that use coordinate velocity. We provide three examples where applying the Galilean matrix Lie-group algebra to robotics problems is straightforward and yields significant insights: inertial navigation above the rotating Earth, manipulator kinematics, and sensor data fusion under temporal uncertainty. We believe that the time is right for the robotics community to benefit from rediscovering and extending this classical material and applying it to modern problems.",
    "pdf_url": "https://arxiv.org/pdf/2510.10468v1",
    "github_url": null,
    "published": "2025-10-12T06:24:03+00:00",
    "updated": "2025-10-12T06:24:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10346v1",
    "title": "sqrtVINS: Robust and Ultrafast Square-Root Filter-based 3D Motion Tracking",
    "authors": [
      "Peng",
      "Chen",
      "Wu"
    ],
    "summary": "In this paper, we develop and open-source, for the first time, a square-root filter (SRF)-based visual-inertial navigation system (VINS), termed sqrtVINS, which is ultra-fast, numerically stable, and capable of dynamic initialization even under extreme conditions (i.e., extremely small time window). Despite recent advancements in VINS, resource constraints and numerical instability on embedded (robotic) systems with limited precision remain critical challenges. A square-root covariance-based filter offers a promising solution by providing numerical stability, efficient memory usage, and guaranteed positive semi-definiteness. However, canonical SRFs suffer from inefficiencies caused by disruptions in the triangular structure of the covariance matrix during updates. The proposed method significantly improves VINS efficiency with a novel Cholesky decomposition (LLT)-based SRF update, by fully exploiting the system structure to preserve the structure. Moreover, we design a fast, robust, dynamic initialization method, which first recovers the minimal states without triangulating 3D features and then efficiently performs iterative SRF update to refine the full states, enabling seamless VINS operation. The proposed LLT-based SRF is extensively verified through numerical studies, demonstrating superior numerical stability and achieving robust efficient performance on 32-bit single-precision floats, operating at twice the speed of state-of-the-art (SOTA) methods. Our initialization method, tested on both mobile workstations and Jetson Nano computers, achieving a high success rate of initialization even within a 100 ms window under minimal conditions. Finally, the proposed sqrtVINS is extensively validated across diverse scenarios, demonstrating strong efficiency, robustness, and reliability. The full open-source implementation is released to support future research and applications.",
    "pdf_url": "https://arxiv.org/pdf/2510.10346v1",
    "github_url": null,
    "published": "2025-10-11T21:41:03+00:00",
    "updated": "2025-10-11T21:41:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10332v2",
    "title": "Towards Safe Maneuvering of Double-Ackermann-Steering Robots with a Soft Actor-Critic Framework",
    "authors": [
      "Deflesselle",
      "Daniel",
      "Magassouba"
    ],
    "summary": "We present a deep reinforcement learning framework based on Soft Actor-Critic (SAC) for safe and precise maneuvering of double-Ackermann-steering mobile robots (DASMRs). Unlike holonomic or simpler non-holonomic robots such as differential-drive robots, DASMRs face strong kinematic constraints that make classical planners brittle in cluttered environments. Our framework leverages the Hindsight Experience Replay (HER) and the CrossQ overlay to encourage maneuvering efficiency while avoiding obstacles. Simulation results with a heavy four-wheel-steering rover show that the learned policy can robustly reach up to 97% of target positions while avoiding obstacles. Our framework does not rely on handcrafted trajectories or expert demonstrations.",
    "pdf_url": "https://arxiv.org/pdf/2510.10332v2",
    "github_url": null,
    "published": "2025-10-11T20:30:37+00:00",
    "updated": "2025-10-14T07:59:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10331v1",
    "title": "LLM-Friendly Knowledge Representation for Customer Support",
    "authors": [
      "Su",
      "Luo",
      "Han"
    ],
    "summary": "We propose a practical approach by integrating Large Language Models (LLMs) with a framework designed to navigate the complexities of Airbnb customer support operations. In this paper, our methodology employs a novel reformatting technique, the Intent, Context, and Action (ICA) format, which transforms policies and workflows into a structure more comprehensible to LLMs. Additionally, we develop a synthetic data generation strategy to create training data with minimal human intervention, enabling cost-effective fine-tuning of our model. Our internal experiments (not applied to Airbnb products) demonstrate that our approach of restructuring workflows and fine-tuning LLMs with synthetic data significantly enhances their performance, setting a new benchmark for their application in customer support. Our solution is not only cost-effective but also improves customer support, as evidenced by both accuracy and manual processing time evaluation metrics.",
    "pdf_url": "https://arxiv.org/pdf/2510.10331v1",
    "github_url": null,
    "published": "2025-10-11T20:24:50+00:00",
    "updated": "2025-10-11T20:24:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10304v1",
    "title": "Sample-Efficient Online Learning in LM Agents via Hindsight Trajectory Rewriting",
    "authors": [
      "Hu",
      "Durme",
      "Andreas"
    ],
    "summary": "Language model (LM) agents deployed in novel environments often exhibit poor sample efficiency when learning from sequential interactions. This significantly hinders the usefulness of such agents in environments where interaction is costly (for example, when they interact with humans or reset physical systems). While a number of existing LM agent architectures incorporate various mechanisms for experience storage and reflection, they make limited use of LMs' abilities to directly generate or reason about full counterfactual trajectories. We introduce ECHO (Experience Consolidation via Hindsight Optimization), a prompting framework that adapts hindsight experience replay from reinforcement learning for language model agents. ECHO generates optimized trajectories for alternative goals that could have been achieved during failed attempts, effectively creating synthetic positive examples from unsuccessful interactions. Our approach consists of two components: a hindsight rule that uses the language model itself to identify relevant subgoals and generate optimized trajectories, and an update rule that maintains compressed trajectory representations in memory. We evaluate ECHO on stateful versions of XMiniGrid, a text-based navigation and planning benchmark, and PeopleJoinQA, a collaborative information-gathering enterprise simulation. Across both domains, ECHO outperforms vanilla language agent baselines by up to 80%; in XMiniGrid, it also outperforms a number of sophisticated agent architectures including Reflexion and AWM, demonstrating faster adaptation to novel environments through more effective utilization of past experiences.",
    "pdf_url": "https://arxiv.org/pdf/2510.10304v1",
    "github_url": null,
    "published": "2025-10-11T18:11:09+00:00",
    "updated": "2025-10-11T18:11:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10278v1",
    "title": "Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models",
    "authors": [
      "Chiu",
      "Pitis",
      "Schaar"
    ],
    "summary": "Clinical reasoning in medicine is a hypothesis-driven process where physicians refine diagnoses from limited information through targeted history, physical examination, and diagnostic investigations. In contrast, current medical benchmarks for large language models (LLMs) primarily assess knowledge recall through single-turn questions, where complete clinical information is provided upfront. To address this gap, we introduce VivaBench, a multi-turn benchmark that evaluates sequential clinical reasoning in LLM agents. Our dataset consists of 1762 physician-curated clinical vignettes structured as interactive scenarios that simulate a (oral) examination in medical training, requiring agents to actively probe for relevant findings, select appropriate investigations, and synthesize information across multiple steps to reach a diagnosis. While current LLMs demonstrate competence in diagnosing conditions from well-described clinical presentations, their performance degrades significantly when required to navigate iterative diagnostic reasoning under uncertainty in our evaluation. Our analysis identified several failure modes that mirror common cognitive errors in clinical practice, including: (1) fixation on initial hypotheses, (2) inappropriate investigation ordering, (3) premature diagnostic closure, and (4) failing to screen for critical conditions. These patterns reveal fundamental limitations in how current LLMs reason and make decisions under uncertainty. Through VivaBench, we provide a standardized benchmark for evaluating conversational medical AI systems for real-world clinical decision support. Beyond medical applications, we contribute to the larger corpus of research on agentic AI by demonstrating how sequential reasoning trajectories can diverge in complex decision-making environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.10278v1",
    "github_url": null,
    "published": "2025-10-11T16:24:35+00:00",
    "updated": "2025-10-11T16:24:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10263v1",
    "title": "Unveiling Gamer Archetypes through Multi modal feature Correlations and Unsupervised Learning",
    "authors": [
      "Kanwal",
      "Siddiqui",
      "Ali"
    ],
    "summary": "Profiling gamers provides critical insights for adaptive game design, behavioral understanding, and digital well-being. This study proposes an integrated, data-driven framework that combines psychological measures, behavioral analytics, and machine learning to reveal underlying gamer personas. A structured survey of 250 participants, including 113 active gamers, captured multidimensional behavioral, motivational, and social data. The analysis pipeline integrated feature engineering, association-network, knowledge-graph analysis, and unsupervised clustering to extract meaningful patterns. Correlation statistics uses Cramers V, Tschuprows T, Theils U, and Spearmans quantified feature associations, and network centrality guided feature selection. Dimensionality-reduction techniques such as PCA, SVD, t-SNE are coupled with clustering algorithms like K-Means, Agglomerative, Spectral, DBSCAN, evaluated using Silhouette, Calinski Harabasz, and Davies Bouldin indices. The PCA with K-Means with k = 4 model achieved optimal cluster quality with Silhouette = 0.4, identifying four archetypes as Immersive Social Story-Seekers, Disciplined Optimizers, Strategic Systems Navigators, and Competitive Team-Builders. This research contributes a reproducible pipeline that links correlation-driven network insights with unsupervised learning. The integration of behavioral correlation networks with clustering not only enhances classification accuracy but also offers a holistic lens to connect gameplay motivations with psychological and wellness outcomes.",
    "pdf_url": "https://arxiv.org/pdf/2510.10263v1",
    "github_url": null,
    "published": "2025-10-11T15:46:44+00:00",
    "updated": "2025-10-11T15:46:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13840v1",
    "title": "Quantum modeling of radical pair magnetic sensor based on electric dipole moment",
    "authors": [
      "Sehati",
      "Soltanmanesh",
      "Abutalebi"
    ],
    "summary": "Photoreduction of cryptochrome protein in the retina is a well-known mechanism of navigation of birds through the geomagnetic field, yet the biosignal nature of the mechanism remains unclear. The absorption of blue light by the flavin adenine dinucleotide (FAD) chromophore can alter the distribution of electrons in cryptochrome and create radical pairs with separated charges. In this study, the spin dynamics of electrons in the radical pair and its coupling with spatial position were investigated by computational modeling from a quantum mechanical perspective. Several interactions were considered in the presence of an external magnetic field, and the resulting electric dipole moment in cryptochrome was computed as the quantity emerging from this coupling. The computations show the induced electric dipole moment clearly depend on the characteristics of the applied magnetic field even after considering dissipative effects. In fact, our findings indicate that the radical pair in cryptochrome protein is a magnetic biosensor, in the sense that in the presence of the geomagnetic field, variations in spin states can influence its electric dipole moment, which may be interpreted via the bird as an orientation signal. The results can be used in the advancement of bio-inspired technologies which replicate animal magnetic sensitivity. On the other hand, with increasing concern about the detrimental effects of electromagnetic fields on wildlife and human health, studying the phenomenon of magnetoreception can contribute to a deeper understanding of how biological structures interact with these fields.",
    "pdf_url": "https://arxiv.org/pdf/2510.13840v1",
    "github_url": null,
    "published": "2025-10-11T12:58:38+00:00",
    "updated": "2025-10-11T12:58:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10154v1",
    "title": "CompassNav: Steering From Path Imitation To Decision Understanding In Navigation",
    "authors": [
      "Li",
      "Zhao",
      "Xie"
    ],
    "summary": "The dominant paradigm for training Large Vision-Language Models (LVLMs) in navigation relies on imitating expert trajectories. This approach reduces the complex navigation task to a sequence-to-sequence replication of a single correct path, fundamentally limiting the agent's ability to explore and generalize. In this work, we argue for and introduce a new paradigm: a shift from Path Imitation to Decision Understanding. The goal of this paradigm is to build agents that do not just follow, but truly understand how to navigate. We materialize this through two core contributions: first, we introduce Compass-Data-22k, a novel 22k-trajectory dataset.Its Reinforcement Fine-Tuning (RFT) subset provides a panoramic view of the decision landscape by annotating all feasible actions with A* geodesic distances. Second, we design a novel gap-aware hybrid reward function that dynamically adapts its feedback to decision certainty, shifting between decisive signals for optimal actions and nuanced scores to encourage exploration. Integrated into an SFT-then-RFT recipe, our CompassNav agent is trained not to memorize static routes, but to develop an internal ``compass'' that constantly intuits the direction to the goal by evaluating the relative quality of all possible moves. This approach enables our 7B agent to set a new state-of-the-art on Goal navigation benchmarks, outperforming even larger proprietary models, and achieve robust real-world goal navigation on a physical robot.",
    "pdf_url": "https://arxiv.org/pdf/2510.10154v1",
    "github_url": null,
    "published": "2025-10-11T10:25:00+00:00",
    "updated": "2025-10-11T10:25:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10059v2",
    "title": "Ionospheric and Plasmaspheric Delay Characterization for Lunar Terrestrial GNSS Receivers with Global Core Plasma Model",
    "authors": [
      "Iiyama",
      "Gao"
    ],
    "summary": "Recent advancements in lunar positioning, navigation, and timing (PNT) have demonstrated that terrestrial GNSS signals, including weak sidelobe transmissions, can be exploited for lunar spacecraft positioning and timing. While GNSS-based navigation at the Moon has been validated recently, unmodeled ionospheric and plasmaspheric delays remain a significant error source, particularly given the unique signal geometry and extended propagation paths. This paper characterizes these delays using the Global Core Plasma Model (GCPM) and a custom low-cost ray-tracing algorithm that iteratively solves for bent signal paths. We simulate first-, second-, and third-order group delays, as well as excess path length from ray bending, for GNSS signals received at both lunar orbit and the lunar south pole under varying solar and geomagnetic conditions. Results show that mean group delays are typically on the order of 1 m, but can exceed 100 m for low-altitude ray paths during high solar activity, while bending delays are generally smaller but non-negligible for low-altitude ray paths. We also quantify the influence of signal frequency, geomagnetic $K_p$ index, and solar R12 index. These findings inform the design of robust positioning and timing algorithms that utilize terrestrial GNSS signals.",
    "pdf_url": "https://arxiv.org/pdf/2510.10059v2",
    "github_url": null,
    "published": "2025-10-11T06:53:01+00:00",
    "updated": "2025-11-21T19:06:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.10002v1",
    "title": "Deliberative Dynamics and Value Alignment in LLM Debates",
    "authors": [
      "Sachdeva",
      "Nuenen"
    ],
    "summary": "As large language models (LLMs) are increasingly deployed in sensitive everyday contexts - offering personal advice, mental health support, and moral guidance - understanding their elicited values in navigating complex moral reasoning is essential. Most evaluations study this sociotechnical alignment through single-turn prompts, but it is unclear if these findings extend to multi-turn settings where values emerge through dialogue, revision, and consensus. We address this gap using LLM debate to examine deliberative dynamics and value alignment in multi-turn settings by prompting subsets of three models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively assign blame in 1,000 everyday dilemmas from Reddit's \"Am I the Asshole\" community. We use both synchronous (parallel responses) and round-robin (sequential responses) formats to test order effects and verdict revision. Our findings show striking behavioral differences. In the synchronous setting, GPT showed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were far more flexible (28-41%). Value patterns also diverged: GPT emphasized personal autonomy and direct communication, while Claude and Gemini prioritized empathetic dialogue. Certain values proved especially effective at driving verdict changes. We further find that deliberation format had a strong impact on model behavior: GPT and Gemini stood out as highly conforming relative to Claude, with their verdict behavior strongly shaped by order effects. These results show how deliberation format and model-specific behaviors shape moral reasoning in multi-turn interactions, underscoring that sociotechnical alignment depends on how systems structure dialogue as much as on their outputs.",
    "pdf_url": "https://arxiv.org/pdf/2510.10002v1",
    "github_url": null,
    "published": "2025-10-11T04:06:07+00:00",
    "updated": "2025-10-11T04:06:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09951v2",
    "title": "Egocentric Visual Navigation through Hippocampal Sequences",
    "authors": [
      "Lin",
      "Yiu",
      "Leibold"
    ],
    "summary": "Sequential activation of place-tuned neurons in an animal during navigation is typically interpreted as reflecting the sequence of input from adjacent positions along the trajectory. More recent theories about such place cells suggest sequences arise from abstract cognitive objectives like planning. Here, we propose a mechanistic and parsimonious interpretation to complement these ideas: hippocampal sequences arise from intrinsic recurrent circuitry that propagates activity without readily available input, acting as a temporal memory buffer for extremely sparse inputs. We implement a minimal sequence generator inspired by neurobiology and pair it with an actor-critic learner for egocentric visual navigation. Our agent reliably solves a continuous maze without explicit geometric cues, with performance depending on the length of the recurrent sequence. Crucially, the model outperforms LSTM cores under sparse input conditions (16 channels, ~2.5% activity), but not under dense input, revealing a strong interaction between representational sparsity and memory architecture. In contrast to LSTM agents, hidden sequence units develop localized place fields, distance-dependent spatial kernels, and task-dependent remapping, while inputs orthogonalize and spatial information increases across layers. These phenomena align with neurobiological data and are causal to performance. Together, our results show that sparse input synergizes with sequence-generating dynamics, providing both a mechanistic account of place cell sequences in the mammalian hippocampus and a simple inductive bias for reinforcement learning based on sparse egocentric inputs in navigation tasks.",
    "pdf_url": "https://arxiv.org/pdf/2510.09951v2",
    "github_url": null,
    "published": "2025-10-11T01:38:23+00:00",
    "updated": "2025-10-15T17:40:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09912v1",
    "title": "SpectralCA: Bi-Directional Cross-Attention for Next-Generation UAV Hyperspectral Vision",
    "authors": [
      "Brovko"
    ],
    "summary": "The relevance of this research lies in the growing demand for unmanned aerial vehicles (UAVs) capable of operating reliably in complex environments where conventional navigation becomes unreliable due to interference, poor visibility, or camouflage. Hyperspectral imaging (HSI) provides unique opportunities for UAV-based computer vision by enabling fine-grained material recognition and object differentiation, which are critical for navigation, surveillance, agriculture, and environmental monitoring. The aim of this work is to develop a deep learning architecture integrating HSI into UAV perception for navigation, object detection, and terrain classification. Objectives include: reviewing existing HSI methods, designing a hybrid 2D/3D convolutional architecture with spectral-spatial cross-attention, training, and benchmarking. The methodology is based on the modification of the Mobile 3D Vision Transformer (MDvT) by introducing the proposed SpectralCA block. This block employs bi-directional cross-attention to fuse spectral and spatial features, enhancing accuracy while reducing parameters and inference time. Experimental evaluation was conducted on the WHU-Hi-HongHu dataset, with results assessed using Overall Accuracy, Average Accuracy, and the Kappa coefficient. The findings confirm that the proposed architecture improves UAV perception efficiency, enabling real-time operation for navigation, object recognition, and environmental monitoring tasks.   Keywords: SpectralCA, deep learning, computer vision, hyperspectral imaging, unmanned aerial vehicle, object detection, semi-supervised learning.",
    "pdf_url": "https://arxiv.org/pdf/2510.09912v1",
    "github_url": null,
    "published": "2025-10-10T22:53:28+00:00",
    "updated": "2025-10-10T22:53:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09872v1",
    "title": "WARC-Bench: Web Archive Based Benchmark for GUI Subtask Executions",
    "authors": [
      "Srivastava",
      "Li",
      "Chang"
    ],
    "summary": "Training web agents to navigate complex, real-world websites requires them to master $\\textit{subtasks}$ - short-horizon interactions on multiple UI components (e.g., choosing the correct date in a date picker, or scrolling in a container to extract information). We introduce WARC-Bench (Web Archive Benchmark), a novel web navigation benchmark featuring 438 tasks designed to evaluate multimodal AI agents on subtasks. WARC-Bench enables sandboxed interactions with dynamic and realistic webpages using Web ARChive files. We show that WARC-Bench is challenging for leading computer-use models, with the highest observed success rate being 64.8%. To improve open source models on subtask, we explore two common training techniques: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). Experiments show that SFT models obtain a 48.8% success rate on the benchmark. Training with RLVR over SFT checkpoints, even in data-scarce settings, improves the score to 52.8% on WARC-Bench, outperforming many frontier models. Our analysis concludes that mastering these subtasks is essential for robust web planning and navigation, and is a capability not extensively evaluated by existing benchmarks.",
    "pdf_url": "https://arxiv.org/pdf/2510.09872v1",
    "github_url": null,
    "published": "2025-10-10T21:20:51+00:00",
    "updated": "2025-10-10T21:20:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.17845v1",
    "title": "MAT-Agent: Adaptive Multi-Agent Training Optimization",
    "authors": [
      "Zhang",
      "Cai",
      "Fan"
    ],
    "summary": "Multi-label image classification demands adaptive training strategies to navigate complex, evolving visual-semantic landscapes, yet conventional methods rely on static configurations that falter in dynamic settings. We propose MAT-Agent, a novel multi-agent framework that reimagines training as a collaborative, real-time optimization process. By deploying autonomous agents to dynamically tune data augmentation, optimizers, learning rates, and loss functions, MAT-Agent leverages non-stationary multi-armed bandit algorithms to balance exploration and exploitation, guided by a composite reward harmonizing accuracy, rare-class performance, and training stability. Enhanced with dual-rate exponential moving average smoothing and mixed-precision training, it ensures robustness and efficiency. Extensive experiments across Pascal VOC, COCO, and VG-256 demonstrate MAT-Agent's superiority: it achieves an mAP of 97.4 (vs. 96.2 for PAT-T), OF1 of 92.3, and CF1 of 91.4 on Pascal VOC; an mAP of 92.8 (vs. 92.0 for HSQ-CvN), OF1 of 88.2, and CF1 of 87.1 on COCO; and an mAP of 60.9, OF1 of 70.8, and CF1 of 61.1 on VG-256. With accelerated convergence and robust cross-domain generalization, MAT-Agent offers a scalable, intelligent solution for optimizing complex visual models, paving the way for adaptive deep learning advancements.",
    "pdf_url": "https://arxiv.org/pdf/2510.17845v1",
    "github_url": null,
    "published": "2025-10-10T19:41:50+00:00",
    "updated": "2025-10-10T19:41:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09577v1",
    "title": "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents",
    "authors": [
      "Yu",
      "Peng",
      "Galley"
    ],
    "summary": "Reasoning models have recently shown remarkable progress in domains such as math and coding. However, their expert-level abilities in math and coding contrast sharply with their performance in long-horizon, interactive tasks such as web navigation and computer/phone-use. Inspired by literature on human cognition, we argue that current AI agents need ''vicarious trial and error'' - the capacity to mentally simulate alternative futures before acting - in order to enhance their understanding and performance in complex interactive environments. We introduce Dyna-Mind, a two-stage training framework that explicitly teaches (V)LM agents to integrate such simulation into their reasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which trains the agent to generate structured reasoning traces from expanded search trees built from real experience gathered through environment interactions. ReSim thus grounds the agent's reasoning in faithful world dynamics and equips it with the ability to anticipate future states in its reasoning. In stage 2, we propose Dyna-GRPO, an online reinforcement learning method to further strengthen the agent's simulation and decision-making ability by using both outcome rewards and intermediate states as feedback from real rollouts. Experiments on two synthetic benchmarks (Sokoban and ALFWorld) and one realistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively infuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome and interaction-level signals to learn better policies for long-horizon, planning-intensive tasks. Together, these results highlight the central role of simulation in enabling AI agents to reason, plan, and act more effectively in the ever more challenging environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.09577v1",
    "github_url": null,
    "published": "2025-10-10T17:30:18+00:00",
    "updated": "2025-10-10T17:30:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09574v1",
    "title": "Zero-shot Structure Learning and Planning for Autonomous Robot Navigation using Active Inference",
    "authors": [
      "tinguy",
      "Verbelen",
      "Gamba"
    ],
    "summary": "Autonomous navigation in unfamiliar environments requires robots to simultaneously explore, localise, and plan under uncertainty, without relying on predefined maps or extensive training. We present a biologically inspired, Active Inference-based framework, Active Inference MAPping and Planning (AIMAPP). This model unifies mapping, localisation, and decision-making within a single generative model. Inspired by hippocampal navigation, it uses topological reasoning, place-cell encoding, and episodic memory to guide behaviour. The agent builds and updates a sparse topological map online, learns state transitions dynamically, and plans actions by minimising Expected Free Energy. This allows it to balance goal-directed and exploratory behaviours. We implemented a ROS-compatible navigation system that is sensor and robot-agnostic, capable of integrating with diverse hardware configurations. It operates in a fully self-supervised manner, is resilient to drift, and supports both exploration and goal-directed navigation without any pre-training. We demonstrate robust performance in large-scale real and simulated environments against state-of-the-art planning models, highlighting the system's adaptability to ambiguous observations, environmental changes, and sensor noise. The model offers a biologically inspired, modular solution to scalable, self-supervised navigation in unstructured settings. AIMAPP is available at https://github.com/decide-ugent/AIMAPP.",
    "pdf_url": "https://arxiv.org/pdf/2510.09574v1",
    "github_url": "https://github.com/decide-ugent/AIMAPP",
    "published": "2025-10-10T17:28:12+00:00",
    "updated": "2025-10-10T17:28:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09539v1",
    "title": "IF-D: A High-Frequency, General-Purpose Inertial Foundation Dataset for Self-Supervised Learning",
    "authors": [
      "Ferreira",
      "Costa"
    ],
    "summary": "We present IF-D, a large-scale inertial dataset designed to enable self-supervised and foundational learning for IMU time series. IF-D comprises continuous, long-duration multichannel recordings (accelerometer, gyroscope, magnetometer) sampled at 200Hz using a UM7 IMU mounted inside a 3D-printed spherical enclosure that promotes diverse, free rotations during vehicle traversal. The collection spans approximately 135 minutes of recording, yielding around 1.6 million samples across nine sensor channels. We describe the data acquisition setup, preprocessing, and calibration procedures (six-orientation accelerometer calibration, stationary gyroscope bias estimation, and ellipsoid fitting for magnetometer hard-/soft-iron correction), and provide quantitative calibration results. IF-D is designed to mitigate platform specific motion bias and expose models to both physical dynamics and typical measurement noise, thereby facilitating robust representation learning and downstream tasks such as event detection, motion mode recognition, and inertial navigation.",
    "pdf_url": "https://arxiv.org/pdf/2510.09539v1",
    "github_url": null,
    "published": "2025-10-10T16:50:57+00:00",
    "updated": "2025-10-10T16:50:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09529v1",
    "title": "Self-Resetting Soft Ring Enables Autonomous and Continuous Leaping under Uniform Light",
    "authors": [
      "Qi",
      "Zhou",
      "Qing"
    ],
    "summary": "Jumping is an efficient locomotion strategy to traverse cluttered, uneven, or unstable environments in nature, yet replicating continuous, autonomous leaping in soft robots remains challenging due to limited energy storage and reliance on human intervention or latches. Here, we report a millimeter-scale, self-resetting soft ring that achieves repeated vertical and stable horizontal leaps under uniform infrared illumination without external control. The ring-shaped liquid crystal elastomer body twists to store elastic energy, which is suddenly released when a rigid tail strikes the ground, propelling the robot. During the airborne phase, the twisted body autonomously untwists, resetting for the next cycle. By tuning geometric asymmetry and the center of mass, the robot transitions between crawling, directional leaping, and vertical jumping. Optimized configurations yield vertical jumps exceeding 80 body heights and directional horizontal leaps over 3 body lengths. Beyond controlled motion on flat ground, the robot demonstrates resilient and robust locomotion across slopes, parallel hurdles, and diverse cluttered natural terrains including grass, wet sand, and mulch. This work establishes a new paradigm of twisting-enabled, photothermally powered soft robots capable of autonomous, continuous leaping, with potential applications in environmental navigation, swarm robotics, and unstructured terrain navigation.",
    "pdf_url": "https://arxiv.org/pdf/2510.09529v1",
    "github_url": null,
    "published": "2025-10-10T16:43:22+00:00",
    "updated": "2025-10-10T16:43:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09497v1",
    "title": "Autonomous Soft Robotic Guidewire Navigation via Imitation Learning",
    "authors": [
      "Barnes",
      "Kim",
      "Di"
    ],
    "summary": "In endovascular surgery, endovascular interventionists push a thin tube called a catheter, guided by a thin wire to a treatment site inside the patient's blood vessels to treat various conditions such as blood clots, aneurysms, and malformations. Guidewires with robotic tips can enhance maneuverability, but they present challenges in modeling and control. Automation of soft robotic guidewire navigation has the potential to overcome these challenges, increasing the precision and safety of endovascular navigation. In other surgical domains, end-to-end imitation learning has shown promising results. Thus, we develop a transformer-based imitation learning framework with goal conditioning, relative action outputs, and automatic contrast dye injections to enable generalizable soft robotic guidewire navigation in an aneurysm targeting task. We train the model on 36 different modular bifurcated geometries, generating 647 total demonstrations under simulated fluoroscopy, and evaluate it on three previously unseen vascular geometries. The model can autonomously drive the tip of the robot to the aneurysm location with a success rate of 83% on the unseen geometries, outperforming several baselines. In addition, we present ablation and baseline studies to evaluate the effectiveness of each design and data collection choice. Project website: https://softrobotnavigation.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.09497v1",
    "github_url": null,
    "published": "2025-10-10T15:57:09+00:00",
    "updated": "2025-10-10T15:57:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09469v1",
    "title": "Scalable Multi-Agent Path Finding using Collision-Aware Dynamic Alert Mask and a Hybrid Execution Strategy",
    "authors": [
      "Muppasani",
      "Dey",
      "Srivastava"
    ],
    "summary": "Multi-agent pathfinding (MAPF) remains a critical problem in robotics and autonomous systems, where agents must navigate shared spaces efficiently while avoiding conflicts. Traditional centralized algorithms that have global information, such as Conflict-Based Search (CBS), provide high-quality solutions but become computationally expensive in large-scale scenarios due to the combinatorial explosion of conflicts that need resolution. Conversely, distributed approaches that have local information, particularly learning-based methods, offer better scalability by operating with relaxed information availability, yet often at the cost of solution quality. To address these limitations, we propose a hybrid framework that combines decentralized path planning with a lightweight centralized coordinator. Our framework leverages reinforcement learning (RL) for decentralized planning, enabling agents to adapt their planning based on minimal, targeted alerts--such as static conflict-cell flags or brief conflict tracks--that are dynamically shared information from the central coordinator for effective conflict resolution. We empirically study the effect of the information available to an agent on its planning performance. Our approach reduces the inter-agent information sharing compared to fully centralized and distributed methods, while still consistently finding feasible, collision-free solutions--even in large-scale scenarios having higher agent counts.",
    "pdf_url": "https://arxiv.org/pdf/2510.09469v1",
    "github_url": null,
    "published": "2025-10-10T15:25:40+00:00",
    "updated": "2025-10-10T15:25:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09396v1",
    "title": "Bridging Research and Practice in Simulation-based Testing of Industrial Robot Navigation Systems",
    "authors": [
      "Khatiri",
      "Barrientos",
      "Wulf"
    ],
    "summary": "Ensuring robust robotic navigation in dynamic environments is a key challenge, as traditional testing methods often struggle to cover the full spectrum of operational requirements. This paper presents the industrial adoption of Surrealist, a simulation-based test generation framework originally for UAVs, now applied to the ANYmal quadrupedal robot for industrial inspection. Our method uses a search-based algorithm to automatically generate challenging obstacle avoidance scenarios, uncovering failures often missed by manual testing. In a pilot phase, generated test suites revealed critical weaknesses in one experimental algorithm (40.3% success rate) and served as an effective benchmark to prove the superior robustness of another (71.2% success rate). The framework was then integrated into the ANYbotics workflow for a six-month industrial evaluation, where it was used to test five proprietary algorithms. A formal survey confirmed its value, showing it enhances the development process, uncovers critical failures, provides objective benchmarks, and strengthens the overall verification pipeline.",
    "pdf_url": "https://arxiv.org/pdf/2510.09396v1",
    "github_url": null,
    "published": "2025-10-10T13:50:32+00:00",
    "updated": "2025-10-10T13:50:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09388v1",
    "title": "HINT: Helping Ineffective Rollouts Navigate Towards Effectiveness",
    "authors": [
      "Wang",
      "Han",
      "Jiang"
    ],
    "summary": "Reinforcement Learning (RL) has become a key driver for enhancing the long chain-of-thought (CoT) reasoning capabilities of Large Language Models (LLMs). However, prevalent methods like GRPO often fail when task difficulty exceeds the model's capacity, leading to reward sparsity and inefficient training. While prior work attempts to mitigate this using off-policy data, such as mixing RL with Supervised Fine-Tuning (SFT) or using hints, they often misguide policy updates In this work, we identify a core issue underlying these failures, which we term low training affinity. This condition arises from a large distributional mismatch between external guidance and the model's policy. To diagnose this, we introduce Affinity, the first quantitative metric for monitoring exploration efficiency and training stability. To improve Affinity, we propose HINT: Helping Ineffective rollouts Navigate Towards effectiveness, an adaptive hinting framework. Instead of providing direct answers, HINT supplies heuristic hints that guide the model to discover solutions on its own, preserving its autonomous reasoning capabilities. Extensive experiments on mathematical reasoning tasks show that HINT consistently outperforms existing methods, achieving state-of-the-art results with models of various scales, while also demonstrating significantly more stable learning and greater data efficiency.Code is available on Github.",
    "pdf_url": "https://arxiv.org/pdf/2510.09388v1",
    "github_url": null,
    "published": "2025-10-10T13:42:03+00:00",
    "updated": "2025-10-10T13:42:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09323v1",
    "title": "Parametrized Topological Complexity for a Multi-Robot System with Variable Tasks",
    "authors": [
      "Dutta",
      "Paul",
      "Sau"
    ],
    "summary": "We study a generalized motion planning problem involving multiple autonomous robots navigating in a $d$-dimensional Euclidean space in the presence of a set of obstacles whose positions are unknown a priori. Each robot is required to visit sequentially a prescribed set of target states, with the number of targets varying between robots. This heterogeneous setting generalizes the framework considered in the prior works on sequential parametrized topological complexity by Farber and the second author of this article. To determine the topological complexity of our problem, we formulate it mathematically by constructing an appropriate fibration. Our main contribution is the determination of this invariant in the generalized setting, which captures the minimal algorithmic instability required for designing collision-free motion planning algorithms under parameter-dependent constraints. We provide a detailed analysis for both odd and even-dimensional ambient spaces, including the essential cohomological computations and explicit constructions of corresponding motion planning algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2510.09323v1",
    "github_url": null,
    "published": "2025-10-10T12:26:26+00:00",
    "updated": "2025-10-10T12:26:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09283v1",
    "title": "Safety Analysis of eVTOL Operations based on STPA",
    "authors": [
      "Elizebeth",
      "Chen",
      "Badaoui"
    ],
    "summary": "Electric Vertical Take-Off and Landing (eVTOL) aircraft are expected to be quieter and more cost-effective than helicopters, offering major economic and social benefits through improved connectivity. Their adoption will require new ground infrastructure and airspace redesign, introducing risks involving multiple stakeholders (Regulators, eVTOL operators, Air navigation service providers, Vertiport operators, OEMs, Pilots, etc.). To assess these risks for the UK airspace, systems-thinking based System Theoretic Process Analysis (STPA) was conducted. To manage the large number of Unsafe Control Actions (UCAs) and requirements generated due to the complexity of the analysis, a novel extension to STPA for the prioritization of results was applied. 317 UCAs were identified in total out of which 110 high-priority UCAs were analyzed (Step-4), resulting in 377 causal factors and 432 requirements. These were prioritized to produce a targeted list of 124 distinct high-priority requirements, 56 of which were identified as gaps in existing aviation regulations, policies, or procedures.. These highlight opportunities for regulatory updates in areas such as organizational performance, certification processes, training, collision avoidance, energy management, and automation. The findings provide regulators with safety considerations that could shape new or updated regulations, compliance methods, and guidance materials for the safe deployment of eVTOLs.",
    "pdf_url": "https://arxiv.org/pdf/2510.09283v1",
    "github_url": null,
    "published": "2025-10-10T11:24:48+00:00",
    "updated": "2025-10-10T11:24:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09250v1",
    "title": "Smart navigation of a gravity-driven glider with adjustable centre-of-mass",
    "authors": [
      "Jiang",
      "Qiu",
      "Gustavsson"
    ],
    "summary": "Artificial gliders are designed to disperse as they settle through a fluid, requiring precise navigation to reach target locations. We show that a compact glider settling in a viscous fluid can navigate by dynamically adjusting its centre-of-mass. Using fully resolved direct numerical simulations (DNS) and reinforcement learning, we find two optimal navigation strategies that allow the glider to reach its target location accurately. These strategies depend sensitively on how the glider interacts with the surrounding fluid. The nature of this interaction changes as the particle Reynolds number Re$_p$ changes. Our results explain how the optimal strategy depends on Re$_p$. At large Re$_p$, the glider learns to tumble rapidly by moving its centre-of-mass as its orientation changes. This generates a large horizontal inertial lift force, which allows the glider to travel far. At small Re$_p$, by contrast, high viscosity hinders tumbling. In this case, the glider learns to adjust its centre-of-mass so that it settles with a steady, inclined orientation that results in a horizontal viscous force. The horizontal range is much smaller than for large Re$_p$, because this viscous force is much smaller than the inertial lift force at large Re$_p$.   *These authors contributed equally.",
    "pdf_url": "https://arxiv.org/pdf/2510.09250v1",
    "github_url": null,
    "published": "2025-10-10T10:41:46+00:00",
    "updated": "2025-10-10T10:41:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09221v1",
    "title": "HANDO: Hierarchical Autonomous Navigation and Dexterous Omni-loco-manipulation",
    "authors": [
      "Sun",
      "Wang",
      "Zhang"
    ],
    "summary": "Seamless loco-manipulation in unstructured environments requires robots to leverage autonomous exploration alongside whole-body control for physical interaction. In this work, we introduce HANDO (Hierarchical Autonomous Navigation and Dexterous Omni-loco-manipulation), a two-layer framework designed for legged robots equipped with manipulators to perform human-centered mobile manipulation tasks. The first layer utilizes a goal-conditioned autonomous exploration policy to guide the robot to semantically specified targets, such as a black office chair in a dynamic environment. The second layer employs a unified whole-body loco-manipulation policy to coordinate the arm and legs for precise interaction tasks-for example, handing a drink to a person seated on the chair. We have conducted an initial deployment of the navigation module, and will continue to pursue finer-grained deployment of whole-body loco-manipulation.",
    "pdf_url": "https://arxiv.org/pdf/2510.09221v1",
    "github_url": null,
    "published": "2025-10-10T10:04:30+00:00",
    "updated": "2025-10-10T10:04:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09188v1",
    "title": "Decentralized Multi-Robot Relative Navigation in Unknown, Structurally Constrained Environments under Limited Communication",
    "authors": [
      "Mao",
      "Wang",
      "Ji"
    ],
    "summary": "Multi-robot navigation in unknown, structurally constrained, and GPS-denied environments presents a fundamental trade-off between global strategic foresight and local tactical agility, particularly under limited communication. Centralized methods achieve global optimality but suffer from high communication overhead, while distributed methods are efficient but lack the broader awareness to avoid deadlocks and topological traps. To address this, we propose a fully decentralized, hierarchical relative navigation framework that achieves both strategic foresight and tactical agility without a unified coordinate system. At the strategic layer, robots build and exchange lightweight topological maps upon opportunistic encounters. This process fosters an emergent global awareness, enabling the planning of efficient, trap-avoiding routes at an abstract level. This high-level plan then inspires the tactical layer, which operates on local metric information. Here, a sampling-based escape point strategy resolves dense spatio-temporal conflicts by generating dynamically feasible trajectories in real time, concurrently satisfying tight environmental and kinodynamic constraints. Extensive simulations and real-world experiments demonstrate that our system significantly outperforms in success rate and efficiency, especially in communication-limited environments with complex topological structures.",
    "pdf_url": "https://arxiv.org/pdf/2510.09188v1",
    "github_url": null,
    "published": "2025-10-10T09:33:20+00:00",
    "updated": "2025-10-10T09:33:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09153v1",
    "title": "Euclid preparation. Cosmology Likelihood for Observables in Euclid (CLOE). 3. Inference and Forecasts",
    "authors": [
      "Collaboration",
      "Cañas-Herrera",
      "Goh"
    ],
    "summary": "The Euclid mission aims to measure the positions, shapes, and redshifts of over a billion galaxies to provide unprecedented constraints on the nature of dark matter and dark energy. Achieving this goal requires a continuous reassessment of the mission's scientific performance, particularly in terms of its ability to constrain cosmological parameters, as our understanding of how to model large-scale structure observables improves. In this study, we present the first scientific forecasts using CLOE (Cosmology Likelihood for Observables in Euclid), a dedicated Euclid cosmological pipeline developed to support this endeavour. Using advanced Bayesian inference techniques applied to synthetic Euclid-like data, we sample the posterior distribution of cosmological and nuisance parameters across a variety of cosmological models and Euclid primary probes: cosmic shear, angular photometric galaxy clustering, galaxy-galaxy lensing, and spectroscopic galaxy clustering. We validate the capability of CLOE to produce reliable cosmological forecasts, showcasing Euclid's potential to achieve a figure of merit for the dark energy parameters $w_0$ and $w_a$ exceeding 400 when combining all primary probes. Furthermore, we illustrate the behaviour of the posterior probability distribution of the parameters of interest given different priors and scale cuts. Finally, we emphasise the importance of addressing computational challenges, proposing further exploration of innovative data science techniques to efficiently navigate the Euclid high-dimensional parameter space in upcoming cosmological data releases.",
    "pdf_url": "https://arxiv.org/pdf/2510.09153v1",
    "github_url": null,
    "published": "2025-10-10T08:57:09+00:00",
    "updated": "2025-10-10T08:57:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09144v2",
    "title": "Online Topological Localization for Navigation Assistance in Bronchoscopy",
    "authors": [
      "Tomasini",
      "Riazuelo",
      "Murillo"
    ],
    "summary": "Video bronchoscopy is a fundamental procedure in respiratory medicine, where medical experts navigate through the bronchial tree of a patient to diagnose or operate the patient. Surgeons need to determine the position of the scope as they go through the airway until they reach the area of interest. This task is very challenging for practitioners due to the complex bronchial tree structure and varying doctor experience and training. Navigation assistance to locate the bronchoscope during the procedure can improve its outcome. Currently used techniques for navigational guidance commonly rely on previous CT scans of the patient to obtain a 3D model of the airway, followed by tracking of the scope with additional sensors or image registration. These methods obtain accurate locations but imply additional setup, scans and training. Accurate metric localization is not always required, and a topological localization with regard to a generic airway model can often suffice to assist the surgeon with navigation. We present an image-based bronchoscopy topological localization pipeline to provide navigation assistance during the procedure, with no need of patient CT scan. Our approach is trained only on phantom data, eliminating the high cost of real data labeling, and presents good generalization capabilities. The results obtained surpass existing methods, particularly on real data test sequences.",
    "pdf_url": "https://arxiv.org/pdf/2510.09144v2",
    "github_url": null,
    "published": "2025-10-10T08:44:39+00:00",
    "updated": "2025-10-14T11:40:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09136v1",
    "title": "Controlled Personalization in Legacy Media Online Services: A Case Study in News Recommendation",
    "authors": [
      "Holzleitner",
      "Leitner",
      "Jorgensen"
    ],
    "summary": "Personalized news recommendations have become a standard feature of large news aggregation services, optimizing user engagement through automated content selection. In contrast, legacy news media often approach personalization cautiously, striving to balance technological innovation with core editorial values. As a result, online platforms of traditional news outlets typically combine editorially curated content with algorithmically selected articles - a strategy we term controlled personalization. In this industry paper, we evaluate the effectiveness of controlled personalization through an A/B test conducted on the website of a major Norwegian legacy news organization. Our findings indicate that even a modest level of personalization yields substantial benefits. Specifically, we observe that users exposed to personalized content demonstrate higher click-through rates and reduced navigation effort, suggesting improved discovery of relevant content. Moreover, our analysis reveals that controlled personalization contributes to greater content diversity and catalog coverage and in addition reduces popularity bias. Overall, our results suggest that controlled personalization can successfully align user needs with editorial goals, offering a viable path for legacy media to adopt personalization technologies while upholding journalistic values.",
    "pdf_url": "https://arxiv.org/pdf/2510.09136v1",
    "github_url": null,
    "published": "2025-10-10T08:37:13+00:00",
    "updated": "2025-10-10T08:37:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09089v1",
    "title": "Robust Visual Teach-and-Repeat Navigation with Flexible Topo-metric Graph Map Representation",
    "authors": [
      "Wang",
      "Cheng",
      "Wang"
    ],
    "summary": "Visual Teach-and-Repeat Navigation is a direct solution for mobile robot to be deployed in unknown environments. However, robust trajectory repeat navigation still remains challenged due to environmental changing and dynamic objects. In this paper, we propose a novel visual teach-and-repeat navigation system, which consists of a flexible map representation, robust map matching and a map-less local navigation module. During the teaching process, the recorded keyframes are formulated as a topo-metric graph and each node can be further extended to save new observations. Such representation also alleviates the requirement of globally consistent mapping. To enhance the place recognition performance during repeating process, instead of using frame-to-frame matching, we firstly implement keyframe clustering to aggregate similar connected keyframes into local map and perform place recognition based on visual frame-tolocal map matching strategy. To promote the local goal persistent tracking performance, a long-term goal management algorithm is constructed, which can avoid the robot getting lost due to environmental changes or obstacle occlusion. To achieve the goal without map, a local trajectory-control candidate optimization algorithm is proposed. Extensively experiments are conducted on our mobile platform. The results demonstrate that our system is superior to the baselines in terms of robustness and effectiveness.",
    "pdf_url": "https://arxiv.org/pdf/2510.09089v1",
    "github_url": null,
    "published": "2025-10-10T07:35:37+00:00",
    "updated": "2025-10-10T07:35:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.13829v1",
    "title": "A Linguistics-Aware LLM Watermarking via Syntactic Predictability",
    "authors": [
      "Park",
      "Park",
      "Ahn"
    ],
    "summary": "As large language models (LLMs) continue to advance rapidly, reliable governance tools have become critical. Publicly verifiable watermarking is particularly essential for fostering a trustworthy AI ecosystem. A central challenge persists: balancing text quality against detection robustness. Recent studies have sought to navigate this trade-off by leveraging signals from model output distributions (e.g., token-level entropy); however, their reliance on these model-specific signals presents a significant barrier to public verification, as the detection process requires access to the logits of the underlying model. We introduce STELA, a novel framework that aligns watermark strength with the linguistic degrees of freedom inherent in language. STELA dynamically modulates the signal using part-of-speech (POS) n-gram-modeled linguistic indeterminacy, weakening it in grammatically constrained contexts to preserve quality and strengthen it in contexts with greater linguistic flexibility to enhance detectability. Our detector operates without access to any model logits, thus facilitating publicly verifiable detection. Through extensive experiments on typologically diverse languages-analytic English, isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior methods in detection robustness. Our code is available at https://github.com/Shinwoo-Park/stela_watermark.",
    "pdf_url": "https://arxiv.org/pdf/2510.13829v1",
    "github_url": "https://github.com/Shinwoo-Park/stela_watermark",
    "published": "2025-10-10T07:26:15+00:00",
    "updated": "2025-10-10T07:26:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.21751v1",
    "title": "Real-time Mixed-Integer Quadratic Programming for Driving Behavior-Inspired Speed Bump Optimal Trajectory Planning",
    "authors": [
      "Dinh",
      "Phan",
      "Dang"
    ],
    "summary": "This paper proposes a novel methodology for trajectory planning in autonomous vehicles (AVs), addressing the complex challenge of negotiating speed bumps within a unified Mixed-Integer Quadratic Programming (MIQP) framework. By leveraging Model Predictive Control (MPC), we develop trajectories that optimize both the traversal of speed bumps and overall passenger comfort. A key contribution of this work is the formulation of speed bump handling constraints that closely emulate human driving behavior, seamlessly integrating these with broader road navigation requirements. Through extensive simulations in varied urban driving environments, we demonstrate the efficacy of our approach, highlighting its ability to ensure smooth speed transitions over speed bumps while maintaining computational efficiency suitable for real-time deployment. The method's capability to handle both static road features and dynamic constraints, alongside expert human driving, represents a significant step forward in trajectory planning for urban",
    "pdf_url": "https://arxiv.org/pdf/2510.21751v1",
    "github_url": null,
    "published": "2025-10-10T07:15:23+00:00",
    "updated": "2025-10-10T07:15:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.09010v1",
    "title": "HERO: Hardware-Efficient RL-based Optimization Framework for NeRF Quantization",
    "authors": [
      "Zhang",
      "Ma",
      "Ge"
    ],
    "summary": "Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction method, delivering high-quality results for AR/VR applications. While quantization methods and hardware accelerators have been proposed to enhance NeRF's computational efficiency, existing approaches face crucial limitations. Current quantization methods operate without considering hardware architecture, resulting in sub-optimal solutions within the vast design space encompassing accuracy, latency, and model size. Additionally, existing NeRF accelerators heavily rely on human experts to explore this design space, making the optimization process time-consuming, inefficient, and unlikely to discover optimal solutions. To address these challenges, we introduce HERO, a reinforcement learning framework performing hardware-aware quantization for NeRF. Our framework integrates a NeRF accelerator simulator to generate real-time hardware feedback, enabling fully automated adaptation to hardware constraints. Experimental results demonstrate that HERO achieves 1.31-1.33 $\\times$ better latency, 1.29-1.33 $\\times$ improved cost efficiency, and a more compact model size compared to CAQ, a previous state-of-the-art NeRF quantization framework. These results validate our framework's capability to effectively navigate the complex design space between hardware and algorithm requirements, discovering superior quantization policies for NeRF implementation. Code is available at https://github.com/ypzhng/HERO.",
    "pdf_url": "https://arxiv.org/pdf/2510.09010v1",
    "github_url": "https://github.com/ypzhng/HERO",
    "published": "2025-10-10T05:21:24+00:00",
    "updated": "2025-10-10T05:21:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08854v1",
    "title": "Optimal Control with Lyapunov Stability Guarantees for Space Applications",
    "authors": [
      "Abhijeet",
      "Mohamed",
      "Sharma"
    ],
    "summary": "This paper investigates the infinite horizon optimal control problem (OCP) for space applications characterized by nonlinear dynamics. The proposed approach divides the problem into a finite horizon OCP with a regularized terminal cost, guiding the system towards a terminal set, and an infinite horizon linear regulation phase within this set. This strategy guarantees global asymptotic stability under specific assumptions. Our method maintains the system's fully nonlinear dynamics until it reaches the terminal set, where the system dynamics is linearized. As the terminal set converges to the origin, the difference in optimal cost incurred reduces to zero, guaranteeing an efficient and stable solution. The approach is tested through simulations on three problems: spacecraft attitude control, rendezvous maneuver, and soft landing. In spacecraft attitude control, we focus on achieving precise orientation and stabilization. For rendezvous maneuvers, we address the navigation of a chaser to meet a target spacecraft. For the soft landing problem, we ensure a controlled descent and touchdown on a planetary surface. We provide numerical results confirming the effectiveness of the proposed method in managing these nonlinear dynamics problems, offering robust solutions essential for successful space missions.",
    "pdf_url": "https://arxiv.org/pdf/2510.08854v1",
    "github_url": null,
    "published": "2025-10-09T23:06:59+00:00",
    "updated": "2025-10-09T23:06:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08825v1",
    "title": "Search-on-Graph: Iterative Informed Navigation for Large Language Model Reasoning on Knowledge Graphs",
    "authors": [
      "Sun",
      "Yu",
      "Gotti"
    ],
    "summary": "Large language models (LLMs) have demonstrated impressive reasoning abilities yet remain unreliable on knowledge-intensive, multi-hop questions -- they miss long-tail facts, hallucinate when uncertain, and their internal knowledge lags behind real-world change. Knowledge graphs (KGs) offer a structured source of relational evidence, but existing KGQA methods face fundamental trade-offs: compiling complete SPARQL queries without knowing available relations proves brittle, retrieving large subgraphs introduces noise, and complex agent frameworks with parallel exploration exponentially expand search spaces. To address these limitations, we propose Search-on-Graph (SoG), a simple yet effective framework that enables LLMs to perform iterative informed graph navigation using a single, carefully designed \\textsc{Search} function. Rather than pre-planning paths or retrieving large subgraphs, SoG follows an ``observe-then-navigate'' principle: at each step, the LLM examines actual available relations from the current entity before deciding on the next hop. This approach further adapts seamlessly to different KG schemas and handles high-degree nodes through adaptive filtering. Across six KGQA benchmarks spanning Freebase and Wikidata, SoG achieves state-of-the-art performance without fine-tuning. We demonstrate particularly strong gains on Wikidata benchmarks (+16\\% improvement over previous best methods) alongside consistent improvements on Freebase benchmarks.",
    "pdf_url": "https://arxiv.org/pdf/2510.08825v1",
    "github_url": null,
    "published": "2025-10-09T21:20:16+00:00",
    "updated": "2025-10-09T21:20:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08730v1",
    "title": "How Reliable is Language Model Micro-Benchmarking?",
    "authors": [
      "Yauney",
      "Warraich",
      "Swayamdipta"
    ],
    "summary": "Micro-benchmarking offers a solution to the often prohibitive time and cost of language model development: evaluate on a very small subset of existing benchmarks. Can these micro-benchmarks, however, rank models as consistently as the full benchmarks they replace? And can they rank models more consistently than selecting a random subset of data points? In many scenarios, we find that the answer is no. We introduce a meta-evaluation measure for micro-benchmarking which investigates how well a micro-benchmark can rank two models as a function of their performance difference on the full benchmark. This approach can determine which model pairs can be ranked correctly by a micro-benchmark, allowing for a finer-grained analysis of the trade-off between micro-benchmark size and reliability. Prior work has suggested selecting as few as 10 examples; we find that no micro-benchmarking method can consistently rank model pairs 3.5 points of accuracy apart on MMLU-Pro or 4 points apart on BIG-bench Hard. In order to consistently rank model pairs with relatively similar performances, we show that often as many as 250 examples must be selected, at which point random sampling is competitive with existing micro-benchmarking methods. When comparing only 8B instruction-tuned models on MMLU-Pro micro-benchmarks with 25 examples, we find that more than half of pairwise comparisons are not likely to be preserved. Our work provides actionable guidance for both micro-benchmark users and developers in navigating the trade-off between evaluation efficiency and reliability.",
    "pdf_url": "https://arxiv.org/pdf/2510.08730v1",
    "github_url": null,
    "published": "2025-10-09T18:37:03+00:00",
    "updated": "2025-10-09T18:37:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08713v1",
    "title": "Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation",
    "authors": [
      "Dong",
      "Wu",
      "Chen"
    ],
    "summary": "Enabling embodied agents to effectively imagine future states is critical for robust and generalizable visual navigation. Current state-of-the-art approaches, however, adopt modular architectures that separate navigation planning from visual world modeling, leading to state-action misalignment and limited adaptability in novel or dynamic scenarios. To overcome this fundamental limitation, we propose UniWM, a unified, memory-augmented world model integrating egocentric visual foresight and planning within a single multimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly grounds action decisions in visually imagined outcomes, ensuring tight alignment between prediction and control. A hierarchical memory mechanism further integrates detailed short-term perceptual cues with longer-term trajectory context, enabling stable, coherent reasoning over extended horizons. Extensive experiments across four challenging benchmarks (Go Stanford, ReCon, SCAND, HuRoN) demonstrate that UniWM substantially improves navigation success rates by up to 30%, significantly reduces trajectory errors compared to strong baselines, and exhibits impressive zero-shot generalization on the unseen TartanDrive dataset. These results highlight UniWM as a principled step toward unified, imagination-driven embodied navigation.",
    "pdf_url": "https://arxiv.org/pdf/2510.08713v1",
    "github_url": null,
    "published": "2025-10-09T18:18:11+00:00",
    "updated": "2025-10-09T18:18:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08553v1",
    "title": "Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation",
    "authors": [
      "Xu",
      "Pan",
      "Liu"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.",
    "pdf_url": "https://arxiv.org/pdf/2510.08553v1",
    "github_url": "https://github.com/xyz9911/Memoir",
    "published": "2025-10-09T17:58:01+00:00",
    "updated": "2025-10-09T17:58:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08521v1",
    "title": "FlowSearch: Advancing deep research with dynamic structured knowledge flow",
    "authors": [
      "Hu",
      "Ma",
      "Fan"
    ],
    "summary": "Deep research is an inherently challenging task that demands both breadth and depth of thinking. It involves navigating diverse knowledge spaces and reasoning over complex, multi-step dependencies, which presents substantial challenges for agentic systems. To address this, we propose FlowSearch, a multi-agent framework that actively constructs and evolves a dynamic structured knowledge flow to drive subtask execution and reasoning. FlowSearch is capable of strategically planning and expanding the knowledge flow to enable parallel exploration and hierarchical task decomposition, while also adjusting the knowledge flow in real time based on feedback from intermediate reasoning outcomes and insights. FlowSearch achieves state-of-the-art performance on both general and scientific benchmarks, including GAIA, HLE, GPQA and TRQA, demonstrating its effectiveness in multi-disciplinary research scenarios and its potential to advance scientific discovery. The code is available at https://github.com/Alpha-Innovator/InternAgent.",
    "pdf_url": "https://arxiv.org/pdf/2510.08521v1",
    "github_url": "https://github.com/Alpha-Innovator/InternAgent",
    "published": "2025-10-09T17:48:12+00:00",
    "updated": "2025-10-09T17:48:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08511v1",
    "title": "AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents",
    "authors": [
      "Du",
      "Yan",
      "Jiang"
    ],
    "summary": "Large language models (LLMs) have shown impressive performance in general programming tasks. However, in Machine Learning Engineering (MLE) scenarios such as AutoML and Kaggle competitions, achieving high performance depends heavily on expert intervention and repeated adjustments rather than simply generating correct code. When applied directly to these tasks, LLMs often lack fine-grained domain priors, and existing MLE approaches that use linear or tree-structured searches limit knowledge transfer to adjacent hierarchical links. As a result, they cannot leverage past full trajectories or share information across branches, limiting self-evolving ability and search space diversity. To address these limitations, we introduce AutoMLGen, an LLM-based coding agent that integrates a domain knowledge base for high-quality prior guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS retains the tree-guided exploration of MCTS while embedding a graph structure into the expansion stage to enable dynamic path reorganization, historical trajectory reuse, and multi-solution fusion to support both self-evolution and collaborative learning. Combined with fine-grained operator sets, this design improves stability and accelerates convergence. Evaluation on the MLE-Bench shows that AutoMLGen achieves state-of-the-art performance in numerous dimensions, such as the average medal rate and the valid submission rate, under a 12-hour budget (half the standard runtime). The code is available at https://github.com/Alpha-Innovator/InternAgent.",
    "pdf_url": "https://arxiv.org/pdf/2510.08511v1",
    "github_url": "https://github.com/Alpha-Innovator/InternAgent",
    "published": "2025-10-09T17:45:05+00:00",
    "updated": "2025-10-09T17:45:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08464v1",
    "title": "Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered",
    "authors": [
      "Jabbour",
      "Kim",
      "Smith"
    ],
    "summary": "Vision-Language-Action (VLA) models have advanced robotic capabilities but remain challenging to deploy on resource-limited hardware. Pruning has enabled efficient compression of large language models (LLMs), yet it is largely understudied in robotics. Surprisingly, we observe that pruning VLA models leads to drastic degradation and increased safety violations. We introduce GLUESTICK, a post-pruning recovery method that restores much of the original model's functionality while retaining sparsity benefits. Our method performs a one-time interpolation between the dense and pruned models in weight-space to compute a corrective term. This correction is used during inference by each pruned layer to recover lost capabilities with minimal overhead. GLUESTICK requires no additional training, is agnostic to the pruning algorithm, and introduces a single hyperparameter that controls the tradeoff between efficiency and accuracy. Across diverse VLA architectures and tasks in manipulation and navigation, GLUESTICK achieves competitive memory efficiency while substantially recovering success rates and reducing safety violations. Additional material can be found at: https://gluestick-vla.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2510.08464v1",
    "github_url": null,
    "published": "2025-10-09T17:07:30+00:00",
    "updated": "2025-10-09T17:07:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08435v1",
    "title": "Navigating Sparsities in High-Dimensional Linear Contextual Bandits",
    "authors": [
      "Zhao",
      "Chen",
      "Zheng"
    ],
    "summary": "High-dimensional linear contextual bandit problems remain a significant challenge due to the curse of dimensionality. Existing methods typically consider either the model parameters to be sparse or the eigenvalues of context covariance matrices to be (approximately) sparse, lacking general applicability due to the rigidity of conventional reward estimators. To overcome this limitation, a powerful pointwise estimator is introduced in this work that adaptively navigates both kinds of sparsity. Based on this pointwise estimator, a novel algorithm, termed HOPE, is proposed. Theoretical analyses demonstrate that HOPE not only achieves improved regret bounds in previously discussed homogeneous settings (i.e., considering only one type of sparsity) but also, for the first time, efficiently handles two new challenging heterogeneous settings (i.e., considering a mixture of two types of sparsity), highlighting its flexibility and generality. Experiments corroborate the superiority of HOPE over existing methods across various scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2510.08435v1",
    "github_url": null,
    "published": "2025-10-09T16:47:14+00:00",
    "updated": "2025-10-09T16:47:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08340v1",
    "title": "Impact of protein corona morphology on nanoparticle diffusion in biological fluids: insights from a mesoscale approach",
    "authors": [
      "Cipriani",
      "Lopez"
    ],
    "summary": "Nanoparticles (NPs) demonstrate considerable potential in medical applications, including targeted drug delivery and diagnostic probes. However, their efficacy depends on their ability to navigate through the complex biological environments inside living organisms. In such environments, NPs interact with a dense mixture of biomolecules, which can reduce their mobility and hinder diffusion. Understanding the factors influencing NP diffusion in these environments is key to improving nanomedicine design and predicting toxicological effects. In this study, we propose a computational approach to model NP diffusion in crowded environments. We introduce a mesoscale model that accounts for the combined effects of the Protein Corona (PC) and the crowded medium on NP movement. By including volume-exclusion interactions and modelling the PC both explicitly and implicitly, we identify key macromolecular descriptors that affect NP diffusion. Our results show that the morphology of the PC can significantly affect the diffusion of NPs, and the role of the occupied volume fraction and the size ratio between tracers and crowders are analysed. The results also show that approximating large macromolecular assemblies with a hydrodynamic single-sphere model leads to inexact diffusion estimates. To overcome the limitations of single-sphere representations, a strategy for an accurate parametrization of NP-PC systems using a single-sphere model is presented.",
    "pdf_url": "https://arxiv.org/pdf/2510.08340v1",
    "github_url": null,
    "published": "2025-10-09T15:26:24+00:00",
    "updated": "2025-10-09T15:26:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08240v1",
    "title": "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety",
    "authors": [
      "Zhang",
      "Wang",
      "Smith"
    ],
    "summary": "Harnessing the power of LLMs requires a delicate dance between being helpful and harmless. This creates a fundamental tension between two competing challenges: vulnerability to adversarial attacks that elicit unsafe content, and a tendency for overrefusal on benign but sensitive prompts. Current approaches often navigate this dance with safeguard models that completely reject any content that contains unsafe portions. This approach cuts the music entirely-it may exacerbate overrefusals and fails to provide nuanced guidance for queries it refuses. To teach models a more coordinated choreography, we propose WaltzRL, a novel multi-agent reinforcement learning framework that formulates safety alignment as a collaborative, positive-sum game. WaltzRL jointly trains a conversation agent and a feedback agent, where the latter is incentivized to provide useful suggestions that improve the safety and helpfulness of the conversation agent's responses. At the core of WaltzRL is a Dynamic Improvement Reward (DIR) that evolves over time based on how well the conversation agent incorporates the feedback. At inference time, unsafe or overrefusing responses from the conversation agent are improved rather than discarded. The feedback agent is deployed together with the conversation agent and only engages adaptively when needed, preserving helpfulness and low latency on safe queries. Our experiments, conducted across five diverse datasets, demonstrate that WaltzRL significantly reduces both unsafe responses (e.g., from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on OR-Bench) compared to various baselines. By enabling the conversation and feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances LLM safety without degrading general capabilities, thereby advancing the Pareto front between helpfulness and harmlessness.",
    "pdf_url": "https://arxiv.org/pdf/2510.08240v1",
    "github_url": null,
    "published": "2025-10-09T14:03:05+00:00",
    "updated": "2025-10-09T14:03:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08184v1",
    "title": "Satellite Navigation and Control using Physics-Informed Artificial Potential Field and Sliding Mode Controller",
    "authors": [
      "Sahoo",
      "Choudhary",
      "Sinha"
    ],
    "summary": "Increase in the number of space exploration missions has led to the accumulation of space debris, posing risk of collision with the operational satellites. Addressing this challenge is crucial for the sustainability of space operations. To plan a safe trajectory in the presence of moving space debris, an integrated approach of artificial potential field and sliding mode controller is proposed and implemented in this paper. The relative 6-DOF kinematics and dynamics of the spacecraft is modelled in the framework of geometric mechanics with the relative configuration expressed through exponential coordinates. Various collision avoidance guidance algorithms have been proposed in the literature but the Artificial Potential Field guidance algorithm is computationally efficient and enables real-time path adjustments to avoid collision with obstacles. However, it is prone to issues such as local minima. In literature, local minima issue is typically avoided by either redefining the potential function such as adding vorticity or by employing search techniques which are computationally expensive. To address these challenges, a physics-informed APF is proposed in this paper where Hamiltonian mechanics is used instead of the traditional Newtonian mechanics-based approach. In this approach, instead of relying on attractive and repulsive forces for path planning, the Hamiltonian approach uses the potential field to define a path of minimum potential. Additionally, to track the desired trajectory planned by the guidance algorithm within a fixed-time frame, a non-singular fixed-time sliding mode controller (FTSMC) is used. The proposed fixed-time sliding surface not only ensures fixed-time convergence of system states but also guarantees the global stability of the closed-loop system without singularity. The simulation results presented support the claims made.",
    "pdf_url": "https://arxiv.org/pdf/2510.08184v1",
    "github_url": null,
    "published": "2025-10-09T13:09:18+00:00",
    "updated": "2025-10-09T13:09:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08173v1",
    "title": "NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions",
    "authors": [
      "Yang",
      "Long",
      "Yu"
    ],
    "summary": "Instruction-following navigation is a key step toward embodied intelligence. Prior benchmarks mainly focus on semantic understanding but overlook systematically evaluating navigation agents' spatial perception and reasoning capabilities. In this work, we introduce the NavSpace benchmark, which contains six task categories and 1,228 trajectory-instruction pairs designed to probe the spatial intelligence of navigation agents. On this benchmark, we comprehensively evaluate 22 navigation agents, including state-of-the-art navigation models and multimodal large language models. The evaluation results lift the veil on spatial intelligence in embodied navigation. Furthermore, we propose SNav, a new spatially intelligent navigation model. SNav outperforms existing navigation agents on NavSpace and real robot tests, establishing a strong baseline for future work.",
    "pdf_url": "https://arxiv.org/pdf/2510.08173v1",
    "github_url": null,
    "published": "2025-10-09T12:59:19+00:00",
    "updated": "2025-10-09T12:59:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08161v1",
    "title": "Attitude and Heading Estimation in Symmetrical Inertial Arrays",
    "authors": [
      "Libero",
      "Klein"
    ],
    "summary": "Attitude and heading reference systems (AHRS) play a central role in autonomous navigation systems on land, air and maritime platforms. AHRS utilize inertial sensor measurements to estimate platform orientation. In recent years, there has been increasing interest in multiple inertial measurement units (MIMU) arrays to improve navigation accuracy and robustness. A particularly challenging MIMU implementation is the gyro-free (GF) configuration, in which angular velocity is derived solely from accelerometer measurements. While the GF configurations have multiple benefits, including outlier detection and in angular acceleration measurements, their main drawbacks are inherent instability and an increased divergence rate. To address these shortcomings, we introduce a novel symmetrical MIMU formulation, in which the IMUs are arranged in symmetric diagonal pairs to decouple linear and rotational acceleration components. To this end, we derive the theoretical foundations for the symmetrical MIMU formulation of the GF equations, develop a nonlinear least squares estimation process, and integrate statistical hypothesis testing into an AHRS error-state extended Kalman filter. We validate our approach using real-world datasets containing 85 minutes of navigation data recorded on both airborne and land platforms. Our results demonstrated a 30\\% average reduction in attitude estimation errors, rotation detection accuracy exceeding 95\\% improvement, and significantly improved stability compared to a standard GF implementation. These results enable reliable GF navigation in applications where gyroscopes are unavailable, unreliable, or energy-constrained. Common examples include miniature platforms, computational-constraint platforms, and long-endurance marine platforms.",
    "pdf_url": "https://arxiv.org/pdf/2510.08161v1",
    "github_url": null,
    "published": "2025-10-09T12:46:28+00:00",
    "updated": "2025-10-09T12:46:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08072v1",
    "title": "When Light Bends to the Collective Will: A Theory and Vision for Adaptive Photonic Scale-up Domains",
    "authors": [
      "Addanki"
    ],
    "summary": "As chip-to-chip silicon photonics gain traction for their bandwidth and energy efficiency, collective communication has emerged as a critical bottleneck in scale-up systems. Programmable photonic interconnects offer a promising path forward: by dynamically reconfiguring the fabric, they can establish direct, high-bandwidth optical paths between communicating endpoints -- \\emph{synchronously and guided by the structure of collective operations} (e.g., AllReduce). However, realizing this vision -- \\emph{when light bends to the collective will} -- requires navigating a fundamental trade-off between reconfiguration delay and the performance gains of adaptive topologies.   In this paper, we present a simple theoretical framework for adaptive photonic scale-up domains that makes this trade-off explicit and clarifies when reconfiguration is worthwhile. Along the way, we highlight a connection -- not surprising but still powerful -- between the Birkhoff--von Neumann (BvN) decomposition, maximum concurrent flow (a classic measure of network throughput), and the well-known $α$-$β$ cost model for collectives. Finally, we outline a research agenda in algorithm design and systems integration that can build on this foundation.",
    "pdf_url": "https://arxiv.org/pdf/2510.08072v1",
    "github_url": null,
    "published": "2025-10-09T10:59:27+00:00",
    "updated": "2025-10-09T10:59:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.08000v1",
    "title": "DemandCast: Global hourly electricity demand forecasting",
    "authors": [
      "Steijn",
      "Goli",
      "Antonini"
    ],
    "summary": "This paper presents a machine learning framework for electricity demand forecasting across diverse geographical regions using the gradient boosting algorithm XGBoost. The model integrates historical electricity demand and comprehensive weather and socioeconomic variables to predict normalized electricity demand profiles. To enable robust training and evaluation, we developed a large-scale dataset spanning multiple years and countries, applying a temporal data-splitting strategy that ensures benchmarking of out-of-sample performance. Our approach delivers accurate and scalable demand forecasts, providing valuable insights for energy system planners and policymakers as they navigate the challenges of the global energy transition.",
    "pdf_url": "https://arxiv.org/pdf/2510.08000v1",
    "github_url": null,
    "published": "2025-10-09T09:39:06+00:00",
    "updated": "2025-10-09T09:39:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07988v1",
    "title": "ReInAgent: A Context-Aware GUI Agent Enabling Human-in-the-Loop Mobile Task Navigation",
    "authors": [
      "Jia",
      "He",
      "Yin"
    ],
    "summary": "Mobile GUI agents exhibit substantial potential to facilitate and automate the execution of user tasks on mobile phones. However, exist mobile GUI agents predominantly privilege autonomous operation and neglect the necessity of active user engagement during task execution. This omission undermines their adaptability to information dilemmas including ambiguous, dynamically evolving, and conflicting task scenarios, leading to execution outcomes that deviate from genuine user requirements and preferences. To address these shortcomings, we propose ReInAgent, a context-aware multi-agent framework that leverages dynamic information management to enable human-in-the-loop mobile task navigation. ReInAgent integrates three specialized agents around a shared memory module: an information-managing agent for slot-based information management and proactive interaction with the user, a decision-making agent for conflict-aware planning, and a reflecting agent for task reflection and information consistency validation. Through continuous contextual information analysis and sustained user-agent collaboration, ReInAgent overcomes the limitation of existing approaches that rely on clear and static task assumptions. Consequently, it enables more adaptive and reliable mobile task navigation in complex, real-world scenarios. Experimental results demonstrate that ReInAgent effectively resolves information dilemmas and produces outcomes that are more closely aligned with genuine user preferences. Notably, on complex tasks involving information dilemmas, ReInAgent achieves a 25% higher success rate than Mobile-Agent-v2.",
    "pdf_url": "https://arxiv.org/pdf/2510.07988v1",
    "github_url": null,
    "published": "2025-10-09T09:22:05+00:00",
    "updated": "2025-10-09T09:22:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07987v1",
    "title": "Quantifying Locomotion Differences Between Virtual Reality Users With and Without Motor Impairments",
    "authors": [
      "Franz",
      "Wobbrock"
    ],
    "summary": "Today's virtual reality (VR) systems and environments assume that users have typical abilities, which can make VR inaccessible to people with physical impairments. However, there is not yet an understanding of how inaccessible locomotion techniques are, and which interactions make them inaccessible. To this end, we conducted a study in which people with and without upper-body impairments navigated a virtual environment with six locomotion techniques to quantify performance differences among groups. We found that groups performed similarly with Sliding Looking on all performance measures, suggesting that this might be a good default locomotion technique for VR apps. To understand the nature of performance differences with the other techniques, we collected low-level interaction data from the controllers and headset and analyzed interaction differences with a set of movement-, button-, and target-related metrics. We found that movement-related metrics from headset data reveal differences among groups with all techniques, suggesting these are good metrics for identifying whether a user has an upper-body impairment. We also identify movement-, button, and target-related metrics that can explain performance differences between groups for particular locomotion techniques.",
    "pdf_url": "https://arxiv.org/pdf/2510.07987v1",
    "github_url": null,
    "published": "2025-10-09T09:21:55+00:00",
    "updated": "2025-10-09T09:21:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07974v2",
    "title": "Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning",
    "authors": [
      "Du",
      "Hou",
      "Fu"
    ],
    "summary": "While large language models (LLMs) excel in mathematical and code reasoning, we observe they struggle with social reasoning tasks, exhibiting cognitive confusion, logical inconsistencies, and conflation between objective world states and subjective belief states. Through deteiled analysis of DeepSeek-R1's reasoning trajectories, we find that LLMs frequently encounter reasoning impasses and tend to output contradictory terms like \"tricky\" and \"confused\" when processing scenarios with multiple participants and timelines, leading to erroneous reasoning or infinite loops. The core issue is their inability to disentangle objective reality from agents' subjective beliefs. To address this, we propose an adaptive world model-enhanced reasoning mechanism that constructs a dynamic textual world model to track entity states and temporal sequences. It dynamically monitors reasoning trajectories for confusion indicators and promptly intervenes by providing clear world state descriptions, helping models navigate through cognitive dilemmas. The mechanism mimics how humans use implicit world models to distinguish between external events and internal beliefs. Evaluations on three social benchmarks demonstrate significant improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational costs (up to 33.8% token reduction), offering a simple yet effective solution for deploying LLMs in social contexts.",
    "pdf_url": "https://arxiv.org/pdf/2510.07974v2",
    "github_url": null,
    "published": "2025-10-09T09:07:31+00:00",
    "updated": "2025-10-11T05:57:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07871v3",
    "title": "Learning to Navigate Socially Through Proactive Risk Perception",
    "authors": [
      "Xiao",
      "Zhang",
      "Tang"
    ],
    "summary": "In this report, we describe the technical details of our submission to the IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on developing RGBD-based perception and navigation systems that enable autonomous agents to navigate safely, efficiently, and socially compliantly in dynamic human-populated indoor environments. The challenge requires agents to operate from an egocentric perspective using only onboard sensors including RGB-D observations and odometry, without access to global maps or privileged information, while maintaining social norm compliance such as safe distances and collision avoidance. Building upon the Falcon model, we introduce a Proactive Risk Perception Module to enhance social navigation performance. Our approach augments Falcon with collision risk understanding that learns to predict distance-based collision risk scores for surrounding humans, which enables the agent to develop more robust spatial awareness and proactive collision avoidance behaviors. The evaluation on the Social-HM3D benchmark demonstrates that our method improves the agent's ability to maintain personal space compliance while navigating toward goals in crowded indoor scenes with dynamic human agents, achieving 2nd place among 16 participating teams in the challenge.",
    "pdf_url": "https://arxiv.org/pdf/2510.07871v3",
    "github_url": null,
    "published": "2025-10-09T07:22:12+00:00",
    "updated": "2025-11-07T07:41:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07869v3",
    "title": "USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots",
    "authors": [
      "Gu",
      "Wu",
      "Si"
    ],
    "summary": "Underwater environments present unique challenges for robotic operation, including complex hydrodynamics, limited visibility, and constrained communication. Although data-driven approaches have advanced embodied intelligence in terrestrial robots and enabled task-specific autonomous underwater robots, developing underwater intelligence capable of autonomously performing multiple tasks remains highly challenging, as large-scale, high-quality underwater datasets are still scarce. To address these limitations, we introduce USIM, a simulation-based multi-task Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over 561K frames from 1,852 trajectories, totaling approximately 15.6 hours of BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from visual navigation to mobile manipulation. Building upon this dataset, we propose U0, a VLA model for general underwater robots, which integrates binocular vision and other sensor modalities through multimodal fusion, and further incorporates a convolution-attention-based perception focus enhancement module (CAP) to improve spatial understanding and mobile manipulation. Across tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking, the framework achieves a success rate of 80%, while in challenging mobile manipulation tasks, it reduces the distance to the target by 21.2% compared with baseline methods, demonstrating its effectiveness. USIM and U0 show that VLA models can be effectively applied to underwater robotic applications, providing a foundation for scalable dataset construction, improved task autonomy, and the practical realization of intelligent general underwater robots.",
    "pdf_url": "https://arxiv.org/pdf/2510.07869v3",
    "github_url": null,
    "published": "2025-10-09T07:19:29+00:00",
    "updated": "2025-10-15T08:39:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.11729v1",
    "title": "Log-free estimate of the full nonlinearity in the three-dimensional Navier-Stokes equations outside the diagonal regime",
    "authors": [
      "Cherevan"
    ],
    "summary": "We investigate the contribution of the full nonlinearity outside the narrow diagonal zone in the three-dimensional Navier-Stokes equations. We consider the off-diagonal components, including lh, hl, as well as part of the resonant block hh -> l for |xi + eta| >= N^(1-delta). The proof relies on three main elements: (i) six-fold integration by parts in the phase Phi(t,x,xi,eta) = x*(xi + eta) + 4trho1rho2 with respect to (t,rho1,rho2); on the window |t| <= N^(-1/2) the phase Hessian A = nabla^2_(t,rho1,rho2) Phi is non-degenerate and provides a reserve |det A| ~ N^(3/2 - delta); (ii) local Strichartz estimates on cylinders of scale N^(-1/2); in Sec. 4 a strengthened version is used to combine with the decoupling scheme, while the unconditional framework is based on heat reduction (App. D) and globalization (App. E); and (iii) bilinear epsilon-free decoupling in folded geometry of rank 4 (Appendix B), yielding a gain of N^(-1/4) for angular tiles of width N^(-1/2). For the narrow corona, suppression of the null-form type symbol is realized when delta > 1/2; for the block hh -> h with output projection P_N this mechanism is not required and is accounted for separately (see App. E.6). The combined count yields an a priori estimate without logarithmic losses in the norm L^1_t H^-1_x over the whole zone |xi + eta| >= N^(1 - delta) for delta in (1/3, 5/8]; the upper bound is imposed by the stability of the phase reserve |det A| ~ N^(3/2 - delta) >> 1 on the window |t| <= N^(-1/2). The full scheme and navigation through the sections are given in the text.",
    "pdf_url": "https://arxiv.org/pdf/2510.11729v1",
    "github_url": null,
    "published": "2025-10-09T06:38:11+00:00",
    "updated": "2025-10-09T06:38:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07825v1",
    "title": "An LLM-Powered Cooperative Framework for Large-Scale Multi-Vehicle Navigation",
    "authors": [
      "Zhou",
      "Lai",
      "Han"
    ],
    "summary": "The rise of Internet of Vehicles (IoV) technologies is transforming traffic management from isolated control to a collective, multi-vehicle process. At the heart of this shift is multi-vehicle dynamic navigation, which requires simultaneously routing large fleets under evolving traffic conditions. Existing path search algorithms and reinforcement learning methods struggle to scale to city-wide networks, often failing to capture the nonlinear, stochastic, and coupled dynamics of urban traffic. To address these challenges, we propose CityNav, a hierarchical, LLM-powered framework for large-scale multi-vehicle navigation. CityNav integrates a global traffic allocation agent, which coordinates strategic traffic flow distribution across regions, with local navigation agents that generate locally adaptive routes aligned with global directives. To enable effective cooperation, we introduce a cooperative reasoning optimization mechanism, in which agents are jointly trained with a dual-reward structure: individual rewards promote per-vehicle efficiency, while shared rewards encourage network-wide coordination and congestion reduction. Extensive experiments on four real-world road networks of varying scales (up to 1.6 million roads and 430,000 intersections) and traffic datasets demonstrate that CityNav consistently outperforms nine classical path search and RL-based baselines in city-scale travel efficiency and congestion mitigation. Our results highlight the potential of LLMs to enable scalable, adaptive, and cooperative city-wide traffic navigation, providing a foundation for intelligent, large-scale vehicle routing in complex urban environments. Our project is available at https://github.com/usail-hkust/CityNav.",
    "pdf_url": "https://arxiv.org/pdf/2510.07825v1",
    "github_url": "https://github.com/usail-hkust/CityNav",
    "published": "2025-10-09T06:14:29+00:00",
    "updated": "2025-10-09T06:14:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07813v1",
    "title": "Strategic Communication under Threat: Learning Information Trade-offs in Pursuit-Evasion Games",
    "authors": [
      "Gatta",
      "Mutzari",
      "Kraus"
    ],
    "summary": "Adversarial environments require agents to navigate a key strategic trade-off: acquiring information enhances situational awareness, but may simultaneously expose them to threats. To investigate this tension, we formulate a PursuitEvasion-Exposure-Concealment Game (PEEC) in which a pursuer agent must decide when to communicate in order to obtain the evader's position. Each communication reveals the pursuer's location, increasing the risk of being targeted. Both agents learn their movement policies via reinforcement learning, while the pursuer additionally learns a communication policy that balances observability and risk. We propose SHADOW (Strategic-communication Hybrid Action Decision-making under partial Observation for Warfare), a multi-headed sequential reinforcement learning framework that integrates continuous navigation control, discrete communication actions, and opponent modeling for behavior prediction. Empirical evaluations show that SHADOW pursuers achieve higher success rates than six competitive baselines. Our ablation study confirms that temporal sequence modeling and opponent modeling are critical for effective decision-making. Finally, our sensitivity analysis reveals that the learned policies generalize well across varying communication risks and physical asymmetries between agents.",
    "pdf_url": "https://arxiv.org/pdf/2510.07813v1",
    "github_url": null,
    "published": "2025-10-09T05:44:00+00:00",
    "updated": "2025-10-09T05:44:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07802v1",
    "title": "Learning to steer quantum many-body dynamics with tree optimization",
    "authors": [
      "Zhang",
      "Peng",
      "Wang"
    ],
    "summary": "High-quality control over complex quantum systems is a key to achieving practical quantum technologies. However, progress is hindered by the exponential growth of quantum state spaces and the challenges posed by realistic experimental conditions. Here, we present an AI framework that learns to design pulse sequences for optimized quantum control over many-body spin systems, providing a powerful alternative to theory-driven methods. The framework combines customized tree search, neural network filtering, and numerical simulation guidance to navigate highly nonlinear optimization landscapes, using only desktop-level computational resources and minimal experimental input. The objective function is set to preserve coherence, a key prerequisite for quantum information processing. Our framework identifies over 900 high-performing sequences that exhibit non-intuitive structures and hence challenge long-standing design principles, while established optimization methods struggle to find such solutions. Experiments in a diamond spin ensemble show that the best AI-designed sequences achieve coherence times exceeding 200 microseconds, representing a 100% improvement over state-of-the-art baselines and approaching the temperature-imposed limit. Beyond spin coherence preservation, our framework is readily extendable through modified objective functions and incorporation of appropriate training data. This work highlights AI's potential to steer complex quantum many-body dynamics, marking a paradigm shift toward data-driven sequence design with broad applicability across spin-based quantum technologies and beyond.",
    "pdf_url": "https://arxiv.org/pdf/2510.07802v1",
    "github_url": null,
    "published": "2025-10-09T05:30:12+00:00",
    "updated": "2025-10-09T05:30:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07799v1",
    "title": "Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models",
    "authors": [
      "Jiang",
      "Wan",
      "Yin"
    ],
    "summary": "The efficiency of multi-agent systems driven by large language models (LLMs) largely hinges on their communication topology. However, designing an optimal topology is a non-trivial challenge, as it requires balancing competing objectives such as task performance, communication cost, and robustness. Existing frameworks often rely on static or hand-crafted topologies, which inherently fail to adapt to diverse task requirements, leading to either excessive token consumption for simple problems or performance bottlenecks for complex ones. To address this challenge, we introduce a novel generative framework called \\textit{Guided Topology Diffusion (GTD)}. Inspired by conditional discrete graph diffusion models, GTD formulates topology synthesis as an iterative construction process. At each step, the generation is steered by a lightweight proxy model that predicts multi-objective rewards (e.g., accuracy, utility, cost), enabling real-time, gradient-free optimization towards task-adaptive topologies. This iterative, guided synthesis process distinguishes GTD from single-step generative frameworks, enabling it to better navigate complex design trade-offs. We validated GTD across multiple benchmarks, and experiments show that this framework can generate highly task-adaptive, sparse, and efficient communication topologies, significantly outperforming existing methods in LLM agent collaboration.",
    "pdf_url": "https://arxiv.org/pdf/2510.07799v1",
    "github_url": null,
    "published": "2025-10-09T05:28:28+00:00",
    "updated": "2025-10-09T05:28:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07740v1",
    "title": "AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?",
    "authors": [
      "Ran",
      "Cao",
      "Wu"
    ],
    "summary": "Large language models (LLMs) have demonstrated remarkable capability in function-level code generation tasks. Unlike isolated functions, real-world applications demand reasoning over the entire software system: developers must orchestrate how different components interact, maintain consistency across states over time, and ensure the application behaves correctly within the lifecycle and framework constraints. Yet, no existing benchmark adequately evaluates whether LLMs can bridge this gap and construct entire software systems from scratch. To address this gap, we propose APPFORGE, a benchmark consisting of 101 software development problems drawn from real-world Android apps. Given a natural language specification detailing the app functionality, a language model is tasked with implementing the functionality into an Android app from scratch. Developing an Android app from scratch requires understanding and coordinating app states, lifecycle management, and asynchronous operations, calling for LLMs to generate context-aware, robust, and maintainable code. To construct APPFORGE, we design a multi-agent system to automatically summarize the main functionalities from app documents and navigate the app to synthesize test cases validating the functional correctness of app implementation. Following rigorous manual verification by Android development experts, APPFORGE incorporates the test cases within an automated evaluation framework that enables reproducible assessment without human intervention, making it easily adoptable for future research. Our evaluation on 12 flagship LLMs show that all evaluated models achieve low effectiveness, with the best-performing model (GPT-5) developing only 18.8% functionally correct applications, highlighting fundamental limitations in current models' ability to handle complex, multi-component software engineering challenges.",
    "pdf_url": "https://arxiv.org/pdf/2510.07740v1",
    "github_url": null,
    "published": "2025-10-09T03:26:05+00:00",
    "updated": "2025-10-09T03:26:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07725v1",
    "title": "Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis",
    "authors": [
      "Muenprasitivej",
      "Zhao",
      "Chou"
    ],
    "summary": "We address the challenge of enabling bipedal robots to traverse rough terrain by developing probabilistically safe planning and control strategies that ensure dynamic feasibility and centroidal robustness under terrain uncertainty. Specifically, we propose a high-level Model Predictive Control (MPC) navigation framework for a bipedal robot with a specified confidence level of safety that (i) enables safe traversal toward a desired goal location across a terrain map with uncertain elevations, and (ii) formally incorporates uncertainty bounds into the centroidal dynamics of locomotion control. To model the rough terrain, we employ Gaussian Process (GP) regression to estimate elevation maps and leverage Conformal Prediction (CP) to construct calibrated confidence intervals that capture the true terrain elevation. Building on this, we formulate contraction-based reachable tubes that explicitly account for terrain uncertainty, ensuring state convergence and tube invariance. In addition, we introduce a contraction-based flywheel torque control law for the reduced-order Linear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum about the center-of-mass (CoM). This formulation provides both probabilistic safety and goal reachability guarantees. For a given confidence level, we establish the forward invariance of the proposed torque control law by demonstrating exponential stabilization of the actual CoM phase-space trajectory and the desired trajectory prescribed by the high-level planner. Finally, we evaluate the effectiveness of our planning framework through physics-based simulations of the Digit bipedal robot in MuJoCo.",
    "pdf_url": "https://arxiv.org/pdf/2510.07725v1",
    "github_url": null,
    "published": "2025-10-09T03:03:09+00:00",
    "updated": "2025-10-09T03:03:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07683v1",
    "title": "A Virtual Fields Method-Genetic Algorithm (VFM-GA) calibration framework for isotropic hyperelastic constitutive models with application to an elastomeric foam material",
    "authors": [
      "Yan",
      "Tao",
      "Franck"
    ],
    "summary": "This work introduces a calibration framework for material parameter identification in isotropic hyperelastic constitutive models. The framework synergizes the Virtual Fields Method (VFM) to define an objective function with a Genetic Algorithm (GA) as the optimization method to facilitate automated calibration. The formulation of the objective function uses experimental displacement fields measured from Digital Image Correlation (DIC) synchronized with load cell data and can accommodate data from experiments involving homogeneous or inhomogeneous deformation fields. The framework places no restrictions on the target isotropic hyperelastic constitutive model, accommodating models with coupled dependencies on deformation invariants and specialized functional forms with a number of material parameters, and assesses material stability, eliminating sets of material parameters that potentially lead to non-physical behavior for the target hyperelastic constitutive model. To minimize the objective function, a GA is deployed as the optimization tool due to its ability to navigate the intricate landscape of material parameter space. The VFM-GA framework is evaluated by applying it to a hyperelastic constitutive model for compressible elastomeric foams. The evaluation process entails a number of tests that employ both homogeneous and inhomogeneous displacement fields collected from DIC experiments on open-cell foam specimens. The results outperform manual fitting, demonstrating the framework's robust and efficient capability to handle material parameter identification for complex hyperelastic constitutive models.",
    "pdf_url": "https://arxiv.org/pdf/2510.07683v1",
    "github_url": null,
    "published": "2025-10-09T02:14:33+00:00",
    "updated": "2025-10-09T02:14:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07609v1",
    "title": "IGUANA: Immersive Guidance, Navigation, and Control for Consumer UAV",
    "authors": [
      "Victor",
      "Krisanty",
      "McGinity"
    ],
    "summary": "As the markets for unmanned aerial vehicles (UAVs) and mixed reality (MR) headsets continue to grow, recent research has increasingly explored their integration, which enables more intuitive, immersive, and situationally aware control systems. We present IGUANA, an MR-based immersive guidance, navigation, and control system for consumer UAVs. IGUANA introduces three key elements beyond conventional control interfaces: (1) a 3D terrain map interface with draggable waypoint markers and live camera preview for high-level control, (2) a novel spatial control metaphor that uses a virtual ball as a physical analogy for low-level control, and (3) a spatial overlay that helps track the UAV when it is not visible with the naked eye or visual line of sight is interrupted. We conducted a user study to evaluate our design, both quantitatively and qualitatively, and found that (1) the 3D map interface is intuitive and easy to use, relieving users from manual control and suggesting improved accuracy and consistency with lower perceived workload relative to conventional dual-stick controller, (2) the virtual ball interface is intuitive but limited by the lack of physical feedback, and (3) the spatial overlay is very useful in enhancing the users' situational awareness.",
    "pdf_url": "https://arxiv.org/pdf/2510.07609v1",
    "github_url": null,
    "published": "2025-10-08T23:07:03+00:00",
    "updated": "2025-10-08T23:07:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07516v1",
    "title": "CompassLLM: A Multi-Agent Approach toward Geo-Spatial Reasoning for Popular Path Query",
    "authors": [
      "Ananto",
      "Fatin",
      "Ali"
    ],
    "summary": "The popular path query - identifying the most frequented routes between locations from historical trajectory data - has important applications in urban planning, navigation optimization, and travel recommendations. While traditional algorithms and machine learning approaches have achieved success in this domain, they typically require model training, parameter tuning, and retraining when accommodating data updates. As Large Language Models (LLMs) demonstrate increasing capabilities in spatial and graph-based reasoning, there is growing interest in exploring how these models can be applied to geo-spatial problems.   We introduce CompassLLM, a novel multi-agent framework that intelligently leverages the reasoning capabilities of LLMs into the geo-spatial domain to solve the popular path query. CompassLLM employs its agents in a two-stage pipeline: the SEARCH stage that identifies popular paths, and a GENERATE stage that synthesizes novel paths in the absence of an existing one in the historical trajectory data. Experiments on real and synthetic datasets show that CompassLLM demonstrates superior accuracy in SEARCH and competitive performance in GENERATE while being cost-effective.",
    "pdf_url": "https://arxiv.org/pdf/2510.07516v1",
    "github_url": null,
    "published": "2025-10-08T20:28:52+00:00",
    "updated": "2025-10-08T20:28:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07458v3",
    "title": "Populism Meets AI: Advancing Populism Research with LLMs",
    "authors": [
      "Jung",
      "Tamaki",
      "Chatterley"
    ],
    "summary": "Measuring the ideational content of populism remains a challenge. Traditional strategies based on textual analysis have been critical for building the field's foundations and providing a valid, objective indicator of populist framing. Yet these approaches are costly, time consuming, and difficult to scale across languages, contexts, and large corpora. Here we present the results from a rubric and anchor guided chain of thought (CoT) prompting approach that mirrors human coder training. By leveraging the Global Populism Database (GPD), a comprehensive dataset of global leaders' speeches annotated for degrees of populism, we replicate the process used to train human coders by prompting the LLM with an adapted version of the same documentation to guide the model's reasoning. We then test multiple proprietary and open weight models by replicating scores in the GPD. Our findings reveal that this domain specific prompting strategy enables the LLM to achieve classification accuracy on par with expert human coders, demonstrating its ability to navigate the nuanced, context sensitive aspects of populism.",
    "pdf_url": "https://arxiv.org/pdf/2510.07458v3",
    "github_url": null,
    "published": "2025-10-08T19:04:15+00:00",
    "updated": "2025-10-25T01:32:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07210v1",
    "title": "HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving",
    "authors": [
      "Pfaffmann",
      "Klusch",
      "Steinmetz"
    ],
    "summary": "We present a novel hybrid learning-assisted planning method, named HyPlan, for solving the collision-free navigation problem for self-driving cars in partially observable traffic environments. HyPlan combines methods for multi-agent behavior prediction, deep reinforcement learning with proximal policy optimization and approximated online POMDP planning with heuristic confidence-based vertical pruning to reduce its execution time without compromising safety of driving. Our experimental performance analysis on the CARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed that HyPlan may navigate safer than selected relevant baselines and perform significantly faster than considered alternative online POMDP planners.",
    "pdf_url": "https://arxiv.org/pdf/2510.07210v1",
    "github_url": null,
    "published": "2025-10-08T16:44:54+00:00",
    "updated": "2025-10-08T16:44:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07200v1",
    "title": "Regulating Social Media: Surveying the Impact of Nepali Government's TikTok Ban",
    "authors": [
      "Khatiwada",
      "Ciuba",
      "Nayak"
    ],
    "summary": "Social media platforms have transformed global communication and interaction, with TikTok emerging as a critical tool for education, connection, and social impact, including in contexts where infrastructural resources are limited. Amid growing political discussions about banning platforms like TikTok, such actions can create significant ripple effects, particularly impacting marginalized communities. We present a study on Nepal, where a TikTok ban was recently imposed and lifted. As a low-resource country in transition where digital communication is rapidly evolving, TikTok enables a space for community engagement and cultural expression. In this context, we conducted an online survey (N=108) to explore user values, experiences, and strategies for navigating online spaces post-ban. By examining these transitions, we aim to improve our understanding of how digital technologies, policy responses, and cultural dynamics interact globally and their implications for governance and societal norms. Our results indicate that users express skepticism toward platform bans but often passively accept them without active opposition. Findings suggest the importance of institutionalizing collective governance models that encourage public deliberation, nuanced control, and socially resonant policy decisions.",
    "pdf_url": "https://arxiv.org/pdf/2510.07200v1",
    "github_url": null,
    "published": "2025-10-08T16:33:14+00:00",
    "updated": "2025-10-08T16:33:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07184v1",
    "title": "Exploring the Feasibility of Gaze-Based Navigation Across Path Types",
    "authors": [
      "Zhang",
      "Zhang",
      "Hu"
    ],
    "summary": "Gaze input, as a modality inherently conveying user intent, offers intuitive and immersive experiences in extended reality (XR). With eye-tracking now being a standard feature in modern XR headsets, gaze has been extensively applied to tasks such as selection, text entry, and object manipulation. However, gaze based navigation despite being a fundamental interaction task remains largely underexplored. In particular, little is known about which path types are well suited for gaze navigation and under what conditions it performs effectively. To bridge this gap, we conducted a controlled user study evaluating gaze-based navigation across three representative path types: linear, narrowing, and circular. Our findings reveal distinct performance characteristics and parameter ranges for each path type, offering design insights and practical guidelines for future gaze-driven navigation systems in XR.",
    "pdf_url": "https://arxiv.org/pdf/2510.07184v1",
    "github_url": null,
    "published": "2025-10-08T16:21:55+00:00",
    "updated": "2025-10-08T16:21:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07116v1",
    "title": "From Neural Sensing to Stimulation: An Interdisciplinary Roadmap for Neurotechnology",
    "authors": [
      "Serrano",
      "Troughton",
      "Mirkhani"
    ],
    "summary": "Neurotechnologies are transforming how we measure, interpret, and modulate brain-body interactions, integrating real-time sensing, computation, and stimulation to enable precise physiological control. They hold transformative potential across clinical and non-clinical domains, from treating disorders to enhancing cognition and performance. Realizing this potential requires navigating complex, interdisciplinary challenges spanning neuroscience, materials science, device engineering, signal processing, computational modelling, and regulatory and ethical frameworks. This Perspective presents a strategic roadmap for neurotechnology development, created by early-career researchers, highlighting their role at the intersection of disciplines and their capacity to bridge traditional silos. We identify five cross-cutting trade-offs that constrain progress across functionality, scalability, adaptability, and translatability, and illustrate how technical domains influence their resolution. Rather than a domain-specific review, we focus on shared challenges and strategic opportunities that transcend disciplines. We propose a unified framework for collaborative innovation and education, highlight ethical and regulatory priorities, and outline a timeline for overcoming key bottlenecks. By aligning technical development with translational and societal needs, this roadmap aims to accelerate equitable, effective, and future-ready adaptive neurotechnologies, guiding coordinated efforts across the global research and innovation community.",
    "pdf_url": "https://arxiv.org/pdf/2510.07116v1",
    "github_url": null,
    "published": "2025-10-08T15:09:54+00:00",
    "updated": "2025-10-08T15:09:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06989v2",
    "title": "Human-aligned AI Model Cards with Weighted Hierarchy Architecture",
    "authors": [
      "Yang",
      "Jin",
      "Zeng"
    ],
    "summary": "The proliferation of Large Language Models (LLMs) has led to a burgeoning ecosystem of specialized, domain-specific models. While this rapid growth accelerates innovation, it has simultaneously created significant challenges in model discovery and adoption. Users struggle to navigate this landscape due to inconsistent, incomplete, and imbalanced documentation across platforms. Existing documentation frameworks, such as Model Cards and FactSheets, attempt to standardize reporting but are often static, predominantly qualitative, and lack the quantitative mechanisms needed for rigorous cross-model comparison. This gap exacerbates model underutilization and hinders responsible adoption. To address these shortcomings, we introduce the Comprehensive Responsible AI Model Card Framework (CRAI-MCF), a novel approach that transitions from static disclosures to actionable, human-aligned documentation. Grounded in Value Sensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240 open-source projects, distilling 217 parameters into an eight-module, value-aligned architecture. Our framework introduces a quantitative sufficiency criterion to operationalize evaluation and enables rigorous cross-model comparison under a unified scheme. By balancing technical, ethical, and operational dimensions, CRAI-MCF empowers practitioners to efficiently assess, select, and adopt LLMs with greater confidence and operational integrity.",
    "pdf_url": "https://arxiv.org/pdf/2510.06989v2",
    "github_url": null,
    "published": "2025-10-08T13:13:18+00:00",
    "updated": "2025-10-11T11:08:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06970v1",
    "title": "Falsification-Driven Reinforcement Learning for Maritime Motion Planning",
    "authors": [
      "Müller",
      "Finkeldei",
      "Krasowski"
    ],
    "summary": "Compliance with maritime traffic rules is essential for the safe operation of autonomous vessels, yet training reinforcement learning (RL) agents to adhere to them is challenging. The behavior of RL agents is shaped by the training scenarios they encounter, but creating scenarios that capture the complexity of maritime navigation is non-trivial, and real-world data alone is insufficient. To address this, we propose a falsification-driven RL approach that generates adversarial training scenarios in which the vessel under test violates maritime traffic rules, which are expressed as signal temporal logic specifications. Our experiments on open-sea navigation with two vessels demonstrate that the proposed approach provides more relevant training scenarios and achieves more consistent rule compliance.",
    "pdf_url": "https://arxiv.org/pdf/2510.06970v1",
    "github_url": null,
    "published": "2025-10-08T12:56:31+00:00",
    "updated": "2025-10-08T12:56:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06931v1",
    "title": "Textual interpretation of transient image classifications from large language models",
    "authors": [
      "Stoppa",
      "Bulmus",
      "Bloemen"
    ],
    "summary": "Modern astronomical surveys deliver immense volumes of transient detections, yet distinguishing real astrophysical signals (for example, explosive events) from bogus imaging artefacts remains a challenge. Convolutional neural networks are effectively used for real versus bogus classification; however, their reliance on opaque latent representations hinders interpretability. Here we show that large language models (LLMs) can approach the performance level of a convolutional neural network on three optical transient survey datasets (Pan-STARRS, MeerLICHT and ATLAS) while simultaneously producing direct, human-readable descriptions for every candidate. Using only 15 examples and concise instructions, Google's LLM, Gemini, achieves a 93% average accuracy across datasets that span a range of resolution and pixel scales. We also show that a second LLM can assess the coherence of the output of the first model, enabling iterative refinement by identifying problematic cases. This framework allows users to define the desired classification behaviour through natural language and examples, bypassing traditional training pipelines. Furthermore, by generating textual descriptions of observed features, LLMs enable users to query classifications as if navigating an annotated catalogue, rather than deciphering abstract latent spaces. As next-generation telescopes and surveys further increase the amount of data available, LLM-based classification could help bridge the gap between automated detection and transparent, human-level understanding.",
    "pdf_url": "https://arxiv.org/pdf/2510.06931v1",
    "github_url": null,
    "published": "2025-10-08T12:12:46+00:00",
    "updated": "2025-10-08T12:12:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06901v2",
    "title": "Adaptive Semantic Communication for UAV/UGV Cooperative Path Planning",
    "authors": [
      "Zhao",
      "Sun",
      "Lan"
    ],
    "summary": "Effective path planning is fundamental to the coordination of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) systems, particularly in applications such as surveillance, navigation, and emergency response. Combining UAVs' broad field of view with UGVs' ground-level operational capability greatly improve the likelihood of successfully achieving task objectives such as locating victims, monitoring target areas, or navigating hazardous terrain. In complex environments, UAVs need to provide precise environmental perception information for UGVs to optimize their routing policy. However, due to severe interference and non-line-of-sight conditions, wireless communication is often unstable in such complex environments, making it difficult to support timely and accurate path planning for UAV-UGV coordination. To this end, this paper proposes a semantic communication (SemCom) framework to enhance UAV/UGV cooperative path planning under unreliable wireless conditions. Unlike traditional methods that transmit raw data, SemCom transmits only the key information for path planning, reducing transmission volume without sacrificing accuracy. The proposed framework is developed by defining key semantics for path planning and designing a transceiver for meeting the requirements of UAV-UGV cooperative path planning. Simulation results show that, compared to conventional SemCom transceivers, the proposed transceiver significantly reduces data transmission volume while maintaining path planning accuracy, thereby enhancing system collaboration efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2510.06901v2",
    "github_url": null,
    "published": "2025-10-08T11:30:57+00:00",
    "updated": "2025-10-21T16:19:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06754v1",
    "title": "UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene",
    "authors": [
      "Maurer",
      "Jauhri",
      "Lueth"
    ],
    "summary": "Comprehensive visual, geometric, and semantic understanding of a 3D scene is crucial for successful execution of robotic tasks, especially in unstructured and complex environments. Additionally, to make robust decisions, it is necessary for the robot to evaluate the reliability of perceived information. While recent advances in 3D neural feature fields have enabled robots to leverage features from pretrained foundation models for tasks such as language-guided manipulation and navigation, existing methods suffer from two critical limitations: (i) they are typically scene-specific, and (ii) they lack the ability to model uncertainty in their predictions. We present UniFField, a unified uncertainty-aware neural feature field that combines visual, semantic, and geometric features in a single generalizable representation while also predicting uncertainty in each modality. Our approach, which can be applied zero shot to any new environment, incrementally integrates RGB-D images into our voxel-based feature representation as the robot explores the scene, simultaneously updating uncertainty estimation. We evaluate our uncertainty estimations to accurately describe the model prediction errors in scene reconstruction and semantic feature prediction. Furthermore, we successfully leverage our feature predictions and their respective uncertainty for an active object search task using a mobile manipulator robot, demonstrating the capability for robust decision-making.",
    "pdf_url": "https://arxiv.org/pdf/2510.06754v1",
    "github_url": null,
    "published": "2025-10-08T08:30:26+00:00",
    "updated": "2025-10-08T08:30:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06690v1",
    "title": "\"It feels like hard work trying to talk to it\": Understanding Older Adults' Experiences of Encountering and Repairing Conversational Breakdowns with AI Systems",
    "authors": [
      "Mathur",
      "Zubatiy",
      "Rozga"
    ],
    "summary": "Designing Conversational AI systems to support older adults requires more than usability and reliability, it also necessitates robustness in handling conversational breakdowns. In this study, we investigate how older adults navigate and repair such breakdowns while interacting with a voice-based AI system deployed in their homes for medication management. Through a 20-week in-home deployment with 7 older adult participant dyads, we analyzed 844 recoded interactions to identify conversational breakdowns and user-initiated repair strategies. Through findings gleaned from post-deployment interviews, we reflect on the nature of these breakdowns and older adults' experiences of mitigating them. We identify four types of conversational breakdowns and demonstrate how older adults draw on their situated knowledge and environment to make sense of and recover from these disruptions, highlighting the cognitive effort required in doing so. Our findings emphasize the collaborative nature of interactions in human-AI contexts, and point to the need for AI systems to better align with users' expectations for memory, their routines, and external resources in their environment. We conclude by discussing opportunities for AI systems to integrate contextual knowledge from older adults' sociotechnical environment and to facilitate more meaningful and user-centered interactions.",
    "pdf_url": "https://arxiv.org/pdf/2510.06690v1",
    "github_url": null,
    "published": "2025-10-08T06:26:12+00:00",
    "updated": "2025-10-08T06:26:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06633v1",
    "title": "Assist-As-Needed: Adaptive Multimodal Robotic Assistance for Medication Management in Dementia Care",
    "authors": [
      "Gangaraju",
      "Inaparthy",
      "Yang"
    ],
    "summary": "People living with dementia (PLWDs) face progressively declining abilities in medication management-from simple forgetfulness to complete task breakdown-yet most assistive technologies fail to adapt to these changing needs. This one-size-fits-all approach undermines autonomy, accelerates dependence, and increases caregiver burden. Occupational therapy principles emphasize matching assistance levels to individual capabilities: minimal reminders for those who merely forget, spatial guidance for those who misplace items, and comprehensive multimodal support for those requiring step-by-step instruction. However, existing robotic systems lack this adaptive, graduated response framework essential for maintaining PLWD independence. We present an adaptive multimodal robotic framework using the Pepper robot that dynamically adjusts assistance based on real-time assessment of user needs. Our system implements a hierarchical intervention model progressing from (1) simple verbal reminders, to (2) verbal + gestural cues, to (3) full multimodal guidance combining physical navigation to medication locations with step-by-step verbal and gestural instructions. Powered by LLM-driven interaction strategies and multimodal sensing, the system continuously evaluates task states to provide just-enough assistance-preserving autonomy while ensuring medication adherence. We conducted a preliminary study with healthy adults and dementia care stakeholders in a controlled lab setting, evaluating the system's usability, comprehensibility, and appropriateness of adaptive feedback mechanisms. This work contributes: (1) a theoretically grounded adaptive assistance framework translating occupational therapy principles into HRI design, (2) a multimodal robotic implementation that preserves PLWD dignity through graduated support, and (3) empirical insights into stakeholder perceptions of adaptive robotic care.",
    "pdf_url": "https://arxiv.org/pdf/2510.06633v1",
    "github_url": null,
    "published": "2025-10-08T04:34:43+00:00",
    "updated": "2025-10-08T04:34:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06587v1",
    "title": "WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks",
    "authors": [
      "Yang",
      "Hou",
      "Wei"
    ],
    "summary": "Large language model (LLM) agents are becoming competent at straightforward web tasks, such as opening an item page or submitting a form, but still struggle with objectives that require long horizon navigation, large scale information extraction, and reasoning under constraints. We present WebDART, a general framework that enables a single LLM to handle such complex chores. WebDART (i) dynamically decomposes each objective into three focused subtasks: navigation, information extraction, and execution, so the model concentrates on one skill at a time, and (ii) continuously replans the decomposition as new webpages are revealed, taking advantage of newly discovered filters or shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena, WebDART lifts success rates by up to 13.7 percentage points over previous SOTA agents, while matching their performance on the easier WebArena suite and completing tasks with up to 14.7 fewer navigation steps.",
    "pdf_url": "https://arxiv.org/pdf/2510.06587v1",
    "github_url": null,
    "published": "2025-10-08T02:34:59+00:00",
    "updated": "2025-10-08T02:34:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06573v1",
    "title": "RAVEN: Realtime Accessibility in Virtual ENvironments for Blind and Low-Vision People",
    "authors": [
      "Cao",
      "Ju",
      "Li"
    ],
    "summary": "As virtual 3D environments become prevalent, equitable access is crucial for blind and low-vision (BLV) users who face challenges with spatial awareness, navigation, and interactions. To address this gap, previous work explored supplementing visual information with auditory and haptic modalities. However, these methods are static and offer limited support for dynamic, in-context adaptation. Recent work in generative AI enables users to query and modify 3D scenes via natural language, introducing a paradigm with increased flexibility and control for accessibility improvements. We present RAVEN, a system that responds to query or modification prompts from BLV users to improve the runtime accessibility of 3D virtual scenes. We evaluated the system with eight BLV people, uncovering key insights into the strengths and shortcomings of generative AI-driven accessibility in virtual 3D environments, pointing to promising results as well as challenges related to system reliability and user trust.",
    "pdf_url": "https://arxiv.org/pdf/2510.06573v1",
    "github_url": null,
    "published": "2025-10-08T02:07:05+00:00",
    "updated": "2025-10-08T02:07:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06541v1",
    "title": "Cluster Paths: Navigating Interpretability in Neural Networks",
    "authors": [
      "Kroeger",
      "Bindschaedler"
    ],
    "summary": "While modern deep neural networks achieve impressive performance in vision tasks, they remain opaque in their decision processes, risking unwarranted trust, undetected biases and unexpected failures. We propose cluster paths, a post-hoc interpretability method that clusters activations at selected layers and represents each input as its sequence of cluster IDs. To assess these cluster paths, we introduce four metrics: path complexity (cognitive load), weighted-path purity (class alignment), decision-alignment faithfulness (predictive fidelity), and path agreement (stability under perturbations). In a spurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts and collapse when the cue is removed. On a five-class CelebA hair-color task, they achieve 90% faithfulness and maintain 96% agreement under Gaussian noise without sacrificing accuracy. Scaling to a Vision Transformer pretrained on ImageNet, we extend cluster paths to concept paths derived from prompting a large language model on minimal path divergences. Finally, we show that cluster paths can serve as an effective out-of-distribution (OOD) detector, reliably flagging anomalous samples before the model generates over-confident predictions. Cluster paths uncover visual concepts, such as color palettes, textures, or object contexts, at multiple network depths, demonstrating that cluster paths scale to large vision models while generating concise and human-readable explanations.",
    "pdf_url": "https://arxiv.org/pdf/2510.06541v1",
    "github_url": null,
    "published": "2025-10-08T00:41:09+00:00",
    "updated": "2025-10-08T00:41:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06518v1",
    "title": "Real-Time Glass Detection and Reprojection using Sensor Fusion Onboard Aerial Robots",
    "authors": [
      "Hopkins",
      "Murali",
      "Kumar"
    ],
    "summary": "Autonomous aerial robots are increasingly being deployed in real-world scenarios, where transparent obstacles present significant challenges to reliable navigation and mapping. These materials pose a unique problem for traditional perception systems because they lack discernible features and can cause conventional depth sensors to fail, leading to inaccurate maps and potential collisions. To ensure safe navigation, robots must be able to accurately detect and map these transparent obstacles. Existing methods often rely on large, expensive sensors or algorithms that impose high computational burdens, making them unsuitable for low Size, Weight, and Power (SWaP) robots. In this work, we propose a novel and computationally efficient framework for detecting and mapping transparent obstacles onboard a sub-300g quadrotor. Our method fuses data from a Time-of-Flight (ToF) camera and an ultrasonic sensor with a custom, lightweight 2D convolution model. This specialized approach accurately detects specular reflections and propagates their depth into corresponding empty regions of the depth map, effectively rendering transparent obstacles visible. The entire pipeline operates in real-time, utilizing only a small fraction of a CPU core on an embedded processor. We validate our system through a series of experiments in both controlled and real-world environments, demonstrating the utility of our method through experiments where the robot maps indoor environments containing glass. Our work is, to our knowledge, the first of its kind to demonstrate a real-time, onboard transparent obstacle mapping system on a low-SWaP quadrotor using only the CPU.",
    "pdf_url": "https://arxiv.org/pdf/2510.06518v1",
    "github_url": null,
    "published": "2025-10-07T23:31:45+00:00",
    "updated": "2025-10-07T23:31:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06483v1",
    "title": "Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review",
    "authors": [
      "Michael",
      "Netz",
      "Rumpe"
    ],
    "summary": "Software applications often pose barriers for users with accessibility needs, e.g., visual impairments. Model-driven engineering (MDE), with its systematic nature of code derivation, offers systematic methods to integrate accessibility concerns into software development while reducing manual effort. This paper presents a systematic literature review on how MDE addresses accessibility for vision impairments. From 447 initially identified papers, 30 primary studies met the inclusion criteria. About two-thirds reference the Web Content Accessibility Guidelines (WCAG), yet their project-specific adaptions and end-user validations hinder wider adoption in MDE. The analyzed studies model user interface structures, interaction and navigation, user capabilities, requirements, and context information. However, only few specify concrete modeling techniques on how to incorporate accessibility needs or demonstrate fully functional systems. Insufficient details on MDE methods, i.e., transformation rules or code templates, hinder the reuse, generalizability, and reproducibility. Furthermore, limited involvement of affected users and limited developer expertise in accessibility contribute to weak empirical validation. Overall, the findings indicate that current MDE research insufficiently supports vision-related accessibility. Our paper concludes with a research agenda outlining how support for vision impairments can be more effectively embedded in MDE processes.",
    "pdf_url": "https://arxiv.org/pdf/2510.06483v1",
    "github_url": null,
    "published": "2025-10-07T21:46:26+00:00",
    "updated": "2025-10-07T21:46:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06481v1",
    "title": "Active Next-Best-View Optimization for Risk-Averse Path Planning",
    "authors": [
      "Khass",
      "Liu",
      "Pandey"
    ],
    "summary": "Safe navigation in uncertain environments requires planning methods that integrate risk aversion with active perception. In this work, we present a unified framework that refines a coarse reference path by constructing tail-sensitive risk maps from Average Value-at-Risk statistics on an online-updated 3D Gaussian-splat Radiance Field. These maps enable the generation of locally safe and feasible trajectories. In parallel, we formulate Next-Best-View (NBV) selection as an optimization problem on the SE(3) pose manifold, where Riemannian gradient descent maximizes an expected information gain objective to reduce uncertainty most critical for imminent motion. Our approach advances the state-of-the-art by coupling risk-averse path refinement with NBV planning, while introducing scalable gradient decompositions that support efficient online updates in complex environments. We demonstrate the effectiveness of the proposed framework through extensive computational studies.",
    "pdf_url": "https://arxiv.org/pdf/2510.06481v1",
    "github_url": null,
    "published": "2025-10-07T21:41:28+00:00",
    "updated": "2025-10-07T21:41:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.07348v1",
    "title": "Bioinspired Tapered-Spring Turbulence Sensor for Underwater Flow Detection",
    "authors": [
      "Jin",
      "Yu",
      "Nanayakkara"
    ],
    "summary": "This paper presents a bio-inspired underwater whisker sensor for robust hydrodynamic disturbance detection and efficient signal analysis based on Physical Reservoir Computing (PRC). The design uses a tapered nylon spring with embedded accelerometers to achieve spatially distributed vibration sensing and frequency separation along the whisker. Towing-tank experiments and computational fluid dynamics simulations confirmed that the whisker effectively distinguishes vortex regimes across different fin angles and maintains Strouhal scaling with flow velocity, where higher speeds increase vibration intensity without affecting the dominant frequencies. Frequency-domain analysis, Shannon entropy, and machine learning further validated the sensing performance: vortex shedding frequencies were identified with less than 10\\% error, entropy captured the transition from coherent vortex streets to turbulence, and logistic regression achieved 86.0\\% classification accuracy with millisecond-level inference. These results demonstrate that structurally encoded whisker sensing provides a scalable and real-time solution for underwater perception, wake tracking, and turbulence-aware navigation in autonomous marine robots.",
    "pdf_url": "https://arxiv.org/pdf/2510.07348v1",
    "github_url": null,
    "published": "2025-10-07T21:23:09+00:00",
    "updated": "2025-10-07T21:23:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06470v1",
    "title": "Terrain-Aided Navigation Using a Point Cloud Measurement Sensor",
    "authors": [
      "Şanlan",
      "Erol",
      "Abu-Khalaf"
    ],
    "summary": "We investigate the use of a point cloud measurement in terrain-aided navigation. Our goal is to aid an inertial navigation system, by exploring ways to generate a useful measurement innovation error for effective nonlinear state estimation. We compare two such measurement models that involve the scanning of a digital terrain elevation model: a) one that is based on typical ray-casting from a given pose, that returns the predicted point cloud measurement from that pose, and b) another computationally less intensive one that does not require raycasting and we refer to herein as a sliding grid. Besides requiring a pose, it requires the pattern of the point cloud measurement itself and returns a predicted point cloud measurement. We further investigate the observability properties of the altitude for both measurement models. As a baseline, we compare the use of a point cloud measurement performance to the use of a radar altimeter and show the gains in accuracy. We conclude by showing that a point cloud measurement outperforms the use of a radar altimeter, and the point cloud measurement model to use depends on the computational resources",
    "pdf_url": "https://arxiv.org/pdf/2510.06470v1",
    "github_url": null,
    "published": "2025-10-07T21:13:20+00:00",
    "updated": "2025-10-07T21:13:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06457v1",
    "title": "Evaluating Node-tree Interfaces for AI Explainability",
    "authors": [
      "Wang",
      "Friedman",
      "Zhu"
    ],
    "summary": "As large language models (LLMs) become ubiquitous in workplace tools and decision-making processes, ensuring explainability and fostering user trust are critical. Although advancements in LLM engineering continue, human-centered design is still catching up, particularly when it comes to embedding transparency and trust into AI interfaces. This study evaluates user experiences with two distinct AI interfaces - node-tree interfaces and chatbot interfaces - to assess their performance in exploratory, follow-up inquiry, decision-making, and problem-solving tasks. Our design-driven approach introduces a node-tree interface that visually structures AI-generated responses into hierarchically organized, interactive nodes, allowing users to navigate, refine, and follow up on complex information. In a comparative study with n=20 business users, we observed that while the chatbot interface effectively supports linear, step-by-step queries, it is the node-tree interface that enhances brainstorming. Quantitative and qualitative findings indicate that node-tree interfaces not only improve task performance and decision-making support but also promote higher levels of user trust by preserving context. Our findings suggest that adaptive AI interfaces capable of switching between structured visualizations and conversational formats based on task requirements can significantly enhance transparency and user confidence in AI-powered systems. This work contributes actionable insights to the fields of human-robot interaction and AI design, particularly for enterprise applications where trust-building is critical for teams.",
    "pdf_url": "https://arxiv.org/pdf/2510.06457v1",
    "github_url": null,
    "published": "2025-10-07T20:48:08+00:00",
    "updated": "2025-10-07T20:48:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06176v1",
    "title": "An integrated photonic millimeter-wave receiver with sub-ambient noise",
    "authors": [
      "Zhang",
      "Zheng",
      "Cai"
    ],
    "summary": "Decades of progress in radiofrequency (RF) transistors and receiver frontends have profoundly impacted wireless communications, remote sensing, navigation, and instrumentation. Growing demands for data throughput in 6G networks, timing precision in positioning systems, and resolution in atmospheric sensing and automotive radar have pushed receiver frontends into the millimeter-wave (mmW) and sub-mmW/THz regimes. At these frequencies, however, the noise performance of field-effect transistors (FETs) degrades rapidly due to parasitic effects, limited carrier mobility, hot electrons, and shot noise. Parametric transducers that couple electromagnetic signals to optical fields offer quantum-limited sensitivity at room temperature. Electro-optic materials enable receivers that convert RF signals into optical phase shifts. While early demonstrations used resonant devices and recent efforts have focused on cryogenic microwave-to-optical quantum transduction, room-temperature electro-optic receivers have yet to achieve noise figures comparable to their electronic counterparts. Here we demonstrate a room-temperature integrated cavity electro-optic mmW receiver on a lithium tantalate (LiTaO3) photonic integrated circuit with 2.5% on-chip photon-number transduction efficiency, achieving 250 K noise temperature at 59.33 GHz--matching state-of-the-art LNAs. We report the first direct resolution of thermal noise in cavity electro-optic transduction, showing the system is fundamentally limited by thermal photon occupation (~100) in the mmW cavity. Our work establishes integrated photonics as a path to surpass electronic LNAs while offering exceptional resilience to strong electromagnetic inputs and immunity to EMI, establishing cavity electro-optics as a low-noise, chip-scale, EMI-resilient receiver frontend for mmW applications and scalable analog processing in the optical domain.",
    "pdf_url": "https://arxiv.org/pdf/2510.06176v1",
    "github_url": null,
    "published": "2025-10-07T17:40:21+00:00",
    "updated": "2025-10-07T17:40:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06166v2",
    "title": "Bridging the Synthesizability Gap in Perovskites by Combining Computations, Literature Data, and PU Learning",
    "authors": [
      "Desai",
      "Ahn",
      "Strachan"
    ],
    "summary": "Among emerging energy materials, halide and chalcogenide perovskites have garnered significant attention over the last decade owing to the abundance of their constituent species, low manufacturing costs, and their highly tunable composition-structure-property space. Navigating the vast perovskite compositional landscape is possible using density functional theory (DFT) computations, but they are not easily extended to predictions of the synthesizability of new materials and their properties. As a result, only a limited number of compositions identified to have desirable optoelectronic properties from these calculations have been realized experimentally. One way to bridge this gap is by learning from the experimental literature about how the perovskite composition-structure space relates to their likelihood of laboratory synthesis. Here, we present our efforts in combining high-throughput DFT data with experimental labels collected from the literature to train classifier models employing various materials descriptors to forecast the synthesizability of any given perovskite compound. Our framework utilizes the positive and unlabeled (PU) learning strategy and makes probabilistic estimates of the synthesis likelihood based on DFT- computed energies and the prior existence of similar synthesized compounds. Our data and models can be readily accessed via a Findable, Accessible, Interoperable, and Reproducible (FAIR) nanoHUB tool.",
    "pdf_url": "https://arxiv.org/pdf/2510.06166v2",
    "github_url": null,
    "published": "2025-10-07T17:31:22+00:00",
    "updated": "2025-11-10T15:19:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.06002v2",
    "title": "Deterministic Legal Agents: A Canonical Primitive API for Auditable Reasoning over Temporal Knowledge Graphs",
    "authors": [
      "Martim"
    ],
    "summary": "For autonomous legal agents to operate safely in high-stakes domains, they require a foundation of absolute determinism and auditability-guarantees that standard Retrieval-Augmented Generation (RAG) frameworks cannot provide. When interacting with temporal knowledge graphs that model the complex evolution of legal norms, agents must navigate versioning, causality, and hierarchical structures with precision, a task for which black-box vector search is ill-suited. This paper introduces a new architectural pattern to solve this: a formal Primitive API designed as a secure execution layer for reasoning over such graphs. Instead of a monolithic query engine, our framework provides a library of canonical primitives-atomic, composable, and auditable primitives. This design empowers planner-guided agents to decompose complex legal questions into transparent execution plans, enabling critical tasks with full verifiability, including: (i) precise point-in-time version retrieval, (ii) robust causal lineage tracing, and (iii) context-aware hybrid search. Ultimately, this architecture transforms opaque retrieval into auditable reasoning, turning the agent's internal process from a black box into a verifiable log of deterministic primitives and providing a blueprint for building the next generation of trustworthy legal AI.",
    "pdf_url": "https://arxiv.org/pdf/2510.06002v2",
    "github_url": null,
    "published": "2025-10-07T15:04:23+00:00",
    "updated": "2025-11-04T16:44:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.05976v1",
    "title": "Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis",
    "authors": [
      "Adhikarla",
      "Liu",
      "Davison"
    ],
    "summary": "Low-light image enhancement (LLIE) is vital for safety-critical applications such as surveillance, autonomous navigation, and medical imaging, where visibility degradation can impair downstream task performance. Recently, diffusion models have emerged as a promising generative paradigm for LLIE due to their capacity to model complex image distributions via iterative denoising. This survey provides an up-to-date critical analysis of diffusion models for LLIE, distinctively featuring an in-depth comparative performance evaluation against Generative Adversarial Network and Transformer-based state-of-the-art methods, a thorough examination of practical deployment challenges, and a forward-looking perspective on the role of emerging paradigms like foundation models. We propose a multi-perspective taxonomy encompassing six categories: Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal, and Autonomous; that map enhancement methods across physical priors, conditioning schemes, and computational efficiency. Our taxonomy is grounded in a hybrid view of both the model mechanism and the conditioning signals. We evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs between interpretability, generalization, and inference efficiency. We also discuss real-world deployment constraints (e.g., memory, energy use) and ethical considerations. This survey aims to guide the next generation of diffusion-based LLIE research by highlighting trends and surfacing open research questions, including novel conditioning, real-time adaptation, and the potential of foundation models.",
    "pdf_url": "https://arxiv.org/pdf/2510.05976v1",
    "github_url": null,
    "published": "2025-10-07T14:30:36+00:00",
    "updated": "2025-10-07T14:30:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.05833v1",
    "title": "The Interplay of Attention and Memory in Visual Enumeration",
    "authors": [
      "Sankar",
      "Sen",
      "Sen"
    ],
    "summary": "Humans navigate and understand complex visual environments by subconsciously quantifying what they see, a process known as visual enumeration. However, traditional studies using flat screens fail to capture the cognitive dynamics of this process over the large visual fields of real-world scenes. To address this gap, we developed an immersive virtual reality system with integrated eye-tracking to investigate the interplay between attention and memory during complex enumeration. We conducted a two-phase experiment where participants enumerated scenes of either simple abstract shapes or complex real-world objects, systematically varying the task intent (e.g., selective vs. exhaustive counting) and the spatial layout of items. Our results reveal that task intent is the dominant factor driving performance, with selective counting imposing a significant cognitive cost that was dramatically amplified by stimulus complexity. The semantic processing required for real-world objects reduced accuracy and suppressed memory recall, while the influence of spatial layout was secondary and statistically non-significant when a higher-order cognitive task intent was driving the human behaviour. We conclude that real-world enumeration is fundamentally constrained by the cognitive load of semantic processing, not just the mechanics of visual search. Our findings demonstrate that under high cognitive demand, the effort to understand what we are seeing directly limits our capacity to remember it.",
    "pdf_url": "https://arxiv.org/pdf/2510.05833v1",
    "github_url": null,
    "published": "2025-10-07T11:56:26+00:00",
    "updated": "2025-10-07T11:56:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.05742v1",
    "title": "Vipera: Blending Visual and LLM-Driven Guidance for Systematic Auditing of Text-to-Image Generative AI",
    "authors": [
      "Huang",
      "Deng",
      "Xiao"
    ],
    "summary": "Despite their increasing capabilities, text-to-image generative AI systems are known to produce biased, offensive, and otherwise problematic outputs. While recent advancements have supported testing and auditing of generative AI, existing auditing methods still face challenges in supporting effectively explore the vast space of AI-generated outputs in a structured way. To address this gap, we conducted formative studies with five AI auditors and synthesized five design goals for supporting systematic AI audits. Based on these insights, we developed Vipera, an interactive auditing interface that employs multiple visual cues including a scene graph to facilitate image sensemaking and inspire auditors to explore and hierarchically organize the auditing criteria. Additionally, Vipera leverages LLM-powered suggestions to facilitate exploration of unexplored auditing directions. Through a controlled experiment with 24 participants experienced in AI auditing, we demonstrate Vipera's effectiveness in helping auditors navigate large AI output spaces and organize their analyses while engaging with diverse criteria.",
    "pdf_url": "https://arxiv.org/pdf/2510.05742v1",
    "github_url": null,
    "published": "2025-10-07T10:01:57+00:00",
    "updated": "2025-10-07T10:01:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.05684v2",
    "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI",
    "authors": [
      "Choi",
      "Jung",
      "Seong"
    ],
    "summary": "Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/",
    "pdf_url": "https://arxiv.org/pdf/2510.05684v2",
    "github_url": null,
    "published": "2025-10-07T08:40:33+00:00",
    "updated": "2025-12-17T19:23:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.05330v2",
    "title": "Adaptive Dynamics Planning for Robot Navigation",
    "authors": [
      "Lu",
      "Mao",
      "Xu"
    ],
    "summary": "Autonomous robot navigation systems often rely on hierarchical planning, where global planners compute collision-free paths without considering dynamics, and local planners enforce dynamics constraints to produce executable commands. This discontinuity in dynamics often leads to trajectory tracking failure in highly constrained environments. Recent approaches integrate dynamics within the entire planning process by gradually decreasing its fidelity, e.g., increasing integration steps and reducing collision checking resolution, for real-time planning efficiency. However, they assume that the fidelity of the dynamics should decrease according to a manually designed scheme. Such static settings fail to adapt to environmental complexity variations, resulting in computational overhead in simple environments or insufficient dynamics consideration in obstacle-rich scenarios. To overcome this limitation, we propose Adaptive Dynamics Planning (ADP), a learning-augmented paradigm that uses reinforcement learning to dynamically adjust robot dynamics properties, enabling planners to adapt across diverse environments. We integrate ADP into three different planners and further design a standalone ADP-based navigation system, benchmarking them against other baselines. Experiments in both simulation and real-world tests show that ADP consistently improves navigation success, safety, and efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2510.05330v2",
    "github_url": null,
    "published": "2025-10-06T19:50:16+00:00",
    "updated": "2025-10-10T20:19:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.04991v2",
    "title": "Efficient Navigation in Unknown Indoor Environments with Vision-Language Models",
    "authors": [
      "Schwartz",
      "Kondo",
      "How"
    ],
    "summary": "We present a novel high-level planning framework that leverages vision-language models (VLMs) to improve autonomous navigation in unknown indoor environments with many dead ends. Traditional exploration methods often take inefficient routes due to limited global reasoning and reliance on local heuristics. In contrast, our approach enables a VLM to reason directly about occupancy maps in a zero-shot manner, selecting subgoals that are likely to yield more efficient paths. At each planning step, we convert a 3D occupancy grid into a partial 2D map of the environment, and generate candidate subgoals. Each subgoal is then evaluated and ranked against other candidates by the model. We integrate this planning scheme into DYNUS \\cite{kondo2025dynus}, a state-of-the-art trajectory planner, and demonstrate improved navigation efficiency in simulation. The VLM infers structural patterns (e.g., rooms, corridors) from incomplete maps and balances the need to make progress toward a goal against the risk of entering unknown space. This reduces common greedy failures (e.g., detouring into small rooms) and achieves about 10\\% shorter paths on average.",
    "pdf_url": "https://arxiv.org/pdf/2510.04991v2",
    "github_url": null,
    "published": "2025-10-06T16:26:16+00:00",
    "updated": "2025-10-11T16:08:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.04942v1",
    "title": "Robust Cislunar Navigation via LFT-Based $\\mathcal{H}_\\infty$ Filtering with Bearing-Only Measurements",
    "authors": [
      "Bhattacharya"
    ],
    "summary": "This paper develops a robust estimation framework for cislunar navigation that embeds the Circular Restricted Three-Body Problem (CR3BP) dynamics and bearing-only optical measurements within a Linear Fractional Transformation (LFT) representation. A full-order $\\mathcal{H}_\\infty$ observer is synthesized with explicit $\\mathcal{L}_2$ performance bounds. The formulation yields a nonlinear estimator that operates directly on the governing equations and avoids reliance on local linearizations. Dominant nonlinearities are expressed as structured real uncertainties, while measurement fidelity is represented through range-dependent weighting with Earth-Moon distances reconstructed from line-of-sight geometry. The sensing architecture assumes passive star-tracker-class optical instruments, eliminating the need for time-of-flight ranging or precision clocks. Simulations demonstrate bounded estimation errors and smooth position tracking over multiple orbital periods, with the largest deviations observed in the out-of-plane states, consistent with the stiffness of the vertical dynamics and the limitations of angle-only observability. Application to a Near Rectilinear Halo Orbit (NRHO) illustrates that the framework can achieve robust onboard navigation with bounded estimation errors with flight-representative sensors.",
    "pdf_url": "https://arxiv.org/pdf/2510.04942v1",
    "github_url": null,
    "published": "2025-10-06T15:45:49+00:00",
    "updated": "2025-10-06T15:45:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.03142v1",
    "title": "MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning",
    "authors": [
      "Xu",
      "Chen",
      "Zhang"
    ],
    "summary": "Visual navigation policy is widely regarded as a promising direction, as it mimics humans by using egocentric visual observations for navigation. However, optical information of visual observations is difficult to be explicitly modeled like LiDAR point clouds or depth maps, which subsequently requires intelligent models and large-scale data. To this end, we propose to leverage the intelligence of the Vision-Language-Action (VLA) model to learn diverse navigation capabilities from synthetic expert data in a teacher-student manner. Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360 observations) based on pretrained large language models and visual foundation models. For large-scale navigation data, we collect expert data from three reinforcement learning (RL) experts trained with privileged depth information in three challenging tailor-made environments for different navigation capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA model using data collected online from RL experts, where the training ratio is dynamically balanced based on performance on individual capabilities. Through extensive experiments in synthetic environments, we demonstrate that our model achieves strong generalization capability. Moreover, we find that our student VLA model outperforms the RL teachers, demonstrating the synergistic effect of integrating multiple capabilities. Extensive real-world experiments further confirm the effectiveness of our method.",
    "pdf_url": "https://arxiv.org/pdf/2510.03142v1",
    "github_url": null,
    "published": "2025-10-03T16:15:09+00:00",
    "updated": "2025-10-03T16:15:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.02941v1",
    "title": "Metrics vs Surveys: Can Quantitative Measures Replace Human Surveys in Social Robot Navigation? A Correlation Analysis",
    "authors": [
      "Trepella",
      "Martini",
      "Pérez-Higueras"
    ],
    "summary": "Social, also called human-aware, navigation is a key challenge for the integration of mobile robots into human environments. The evaluation of such systems is complex, as factors such as comfort, safety, and legibility must be considered. Human-centered assessments, typically conducted through surveys, provide reliable insights but are costly, resource-intensive, and difficult to reproduce or compare across systems. Alternatively, numerical social navigation metrics are easy to compute and facilitate comparisons, yet the community lacks consensus on a standard set of metrics.   This work explores the relationship between numerical metrics and human-centered evaluations to identify potential correlations. If specific quantitative measures align with human perceptions, they could serve as standardized evaluation tools, reducing the dependency on surveys. Our results indicate that while current metrics capture some aspects of robot navigation behavior, important subjective factors remain insufficiently represented and new metrics are necessary.",
    "pdf_url": "https://arxiv.org/pdf/2510.02941v1",
    "github_url": null,
    "published": "2025-10-03T12:34:32+00:00",
    "updated": "2025-10-03T12:34:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.02885v1",
    "title": "Point Cloud-Based Control Barrier Functions for Model Predictive Control in Safety-Critical Navigation of Autonomous Mobile Robots",
    "authors": [
      "Liang",
      "Yang",
      "Dai"
    ],
    "summary": "In this work, we propose a novel motion planning algorithm to facilitate safety-critical navigation for autonomous mobile robots. The proposed algorithm integrates a real-time dynamic obstacle tracking and mapping system that categorizes point clouds into dynamic and static components. For dynamic point clouds, the Kalman filter is employed to estimate and predict their motion states. Based on these predictions, we extrapolate the future states of dynamic point clouds, which are subsequently merged with static point clouds to construct the forward-time-domain (FTD) map. By combining control barrier functions (CBFs) with nonlinear model predictive control, the proposed algorithm enables the robot to effectively avoid both static and dynamic obstacles. The CBF constraints are formulated based on risk points identified through collision detection between the predicted future states and the FTD map. Experimental results from both simulated and real-world scenarios demonstrate the efficacy of the proposed algorithm in complex environments. In simulation experiments, the proposed algorithm is compared with two baseline approaches, showing superior performance in terms of safety and robustness in obstacle avoidance. The source code is released for the reference of the robotics community.",
    "pdf_url": "https://arxiv.org/pdf/2510.02885v1",
    "github_url": null,
    "published": "2025-10-03T10:43:48+00:00",
    "updated": "2025-10-03T10:43:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.02728v2",
    "title": "Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4",
    "authors": [
      "Zhang",
      "Xiao",
      "Zhang"
    ],
    "summary": "Cross-modal drone navigation remains a challenging task in robotics, requiring efficient retrieval of relevant images from large-scale databases based on natural language descriptions. The RoboSense 2025 Track 4 challenge addresses this challenge, focusing on robust, natural language-guided cross-view image retrieval across multiple platforms (drones, satellites, and ground cameras). Current baseline methods, while effective for initial retrieval, often struggle to achieve fine-grained semantic matching between text queries and visual content, especially in complex aerial scenes. To address this challenge, we propose a two-stage retrieval refinement method: Caption-Guided Retrieval System (CGRS) that enhances the baseline coarse ranking through intelligent reranking. Our method first leverages a baseline model to obtain an initial coarse ranking of the top 20 most relevant images for each query. We then use Vision-Language-Model (VLM) to generate detailed captions for these candidate images, capturing rich semantic descriptions of their visual content. These generated captions are then used in a multimodal similarity computation framework to perform fine-grained reranking of the original text query, effectively building a semantic bridge between the visual content and natural language descriptions. Our approach significantly improves upon the baseline, achieving a consistent 5\\% improvement across all key metrics (Recall@1, Recall@5, and Recall@10). Our approach win TOP-2 in the challenge, demonstrating the practical value of our semantic refinement strategy in real-world robotic navigation scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2510.02728v2",
    "github_url": null,
    "published": "2025-10-03T05:13:19+00:00",
    "updated": "2025-11-06T03:23:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.02624v2",
    "title": "Multi-robot Rigid Formation Navigation via Synchronous Motion and Discrete-time Communication-Control Optimization",
    "authors": [
      "Yang",
      "Liew"
    ],
    "summary": "Rigid-formation navigation of multiple robots is essential for applications such as cooperative transportation. This process involves a team of collaborative robots maintaining a predefined geometric configuration, such as a square, while in motion. For untethered collaborative motion, inter-robot communication must be conducted through a wireless network. Notably, few existing works offer a comprehensive solution for multi-robot formation navigation executable on microprocessor platforms via wireless networks, particularly for formations that must traverse complex curvilinear paths. To address this gap, we introduce a novel \"hold-and-hit\" communication-control framework designed to work seamlessly with the widely-used Robotic Operating System (ROS) platform. The hold-and-hit framework synchronizes robot movements in a manner robust against wireless network delays and packet loss. It operates over discrete-time communication-control cycles, making it suitable for implementation on contemporary microprocessors. Complementary to hold-and-hit, we propose an intra-cycle optimization approach that enables rigid formations to closely follow desired curvilinear paths, even under the nonholonomic movement constraints inherent to most vehicular robots. The combination of hold-and-hit and intra-cycle optimization ensures precise and reliable navigation even in challenging scenarios. Simulations in a virtual environment demonstrate the superiority of our method in maintaining a four-robot square formation along an S-shaped path, outperforming two existing approaches. Furthermore, real-world experiments validate the effectiveness of our framework: the robots maintained an inter-distance error within $\\pm 0.069m$ and an inter-angular orientation error within $\\pm19.15^{\\circ}$ while navigating along an S-shaped path at a fixed linear velocity of $0.1 m/s$.",
    "pdf_url": "https://arxiv.org/pdf/2510.02624v2",
    "github_url": null,
    "published": "2025-10-03T00:08:13+00:00",
    "updated": "2025-10-10T13:25:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.02533v1",
    "title": "Head and Eye Control in Persons with Low Vision during Urban Navigation",
    "authors": [
      "Beheshti",
      "Rizzo",
      "Bergquist-Kosumi"
    ],
    "summary": "Low vision involves a range of visual impairments that significantly impact daily activities, particularly navigation in urban environments. Individuals with low vision often develop adaptive strategies to compensate for visual deficits, relying on head movements to bring objects into their remaining functional field of vision. Research suggests that they focus on road surface markings and building edges to aid in wayfinding and collision avoidance. However, urban navigation presents additional challenges, as obstacles, moving hazards, and tripping dangers may enter their visual loss field, increasing the risk of injury. Traditional eye movement studies are typically conducted in controlled laboratory settings with fixed head positions, limiting the understanding of head-eye coordination in real-world environments. To bridge this gap, we designed a naturalistic, \"free-head\" experiment using eye-tracking technology to examine head and eye movement patterns during urban navigation. Participants with low vision were compared to a control cohort without visual impairment to test the hypothesis that eye and head movements become decoupled in visually impaired individuals. Findings indicate that individuals with peripheral field loss exhibit significant eye-head decoupling, while those with acuity loss demonstrate more synchronized movements. Results for individuals with central field loss were inconclusive but revealed distinct movement patterns. These insights provide valuable direction for rehabilitation strategies, assistive-mobility technologies, and urban design improvements. By expanding research on eye-head coordination, this study contributes to the development of interventions that enhance safety, mobility, and independence for individuals with low vision in complex urban environments.",
    "pdf_url": "https://arxiv.org/pdf/2510.02533v1",
    "github_url": null,
    "published": "2025-10-02T20:03:40+00:00",
    "updated": "2025-10-02T20:03:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.02248v1",
    "title": "Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0",
    "authors": [
      "Miao",
      "Yuceel",
      "Fainekos"
    ],
    "summary": "Visual policy design is crucial for aerial navigation. However, state-of-the-art visual policies often overfit to a single track and their performance degrades when track geometry changes. We develop FalconGym 2.0, a photorealistic simulation framework built on Gaussian Splatting (GSplat) with an Edit API that programmatically generates diverse static and dynamic tracks in milliseconds. Leveraging FalconGym 2.0's editability, we propose a Performance-Guided Refinement (PGR) algorithm, which concentrates visual policy's training on challenging tracks while iteratively improving its performance. Across two case studies (fixed-wing UAVs and quadrotors) with distinct dynamics and environments, we show that a single visual policy trained with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in generalization and robustness: it generalizes to three unseen tracks with 100% success without per-track retraining and maintains higher success rates under gate-pose perturbations. Finally, we demonstrate that the visual policy trained with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30 trials spanning two three-gate tracks and a moving-gate track.",
    "pdf_url": "https://arxiv.org/pdf/2510.02248v1",
    "github_url": null,
    "published": "2025-10-02T17:36:50+00:00",
    "updated": "2025-10-02T17:36:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.02418v2",
    "title": "BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks",
    "authors": [
      "Anupam",
      "Brown",
      "Li"
    ],
    "summary": "LLM web agents now browse and take actions on the open web, yet current agent evaluations are constrained to sandboxed environments or artificial tasks. We introduce BrowserArena, a live open-web agent evaluation platform that collects user-submitted tasks, runs Arena-style head-to-head comparisons, and uses step-level human feedback to surface failure modes. Collecting and analyzing step-level annotations on the agent traces, we identify three consistent failure modes: captcha resolution, pop-up banner removal, and direct navigation to URLs. By constructing targeted datasets to further study these tasks, we discover variations in how different language models navigate these failure modes. We find, for example, that o4-mini deploys a wider variety of strategies to circumvent captcha resolution than other models and DeepSeek-R1 consistently misleads users about pop-up banner closure. Our findings surface both the diversity and brittleness of current web agents. More broadly, our benchmarking methodology provides an approach to evaluating and understanding web agent failure modes at scale.",
    "pdf_url": "https://arxiv.org/pdf/2510.02418v2",
    "github_url": null,
    "published": "2025-10-02T15:22:21+00:00",
    "updated": "2025-10-07T15:12:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.01830v1",
    "title": "What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework",
    "authors": [
      "Wang",
      "Sun",
      "Xing"
    ],
    "summary": "Object-Goal Navigation (ObjectNav) is a critical component toward deploying mobile robots in everyday, uncontrolled environments such as homes, schools, and workplaces. In this context, a robot must locate target objects in previously unseen environments using only its onboard perception. Success requires the integration of semantic understanding, spatial reasoning, and long-horizon planning, which is a combination that remains extremely challenging. While reinforcement learning (RL) has become the dominant paradigm, progress has spanned a wide range of design choices, yet the field still lacks a unifying analysis to determine which components truly drive performance. In this work, we conduct a large-scale empirical study of modular RL-based ObjectNav systems, decomposing them into three key components: perception, policy, and test-time enhancement. Through extensive controlled experiments, we isolate the contribution of each and uncover clear trends: perception quality and test-time strategies are decisive drivers of performance, whereas policy improvements with current methods yield only marginal gains. Building on these insights, we propose practical design guidelines and demonstrate an enhanced modular system that surpasses State-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We also introduce a human baseline under identical conditions, where experts achieve an average 98% success, underscoring the gap between RL agents and human-level navigation. Our study not only sets the SotA performance but also provides principled guidance for future ObjectNav development and evaluation.",
    "pdf_url": "https://arxiv.org/pdf/2510.01830v1",
    "github_url": null,
    "published": "2025-10-02T09:24:32+00:00",
    "updated": "2025-10-02T09:24:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.01795v2",
    "title": "Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving",
    "authors": [
      "Hu",
      "Huang",
      "Wang"
    ],
    "summary": "Vision-Language Models (VLMs) are increasingly applied in autonomous driving for unified perception and reasoning, but high inference latency hinders real-time deployment. Early-exit reduces latency by terminating inference at intermediate layers, yet its task-dependent nature limits generalization across diverse scenarios. We observe that this limitation aligns with autonomous driving: navigation systems can anticipate upcoming contexts (e.g., intersections, traffic lights), indicating which tasks will be required. We propose Nav-EE, a navigation-guided early-exit framework that precomputes task-specific exit layers offline and dynamically applies them online based on navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE achieves accuracy comparable to full inference while reducing latency by up to 63.9%. Real-vehicle integration with Autoware Universe further demonstrates reduced inference latency (600ms to 300ms), supporting faster decision-making in complex scenarios. These results suggest that coupling navigation foresight with early-exit offers a viable path toward efficient deployment of large models in autonomous systems. Code and data are available at our anonymous repository: https://anonymous.4open.science/r/Nav-EE-BBC4",
    "pdf_url": "https://arxiv.org/pdf/2510.01795v2",
    "github_url": null,
    "published": "2025-10-02T08:37:58+00:00",
    "updated": "2025-10-10T08:31:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.01519v1",
    "title": "Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments",
    "authors": [
      "Chen",
      "Liu",
      "Buynitsky"
    ],
    "summary": "Robot navigation in large, complex, and unknown indoor environments is a challenging problem. The existing approaches, such as traditional sampling-based methods, struggle with resolution control and scalability, while imitation learning-based methods require a large amount of demonstration data. Active Neural Time Fields (ANTFields) have recently emerged as a promising solution by using local observations to learn cost-to-go functions without relying on demonstrations. Despite their potential, these methods are hampered by challenges such as spectral bias and catastrophic forgetting, which diminish their effectiveness in complex scenarios. To address these issues, our approach decomposes the planning problem into a hierarchical structure. At the high level, a sparse graph captures the environment's global connectivity, while at the low level, a planner based on neural fields navigates local obstacles by solving the Eikonal PDE. This physics-informed strategy overcomes common pitfalls like spectral bias and neural field fitting difficulties, resulting in a smooth and precise representation of the cost landscape. We validate our framework in large-scale environments, demonstrating its enhanced adaptability and precision compared to previous methods, and highlighting its potential for online exploration, mapping, and real-world navigation.",
    "pdf_url": "https://arxiv.org/pdf/2510.01519v1",
    "github_url": null,
    "published": "2025-10-01T23:29:56+00:00",
    "updated": "2025-10-01T23:29:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.01483v1",
    "title": "VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs",
    "authors": [
      "Mdfaa",
      "Lukina",
      "Akhtyamov"
    ],
    "summary": "Vision-language models (VLMs) have shown potential for robot navigation but encounter fundamental limitations: they lack persistent scene memory, offer limited spatial reasoning, and do not scale effectively with video duration for real-time application. We present VL-KnG, a Visual Scene Understanding system that tackles these challenges using spatiotemporal knowledge graph construction and computationally efficient query processing for navigation goal identification. Our approach processes video sequences in chunks utilizing modern VLMs, creates persistent knowledge graphs that maintain object identity over time, and enables explainable spatial reasoning through queryable graph structures. We also introduce WalkieKnowledge, a new benchmark with about 200 manually annotated questions across 8 diverse trajectories spanning approximately 100 minutes of video data, enabling fair comparison between structured approaches and general-purpose VLMs. Real-world deployment on a differential drive robot demonstrates practical applicability, with our method achieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5 Pro performance while providing explainable reasoning supported by the knowledge graph, computational efficiency for real-time deployment across different tasks, such as localization, navigation and planning. Code and dataset will be released after acceptance.",
    "pdf_url": "https://arxiv.org/pdf/2510.01483v1",
    "github_url": null,
    "published": "2025-10-01T21:53:44+00:00",
    "updated": "2025-10-01T21:53:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.01388v1",
    "title": "VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation",
    "authors": [
      "Zhang",
      "Meng",
      "Calliari"
    ],
    "summary": "Robots must adapt to diverse human instructions and operate safely in unstructured, open-world environments. Recent Vision-Language models (VLMs) offer strong priors for grounding language and perception, but remain difficult to steer for navigation due to differences in action spaces and pretraining objectives that hamper transferability to robotics tasks. Towards addressing this, we introduce VENTURA, a vision-language navigation system that finetunes internet-pretrained image diffusion models for path planning. Instead of directly predicting low-level actions, VENTURA generates a path mask (i.e. a visual plan) in image space that captures fine-grained, context-aware navigation behaviors. A lightweight behavior-cloning policy grounds these visual plans into executable trajectories, yielding an interface that follows natural language instructions to generate diverse robot behaviors. To scale training, we supervise on path masks derived from self-supervised tracking models paired with VLM-augmented captions, avoiding manual pixel-level annotation or highly engineered data collection setups. In extensive real-world evaluations, VENTURA outperforms state-of-the-art foundation model baselines on object reaching, obstacle avoidance, and terrain preference tasks, improving success rates by 33% and reducing collisions by 54% across both seen and unseen scenarios. Notably, we find that VENTURA generalizes to unseen combinations of distinct tasks, revealing emergent compositional capabilities. Videos, code, and additional materials: https://venturapath.github.io",
    "pdf_url": "https://arxiv.org/pdf/2510.01388v1",
    "github_url": null,
    "published": "2025-10-01T19:21:28+00:00",
    "updated": "2025-10-01T19:21:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.01348v1",
    "title": "Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge",
    "authors": [
      "Werner",
      "Čapek",
      "Musil"
    ],
    "summary": "Reliable long-range flight of unmanned aerial vehicles (UAVs) in GNSS-denied environments is challenging: integrating odometry leads to drift, loop closures are unavailable in previously unseen areas and embedded platforms provide limited computational power. We present a fully onboard UAV system developed for the SPRIN-D Funke Fully Autonomous Flight Challenge, which required 9 km long-range waypoint navigation below 25 m AGL (Above Ground Level) without GNSS or prior dense mapping. The system integrates perception, mapping, planning, and control with a lightweight drift-correction method that matches LiDAR-derived local heightmaps to a prior geo-data heightmap via gradient-template matching and fuses the evidence with odometry in a clustered particle filter. Deployed during the competition, the system executed kilometer-scale flights across urban, forest, and open-field terrain and reduced drift substantially relative to raw odometry, while running in real time on CPU-only hardware. We describe the system architecture, the localization pipeline, and the competition evaluation, and we report practical insights from field deployment that inform the design of GNSS-denied UAV autonomy.",
    "pdf_url": "https://arxiv.org/pdf/2510.01348v1",
    "github_url": null,
    "published": "2025-10-01T18:23:42+00:00",
    "updated": "2025-10-01T18:23:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.00942v1",
    "title": "Non-submodular Visual Attention for Robot Navigation",
    "authors": [
      "Vafaee",
      "Behzad",
      "Siami"
    ],
    "summary": "This paper presents a task-oriented computational framework to enhance Visual-Inertial Navigation (VIN) in robots, addressing challenges such as limited time and energy resources. The framework strategically selects visual features using a Mean Squared Error (MSE)-based, non-submodular objective function and a simplified dynamic anticipation model. To address the NP-hardness of this problem, we introduce four polynomial-time approximation algorithms: a classic greedy method with constant-factor guarantees; a low-rank greedy variant that significantly reduces computational complexity; a randomized greedy sampler that balances efficiency and solution quality; and a linearization-based selector based on a first-order Taylor expansion for near-constant-time execution. We establish rigorous performance bounds by leveraging submodularity ratios, curvature, and element-wise curvature analyses. Extensive experiments on both standardized benchmarks and a custom control-aware platform validate our theoretical results, demonstrating that these methods achieve strong approximation guarantees while enabling real-time deployment.",
    "pdf_url": "https://arxiv.org/pdf/2510.00942v1",
    "github_url": null,
    "published": "2025-10-01T14:17:06+00:00",
    "updated": "2025-10-01T14:17:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.00604v1",
    "title": "Disentangling Foreground and Background for vision-Language Navigation via Online Augmentation",
    "authors": [
      "Xu",
      "Zhang",
      "Li"
    ],
    "summary": "Following language instructions, vision-language navigation (VLN) agents are tasked with navigating unseen environments. While augmenting multifaceted visual representations has propelled advancements in VLN, the significance of foreground and background in visual observations remains underexplored. Intuitively, foreground regions provide semantic cues, whereas the background encompasses spatial connectivity information. Inspired on this insight, we propose a Consensus-driven Online Feature Augmentation strategy (COFA) with alternative foreground and background features to facilitate the navigable generalization. Specifically, we first leverage semantically-enhanced landmark identification to disentangle foreground and background as candidate augmented features. Subsequently, a consensus-driven online augmentation strategy encourages the agent to consolidate two-stage voting results on feature preferences according to diverse instructions and navigational locations. Experiments on REVERIE and R2R demonstrate that our online foreground-background augmentation boosts the generalization of baseline and attains state-of-the-art performance.",
    "pdf_url": "https://arxiv.org/pdf/2510.00604v1",
    "github_url": null,
    "published": "2025-10-01T07:32:36+00:00",
    "updated": "2025-10-01T07:32:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.00466v1",
    "title": "Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation",
    "authors": [
      "Su",
      "Fu",
      "Zhou"
    ],
    "summary": "Offline reinforcement learning (RL) has emerged as a promising framework for addressing robot social navigation challenges. However, inherent uncertainties in pedestrian behavior and limited environmental interaction during training often lead to suboptimal exploration and distributional shifts between offline training and online deployment. To overcome these limitations, this paper proposes a novel offline-to-online fine-tuning RL algorithm for robot social navigation by integrating Return-to-Go (RTG) prediction into a causal Transformer architecture. Our algorithm features a spatiotem-poral fusion model designed to precisely estimate RTG values in real-time by jointly encoding temporal pedestrian motion patterns and spatial crowd dynamics. This RTG prediction framework mitigates distribution shift by aligning offline policy training with online environmental interactions. Furthermore, a hybrid offline-online experience sampling mechanism is built to stabilize policy updates during fine-tuning, ensuring balanced integration of pre-trained knowledge and real-time adaptation. Extensive experiments in simulated social navigation environments demonstrate that our method achieves a higher success rate and lower collision rate compared to state-of-the-art baselines. These results underscore the efficacy of our algorithm in enhancing navigation policy robustness and adaptability. This work paves the way for more reliable and adaptive robotic navigation systems in real-world applications.",
    "pdf_url": "https://arxiv.org/pdf/2510.00466v1",
    "github_url": null,
    "published": "2025-10-01T03:37:02+00:00",
    "updated": "2025-10-01T03:37:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2510.00441v3",
    "title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation",
    "authors": [
      "Pan",
      "Xu",
      "Liu"
    ],
    "summary": "Visual navigation is a fundamental problem in embodied AI, yet practical deployments demand long-horizon planning capabilities to address multi-objective tasks. A major bottleneck is data scarcity: policies learned from limited data often overfit and fail to generalize OOD. Existing neural network-based agents typically increase architectural complexity that paradoxically become counterproductive in the small-sample regime. This paper introduce NeuRO, a integrated learning-to-optimize framework that tightly couples perception networks with downstream task-level robust optimization. Specifically, NeuRO addresses core difficulties in this integration: (i) it transforms noisy visual predictions under data scarcity into convex uncertainty sets using Partially Input Convex Neural Networks (PICNNs) with conformal calibration, which directly parameterize the optimization constraints; and (ii) it reformulates planning under partial observability as a robust optimization problem, enabling uncertainty-aware policies that transfer across environments. Extensive experiments on both unordered and sequential multi-object navigation tasks demonstrate that NeuRO establishes SoTA performance, particularly in generalization to unseen environments. Our work thus presents a significant advancement for developing robust, generalizable autonomous agents.",
    "pdf_url": "https://arxiv.org/pdf/2510.00441v3",
    "github_url": null,
    "published": "2025-10-01T02:48:28+00:00",
    "updated": "2025-10-21T17:55:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.26513v1",
    "title": "Learning from Hallucinating Critical Points for Navigation in Dynamic Environments",
    "authors": [
      "Ghani",
      "Lee",
      "Xiao"
    ],
    "summary": "Generating large and diverse obstacle datasets to learn motion planning in environments with dynamic obstacles is challenging due to the vast space of possible obstacle trajectories. Inspired by hallucination-based data synthesis approaches, we propose Learning from Hallucinating Critical Points (LfH-CP), a self-supervised framework for creating rich dynamic obstacle datasets based on existing optimal motion plans without requiring expensive expert demonstrations or trial-and-error exploration. LfH-CP factorizes hallucination into two stages: first identifying when and where obstacles must appear in order to result in an optimal motion plan, i.e., the critical points, and then procedurally generating diverse trajectories that pass through these points while avoiding collisions. This factorization avoids generative failures such as mode collapse and ensures coverage of diverse dynamic behaviors. We further introduce a diversity metric to quantify dataset richness and show that LfH-CP produces substantially more varied training data than existing baselines. Experiments in simulation demonstrate that planners trained on LfH-CP datasets achieves higher success rates compared to a prior hallucination method.",
    "pdf_url": "https://arxiv.org/pdf/2509.26513v1",
    "github_url": null,
    "published": "2025-09-30T16:52:13+00:00",
    "updated": "2025-09-30T16:52:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.26339v1",
    "title": "Kinodynamic Motion Planning for Mobile Robot Navigation across Inconsistent World Models",
    "authors": [
      "Damm",
      "Howard"
    ],
    "summary": "Mobile ground robots lacking prior knowledge of an environment must rely on sensor data to develop a model of their surroundings. In these scenarios, consistent identification of obstacles and terrain features can be difficult due to noise and algorithmic shortcomings, which can make it difficult for motion planning systems to generate safe motions. One particular difficulty to overcome is when regions of the cost map switch between being marked as obstacles and free space through successive planning cycles. One potential solution to this, which we refer to as Valid in Every Hypothesis (VEH), is for the planning system to plan motions that are guaranteed to be safe through a history of world models. Another approach is to track a history of world models, and adjust node costs according to the potential penalty of needing to reroute around previously hazardous areas. This work discusses three major iterations on this idea. The first iteration, called PEH, invokes a sub-search for every node expansion that crosses through a divergence point in the world models. The second and third iterations, called GEH and GEGRH respectively, defer the sub-search until after an edge expands into the goal region. GEGRH uses an additional step to revise the graph based on divergent nodes in each world. Initial results showed that, although PEH and GEH find more optimistic solutions than VEH, they are unable to generate solutions in less than one-second, which exceeds our requirements for field deployment. Analysis of results from a field experiment in an unstructured, off-road environment on a Clearpath Robotics Warthog UGV indicate that GEGRH finds lower cost trajectories and has faster average planning times than VEH. Compared to single-hypothesis (SH) search, where only the latest world model is considered, GEGRH generates more conservative plans with a small increase in average planning time.",
    "pdf_url": "https://arxiv.org/pdf/2509.26339v1",
    "github_url": null,
    "published": "2025-09-30T14:45:30+00:00",
    "updated": "2025-09-30T14:45:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.25966v1",
    "title": "MUVLA: Learning to Explore Object Navigation via Map Understanding",
    "authors": [
      "Han",
      "Jia",
      "Zhang"
    ],
    "summary": "In this paper, we present MUVLA, a Map Understanding Vision-Language-Action model tailored for object navigation. It leverages semantic map abstractions to unify and structure historical information, encoding spatial context in a compact and consistent form. MUVLA takes the current and history observations, as well as the semantic map, as inputs and predicts the action sequence based on the description of goal object. Furthermore, it amplifies supervision through reward-guided return modeling based on dense short-horizon progress signals, enabling the model to develop a detailed understanding of action value for reward maximization. MUVLA employs a three-stage training pipeline: learning map-level spatial understanding, imitating behaviors from mixed-quality demonstrations, and reward amplification. This strategy allows MUVLA to unify diverse demonstrations into a robust spatial representation and generate more rational exploration strategies. Experiments on HM3D and Gibson benchmarks demonstrate that MUVLA achieves great generalization and learns effective exploration behaviors even from low-quality or partially successful trajectories.",
    "pdf_url": "https://arxiv.org/pdf/2509.25966v1",
    "github_url": null,
    "published": "2025-09-30T09:02:58+00:00",
    "updated": "2025-09-30T09:02:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.25687v2",
    "title": "OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation",
    "authors": [
      "Xue",
      "Hu",
      "Luo"
    ],
    "summary": "Embodied navigation presents a core challenge for intelligent robots, requiring the comprehension of visual environments, natural language instructions, and autonomous exploration. Existing models often fall short in offering a unified solution across diverse navigation paradigms, resulting in low success rates and limited generalization. We introduce OmniNav, a unified framework addressing instruct-goal, object-goal, point-goal navigation, and frontier-based exploration within a single architecture. Our approach features a lightweight, low-latency policy that accurately predicts continuous-space waypoints (coordinates and orientations). This policy surpasses action-chunk methods in precision and supports real-world deployment at control frequencies up to 5 Hz. Architecturally, OmniNav employs a fast-slow system design: a fast module generates waypoints using short-horizon visual context and subtasks, while a slow module performs deliberative planning with long-horizon observations and candidate frontiers to select subsequent subgoals and subtasks. This collaboration enhances path efficiency and maintains trajectory coherence, particularly in exploration and memory-intensive scenarios. Crucially, we identify that the primary bottleneck isn't merely navigation policy learning, but a robust understanding of general instructions and objects. To boost generalization, OmniNav integrates large-scale, general-purpose training datasets, including those for image captioning and visual recognition, into a joint multi-task regimen. This significantly improves success rates and robustness. Extensive experiments confirm OmniNav's state-of-the-art performance across various navigation benchmarks, with real-world deployment further validating its efficacy. OmniNav provides practical insights for embodied navigation, charting a scalable path towards versatile, highly generalizable robotic intelligence.",
    "pdf_url": "https://arxiv.org/pdf/2509.25687v2",
    "github_url": null,
    "published": "2025-09-30T02:44:28+00:00",
    "updated": "2025-10-09T08:47:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.25655v1",
    "title": "Landmark-Guided Knowledge for Vision-and-Language Navigation",
    "authors": [
      "Yang",
      "Zhu",
      "Yu"
    ],
    "summary": "Vision-and-language navigation is one of the core tasks in embodied intelligence, requiring an agent to autonomously navigate in an unfamiliar environment based on natural language instructions. However, existing methods often fail to match instructions with environmental information in complex scenarios, one reason being the lack of common-sense reasoning ability. This paper proposes a vision-and-language navigation method called Landmark-Guided Knowledge (LGK), which introduces an external knowledge base to assist navigation, addressing the misjudgment issues caused by insufficient common sense in traditional methods. Specifically, we first construct a knowledge base containing 630,000 language descriptions and use knowledge Matching to align environmental subviews with the knowledge base, extracting relevant descriptive knowledge. Next, we design a Knowledge-Guided by Landmark (KGL) mechanism, which guides the agent to focus on the most relevant parts of the knowledge by leveraging landmark information in the instructions, thereby reducing the data bias that may arise from incorporating external knowledge. Finally, we propose Knowledge-Guided Dynamic Augmentation (KGDA), which effectively integrates language, knowledge, vision, and historical information. Experimental results demonstrate that the LGK method outperforms existing state-of-the-art methods on the R2R and REVERIE vision-and-language navigation datasets, particularly in terms of navigation error, success rate, and path efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2509.25655v1",
    "github_url": null,
    "published": "2025-09-30T01:54:27+00:00",
    "updated": "2025-09-30T01:54:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.25652v1",
    "title": "Iterative Residual Cross-Attention Mechanism: An Integrated Approach for Audio-Visual Navigation Tasks",
    "authors": [
      "Zhang",
      "Yu",
      "Wang"
    ],
    "summary": "Audio-visual navigation represents a significant area of research in which intelligent agents utilize egocentric visual and auditory perceptions to identify audio targets. Conventional navigation methodologies typically adopt a staged modular design, which involves first executing feature fusion, then utilizing Gated Recurrent Unit (GRU) modules for sequence modeling, and finally making decisions through reinforcement learning. While this modular approach has demonstrated effectiveness, it may also lead to redundant information processing and inconsistencies in information transmission between the various modules during the feature fusion and GRU sequence modeling phases. This paper presents IRCAM-AVN (Iterative Residual Cross-Attention Mechanism for Audiovisual Navigation), an end-to-end framework that integrates multimodal information fusion and sequence modeling within a unified IRCAM module, thereby replacing the traditional separate components for fusion and GRU. This innovative mechanism employs a multi-level residual design that concatenates initial multimodal sequences with processed information sequences. This methodological shift progressively optimizes the feature extraction process while reducing model bias and enhancing the model's stability and generalization capabilities. Empirical results indicate that intelligent agents employing the iterative residual cross-attention mechanism exhibit superior navigation performance.",
    "pdf_url": "https://arxiv.org/pdf/2509.25652v1",
    "github_url": null,
    "published": "2025-09-30T01:52:57+00:00",
    "updated": "2025-09-30T01:52:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.25518v2",
    "title": "World Model for AI Autonomous Navigation in Mechanical Thrombectomy",
    "authors": [
      "Robertshaw",
      "Wu",
      "Granados"
    ],
    "summary": "Autonomous navigation for mechanical thrombectomy (MT) remains a critical challenge due to the complexity of vascular anatomy and the need for precise, real-time decision-making. Reinforcement learning (RL)-based approaches have demonstrated potential in automating endovascular navigation, but current methods often struggle with generalization across multiple patient vasculatures and long-horizon tasks. We propose a world model for autonomous endovascular navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL agent across multiple endovascular navigation tasks in ten real patient vasculatures, comparing performance against the state-of-the-art Soft Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly outperforms SAC in multi-task learning, achieving a 65% mean success rate compared to SAC's 37%, with notable improvements in path ratio. TD-MPC2 exhibited increased procedure times, suggesting a trade-off between success rate and execution speed. These findings highlight the potential of world models for improving autonomous endovascular navigation and lay the foundation for future research in generalizable AI-driven robotic interventions.",
    "pdf_url": "https://arxiv.org/pdf/2509.25518v2",
    "github_url": null,
    "published": "2025-09-29T21:21:30+00:00",
    "updated": "2025-10-02T05:02:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.25139v1",
    "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs",
    "authors": [
      "Zhang",
      "Ma",
      "Wang"
    ],
    "summary": "Integrating large language models (LLMs) into embodied AI models is becoming increasingly prevalent. However, existing zero-shot LLM-based Vision-and-Language Navigation (VLN) agents either encode images as textual scene descriptions, potentially oversimplifying visual details, or process raw image inputs, which can fail to capture abstract semantics required for high-level reasoning. In this paper, we improve the navigation agent's contextual understanding by incorporating textual descriptions from multiple perspectives that facilitate analogical reasoning across images. By leveraging text-based analogical reasoning, the agent enhances its global scene understanding and spatial reasoning, leading to more accurate action decisions. We evaluate our approach on the R2R dataset, where our experiments demonstrate significant improvements in navigation performance.",
    "pdf_url": "https://arxiv.org/pdf/2509.25139v1",
    "github_url": null,
    "published": "2025-09-29T17:51:01+00:00",
    "updated": "2025-09-29T17:51:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.25056v2",
    "title": "AgriCruiser: An Open Source Agriculture Robot for Over-the-row Navigation",
    "authors": [
      "Truong",
      "Lee",
      "Irie"
    ],
    "summary": "We present the AgriCruiser, an open-source over-the-row agricultural robot developed for low-cost deployment and rapid adaptation across diverse crops and row layouts. The chassis provides an adjustable track width of 1.42 m to 1.57 m, along with a ground clearance of 0.94 m. The AgriCruiser achieves compact pivot turns with radii of 0.71 m to 0.79 m, enabling efficient headland maneuvers. The platform is designed for the integration of the other subsystems, and in this study, a precision spraying system was implemented to assess its effectiveness in weed management. In twelve flax plots, a single robotic spray pass reduced total weed populations (pigweed and Venice mallow) by 24- to 42-fold compared to manual weeding in four flax plots, while also causing less crop damage. Mobility experiments conducted on concrete, asphalt, gravel, grass, and both wet and dry soil confirmed reliable traversal consistent with torque sizing. The complete chassis can be constructed from commodity T-slot extrusion with minimal machining, resulting in a bill of materials costing approximately $5,000 - $6,000, which enables replication and customization. The mentioned results demonstrate that low-cost, reconfigurable over-the-row robots can achieve effective weed management with reduced crop damage and labor requirements, while providing a versatile foundation for phenotyping, sensing, and other agriculture applications. Design files and implementation details are released to accelerate research and adoption of modular agricultural robotics.",
    "pdf_url": "https://arxiv.org/pdf/2509.25056v2",
    "github_url": null,
    "published": "2025-09-29T17:06:25+00:00",
    "updated": "2025-09-30T13:08:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.24910v1",
    "title": "Learning Goal-Oriented Language-Guided Navigation with Self-Improving Demonstrations at Scale",
    "authors": [
      "Li",
      "Wang",
      "Zhou"
    ],
    "summary": "Goal-oriented language-guided navigation requires robust exploration capabilities for agents to navigate to specified goals in unknown environments without step-by-step instructions. Existing methods tend to exclusively utilize shortest-path trajectories, lacking effective exploration priors for training navigation agents. To address the above challenges, we present SID, a goal-oriented language-guided navigation learning approach with Self-Improving Demonstrations. Specifically, SID learns an initial agent on the shortest-path data sampled from environments and then leverages this agent to generate novel exploration trajectories. The novel rollouts provide demonstrations with stronger exploration strategies to train a better agent, which in turn produces higher-quality agent demonstrations for the next round of training. We show that this iterative self-improving pipeline readily scales to new environments, and the resulting demonstrations can be transferred across a variety of language-guided navigation tasks, elevating the performance ceiling in diverse goal-oriented navigation tasks. Extensive experiments demonstrate that SID significantly boosts the exploration capabilities and generalization of navigation agents. The resulting agent achieves new state-of-the-art performance on goal-oriented language-guided navigation tasks, including REVERIE, SOON, notably achieving a 50.9% success rate on the unseen validation splits of SOON, surpassing the prior leading approaches by a margin of 13.9%.",
    "pdf_url": "https://arxiv.org/pdf/2509.24910v1",
    "github_url": null,
    "published": "2025-09-29T15:15:54+00:00",
    "updated": "2025-09-29T15:15:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.24907v2",
    "title": "Real-time Recognition of Human Interactions from a Single RGB-D Camera for Socially-Aware Robot Navigation",
    "authors": [
      "Nguyen",
      "Nguyen",
      "Nu"
    ],
    "summary": "{Recognizing human interactions is essential for social robots as it enables them to navigate safely and naturally in shared environments. Conventional robotic systems however often focus on obstacle avoidance, neglecting social cues necessary for seamless human-robot interaction. To address this gap, we propose a framework to recognize human group interactions for socially aware navigation. Our method utilizes color and depth frames from a monocular RGB-D camera to estimate 3D human keypoints and positions. Principal component analysis (PCA) is then used to determine dominant interaction directions. The shoelace formula is finally applied to compute interest points and engagement areas. Extensive experiments have been conducted to evaluate the validity of the proposed method. The results show that our method is capable of recognizing group interactions across different scenarios with varying numbers of individuals. It also achieves high-speed performance, processing each frame in approximately 4 ms on a single-board computer used in robotic systems. The method is implemented as a ROS 2 package making it simple to integrate into existing navigation systems. Source code is available at https://github.com/thanhlong103/social-interaction-detector",
    "pdf_url": "https://arxiv.org/pdf/2509.24907v2",
    "github_url": "https://github.com/thanhlong103/social-interaction-detector",
    "published": "2025-09-29T15:14:56+00:00",
    "updated": "2025-10-17T14:01:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.24763v1",
    "title": "SSR-ZSON: Zero-Shot Object Navigation via Spatial-Semantic Relations within a Hierarchical Exploration Framework",
    "authors": [
      "Meng",
      "Li",
      "Mao"
    ],
    "summary": "Zero-shot object navigation in unknown environments presents significant challenges, mainly due to two key limitations: insufficient semantic guidance leads to inefficient exploration, while limited spatial memory resulting from environmental structure causes entrapment in local regions. To address these issues, we propose SSR-ZSON, a spatial-semantic relative zero-shot object navigation method based on the TARE hierarchical exploration framework, integrating a viewpoint generation strategy balancing spatial coverage and semantic density with an LLM-based global guidance mechanism. The performance improvement of the proposed method is due to two key innovations. First, the viewpoint generation strategy prioritizes areas of high semantic density within traversable sub-regions to maximize spatial coverage and minimize invalid exploration. Second, coupled with an LLM-based global guidance mechanism, it assesses semantic associations to direct navigation toward high-value spaces, preventing local entrapment and ensuring efficient exploration. Deployed on hybrid Habitat-Gazebo simulations and physical platforms, SSR-ZSON achieves real-time operation and superior performance. On Matterport3D and Habitat-Matterport3D datasets, it improves the Success Rate(SR) by 18.5\\% and 11.2\\%, and the Success weighted by Path Length(SPL) by 0.181 and 0.140, respectively, over state-of-the-art methods.",
    "pdf_url": "https://arxiv.org/pdf/2509.24763v1",
    "github_url": null,
    "published": "2025-09-29T13:28:24+00:00",
    "updated": "2025-09-29T13:28:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.24528v3",
    "title": "CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D",
    "authors": [
      "Mirzaei",
      "Amoie",
      "Ekhterachian"
    ],
    "summary": "3D scene understanding is fundamental for embodied AI and robotics, supporting reliable perception for interaction and navigation. Recent approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning embedding vectors to 2D class-agnostic masks generated via vision-language models (VLMs) and projecting these into 3D. However, these methods often produce fragmented masks and inaccurate semantic assignments due to the direct use of raw masks, limiting their effectiveness in complex environments. To address this, we leverage SemanticSAM with progressive granularity refinement to generate more accurate and numerous object-level masks, mitigating the over-segmentation commonly observed in mask generation models such as vanilla SAM, and improving downstream 3D semantic segmentation. To further enhance semantic context, we employ a context-aware CLIP encoding strategy that integrates multiple contextual views of each mask using empirically determined weighting, providing much richer visual context. We evaluate our approach on multiple 3D scene understanding tasks, including 3D semantic segmentation and object retrieval from language queries, across several benchmark datasets. Experimental results demonstrate significant improvements over existing methods, highlighting the effectiveness of our approach.",
    "pdf_url": "https://arxiv.org/pdf/2509.24528v3",
    "github_url": null,
    "published": "2025-09-29T09:43:00+00:00",
    "updated": "2025-12-07T23:06:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.24387v1",
    "title": "AdaNav: Adaptive Reasoning with Uncertainty for Vision-Language Navigation",
    "authors": [
      "Ding",
      "Wei",
      "Yang"
    ],
    "summary": "Vision Language Navigation (VLN) requires agents to follow natural language instructions by grounding them in sequential visual observations over long horizons. Explicit reasoning could enhance temporal consistency and perception action alignment, but reasoning at fixed steps often leads to suboptimal performance and unnecessary computation. To address this, we propose AdaNav, an uncertainty-based adaptive reasoning framework for VLN. At its core is the Uncertainty Adaptive Reasoning Block (UAR), a lightweight plugin that dynamically triggers reasoning. We introduce Action Entropy as a policy prior for UAR and progressively refine it through a Heuristics to RL training method, enabling agents to learn difficulty aware reasoning policies under the strict data limitations of embodied tasks. Results show that with only 6K training samples, AdaNav achieves substantial gains over closed source models trained on million scale data, improving success rate by 20% on R2R val-unseen, 11.7% on RxR-CE, and 11.4% in real world scenes. The code is available at https://github.com/xinding-sys/AdaNav.",
    "pdf_url": "https://arxiv.org/pdf/2509.24387v1",
    "github_url": "https://github.com/xinding-sys/AdaNav",
    "published": "2025-09-29T07:36:45+00:00",
    "updated": "2025-09-29T07:36:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.24321v1",
    "title": "SONAR: Semantic-Object Navigation with Aggregated Reasoning through a Cross-Modal Inference Paradigm",
    "authors": [
      "Wang",
      "Sun",
      "Chi"
    ],
    "summary": "Understanding human instructions and accomplishing Vision-Language Navigation tasks in unknown environments is essential for robots. However, existing modular approaches heavily rely on the quality of training data and often exhibit poor generalization. Vision-Language Model based methods, while demonstrating strong generalization capabilities, tend to perform unsatisfactorily when semantic cues are weak. To address these issues, this paper proposes SONAR, an aggregated reasoning approach through a cross modal paradigm. The proposed method integrates a semantic map based target prediction module with a Vision-Language Model based value map module, enabling more robust navigation in unknown environments with varying levels of semantic cues, and effectively balancing generalization ability with scene adaptability. In terms of target localization, we propose a strategy that integrates multi-scale semantic maps with confidence maps, aiming to mitigate false detections of target objects. We conducted an evaluation of the SONAR within the Gazebo simulator, leveraging the most challenging Matterport 3D (MP3D) dataset as the experimental benchmark. Experimental results demonstrate that SONAR achieves a success rate of 38.4% and an SPL of 17.7%.",
    "pdf_url": "https://arxiv.org/pdf/2509.24321v1",
    "github_url": null,
    "published": "2025-09-29T06:09:28+00:00",
    "updated": "2025-09-29T06:09:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.23563v1",
    "title": "RAVEN: Resilient Aerial Navigation via Open-Set Semantic Memory and Behavior Adaptation",
    "authors": [
      "Kim",
      "Alama",
      "Kurdydyk"
    ],
    "summary": "Aerial outdoor semantic navigation requires robots to explore large, unstructured environments to locate target objects. Recent advances in semantic navigation have demonstrated open-set object-goal navigation in indoor settings, but these methods remain limited by constrained spatial ranges and structured layouts, making them unsuitable for long-range outdoor search. While outdoor semantic navigation approaches exist, they either rely on reactive policies based on current observations, which tend to produce short-sighted behaviors, or precompute scene graphs offline for navigation, limiting adaptability to online deployment. We present RAVEN, a 3D memory-based, behavior tree framework for aerial semantic navigation in unstructured outdoor environments. It (1) uses a spatially consistent semantic voxel-ray map as persistent memory, enabling long-horizon planning and avoiding purely reactive behaviors, (2) combines short-range voxel search and long-range ray search to scale to large environments, (3) leverages a large vision-language model to suggest auxiliary cues, mitigating sparsity of outdoor targets. These components are coordinated by a behavior tree, which adaptively switches behaviors for robust operation. We evaluate RAVEN in 10 photorealistic outdoor simulation environments over 100 semantic tasks, encompassing single-object search, multi-class, multi-instance navigation and sequential task changes. Results show RAVEN outperforms baselines by 85.25% in simulation and demonstrate its real-world applicability through deployment on an aerial robot in outdoor field tests.",
    "pdf_url": "https://arxiv.org/pdf/2509.23563v1",
    "github_url": null,
    "published": "2025-09-28T01:43:25+00:00",
    "updated": "2025-09-28T01:43:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.23203v2",
    "title": "CE-Nav: Flow-Guided Reinforcement Refinement for Cross-Embodiment Local Navigation",
    "authors": [
      "Yang",
      "Zhang",
      "Wang"
    ],
    "summary": "Generalizing local navigation policies across diverse robot morphologies is a critical challenge. Progress is often hindered by the need for costly and embodiment-specific data, the tight coupling of planning and control, and the \"disastrous averaging\" problem where deterministic models fail to capture multi-modal decisions (e.g., turning left or right). We introduce CE-Nav, a novel two-stage (IL-then-RL) framework that systematically decouples universal geometric reasoning from embodiment-specific dynamic adaptation. First, we train an embodiment-agnostic General Expert offline using imitation learning. This expert, a conditional normalizing flow model named VelFlow, learns the full distribution of kinematically-sound actions from a large-scale dataset generated by a classical planner, completely avoiding real robot data and resolving the multi-modality issue. Second, for a new robot, we freeze the expert and use it as a guiding prior to train a lightweight, Dynamics-Aware Refiner via online reinforcement learning. This refiner rapidly learns to compensate for the target robot's specific dynamics and controller imperfections with minimal environmental interaction. Extensive experiments on quadrupeds, bipeds, and quadrotors show that CE-Nav achieves state-of-the-art performance while drastically reducing adaptation cost. Successful real-world deployments further validate our approach as an efficient and scalable solution for building generalizable navigation systems. Code is available at https://github.com/amap-cvlab/CE-Nav.",
    "pdf_url": "https://arxiv.org/pdf/2509.23203v2",
    "github_url": "https://github.com/amap-cvlab/CE-Nav",
    "published": "2025-09-27T09:24:51+00:00",
    "updated": "2025-10-23T03:54:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.23118v1",
    "title": "EKF-Based Fusion of Wi-Fi/LiDAR/IMU for Indoor Localization and Navigation",
    "authors": [
      "Li",
      "Tang",
      "Kim"
    ],
    "summary": "Conventional Wi-Fi received signal strength indicator (RSSI) fingerprinting cannot meet the growing demand for accurate indoor localization and navigation due to its lower accuracy, while solutions based on light detection and ranging (LiDAR) can provide better localization performance but is limited by their higher deployment cost and complexity. To address these issues, we propose a novel indoor localization and navigation framework integrating Wi-Fi RSSI fingerprinting, LiDAR-based simultaneous localization and mapping (SLAM), and inertial measurement unit (IMU) navigation based on an extended Kalman filter (EKF). Specifically, coarse localization by deep neural network (DNN)-based Wi-Fi RSSI fingerprinting is refined by IMU-based dynamic positioning using a Gmapping-based SLAM to generate an occupancy grid map and output high-frequency attitude estimates, which is followed by EKF prediction-update integrating sensor information while effectively suppressing Wi-Fi-induced noise and IMU drift errors. Multi-group real-world experiments conducted on the IR building at Xi'an Jiaotong-Liverpool University demonstrates that the proposed multi-sensor fusion framework suppresses the instability caused by individual approaches and thereby provides stable accuracy across all path configurations with mean two-dimensional (2D) errors ranging from 0.2449 m to 0.3781 m. In contrast, the mean 2D errors of Wi-Fi RSSI fingerprinting reach up to 1.3404 m in areas with severe signal interference, and those of LiDAR/IMU localization are between 0.6233 m and 2.8803 m due to cumulative drift.",
    "pdf_url": "https://arxiv.org/pdf/2509.23118v1",
    "github_url": null,
    "published": "2025-09-27T04:59:04+00:00",
    "updated": "2025-09-27T04:59:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.22653v1",
    "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation",
    "authors": [
      "Hu",
      "Lin",
      "Lee"
    ],
    "summary": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harnesses VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, outperforming the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs. Project page: https://spf-web.pages.dev",
    "pdf_url": "https://arxiv.org/pdf/2509.22653v1",
    "github_url": null,
    "published": "2025-09-26T17:59:59+00:00",
    "updated": "2025-09-26T17:59:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.22548v1",
    "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation",
    "authors": [
      "Zeng",
      "Qi",
      "Chang"
    ],
    "summary": "Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2509.22548v1",
    "github_url": null,
    "published": "2025-09-26T16:29:37+00:00",
    "updated": "2025-09-26T16:29:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.22441v1",
    "title": "UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation",
    "authors": [
      "Wang",
      "Zhu",
      "Yan"
    ],
    "summary": "This paper presents UnderwaterVLA, a novel framework for autonomous underwater navigation that integrates multimodal foundation models with embodied intelligence systems. Underwater operations remain difficult due to hydrodynamic disturbances, limited communication bandwidth, and degraded sensing in turbid waters. To address these challenges, we introduce three innovations. First, a dual-brain architecture decouples high-level mission reasoning from low-level reactive control, enabling robust operation under communication and computational constraints. Second, we apply Vision-Language-Action(VLA) models to underwater robotics for the first time, incorporating structured chain-of-thought reasoning for interpretable decision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC) scheme compensates for fluid effects in real time without costly task-specific training. Experimental results in field tests show that UnderwaterVLA reduces navigation errors in degraded visual conditions while maintaining higher task completion by 19% to 27% over baseline. By minimizing reliance on underwater-specific training data and improving adaptability across environments, UnderwaterVLA provides a scalable and cost-effective path toward the next generation of intelligent AUVs.",
    "pdf_url": "https://arxiv.org/pdf/2509.22441v1",
    "github_url": null,
    "published": "2025-09-26T15:01:52+00:00",
    "updated": "2025-09-26T15:01:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.21930v1",
    "title": "DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation",
    "authors": [
      "Wang",
      "Chen"
    ],
    "summary": "Visual navigation is essential for robotics and embodied AI. However, existing foundation models, particularly those with transformer decoders, suffer from high computational overhead and lack interpretability, limiting their deployment in resource-tight scenarios. To address this, we propose DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer selection based on scene complexity. It employs a trainable hard feature selector for sparse operations, enhancing efficiency and interpretability. Additionally, we integrate feature selection into an early-exit mechanism, with Bayesian Optimization determining optimal exit thresholds to reduce computational cost. Extensive experiments in real-world-based datasets and simulated environments demonstrate the effectiveness of DynaNav. Compared to ViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time, and 32.8% lower memory usage, while improving navigation performance across four public datasets.",
    "pdf_url": "https://arxiv.org/pdf/2509.21930v1",
    "github_url": null,
    "published": "2025-09-26T06:15:31+00:00",
    "updated": "2025-09-26T06:15:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.21657v2",
    "title": "FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction",
    "authors": [
      "Dai",
      "Jiang",
      "Wang"
    ],
    "summary": "High-quality 3D world models are pivotal for embodied intelligence and Artificial General Intelligence (AGI), underpinning applications such as AR/VR content creation and robotic navigation. Despite the established strong imaginative priors, current video foundation models lack explicit 3D grounding capabilities, thus being limited in both spatial consistency and their utility for downstream 3D reasoning tasks. In this work, we present FantasyWorld, a geometry-enhanced framework that augments frozen video foundation models with a trainable geometric branch, enabling joint modeling of video latents and an implicit 3D field in a single forward pass. Our approach introduces cross-branch supervision, where geometry cues guide video generation and video priors regularize 3D prediction, thus yielding consistent and generalizable 3D-aware video representations. Notably, the resulting latents from the geometric branch can potentially serve as versatile representations for downstream 3D tasks such as novel view synthesis and navigation, without requiring per-scene optimization or fine-tuning. Extensive experiments show that FantasyWorld effectively bridges video imagination and 3D perception, outperforming recent geometry-consistent baselines in multi-view coherence and style consistency. Ablation studies further confirm that these gains stem from the unified backbone and cross-branch information exchange.",
    "pdf_url": "https://arxiv.org/pdf/2509.21657v2",
    "github_url": null,
    "published": "2025-09-25T22:24:23+00:00",
    "updated": "2025-10-31T08:16:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.21189v1",
    "title": "Human-like Navigation in a World Built for Humans",
    "authors": [
      "Chandaka",
      "Wang",
      "Chen"
    ],
    "summary": "When navigating in a man-made environment they haven't visited before--like an office building--humans employ behaviors such as reading signs and asking others for directions. These behaviors help humans reach their destinations efficiently by reducing the need to search through large areas. Existing robot navigation systems lack the ability to execute such behaviors and are thus highly inefficient at navigating within large environments. We present ReasonNav, a modular navigation system which integrates these human-like navigation skills by leveraging the reasoning capabilities of a vision-language model (VLM). We design compact input and output abstractions based on navigation landmarks, allowing the VLM to focus on language understanding and reasoning. We evaluate ReasonNav on real and simulated navigation tasks and show that the agent successfully employs higher-order reasoning to navigate efficiently in large, complex buildings.",
    "pdf_url": "https://arxiv.org/pdf/2509.21189v1",
    "github_url": null,
    "published": "2025-09-25T14:04:17+00:00",
    "updated": "2025-09-25T14:04:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.20839v1",
    "title": "SemSight: Probabilistic Bird's-Eye-View Prediction of Multi-Level Scene Semantics for Navigation",
    "authors": [
      "He",
      "Ren",
      "Yan"
    ],
    "summary": "In target-driven navigation and autonomous exploration, reasonable prediction of unknown regions is crucial for efficient navigation and environment understanding. Existing methods mostly focus on single objects or geometric occupancy maps, lacking the ability to model room-level semantic structures. We propose SemSight, a probabilistic bird's-eye-view prediction model for multi-level scene semantics. The model jointly infers structural layouts, global scene context, and target area distributions, completing semantic maps of unexplored areas while estimating probability maps for target categories. To train SemSight, we simulate frontier-driven exploration on 2,000 indoor layout graphs, constructing a diverse dataset of 40,000 sequential egocentric observations paired with complete semantic maps. We adopt an encoder-decoder network as the core architecture and introduce a mask-constrained supervision strategy. This strategy applies a binary mask of unexplored areas so that supervision focuses only on unknown regions, forcing the model to infer semantic structures from the observed context. Experimental results show that SemSight improves prediction performance for key functional categories in unexplored regions and outperforms non-mask-supervised approaches on metrics such as Structural Consistency (SC) and Region Recognition Accuracy (PA). It also enhances navigation efficiency in closed-loop simulations, reducing the number of search steps when guiding robots toward target areas.",
    "pdf_url": "https://arxiv.org/pdf/2509.20839v1",
    "github_url": null,
    "published": "2025-09-25T07:25:42+00:00",
    "updated": "2025-09-25T07:25:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.20748v1",
    "title": "AI-Enabled Crater-Based Navigation for Lunar Mapping",
    "authors": [
      "McLeod",
      "Chng",
      "Rodda"
    ],
    "summary": "Crater-Based Navigation (CBN) uses the ubiquitous impact craters of the Moon observed on images as natural landmarks to determine the six degrees of freedom pose of a spacecraft. To date, CBN has primarily been studied in the context of powered descent and landing. These missions are typically short in duration, with high-frequency imagery captured from a nadir viewpoint over well-lit terrain. In contrast, lunar mapping missions involve sparse, oblique imagery acquired under varying illumination conditions over potentially year-long campaigns, posing significantly greater challenges for pose estimation. We bridge this gap with STELLA - the first end-to-end CBN pipeline for long-duration lunar mapping. STELLA combines a Mask R-CNN-based crater detector, a descriptor-less crater identification module, a robust perspective-n-crater pose solver, and a batch orbit determination back-end. To rigorously test STELLA, we introduce CRESENT-365 - the first public dataset that emulates a year-long lunar mapping mission. Each of its 15,283 images is rendered from high-resolution digital elevation models with SPICE-derived Sun angles and Moon motion, delivering realistic global coverage, illumination cycles, and viewing geometries. Experiments on CRESENT+ and CRESENT-365 show that STELLA maintains metre-level position accuracy and sub-degree attitude accuracy on average across wide ranges of viewing angles, illumination conditions, and lunar latitudes. These results constitute the first comprehensive assessment of CBN in a true lunar mapping setting and inform operational conditions that should be considered for future missions.",
    "pdf_url": "https://arxiv.org/pdf/2509.20748v1",
    "github_url": null,
    "published": "2025-09-25T05:09:41+00:00",
    "updated": "2025-09-25T05:09:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.20739v1",
    "title": "SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning",
    "authors": [
      "Zhao",
      "Li",
      "Qi"
    ],
    "summary": "Conventional SLAM pipelines for legged robot navigation are fragile under rapid motion, calibration demands, and sensor drift, while offering limited semantic reasoning for task-driven exploration. To deal with these issues, we propose a vision-only, SLAM-free navigation framework that replaces dense geometry with semantic reasoning and lightweight topological representations. A hierarchical vision-language perception module fuses scene-level context with object-level cues for robust semantic inference. And a semantic-probabilistic topological map supports coarse-to-fine planning: LLM-based global reasoning for subgoal selection and vision-based local planning for obstacle avoidance. Integrated with reinforcement-learning locomotion controllers, the framework is deployable across diverse legged robot platforms. Experiments in simulation and real-world settings demonstrate consistent improvements in semantic accuracy, planning quality, and navigation success, while ablation studies further showcase the necessity of both hierarchical perception and fine local planning. This work introduces a new paradigm for SLAM-free, vision-language-driven navigation, shifting robotic exploration from geometry-centric mapping to semantics-driven decision making.",
    "pdf_url": "https://arxiv.org/pdf/2509.20739v1",
    "github_url": null,
    "published": "2025-09-25T04:38:45+00:00",
    "updated": "2025-09-25T04:38:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.20623v1",
    "title": "Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation",
    "authors": [
      "Das",
      "Chiu",
      "Huang"
    ],
    "summary": "Reinforcement learning has enabled significant progress in complex domains such as coordinating and navigating multiple quadrotors. However, even well-trained policies remain vulnerable to collisions in obstacle-rich environments. Addressing these infrequent but critical safety failures through retraining or fine-tuning is costly and risks degrading previously learned skills. Inspired by activation steering in large language models and latent editing in computer vision, we introduce a framework for inference-time Latent Activation Editing (LAE) that refines the behavior of pre-trained policies without modifying their weights or architecture. The framework operates in two stages: (i) an online classifier monitors intermediate activations to detect states associated with undesired behaviors, and (ii) an activation editing module that selectively modifies flagged activations to shift the policy towards safer regimes. In this work, we focus on improving safety in multi-quadrotor navigation. We hypothesize that amplifying a policy's internal perception of risk can induce safer behaviors. We instantiate this idea through a latent collision world model trained to predict future pre-collision activations, thereby prompting earlier and more cautious avoidance responses. Extensive simulations and real-world Crazyflie experiments demonstrate that LAE achieves statistically significant reduction in collisions (nearly 90% fewer cumulative collisions compared to the unedited baseline) and substantially increases the fraction of collision-free trajectories, while preserving task completion. More broadly, our results establish LAE as a lightweight paradigm, feasible on resource-constrained hardware, for post-deployment refinement of learned robot policies.",
    "pdf_url": "https://arxiv.org/pdf/2509.20623v1",
    "github_url": null,
    "published": "2025-09-24T23:58:23+00:00",
    "updated": "2025-09-24T23:58:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.20499v1",
    "title": "Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting",
    "authors": [
      "Li",
      "Li",
      "Wang"
    ],
    "summary": "With the rapid progress of foundation models and robotics, vision-language navigation (VLN) has emerged as a key task for embodied agents with broad practical applications. We address VLN in continuous environments, a particularly challenging setting where an agent must jointly interpret natural language instructions, perceive its surroundings, and plan low-level actions. We propose a zero-shot framework that integrates a simplified yet effective waypoint predictor with a multimodal large language model (MLLM). The predictor operates on an abstract obstacle map, producing linearly reachable waypoints, which are incorporated into a dynamically updated topological graph with explicit visitation records. The graph and visitation information are encoded into the prompt, enabling reasoning over both spatial structure and exploration history to encourage exploration and equip MLLM with local path planning for error correction. Extensive experiments on R2R-CE and RxR-CE show that our method achieves state-of-the-art zero-shot performance, with success rates of 41% and 36%, respectively, outperforming prior state-of-the-art methods.",
    "pdf_url": "https://arxiv.org/pdf/2509.20499v1",
    "github_url": null,
    "published": "2025-09-24T19:21:39+00:00",
    "updated": "2025-09-24T19:21:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.19970v1",
    "title": "Control and Navigation of a 2-D Electric Rocket",
    "authors": [
      "Fonte",
      "Santos",
      "Oliveira"
    ],
    "summary": "This work addresses the control and navigation of a simulated two-dimensional electric rocket. The model provides a simplified framework that neglects actuator dynamics and aerodynamic effects while capturing the complexities of underactuation and state coupling. Trajectory tracking is achieved through a modularized and layered control architecture, with employement of a Linear Quadratic Regulator (LQR) and Lyapunov theory. Full-state estimation is achieved through Kalman filtering techniques, part of the navigation module. The solutions are thoroughly evaluated in a custom-built MATLAB/Simulink testbed, simulating real-world conditions while maintaining a simplified setup. The results reveal limitations along the lateral axis, whose resolution is suggested for future work.",
    "pdf_url": "https://arxiv.org/pdf/2509.19970v1",
    "github_url": null,
    "published": "2025-09-24T10:26:03+00:00",
    "updated": "2025-09-24T10:26:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.19954v1",
    "title": "Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation",
    "authors": [
      "Song",
      "Du",
      "Saussus"
    ],
    "summary": "We propose a probabilistic shared-control solution for navigation, called Robot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe, effective assistance in human-robot interaction. RT-V2 jointly models a user's long-term behavioral patterns and their noisy, low-dimensional control signals by combining a prior intent model with a posterior update that accounts for real-time user input and environmental context. The prior captures the multimodal and history-dependent nature of user intent using recurrent neural networks and conditional variational autoencoders, while the posterior integrates this with uncertain user commands to infer desired actions. We conduct extensive experiments to validate RT-V2 across synthetic benchmarks, human-computer interaction studies with keyboard input, and brain-machine interface experiments with non-human primates. Results show that RT-V2 outperforms the state of the art in intent estimation, provides safe and efficient navigation support, and adequately balances user autonomy with assistive intervention. By unifying probabilistic modeling, reinforcement learning, and safe optimization, RT-V2 offers a principled and generalizable approach to shared control for diverse assistive technologies.",
    "pdf_url": "https://arxiv.org/pdf/2509.19954v1",
    "github_url": null,
    "published": "2025-09-24T10:02:33+00:00",
    "updated": "2025-09-24T10:02:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.19843v1",
    "title": "PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents",
    "authors": [
      "Ziliotto",
      "Akkara",
      "Daniele"
    ],
    "summary": "Recent advances in Embodied AI have enabled agents to perform increasingly complex tasks and adapt to diverse environments. However, deploying such agents in realistic human-centered scenarios, such as domestic households, remains challenging, particularly due to the difficulty of modeling individual human preferences and behaviors. In this work, we introduce PersONAL (PERSonalized Object Navigation And Localization, a comprehensive benchmark designed to study personalization in Embodied AI. Agents must identify, retrieve, and navigate to objects associated with specific users, responding to natural-language queries such as \"find Lily's backpack\". PersONAL comprises over 2,000 high-quality episodes across 30+ photorealistic homes from the HM3D dataset. Each episode includes a natural-language scene description with explicit associations between objects and their owners, requiring agents to reason over user-specific semantics. The benchmark supports two evaluation modes: (1) active navigation in unseen environments, and (2) object grounding in previously mapped scenes. Experiments with state-of-the-art baselines reveal a substantial gap to human performance, highlighting the need for embodied agents capable of perceiving, reasoning, and memorizing over personalized information; paving the way towards real-world assistive robot.",
    "pdf_url": "https://arxiv.org/pdf/2509.19843v1",
    "github_url": null,
    "published": "2025-09-24T07:39:16+00:00",
    "updated": "2025-09-24T07:39:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.19480v1",
    "title": "OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation",
    "authors": [
      "Hirose",
      "Glossop",
      "Shah"
    ],
    "summary": "Humans can flexibly interpret and compose different goal specifications, such as language instructions, spatial coordinates, or visual references, when navigating to a destination. In contrast, most existing robotic navigation policies are trained on a single modality, limiting their adaptability to real-world scenarios where different forms of goal specification are natural and complementary. In this work, we present a training framework for robotic foundation models that enables omni-modal goal conditioning for vision-based navigation. Our approach leverages a high-capacity vision-language-action (VLA) backbone and trains with three primary goal modalities: 2D poses, egocentric images, and natural language, as well as their combinations, through a randomized modality fusion strategy. This design not only expands the pool of usable datasets but also encourages the policy to develop richer geometric, semantic, and visual representations. The resulting model, OmniVLA, achieves strong generalization to unseen environments, robustness to scarce modalities, and the ability to follow novel natural language instructions. We demonstrate that OmniVLA outperforms specialist baselines across modalities and offers a flexible foundation for fine-tuning to new modalities and tasks. We believe OmniVLA provides a step toward broadly generalizable and flexible navigation policies, and a scalable path for building omni-modal robotic foundation models. We present videos showcasing OmniVLA performance and will release its checkpoints and training code on our project page.",
    "pdf_url": "https://arxiv.org/pdf/2509.19480v1",
    "github_url": null,
    "published": "2025-09-23T18:40:29+00:00",
    "updated": "2025-09-23T18:40:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.20401v2",
    "title": "SGAligner++: Cross-Modal Language-Aided 3D Scene Graph Alignment",
    "authors": [
      "Singh",
      "Sarkar",
      "Armeni"
    ],
    "summary": "Aligning 3D scene graphs is a crucial initial step for several applications in robot navigation and embodied perception. Current methods in 3D scene graph alignment often rely on single-modality point cloud data and struggle with incomplete or noisy input. We introduce SGAligner++, a cross-modal, language-aided framework for 3D scene graph alignment. Our method addresses the challenge of aligning partially overlapping scene observations across heterogeneous modalities by learning a unified joint embedding space, enabling accurate alignment even under low-overlap conditions and sensor noise. By employing lightweight unimodal encoders and attention-based fusion, SGAligner++ enhances scene understanding for tasks such as visual localization, 3D reconstruction, and navigation, while ensuring scalability and minimal computational overhead. Extensive evaluations on real-world datasets demonstrate that SGAligner++ outperforms state-of-the-art methods by up to 40% on noisy real-world reconstructions, while enabling cross-modal generalization.",
    "pdf_url": "https://arxiv.org/pdf/2509.20401v2",
    "github_url": null,
    "published": "2025-09-23T18:31:29+00:00",
    "updated": "2025-10-16T15:30:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.19452v3",
    "title": "HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames",
    "authors": [
      "Saviolo",
      "Mao",
      "Loianno"
    ],
    "summary": "Search and rescue operations require unmanned aerial vehicles to both traverse unknown unstructured environments at high speed and track targets once detected. Achieving both capabilities under degraded sensing and without global localization remains an open challenge. Recent works on relative navigation have shown robust tracking by anchoring planning and control to a visible detected object, but cannot address navigation when no target is in the field of view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time framework that unifies traversal, acquisition, and tracking within a single relative formulation. HUNT defines navigation objectives directly from onboard instantaneous observables such as attitude, altitude, and velocity, enabling reactive high-speed flight during search. Once a target is detected, the same perception-control pipeline transitions seamlessly to tracking. Outdoor experiments in dense forests, container compounds, and search-and-rescue operations with vehicles and mannequins demonstrate robust autonomy where global methods fail.",
    "pdf_url": "https://arxiv.org/pdf/2509.19452v3",
    "github_url": null,
    "published": "2025-09-23T18:07:10+00:00",
    "updated": "2025-09-28T18:17:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.19168v1",
    "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination",
    "authors": [
      "Gonzales",
      "Oh",
      "Moore"
    ],
    "summary": "In this paper, we present a receding-horizon, sampling-based planner capable of reasoning over multimodal policy distributions. By using the cross-entropy method to optimize a multimodal policy under a common cost function, our approach increases robustness against local minima and promotes effective exploration of the solution space. We show that our approach naturally extends to multi-robot collision-free planning, enables agents to share diverse candidate policies to avoid deadlocks, and allows teams to minimize a global objective without incurring the computational complexity of centralized optimization. Numerical simulations demonstrate that employing multiple modes significantly improves success rates in trap environments and in multi-robot collision avoidance. Hardware experiments further validate the approach's real-time feasibility and practical performance.",
    "pdf_url": "https://arxiv.org/pdf/2509.19168v1",
    "github_url": null,
    "published": "2025-09-23T15:43:18+00:00",
    "updated": "2025-09-23T15:43:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.19105v2",
    "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation",
    "authors": [
      "Prajapati",
      "Trivedi",
      "Hanson"
    ],
    "summary": "Successful navigation in outdoor environments requires accurate prediction of the physical interactions between the robot and the terrain. Many prior methods rely on geometric or semantic labels to classify traversable surfaces. However, such labels cannot distinguish visually similar surfaces that differ in material properties. Spectral sensors enable inference of material composition from surface reflectance measured across multiple wavelength bands. Although spectral sensing is gaining traction in robotics, widespread deployment remains constrained by the need for custom hardware integration, high sensor costs, and compute-intensive processing pipelines. In this paper, we present the RGB Image to Spectral Signature Neural Network (RS-Net), a deep neural network designed to bridge the gap between the accessibility of RGB sensing and the rich material information provided by spectral data. RS-Net predicts spectral signatures from RGB patches, which we map to terrain labels and friction coefficients. The resulting terrain classifications are integrated into a sampling-based motion planner for a wheeled robot operating in outdoor environments. Likewise, the friction estimates are incorporated into a contact-force-based MPC for a quadruped robot navigating slippery surfaces. Overall, our framework learns the task-relevant physical properties offline during training and thereafter relies solely on RGB sensing at run time.",
    "pdf_url": "https://arxiv.org/pdf/2509.19105v2",
    "github_url": null,
    "published": "2025-09-23T14:49:48+00:00",
    "updated": "2025-11-28T00:09:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.19002v2",
    "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction",
    "authors": [
      "Wang",
      "Murata",
      "Zhang"
    ],
    "summary": "Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.",
    "pdf_url": "https://arxiv.org/pdf/2509.19002v2",
    "github_url": null,
    "published": "2025-09-23T13:46:31+00:00",
    "updated": "2025-11-15T10:09:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.21377v1",
    "title": "Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation",
    "authors": [
      "Yu",
      "Zhang",
      "Zhu"
    ],
    "summary": "Audiovisual embodied navigation enables robots to locate audio sources by dynamically integrating visual observations from onboard sensors with the auditory signals emitted by the target. The core challenge lies in effectively leveraging multimodal cues to guide navigation. While prior works have explored basic fusion of visual and audio data, they often overlook deeper perceptual context. To address this, we propose the Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target architecture coupled with a refined Transformer mechanism to filter and selectively fuse cross-modal information. Extensive experiments on the Replica and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art performance, outperforming existing methods in success rate (SR), path efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits strong scalability and generalizability, paving the way for advanced multimodal fusion strategies in robotic navigation. The code and videos are available at   https://github.com/zzzmmm-svg/DMTF.",
    "pdf_url": "https://arxiv.org/pdf/2509.21377v1",
    "github_url": "https://github.com/zzzmmm-svg/DMTF",
    "published": "2025-09-23T09:31:00+00:00",
    "updated": "2025-09-23T09:31:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.18734v2",
    "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation",
    "authors": [
      "Doshi",
      "Sutavani",
      "Gujar"
    ],
    "summary": "One of the challenges faced by Autonomous Aerial Vehicles is reliable navigation through urban environments. Factors like reduction in precision of Global Positioning System (GPS), narrow spaces and dynamically moving obstacles make the path planning of an aerial robot a complicated task. One of the skills required for the agent to effectively navigate through such an environment is to develop an ability to avoid collisions using information from onboard depth sensors. In this paper, we propose Reinforcement Learning of a virtual quadcopter robot agent equipped with a Depth Camera to navigate through a simulated urban environment.",
    "pdf_url": "https://arxiv.org/pdf/2509.18734v2",
    "github_url": null,
    "published": "2025-09-23T07:27:48+00:00",
    "updated": "2025-12-14T20:22:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.18671v1",
    "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout",
    "authors": [
      "Chai",
      "Lee",
      "Lim"
    ],
    "summary": "In mobile manipulation, the manipulation policy has strong preferences for initial poses where it is executed. However, the navigation module focuses solely on reaching the task area, without considering which initial pose is preferable for downstream manipulation. To address this misalignment, we introduce N2M, a transition module that guides the robot to a preferable initial pose after reaching the task area, thereby substantially improving task success rates. N2M features five key advantages: (1) reliance solely on ego-centric observation without requiring global or historical information; (2) real-time adaptation to environmental changes; (3) reliable prediction with high viewpoint robustness; (4) broad applicability across diverse tasks, manipulation policies, and robot hardware; and (5) remarkable data efficiency and generalizability. We demonstrate the effectiveness of N2M through extensive simulation and real-world experiments. In the PnPCounterToCab task, N2M improves the averaged success rate from 3% with the reachability-based baseline to 54%. Furthermore, in the Toybox Handover task, N2M provides reliable predictions even in unseen environments with only 15 data samples, showing remarkable data efficiency and generalizability.",
    "pdf_url": "https://arxiv.org/pdf/2509.18671v1",
    "github_url": null,
    "published": "2025-09-23T05:41:59+00:00",
    "updated": "2025-09-23T05:41:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.18610v1",
    "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones",
    "authors": [
      "Adang",
      "Low",
      "Shorinwa"
    ],
    "summary": "Large vision-language models have driven remarkable progress in open-vocabulary robot policies, e.g., generalist robot manipulation policies, that enable robots to complete complex tasks specified in natural language. Despite these successes, open-vocabulary autonomous drone navigation remains an unsolved challenge due to the scarcity of large-scale demonstrations, real-time control demands of drones for stabilization, and lack of reliable external pose estimation modules. In this work, we present SINGER for language-guided autonomous drone navigation in the open world using only onboard sensing and compute. To train robust, open-vocabulary navigation policies, SINGER leverages three central components: (i) a photorealistic language-embedded flight simulator with minimal sim-to-real gap using Gaussian Splatting for efficient data generation, (ii) an RRT-inspired multi-trajectory generation expert for collision-free navigation demonstrations, and these are used to train (iii) a lightweight end-to-end visuomotor policy for real-time closed-loop control. Through extensive hardware flight experiments, we demonstrate superior zero-shot sim-to-real transfer of our policy to unseen environments and unseen language-conditioned goal objects. When trained on ~700k-1M observation action pairs of language conditioned visuomotor data and deployed on hardware, SINGER outperforms a velocity-controlled semantic guidance baseline by reaching the query 23.33% more on average, and maintains the query in the field of view 16.67% more on average, with 10% fewer collisions.",
    "pdf_url": "https://arxiv.org/pdf/2509.18610v1",
    "github_url": null,
    "published": "2025-09-23T03:57:34+00:00",
    "updated": "2025-09-23T03:57:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.18608v2",
    "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning",
    "authors": [
      "Mineiro",
      "Affonso",
      "Becker"
    ],
    "summary": "Reliable navigation in under-canopy agricultural environments remains a challenge due to GNSS unreliability, cluttered rows, and variable lighting. To address these limitations, we present an end-to-end learning-based navigation system that maps raw 3D LiDAR data directly to control commands using a deep reinforcement learning policy trained entirely in simulation. Our method includes a voxel-based downsampling strategy that reduces LiDAR input size by 95.83%, enabling efficient policy learning without relying on labeled datasets or manually designed control interfaces. The policy was validated in simulation, achieving a 100% success rate in straight-row plantations and showing a gradual decline in performance as row curvature increased, tested across varying sinusoidal frequencies and amplitudes.",
    "pdf_url": "https://arxiv.org/pdf/2509.18608v2",
    "github_url": null,
    "published": "2025-09-23T03:56:10+00:00",
    "updated": "2025-11-03T20:29:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.18592v1",
    "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
    "authors": [
      "Bhatt",
      "Yang",
      "Siva"
    ],
    "summary": "Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task-location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments. Codebase, datasets, and videos for VLN-Zero are available at: https://vln-zero.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2509.18592v1",
    "github_url": null,
    "published": "2025-09-23T03:23:03+00:00",
    "updated": "2025-09-23T03:23:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.18501v1",
    "title": "BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation",
    "authors": [
      "Fehrentz",
      "Winkler",
      "Heiliger"
    ],
    "summary": "We introduce BridgeSplat, a novel approach for deformable surgical navigation that couples intraoperative 3D reconstruction with preoperative CT data to bridge the gap between surgical video and volumetric patient data. Our method rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian parameters and mesh deformation through photometric supervision. By parametrizing each Gaussian relative to its parent mesh triangle, we enforce alignment between Gaussians and mesh and obtain deformations that can be propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on visceral pig surgeries and synthetic data of a human liver under simulation, showing sensible deformations of the preoperative CT on monocular RGB data. Code, data, and additional resources can be found at https://maxfehrentz.github.io/ct-informed-splatting/ .",
    "pdf_url": "https://arxiv.org/pdf/2509.18501v1",
    "github_url": null,
    "published": "2025-09-23T01:09:36+00:00",
    "updated": "2025-09-23T01:09:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.18407v1",
    "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections",
    "authors": [
      "Tiwari",
      "Vazhaeparampil",
      "Preston"
    ],
    "summary": "Uncontrolled intersections account for a significant fraction of roadway crashes due to ambiguous right-of-way rules, occlusions, and unpredictable driver behavior. While autonomous vehicle research has explored uncertainty-aware decision making, few systems exist to retrofit human-operated vehicles with assistive navigation support. We present a driver-assist framework for right-of-way reasoning at uncontrolled intersections, formulated as a Partially Observable Markov Decision Process (POMDP). Using a custom simulation testbed with stochastic traffic agents, pedestrians, occlusions, and adversarial scenarios, we evaluate four decision-making approaches: a deterministic finite state machine (FSM), and three probabilistic planners: QMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform the rule-based baseline, achieving up to 97.5 percent collision-free navigation under partial observability, with POMCP prioritizing safety and DESPOT balancing efficiency and runtime feasibility. Our findings highlight the importance of uncertainty-aware planning for driver assistance and motivate future integration of sensor fusion and environment perception modules for real-time deployment in realistic traffic environments.",
    "pdf_url": "https://arxiv.org/pdf/2509.18407v1",
    "github_url": null,
    "published": "2025-09-22T20:46:23+00:00",
    "updated": "2025-09-22T20:46:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.18384v1",
    "title": "AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback",
    "authors": [
      "Yang",
      "Hong",
      "Perin"
    ],
    "summary": "Large language models (LLMs) can translate natural language instructions into executable action plans for robotics, autonomous driving, and other domains. Yet, deploying LLM-driven planning in the physical world demands strict adherence to safety and regulatory constraints, which current models often violate due to hallucination or weak alignment. Traditional data-driven alignment methods, such as Direct Preference Optimization (DPO), require costly human labeling, while recent formal-feedback approaches still depend on resource-intensive fine-tuning. In this paper, we propose LAD-VF, a fine-tuning-free framework that leverages formal verification feedback for automated prompt engineering. By introducing a formal-verification-informed text loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts rather than model parameters. This yields three key benefits: (i) scalable adaptation without fine-tuning; (ii) compatibility with modular LLM architectures; and (iii) interpretable refinement via auditable prompts. Experiments in robot navigation and manipulation tasks demonstrate that LAD-VF substantially enhances specification compliance, improving success rates from 60% to over 90%. Our method thus presents a scalable and interpretable pathway toward trustworthy, formally-verified LLM-driven control systems.",
    "pdf_url": "https://arxiv.org/pdf/2509.18384v1",
    "github_url": null,
    "published": "2025-09-22T20:14:32+00:00",
    "updated": "2025-09-22T20:14:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.17941v1",
    "title": "ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion",
    "authors": [
      "Hu",
      "Tang",
      "Munje"
    ],
    "summary": "This paper considers the problem of enabling robots to navigate dynamic environments while following instructions. The challenge lies in the combinatorial nature of instruction specifications: each instruction can include multiple specifications, and the number of possible specification combinations grows exponentially as the robot's skill set expands. For example, \"overtake the pedestrian while staying on the right side of the road\" consists of two specifications: \"overtake the pedestrian\" and \"walk on the right side of the road.\" To tackle this challenge, we propose ComposableNav, based on the intuition that following an instruction involves independently satisfying its constituent specifications, each corresponding to a distinct motion primitive. Using diffusion models, ComposableNav learns each primitive separately, then composes them in parallel at deployment time to satisfy novel combinations of specifications unseen in training. Additionally, to avoid the onerous need for demonstrations of individual motion primitives, we propose a two-stage training procedure: (1) supervised pre-training to learn a base diffusion model for dynamic navigation, and (2) reinforcement learning fine-tuning that molds the base model into different motion primitives. Through simulation and real-world experiments, we show that ComposableNav enables robots to follow instructions by generating trajectories that satisfy diverse and unseen combinations of specifications, significantly outperforming both non-compositional VLM-based policies and costmap composing baselines. Videos and additional materials can be found on the project page: https://amrl.cs.utexas.edu/ComposableNav/",
    "pdf_url": "https://arxiv.org/pdf/2509.17941v1",
    "github_url": null,
    "published": "2025-09-22T16:04:50+00:00",
    "updated": "2025-09-22T16:04:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.17500v1",
    "title": "SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge",
    "authors": [
      "Xie",
      "Zhang",
      "Liu"
    ],
    "summary": "Large-scale Video Object Segmentation (LSVOS) addresses the challenge of accurately tracking and segmenting objects in long video sequences, where difficulties stem from object reappearance, small-scale targets, heavy occlusions, and crowded scenes. Existing approaches predominantly adopt SAM2-based frameworks with various memory mechanisms for complex video mask generation. In this report, we proposed Segment Anything with Memory Strengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE track of ICCV 2025, which integrates the strengths of stateof-the-art VOS models into an effective paradigm. To handle visually similar instances and long-term object disappearance in MOSE, we incorporate a long-term memorymodule for reliable object re-identification. Additionly, we adopt SAM2Long as a post-processing strategy to reduce error accumulation and enhance segmentation stability in long video sequences. Our method achieved a final performance of 0.8427 in terms of J &F in the test-set leaderboard.",
    "pdf_url": "https://arxiv.org/pdf/2509.17500v1",
    "github_url": null,
    "published": "2025-09-22T08:30:34+00:00",
    "updated": "2025-09-22T08:30:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.17435v1",
    "title": "GPS Denied IBVS-Based Navigation and Collision Avoidance of UAV Using a Low-Cost RGB Camera",
    "authors": [
      "Wang",
      "Tan",
      "Leong"
    ],
    "summary": "This paper proposes an image-based visual servoing (IBVS) framework for UAV navigation and collision avoidance using only an RGB camera. While UAV navigation has been extensively studied, it remains challenging to apply IBVS in missions involving multiple visual targets and collision avoidance. The proposed method achieves navigation without explicit path planning, and collision avoidance is realized through AI-based monocular depth estimation from RGB images. Unlike approaches that rely on stereo cameras or external workstations, our framework runs fully onboard a Jetson platform, ensuring a self-contained and deployable system. Experimental results validate that the UAV can navigate across multiple AprilTags and avoid obstacles effectively in GPS-denied environments.",
    "pdf_url": "https://arxiv.org/pdf/2509.17435v1",
    "github_url": null,
    "published": "2025-09-22T07:26:40+00:00",
    "updated": "2025-09-22T07:26:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.17430v2",
    "title": "EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device",
    "authors": [
      "Chhablani",
      "Ye",
      "Irshad"
    ],
    "summary": "The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20% and 40% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87-0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: https://gchhablani.github.io/embodied-splat.",
    "pdf_url": "https://arxiv.org/pdf/2509.17430v2",
    "github_url": null,
    "published": "2025-09-22T07:22:31+00:00",
    "updated": "2025-09-23T03:58:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.17340v1",
    "title": "AERO-MPPI: Anchor-Guided Ensemble Trajectory Optimization for Agile Mapless Drone Navigation",
    "authors": [
      "Chen",
      "Huang",
      "Tang"
    ],
    "summary": "Agile mapless navigation in cluttered 3D environments poses significant challenges for autonomous drones. Conventional mapping-planning-control pipelines incur high computational cost and propagate estimation errors. We present AERO-MPPI, a fully GPU-accelerated framework that unifies perception and planning through an anchor-guided ensemble of Model Predictive Path Integral (MPPI) optimizers. Specifically, we design a multi-resolution LiDAR point-cloud representation that rapidly extracts spatially distributed \"anchors\" as look-ahead intermediate endpoints, from which we construct polynomial trajectory guides to explore distinct homotopy path classes. At each planning step, we run multiple MPPI instances in parallel and evaluate them with a two-stage multi-objective cost that balances collision avoidance and goal reaching. Implemented entirely with NVIDIA Warp GPU kernels, AERO-MPPI achieves real-time onboard operation and mitigates the local-minima failures of single-MPPI approaches. Extensive simulations in forests, verticals, and inclines demonstrate sustained reliable flight above 7 m/s, with success rates above 80% and smoother trajectories compared to state-of-the-art baselines. Real-world experiments on a LiDAR-equipped quadrotor with NVIDIA Jetson Orin NX 16G confirm that AERO-MPPI runs in real time onboard and consistently achieves safe, agile, and robust flight in complex cluttered environments. The code will be open-sourced upon acceptance of the paper.",
    "pdf_url": "https://arxiv.org/pdf/2509.17340v1",
    "github_url": null,
    "published": "2025-09-22T03:21:51+00:00",
    "updated": "2025-09-22T03:21:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.17204v2",
    "title": "Ratatouille: Imitation Learning Ingredients for Real-world Social Robot Navigation",
    "authors": [
      "Han",
      "Vanniasinghe",
      "Sahak"
    ],
    "summary": "Scaling Reinforcement Learning to in-the-wild social robot navigation is both data-intensive and unsafe, since policies must learn through direct interaction and inevitably encounter collisions. Offline Imitation learning (IL) avoids these risks by collecting expert demonstrations safely, training entirely offline, and deploying policies zero-shot. However, we find that naively applying Behaviour Cloning (BC) to social navigation is insufficient; achieving strong performance requires careful architectural and training choices. We present Ratatouille, a pipeline and model architecture that, without changing the data, reduces collisions per meter by 6 times and improves success rate by 3 times compared to naive BC. We validate our approach in both simulation and the real world, where we collected over 11 hours of data on a dense university campus. We further demonstrate qualitative results in a public food court. Our findings highlight that thoughtful IL design, rather than additional data, can substantially improve safety and reliability in real-world social navigation. Video: https://youtu.be/tOdLTXsaYLQ. Code will be released after acceptance.",
    "pdf_url": "https://arxiv.org/pdf/2509.17204v2",
    "github_url": null,
    "published": "2025-09-21T19:17:39+00:00",
    "updated": "2025-09-23T14:11:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.16924v1",
    "title": "Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation",
    "authors": [
      "Li",
      "Yu",
      "Wang"
    ],
    "summary": "In audio-visual navigation (AVN) tasks, an embodied agent must autonomously localize a sound source in unknown and complex 3D environments based on audio-visual signals. Existing methods often rely on static modality fusion strategies and neglect the spatial cues embedded in stereo audio, leading to performance degradation in cluttered or occluded scenes. To address these issues, we propose an end-to-end reinforcement learning-based AVN framework with two key innovations: (1) a \\textbf{S}tereo-Aware \\textbf{A}ttention \\textbf{M}odule (\\textbf{SAM}), which learns and exploits the spatial disparity between left and right audio channels to enhance directional sound perception; and (2) an \\textbf{A}udio-\\textbf{G}uided \\textbf{D}ynamic \\textbf{F}usion Module (\\textbf{AGDF}), which dynamically adjusts the fusion ratio between visual and auditory features based on audio cues, thereby improving robustness to environmental changes. Extensive experiments are conducted on two realistic 3D scene datasets, Replica and Matterport3D, demonstrating that our method significantly outperforms existing approaches in terms of navigation success rate and path efficiency. Notably, our model achieves over 40\\% improvement under audio-only conditions compared to the best-performing baselines. These results highlight the importance of explicitly modeling spatial cues from stereo channels and performing deep multi-modal fusion for robust and efficient audio-visual navigation.",
    "pdf_url": "https://arxiv.org/pdf/2509.16924v1",
    "github_url": null,
    "published": "2025-09-21T05:11:09+00:00",
    "updated": "2025-09-21T05:11:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.22698v1",
    "title": "Advancing Audio-Visual Navigation Through Multi-Agent Collaboration in 3D Environments",
    "authors": [
      "Zhang",
      "Yu",
      "Wang"
    ],
    "summary": "Intelligent agents often require collaborative strategies to achieve complex tasks beyond individual capabilities in real-world scenarios. While existing audio-visual navigation (AVN) research mainly focuses on single-agent systems, their limitations emerge in dynamic 3D environments where rapid multi-agent coordination is critical, especially for time-sensitive applications like emergency response. This paper introduces MASTAVN (Multi-Agent Scalable Transformer Audio-Visual Navigation), a scalable framework enabling two agents to collaboratively localize and navigate toward an audio target in shared 3D environments. By integrating cross-agent communication protocols and joint audio-visual fusion mechanisms, MASTAVN enhances spatial reasoning and temporal synchronization. Through rigorous evaluation in photorealistic 3D simulators (Replica and Matterport3D), MASTAVN achieves significant reductions in task completion time and notable improvements in navigation success rates compared to single-agent and non-collaborative baselines. This highlights the essential role of spatiotemporal coordination in multi-agent systems. Our findings validate MASTAVN's effectiveness in time-sensitive emergency scenarios and establish a paradigm for advancing scalable multi-agent embodied intelligence in complex 3D environments.",
    "pdf_url": "https://arxiv.org/pdf/2509.22698v1",
    "github_url": null,
    "published": "2025-09-21T05:05:26+00:00",
    "updated": "2025-09-21T05:05:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.18200v1",
    "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought",
    "authors": [
      "Huang"
    ],
    "summary": "Conversational agents must translate egocentric utterances (e.g., \"on my right\") into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation.",
    "pdf_url": "https://arxiv.org/pdf/2509.18200v1",
    "github_url": null,
    "published": "2025-09-20T05:25:32+00:00",
    "updated": "2025-09-20T05:25:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.16445v1",
    "title": "FiLM-Nav: Efficient and Generalizable Navigation via VLM Fine-tuning",
    "authors": [
      "Yokoyama",
      "Ha"
    ],
    "summary": "Enabling robotic assistants to navigate complex environments and locate objects described in free-form language is a critical capability for real-world deployment. While foundation models, particularly Vision-Language Models (VLMs), offer powerful semantic understanding, effectively adapting their web-scale knowledge for embodied decision-making remains a key challenge. We present FiLM-Nav (Fine-tuned Language Model for Navigation), an approach that directly fine-tunes pre-trained VLM as the navigation policy. In contrast to methods that use foundation models primarily in a zero-shot manner or for map annotation, FiLM-Nav learns to select the next best exploration frontier by conditioning directly on raw visual trajectory history and the navigation goal. Leveraging targeted simulated embodied experience allows the VLM to ground its powerful pre-trained representations in the specific dynamics and visual patterns relevant to goal-driven navigation. Critically, fine-tuning on a diverse data mixture combining ObjectNav, OVON, ImageNav, and an auxiliary spatial reasoning task proves essential for achieving robustness and broad generalization. FiLM-Nav sets a new state-of-the-art in both SPL and success rate on HM3D ObjectNav among open-vocabulary methods, and sets a state-of-the-art SPL on the challenging HM3D-OVON benchmark, demonstrating strong generalization to unseen object categories. Our work validates that directly fine-tuning VLMs on diverse simulated embodied data is a highly effective pathway towards generalizable and efficient semantic navigation capabilities.",
    "pdf_url": "https://arxiv.org/pdf/2509.16445v1",
    "github_url": null,
    "published": "2025-09-19T21:51:42+00:00",
    "updated": "2025-09-19T21:51:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.16412v1",
    "title": "Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation",
    "authors": [
      "Deng",
      "Gao",
      "Jose"
    ],
    "summary": "Coordinated multi-robot navigation is essential for robots to operate as a team in diverse environments. During navigation, robot teams usually need to maintain specific formations, such as circular formations to protect human teammates at the center. However, in complex scenarios such as narrow corridors, rigidly preserving predefined formations can become infeasible. Therefore, robot teams must be capable of dynamically splitting into smaller subteams and adaptively controlling the subteams to navigate through such scenarios while preserving formations. To enable this capability, we introduce a novel method for SubTeaming and Adaptive Formation (STAF), which is built upon a unified hierarchical learning framework: (1) high-level deep graph cut for team splitting, (2) intermediate-level graph learning for facilitating coordinated navigation among subteams, and (3) low-level policy learning for controlling individual mobile robots to reach their goal positions while avoiding collisions. To evaluate STAF, we conducted extensive experiments in both indoor and outdoor environments using robotics simulations and physical robot teams. Experimental results show that STAF enables the novel capability for subteaming and adaptive formation control, and achieves promising performance in coordinated multi-robot navigation through challenging scenarios. More details are available on the project website: https://hcrlab.gitlab.io/project/STAF.",
    "pdf_url": "https://arxiv.org/pdf/2509.16412v1",
    "github_url": null,
    "published": "2025-09-19T20:49:48+00:00",
    "updated": "2025-09-19T20:49:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.16353v1",
    "title": "Tactile-Based Human Intent Recognition for Robot Assistive Navigation",
    "authors": [
      "Peng",
      "Crowder",
      "Yuan"
    ],
    "summary": "Robot assistive navigation (RAN) is critical for enhancing the mobility and independence of the growing population of mobility-impaired individuals. However, existing systems often rely on interfaces that fail to replicate the intuitive and efficient physical communication observed between a person and a human caregiver, limiting their effectiveness. In this paper, we introduce Tac-Nav, a RAN system that leverages a cylindrical tactile skin mounted on a Stretch 3 mobile manipulator to provide a more natural and efficient interface for human navigational intent recognition. To robustly classify the tactile data, we developed the Cylindrical Kernel Support Vector Machine (CK-SVM), an algorithm that explicitly models the sensor's cylindrical geometry and is consequently robust to the natural rotational shifts present in a user's grasp. Comprehensive experiments were conducted to demonstrate the effectiveness of our classification algorithm and the overall system. Results show that CK-SVM achieved superior classification accuracy on both simulated (97.1%) and real-world (90.8%) datasets compared to four baseline models. Furthermore, a pilot study confirmed that users more preferred the Tac-Nav tactile interface over conventional joystick and voice-based controls.",
    "pdf_url": "https://arxiv.org/pdf/2509.16353v1",
    "github_url": null,
    "published": "2025-09-19T18:50:01+00:00",
    "updated": "2025-09-19T18:50:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.15917v1",
    "title": "An MPC framework for efficient navigation of mobile robots in cluttered environments",
    "authors": [
      "Köhler",
      "Zhang",
      "Soloperto"
    ],
    "summary": "We present a model predictive control (MPC) framework for efficient navigation of mobile robots in cluttered environments. The proposed approach integrates a finite-segment shortest path planner into the finite-horizon trajectory optimization of the MPC. This formulation ensures convergence to dynamically selected targets and guarantees collision avoidance, even under general nonlinear dynamics and cluttered environments. The approach is validated through hardware experiments on a small ground robot, where a human operator dynamically assigns target locations. The robot successfully navigated through complex environments and reached new targets within 2-3 seconds.",
    "pdf_url": "https://arxiv.org/pdf/2509.15917v1",
    "github_url": null,
    "published": "2025-09-19T12:13:16+00:00",
    "updated": "2025-09-19T12:13:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.15400v1",
    "title": "Exploring multimodal implicit behavior learning for vehicle navigation in simulated cities",
    "authors": [
      "Antonelo",
      "Couto",
      "Möller"
    ],
    "summary": "Standard Behavior Cloning (BC) fails to learn multimodal driving decisions, where multiple valid actions exist for the same scenario. We explore Implicit Behavioral Cloning (IBC) with Energy-Based Models (EBMs) to better capture this multimodality. We propose Data-Augmented IBC (DA-IBC), which improves learning by perturbing expert actions to form the counterexamples of IBC training and using better initialization for derivative-free inference. Experiments in the CARLA simulator with Bird's-Eye View inputs demonstrate that DA-IBC outperforms standard IBC in urban driving tasks designed to evaluate multimodal behavior learning in a test environment. The learned energy landscapes are able to represent multimodal action distributions, which BC fails to achieve.",
    "pdf_url": "https://arxiv.org/pdf/2509.15400v1",
    "github_url": null,
    "published": "2025-09-18T20:17:29+00:00",
    "updated": "2025-09-18T20:17:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.15180v1",
    "title": "Parallel Simulation of Contact and Actuation for Soft Growing Robots",
    "authors": [
      "Gao",
      "Chen",
      "Bhovad"
    ],
    "summary": "Soft growing robots, commonly referred to as vine robots, have demonstrated remarkable ability to interact safely and robustly with unstructured and dynamic environments. It is therefore natural to exploit contact with the environment for planning and design optimization tasks. Previous research has focused on planning under contact for passively deforming robots with pre-formed bends. However, adding active steering to these soft growing robots is necessary for successful navigation in more complex environments. To this end, we develop a unified modeling framework that integrates vine robot growth, bending, actuation, and obstacle contact. We extend the beam moment model to include the effects of actuation on kinematics under growth and then use these models to develop a fast parallel simulation framework. We validate our model and simulator with real robot experiments. To showcase the capabilities of our framework, we apply our model in a design optimization task to find designs for vine robots navigating through cluttered environments, identifying designs that minimize the number of required actuators by exploiting environmental contacts. We show the robustness of the designs to environmental and manufacturing uncertainties. Finally, we fabricate an optimized design and successfully deploy it in an obstacle-rich environment.",
    "pdf_url": "https://arxiv.org/pdf/2509.15180v1",
    "github_url": null,
    "published": "2025-09-18T17:38:17+00:00",
    "updated": "2025-09-18T17:38:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.15062v1",
    "title": "Energy-Constrained Navigation for Planetary Rovers under Hybrid RTG-Solar Power",
    "authors": [
      "Hu",
      "Guo",
      "Liu"
    ],
    "summary": "Future planetary exploration rovers must operate for extended durations on hybrid power inputs that combine steady radioisotope thermoelectric generator (RTG) output with variable solar photovoltaic (PV) availability. While energy-aware planning has been studied for aerial and underwater robots under battery limits, few works for ground rovers explicitly model power flow or enforce instantaneous power constraints. Classical terrain-aware planners emphasize slope or traversability, and trajectory optimization methods typically focus on geometric smoothness and dynamic feasibility, neglecting energy feasibility. We present an energy-constrained trajectory planning framework that explicitly integrates physics-based models of translational, rotational, and resistive power with baseline subsystem loads, under hybrid RTG-solar input. By incorporating both cumulative energy budgets and instantaneous power constraints into SE(2)-based polynomial trajectory optimization, the method ensures trajectories that are simultaneously smooth, dynamically feasible, and power-compliant. Simulation results on lunar-like terrain show that our planner generates trajectories with peak power within 0.55 percent of the prescribed limit, while existing methods exceed limits by over 17 percent. This demonstrates a principled and practical approach to energy-aware autonomy for long-duration planetary missions.",
    "pdf_url": "https://arxiv.org/pdf/2509.15062v1",
    "github_url": null,
    "published": "2025-09-18T15:25:56+00:00",
    "updated": "2025-09-18T15:25:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.14978v1",
    "title": "PA-MPPI: Perception-Aware Model Predictive Path Integral Control for Quadrotor Navigation in Unknown Environments",
    "authors": [
      "Zhai",
      "Reiter",
      "Scaramuzza"
    ],
    "summary": "Quadrotor navigation in unknown environments is critical for practical missions such as search-and-rescue. Solving it requires addressing three key challenges: the non-convexity of free space due to obstacles, quadrotor-specific dynamics and objectives, and the need for exploration of unknown regions to find a path to the goal. Recently, the Model Predictive Path Integral (MPPI) method has emerged as a promising solution that solves the first two challenges. By leveraging sampling-based optimization, it can effectively handle non-convex free space while directly optimizing over the full quadrotor dynamics, enabling the inclusion of quadrotor-specific costs such as energy consumption. However, its performance in unknown environments is limited, as it lacks the ability to explore unknown regions when blocked by large obstacles. To solve this issue, we introduce Perception-Aware MPPI (PA-MPPI). Here, perception-awareness is defined as adapting the trajectory online based on perception objectives. Specifically, when the goal is occluded, PA-MPPI's perception cost biases trajectories that can perceive unknown regions. This expands the mapped traversable space and increases the likelihood of finding alternative paths to the goal. Through hardware experiments, we demonstrate that PA-MPPI, running at 50 Hz with our efficient perception and mapping module, performs up to 100% better than the baseline in our challenging settings where the state-of-the-art MPPI fails. In addition, we demonstrate that PA-MPPI can be used as a safe and robust action policy for navigation foundation models, which often provide goal poses that are not directly reachable.",
    "pdf_url": "https://arxiv.org/pdf/2509.14978v1",
    "github_url": null,
    "published": "2025-09-18T14:08:32+00:00",
    "updated": "2025-09-18T14:08:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.15273v2",
    "title": "Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation Platform for Embodied AI",
    "authors": [
      "Ni",
      "Zhang",
      "Li"
    ],
    "summary": "Embodied AI development significantly lags behind large foundation models due to three critical challenges: (1) lack of systematic understanding of core capabilities needed for Embodied AI, making research lack clear objectives; (2) absence of unified and standardized evaluation systems, rendering cross-benchmark evaluation infeasible; and (3) underdeveloped automated and scalable acquisition methods for embodied data, creating critical bottlenecks for model scaling. To address these obstacles, we present Embodied Arena, a comprehensive, unified, and evolving evaluation platform for Embodied AI. Our platform establishes a systematic embodied capability taxonomy spanning three levels (perception, reasoning, task execution), seven core capabilities, and 25 fine-grained dimensions, enabling unified evaluation with systematic research objectives. We introduce a standardized evaluation system built upon unified infrastructure supporting flexible integration of 22 diverse benchmarks across three domains (2D/3D Embodied Q&A, Navigation, Task Planning) and 30+ advanced models from 20+ worldwide institutes. Additionally, we develop a novel LLM-driven automated generation pipeline ensuring scalable embodied evaluation data with continuous evolution for diversity and comprehensiveness. Embodied Arena publishes three real-time leaderboards (Embodied Q&A, Navigation, Task Planning) with dual perspectives (benchmark view and capability view), providing comprehensive overviews of advanced model capabilities. Especially, we present nine findings summarized from the evaluation results on the leaderboards of Embodied Arena. This helps to establish clear research veins and pinpoint critical research problems, thereby driving forward progress in the field of Embodied AI.",
    "pdf_url": "https://arxiv.org/pdf/2509.15273v2",
    "github_url": null,
    "published": "2025-09-18T11:53:37+00:00",
    "updated": "2025-09-23T16:30:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.15264v1",
    "title": "GiAnt: A Bio-Inspired Hexapod for Adaptive Terrain Navigation and Object Detection",
    "authors": [
      "Bhuiyan",
      "Mehda",
      "Puspo"
    ],
    "summary": "This paper presents the design, development and testing of GiAnt, an affordable hexapod which is inspired by the efficient motions of ants. The decision to model GiAnt after ants rather than other insects is rooted in ants' natural adaptability to a variety of terrains. This bio-inspired approach gives it a significant advantage in outdoor applications, offering terrain flexibility along with efficient energy use. It features a lightweight 3D-printed and laser cut structure weighing 1.75 kg with dimensions of 310 mm x 200 mm x 120 mm. Its legs have been designed with a simple Single Degree of Freedom (DOF) using a link and crank mechanism. It is great for conquering challenging terrains such as grass, rocks, and steep surfaces. Unlike traditional robots using four wheels for motion, its legged design gives superior adaptability to uneven and rough surfaces. GiAnt's control system is built on Arduino, allowing manual operation. An effective way of controlling the legs of GiAnt was achieved by gait analysis. It can move up to 8 cm of height easily with its advanced leg positioning system. Furthermore, equipped with machine learning and image processing technology, it can identify 81 different objects in a live monitoring system. It represents a significant step towards creating accessible hexapod robots for research, exploration, and surveying, offering unique advantages in adaptability and control simplicity.",
    "pdf_url": "https://arxiv.org/pdf/2509.15264v1",
    "github_url": null,
    "published": "2025-09-18T09:05:43+00:00",
    "updated": "2025-09-18T09:05:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.15250v2",
    "title": "Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning",
    "authors": [
      "Qin",
      "Burns",
      "Plummer"
    ],
    "summary": "Large models achieve strong performance on Vision-and-Language Navigation (VLN) tasks, but are costly to run in resource-limited environments. Token pruning offers appealing tradeoffs for efficiency with minimal performance loss by reducing model input size, but prior work overlooks VLN-specific challenges. For example, information loss from pruning can effectively increase computational cost due to longer walks. Thus, the inability to identify uninformative tokens undermines the supposed efficiency gains from pruning. To address this, we propose Navigation-Aware Pruning (NAP), which uses navigation-specific traits to simplify the pruning process by pre-filtering tokens into foreground and background. For example, image views are filtered based on whether the agent can navigate in that direction. We also extract navigation-relevant instructions using a Large Language Model. After filtering, we focus pruning on background tokens, minimizing information loss. To further help avoid increases in navigation length, we discourage backtracking by removing low-importance navigation nodes. Experiments on standard VLN benchmarks show NAP significantly outperforms prior work, preserving higher success rates while saving more than 50% FLOPS.",
    "pdf_url": "https://arxiv.org/pdf/2509.15250v2",
    "github_url": null,
    "published": "2025-09-18T01:05:37+00:00",
    "updated": "2025-09-22T01:18:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.14195v1",
    "title": "Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning",
    "authors": [
      "Manir",
      "Oates"
    ],
    "summary": "Mental representation, characterized by structured internal models mirroring external environments, is fundamental to advanced cognition but remains challenging to investigate empirically. Existing theory hypothesizes that second-order learning -- learning mechanisms that adapt first-order learning (i.e., learning about the task/domain) -- promotes the emergence of such environment-cognition isomorphism. In this paper, we empirically validate this hypothesis by proposing a hierarchical architecture comprising a Graph Convolutional Network (GCN) as a first-order learner and an MLP controller as a second-order learner. The GCN directly maps node-level features to predictions of optimal navigation paths, while the MLP dynamically adapts the GCN's parameters when confronting structurally novel maze environments. We demonstrate that second-order learning is particularly effective when the cognitive system develops an internal mental map structurally isomorphic to the environment. Quantitative and qualitative results highlight significant performance improvements and robust generalization on unseen maze tasks, providing empirical support for the pivotal role of structured mental representations in maximizing the effectiveness of second-order learning.",
    "pdf_url": "https://arxiv.org/pdf/2509.14195v1",
    "github_url": null,
    "published": "2025-09-17T17:30:58+00:00",
    "updated": "2025-09-17T17:30:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.14056v1",
    "title": "EEG-Based Cognitive Load Classification During Landmark-Based VR Navigation",
    "authors": [
      "An",
      "Cheng",
      "Rudyka"
    ],
    "summary": "Brain computer interfaces enable real-time monitoring of cognitive load, but their effectiveness in dynamic navigation contexts is not well established. Using an existing VR navigation dataset, we examined whether EEG signals can classify cognitive load during map-based wayfinding and whether classification accuracy depends more on task complexity or on individual traits. EEG recordings from forty-six participants navigating routes with 3, 5, or 7 map landmarks were analyzed with a nested cross-validation framework across multiple machine learning models. Classification achieved mean accuracies up to 90.8% for binary contrasts (3 vs. 7 landmarks) and 78.7% for the three-class problem, both well above chance. Demographic and cognitive variables (age, gender, spatial ability, working memory) showed no significant influence. These findings demonstrate that task demands outweigh individual differences in shaping classification performance, highlighting the potential for task-adaptive navigation systems that dynamically adjust map complexity in response to real-time cognitive states.",
    "pdf_url": "https://arxiv.org/pdf/2509.14056v1",
    "github_url": null,
    "published": "2025-09-17T15:03:06+00:00",
    "updated": "2025-09-17T15:03:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.13965v1",
    "title": "MetricNet: Recovering Metric Scale in Generative Navigation Policies",
    "authors": [
      "Nayak",
      "Oliveira",
      "Gode"
    ],
    "summary": "Generative navigation policies have made rapid progress in improving end-to-end learned navigation. Despite their promising results, this paradigm has two structural problems. First, the sampled trajectories exist in an abstract, unscaled space without metric grounding. Second, the control strategy discards the full path, instead moving directly towards a single waypoint. This leads to short-sighted and unsafe actions, moving the robot towards obstacles that a complete and correctly scaled path would circumvent. To address these issues, we propose MetricNet, an effective add-on for generative navigation that predicts the metric distance between waypoints, grounding policy outputs in real-world coordinates. We evaluate our method in simulation with a new benchmarking framework and show that executing MetricNet-scaled waypoints significantly improves both navigation and exploration performance. Beyond simulation, we further validate our approach in real-world experiments. Finally, we propose MetricNav, which integrates MetricNet into a navigation policy to guide the robot away from obstacles while still moving towards the goal.",
    "pdf_url": "https://arxiv.org/pdf/2509.13965v1",
    "github_url": null,
    "published": "2025-09-17T13:37:13+00:00",
    "updated": "2025-09-17T13:37:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.13943v1",
    "title": "Reinforcement Learning for Autonomous Point-to-Point UAV Navigation",
    "authors": [
      "Oyinlola",
      "Subedi",
      "Sarkar"
    ],
    "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly used in automated inspection, delivery, and navigation tasks that require reliable autonomy. This project develops a reinforcement learning (RL) approach to enable a single UAV to autonomously navigate between predefined points without manual intervention. The drone learns navigation policies through trial-and-error interaction, using a custom reward function that encourages goal-reaching efficiency while penalizing collisions and unsafe behavior. The control system integrates ROS with a Gym-compatible training environment, enabling flexible deployment and testing. After training, the learned policy is deployed on a real UAV platform and evaluated under practical conditions. Results show that the UAV can successfully perform autonomous navigation with minimal human oversight, demonstrating the viability of RL-based control for point-to-point drone operations in real-world scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2509.13943v1",
    "github_url": null,
    "published": "2025-09-17T13:12:52+00:00",
    "updated": "2025-09-17T13:12:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.13816v1",
    "title": "Agile in the Face of Delay: Asynchronous End-to-End Learning for Real-World Aerial Navigation",
    "authors": [
      "Li",
      "Zhou",
      "Li"
    ],
    "summary": "Robust autonomous navigation for Autonomous Aerial Vehicles (AAVs) in complex environments is a critical capability. However, modern end-to-end navigation faces a key challenge: the high-frequency control loop needed for agile flight conflicts with low-frequency perception streams, which are limited by sensor update rates and significant computational cost. This mismatch forces conventional synchronous models into undesirably low control rates. To resolve this, we propose an asynchronous reinforcement learning framework that decouples perception and control, enabling a high-frequency policy to act on the latest IMU state for immediate reactivity, while incorporating perception features asynchronously. To manage the resulting data staleness, we introduce a theoretically-grounded Temporal Encoding Module (TEM) that explicitly conditions the policy on perception delays, a strategy complemented by a two-stage curriculum to ensure stable and efficient training. Validated in extensive simulations, our method was successfully deployed in zero-shot sim-to-real transfer on an onboard NUC, where it sustains a 100~Hz control rate and demonstrates robust, agile navigation in cluttered real-world environments. Our source code will be released for community reference.",
    "pdf_url": "https://arxiv.org/pdf/2509.13816v1",
    "github_url": null,
    "published": "2025-09-17T08:32:14+00:00",
    "updated": "2025-09-17T08:32:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.13733v3",
    "title": "FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph",
    "authors": [
      "Zhou",
      "Xiao",
      "Liu"
    ],
    "summary": "Visual-Language Navigation (VLN) is a fundamental challenge in robotic systems, with broad applications for the deployment of embodied agents in real-world environments. Despite recent advances, existing approaches are limited in long-range spatial reasoning, often exhibiting low success rates and high inference latency, particularly in long-range navigation tasks. To address these limitations, we propose FSR-VLN, a vision-language navigation system that combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation supporting progressive retrieval, from coarse room-level localization to fine-grained goal view and object identification. Building on HMSG, FSR first performs fast matching to efficiently select candidate rooms, views, and objects, then applies VLM-driven refinement for final goal selection. We evaluated FSR-VLN across four comprehensive indoor datasets collected by humanoid robots, utilizing 87 instructions that encompass a diverse range of object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all datasets, measured by the retrieval success rate (RSR), while reducing the response time by 82% compared to VLM-based methods on tour videos by activating slow reasoning only when fast intuition fails. Furthermore, we integrate FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1 humanoid robot, enabling natural language interaction and real-time navigation.",
    "pdf_url": "https://arxiv.org/pdf/2509.13733v3",
    "github_url": null,
    "published": "2025-09-17T06:36:41+00:00",
    "updated": "2025-11-25T09:17:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.13720v1",
    "title": "EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility",
    "authors": [
      "Zeng",
      "Peng",
      "Ye"
    ],
    "summary": "Zero-shot object navigation (ZSON) in large-scale outdoor environments faces many challenges; we specifically address a coupled one: long-range targets that reduce to tiny projections and intermittent visibility due to partial or complete occlusion. We present a unified, lightweight closed-loop system built on an aligned multi-scale image tile hierarchy. Through hierarchical target-saliency fusion, it summarizes localized semantic contrast into a stable coarse-layer regional saliency that provides the target direction and indicates target visibility. This regional saliency supports visibility-aware heading maintenance through keyframe memory, saliency-weighted fusion of historical headings, and active search during temporary invisibility. The system avoids whole-image rescaling, enables deterministic bottom-up aggregation, supports zero-shot navigation, and runs efficiently on a mobile robot. Across simulation and real-world outdoor trials, the system detects semantic targets beyond 150m, maintains a correct heading through visibility changes with 82.6% probability, and improves overall task success by 17.5% compared with the SOTA methods, demonstrating robust ZSON toward distant and intermittently observable targets.",
    "pdf_url": "https://arxiv.org/pdf/2509.13720v1",
    "github_url": null,
    "published": "2025-09-17T06:15:06+00:00",
    "updated": "2025-09-17T06:15:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.13132v1",
    "title": "An Uncertainty-Weighted Decision Transformer for Navigation in Dense, Complex Driving Scenarios",
    "authors": [
      "Zhang",
      "Peng",
      "Zhu"
    ],
    "summary": "Autonomous driving in dense, dynamic environments requires decision-making systems that can exploit both spatial structure and long-horizon temporal dependencies while remaining robust to uncertainty. This work presents a novel framework that integrates multi-channel bird's-eye-view occupancy grids with transformer-based sequence modeling for tactical driving in complex roundabout scenarios. To address the imbalance between frequent low-risk states and rare safety-critical decisions, we propose the Uncertainty-Weighted Decision Transformer (UWDT). UWDT employs a frozen teacher transformer to estimate per-token predictive entropy, which is then used as a weight in the student model's loss function. This mechanism amplifies learning from uncertain, high-impact states while maintaining stability across common low-risk transitions. Experiments in a roundabout simulator, across varying traffic densities, show that UWDT consistently outperforms other baselines in terms of reward, collision rate, and behavioral stability. The results demonstrate that uncertainty-aware, spatial-temporal transformers can deliver safer and more efficient decision-making for autonomous driving in complex traffic environments.",
    "pdf_url": "https://arxiv.org/pdf/2509.13132v1",
    "github_url": null,
    "published": "2025-09-16T14:48:52+00:00",
    "updated": "2025-09-16T14:48:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.13386v1",
    "title": "VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization",
    "authors": [
      "Lim",
      "Im",
      "Boyack"
    ],
    "summary": "Demands for software-defined vehicles (SDV) are rising and electric vehicles (EVs) are increasingly being equipped with powerful computers. This enables onboard AI systems to optimize charge-aware path optimization customized to reflect vehicle's current condition and environment. We present VEGA, a charge-aware EV navigation agent that plans over a charger-annotated road graph using Proximal Policy Optimization (PPO) with budgeted A* teacher-student guidance under state-of-charge (SoC) feasibility. VEGA consists of two modules. First, a physics-informed neural operator (PINO), trained on real vehicle speed and battery-power logs, uses recent vehicle speed logs to estimate aerodynamic drag, rolling resistance, mass, motor and regenerative-braking efficiencies, and auxiliary load by learning a vehicle-custom dynamics. Second, a Reinforcement Learning (RL) agent uses these dynamics to optimize a path with optimal charging stops and dwell times under SoC constraints. VEGA requires no additional sensors and uses only vehicle speed signals. It may serve as a virtual sensor for power and efficiency to potentially reduce EV cost. In evaluation on long routes like San Francisco to New York, VEGA's stops, dwell times, SoC management, and total travel time closely track Tesla Trip Planner while being slightly more conservative, presumably due to real vehicle conditions such as vehicle parameter drift due to deterioration. Although trained only in U.S. regions, VEGA was able to compute optimal charge-aware paths in France and Japan, demonstrating generalizability. It achieves practical integration of physics-informed learning and RL for EV eco-routing.",
    "pdf_url": "https://arxiv.org/pdf/2509.13386v1",
    "github_url": null,
    "published": "2025-09-16T11:27:50+00:00",
    "updated": "2025-09-16T11:27:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.12912v1",
    "title": "Spotting the Unfriendly Robot - Towards better Metrics for Interactions",
    "authors": [
      "Wenzel",
      "Probst"
    ],
    "summary": "Establishing standardized metrics for Social Robot Navigation (SRN) algorithms for assessing the quality and social compliance of robot behavior around humans is essential for SRN research. Currently, commonly used evaluation metrics lack the ability to quantify how cooperative an agent behaves in interaction with humans. Concretely, in a simple frontal approach scenario, no metric specifically captures if both agents cooperate or if one agent stays on collision course and the other agent is forced to evade. To address this limitation, we propose two new metrics, a conflict intensity metric and the responsibility metric. Together, these metrics are capable of evaluating the quality of human-robot interactions by showing how much a given algorithm has contributed to reducing a conflict and which agent actually took responsibility of the resolution. This work aims to contribute to the development of a comprehensive and standardized evaluation methodology for SRN, ultimately enhancing the safety, efficiency, and social acceptance of robots in human-centric environments.",
    "pdf_url": "https://arxiv.org/pdf/2509.12912v1",
    "github_url": null,
    "published": "2025-09-16T10:05:52+00:00",
    "updated": "2025-09-16T10:05:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.12894v1",
    "title": "DialNav: Multi-turn Dialog Navigation with a Remote Guide",
    "authors": [
      "Han",
      "Min",
      "Hwangbo"
    ],
    "summary": "We introduce DialNav, a novel collaborative embodied dialog task, where a navigation agent (Navigator) and a remote guide (Guide) engage in multi-turn dialog to reach a goal location. Unlike prior work, DialNav aims for holistic evaluation and requires the Guide to infer the Navigator's location, making communication essential for task success. To support this task, we collect and release the Remote Assistance in Navigation (RAIN) dataset, human-human dialog paired with navigation trajectories in photorealistic environments. We design a comprehensive benchmark to evaluate both navigation and dialog, and conduct extensive experiments analyzing the impact of different Navigator and Guide models. We highlight key challenges and publicly release the dataset, code, and evaluation framework to foster future research in embodied dialog.",
    "pdf_url": "https://arxiv.org/pdf/2509.12894v1",
    "github_url": null,
    "published": "2025-09-16T09:49:34+00:00",
    "updated": "2025-09-16T09:49:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.12890v1",
    "title": "Responsibility and Engagement - Evaluating Interactions in Social Robot Navigation",
    "authors": [
      "Probst",
      "Wenzel",
      "Dasi"
    ],
    "summary": "In Social Robot Navigation (SRN), the availability of meaningful metrics is crucial for evaluating trajectories from human-robot interactions. In the SRN context, such interactions often relate to resolving conflicts between two or more agents. Correspondingly, the shares to which agents contribute to the resolution of such conflicts are important. This paper builds on recent work, which proposed a Responsibility metric capturing such shares. We extend this framework in two directions: First, we model the conflict buildup phase by introducing a time normalization. Second, we propose the related Engagement metric, which captures how the agents' actions intensify a conflict. In a comprehensive series of simulated scenarios with dyadic, group and crowd interactions, we show that the metrics carry meaningful information about the cooperative resolution of conflicts in interactions. They can be used to assess behavior quality and foresightedness. We extensively discuss applicability, design choices and limitations of the proposed metrics.",
    "pdf_url": "https://arxiv.org/pdf/2509.12890v1",
    "github_url": null,
    "published": "2025-09-16T09:44:08+00:00",
    "updated": "2025-09-16T09:44:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.12747v2",
    "title": "NavMoE: Hybrid Model- and Learning-based Traversability Estimation for Local Navigation via Mixture of Experts",
    "authors": [
      "He",
      "Shahidzadeh",
      "Chen"
    ],
    "summary": "This paper explores traversability estimation for robot navigation. A key bottleneck in traversability estimation lies in efficiently achieving reliable and robust predictions while accurately encoding both geometric and semantic information across diverse environments. We introduce Navigation via Mixture of Experts (NAVMOE), a hierarchical and modular approach for traversability estimation and local navigation. NAVMOE combines multiple specialized models for specific terrain types, each of which can be either a classical model-based or a learning-based approach that predicts traversability for specific terrain types. NAVMOE dynamically weights the contributions of different models based on the input environment through a gating network. Overall, our approach offers three advantages: First, NAVMOE enables traversability estimation to adaptively leverage specialized approaches for different terrains, which enhances generalization across diverse and unseen environments. Second, our approach significantly improves efficiency with negligible cost of solution quality by introducing a training-free lazy gating mechanism, which is designed to minimize the number of activated experts during inference. Third, our approach uses a two-stage training strategy that enables the training for the gating networks within the hybrid MoE method that contains nondifferentiable modules. Extensive experiments show that NAVMOE delivers a better efficiency and performance balance than any individual expert or full ensemble across different domains, improving cross-domain generalization and reducing average computational cost by 81.2% via lazy gating, with less than a 2% loss in path quality.",
    "pdf_url": "https://arxiv.org/pdf/2509.12747v2",
    "github_url": null,
    "published": "2025-09-16T07:07:35+00:00",
    "updated": "2025-09-17T06:31:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.12723v2",
    "title": "NAMOUnc: Navigation Among Movable Obstacles with Decision Making on Uncertainty Interval",
    "authors": [
      "Zhang",
      "Lucet",
      "Sandretto"
    ],
    "summary": "Navigation among movable obstacles (NAMO) is a critical task in robotics, often challenged by real-world uncertainties such as observation noise, model approximations, action failures, and partial observability. Existing solutions frequently assume ideal conditions, leading to suboptimal or risky decisions. This paper introduces NAMOUnc, a novel framework designed to address these uncertainties by integrating them into the decision-making process. We first estimate them and compare the corresponding time cost intervals for removing and bypassing obstacles, optimizing both the success rate and time efficiency, ensuring safer and more efficient navigation. We validate our method through extensive simulations and real-world experiments, demonstrating significant improvements over existing NAMO frameworks. More details can be found in our website: https://kai-zhang-er.github.io/namo-uncertainty/",
    "pdf_url": "https://arxiv.org/pdf/2509.12723v2",
    "github_url": null,
    "published": "2025-09-16T06:25:41+00:00",
    "updated": "2025-10-10T01:26:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.12618v1",
    "title": "ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation",
    "authors": [
      "Zhang",
      "Zhu",
      "Pan"
    ],
    "summary": "The Vision-and-Language Navigation (VLN) task requires an agent to follow natural language instructions and navigate through complex environments. Existing MLLM-based VLN methods primarily rely on imitation learning (IL) and often use DAgger for post-training to mitigate covariate shift. While effective, these approaches incur substantial data collection and training costs. Reinforcement learning (RL) offers a promising alternative. However, prior VLN RL methods lack dynamic interaction with the environment and depend on expert trajectories for reward shaping, rather than engaging in open-ended active exploration. This restricts the agent's ability to discover diverse and plausible navigation routes. To address these limitations, we propose ActiveVLN, a VLN framework that explicitly enables active exploration through multi-turn RL. In the first stage, a small fraction of expert trajectories is used for IL to bootstrap the agent. In the second stage, the agent iteratively predicts and executes actions, automatically collects diverse trajectories, and optimizes multiple rollouts via the GRPO objective. To further improve RL efficiency, we introduce a dynamic early-stopping strategy to prune long-tail or likely failed trajectories, along with additional engineering optimizations. Experiments show that ActiveVLN achieves the largest performance gains over IL baselines compared to both DAgger-based and prior RL-based post-training methods, while reaching competitive performance with state-of-the-art approaches despite using a smaller model. Code and data will be released soon.",
    "pdf_url": "https://arxiv.org/pdf/2509.12618v1",
    "github_url": null,
    "published": "2025-09-16T03:31:46+00:00",
    "updated": "2025-09-16T03:31:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.12129v2",
    "title": "Embodied Navigation Foundation Model",
    "authors": [
      "Zhang",
      "Li",
      "Qi"
    ],
    "summary": "Navigation is a fundamental capability in embodied AI, representing the intelligence required to perceive and interact within physical environments following language instructions. Despite significant progress in large Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance on general vision-language tasks, their generalization ability in embodied navigation remains largely confined to narrow task settings and embodiment-specific architectures. In this work, we introduce a cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained on eight million navigation samples that encompass quadrupeds, drones, wheeled robots, and vehicles, and spanning diverse tasks such as vision-and-language navigation, object searching, target tracking, and autonomous driving. NavFoM employs a unified architecture that processes multimodal navigation inputs from varying camera configurations and navigation horizons. To accommodate diverse camera setups and temporal horizons, NavFoM incorporates identifier tokens that embed camera view information of embodiments and the temporal context of tasks. Furthermore, to meet the demands of real-world deployment, NavFoM controls all observation tokens using a dynamically adjusted sampling strategy under a limited token length budget. Extensive evaluations on public benchmarks demonstrate that our model achieves state-of-the-art or highly competitive performance across multiple navigation tasks and embodiments without requiring task-specific fine-tuning. Additional real-world experiments further confirm the strong generalization capability and practical applicability of our approach.",
    "pdf_url": "https://arxiv.org/pdf/2509.12129v2",
    "github_url": null,
    "published": "2025-09-15T16:52:43+00:00",
    "updated": "2025-09-16T18:15:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.11791v1",
    "title": "Synthetic vs. Real Training Data for Visual Navigation",
    "authors": [
      "Suomela",
      "Arachchige",
      "Torres"
    ],
    "summary": "This paper investigates how the performance of visual navigation policies trained in simulation compares to policies trained with real-world data. Performance degradation of simulator-trained policies is often significant when they are evaluated in the real world. However, despite this well-known sim-to-real gap, we demonstrate that simulator-trained policies can match the performance of their real-world-trained counterparts.   Central to our approach is a navigation policy architecture that bridges the sim-to-real appearance gap by leveraging pretrained visual representations and runs real-time on robot hardware. Evaluations on a wheeled mobile robot show that the proposed policy, when trained in simulation, outperforms its real-world-trained version by 31% and the prior state-of-the-art methods by 50% in navigation success rate. Policy generalization is verified by deploying the same model onboard a drone.   Our results highlight the importance of diverse image encoder pretraining for sim-to-real generalization, and identify on-policy learning as a key advantage of simulated training over training with real data.",
    "pdf_url": "https://arxiv.org/pdf/2509.11791v1",
    "github_url": null,
    "published": "2025-09-15T11:22:40+00:00",
    "updated": "2025-09-15T11:22:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.11530v1",
    "title": "Residual Gaze Behavior During Navigation in Blindness and Low Vision",
    "authors": [
      "Feng",
      "Garcia-Pina",
      "Beheshti"
    ],
    "summary": "Background: Outdoor navigation poses significant challenges for people with blindness or low vision, yet the role of gaze behavior in supporting mobility remains underexplored. Fully sighted individuals typically adopt consistent scanning strategies, whereas those with visual impairments rely on heterogeneous adaptations shaped by residual vision and experience.   Methods: We conducted a comparative eye-tracking study of fully sighted, low vision, blind, and fully blind participants navigating outdoor routes. Using a wearable eye tracker, we quantified fixation counts, fixation rate, fixation area, direction, peak fixation location, and walking speed.   Results: Walking speed declined systematically with worsening vision. Fixation count increased with greater impairment, reflecting slower travel times and more frequent sampling. Fixation rate rose with worsening vision, though between-group differences were generally not significant between most groups. Fixation spatial coverage decreased along the continuum of vision loss. Fixation patterns were most consistent in the fully sighted group. Peak fixation locations were centered in fully sighted participants but shifted outward and became more variable with impairment.   Conclusion: Gaze strategies during navigation form a graded continuum across vision groups, with fully sighted and fully blind participants at opposite poles and low vision and blind groups spanning the middle. Visual acuity alone does not predict functional gaze use, as rehabilitation experience and adaptive strategies strongly shape behavior. These findings highlight the need for personalized rehabilitation and assistive technologies, with residual gaze patterns offering insight into mobility capacity and training opportunities for safer navigation.",
    "pdf_url": "https://arxiv.org/pdf/2509.11530v1",
    "github_url": null,
    "published": "2025-09-15T02:44:23+00:00",
    "updated": "2025-09-15T02:44:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.11388v1",
    "title": "Quantum deep reinforcement learning for humanoid robot navigation task",
    "authors": [
      "Lokossou",
      "Girma",
      "Tonguz"
    ],
    "summary": "Classical reinforcement learning (RL) methods often struggle in complex, high-dimensional environments because of their extensive parameter requirements and challenges posed by stochastic, non-deterministic settings. This study introduces quantum deep reinforcement learning (QDRL) to train humanoid agents efficiently. While previous quantum RL models focused on smaller environments, such as wheeled robots and robotic arms, our work pioneers the application of QDRL to humanoid robotics, specifically in environments with substantial observation and action spaces, such as MuJoCo's Humanoid-v4 and Walker2d-v4. Using parameterized quantum circuits, we explored a hybrid quantum-classical setup to directly navigate high-dimensional state spaces, bypassing traditional mapping and planning. By integrating quantum computing with deep RL, we aim to develop models that can efficiently learn complex navigation tasks in humanoid robots. We evaluated the performance of the Soft Actor-Critic (SAC) in classical RL against its quantum implementation. The results show that the quantum SAC achieves an 8% higher average return (246.40) than the classical SAC (228.36) after 92% fewer steps, highlighting the accelerated learning potential of quantum computing in RL tasks.",
    "pdf_url": "https://arxiv.org/pdf/2509.11388v1",
    "github_url": null,
    "published": "2025-09-14T18:45:19+00:00",
    "updated": "2025-09-14T18:45:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.11197v1",
    "title": "DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "Fang",
      "Wang"
    ],
    "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE), which links language instructions to perception and control in the real world, is a core capability of embodied robots. Recently, large-scale pretrained foundation models have been leveraged as shared priors for perception, reasoning, and action, enabling zero-shot VLN without task-specific training. However, existing zero-shot VLN methods depend on costly perception and passive scene understanding, collapsing control to point-level choices. As a result, they are expensive to deploy, misaligned in action semantics, and short-sighted in planning. To address these issues, we present DreamNav that focuses on the following three aspects: (1) for reducing sensory cost, our EgoView Corrector aligns viewpoints and stabilizes egocentric perception; (2) instead of point-level actions, our Trajectory Predictor favors global trajectory-level planning to better align with instruction semantics; and (3) to enable anticipatory and long-horizon planning, we propose an Imagination Predictor to endow the agent with proactive thinking capability. On VLN-CE and real-world tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the strongest egocentric baseline with extra information by up to 7.49\\% and 18.15\\% in terms of SR and SPL metrics. To our knowledge, this is the first zero-shot VLN method to unify trajectory-level planning and active imagination while using only egocentric inputs.",
    "pdf_url": "https://arxiv.org/pdf/2509.11197v1",
    "github_url": null,
    "published": "2025-09-14T09:54:20+00:00",
    "updated": "2025-09-14T09:54:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.10884v1",
    "title": "Nav-R1: Reasoning and Navigation in Embodied Scenes",
    "authors": [
      "Liu",
      "Huang",
      "Zhang"
    ],
    "summary": "Embodied navigation requires agents to integrate perception, reasoning, and action for robust interaction in complex 3D environments. Existing approaches often suffer from incoherent and unstable reasoning traces that hinder generalization across diverse environments, and difficulty balancing long-horizon semantic reasoning with low-latency control for real-time navigation. To address these challenges, we propose Nav-R1, an embodied foundation model that unifies reasoning in embodied environments. We first construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought (CoT) for embodied tasks, which enables cold-start initialization with structured reasoning. Building on this foundation, we design a GRPO-based reinforcement learning framework with three complementary rewards: format, understanding, and navigation, to improve structural adherence, semantic grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow reasoning paradigm, decoupling deliberate semantic reasoning from low-latency reactive control for efficient yet coherent navigation. Extensive evaluations on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms strong baselines, with over 8% average improvement in reasoning and navigation performance. Real-world deployment on a mobile robot further validates its robustness under limited onboard resources. Code: https://github.com/AIGeeksGroup/Nav-R1. Website: https://aigeeksgroup.github.io/Nav-R1.",
    "pdf_url": "https://arxiv.org/pdf/2509.10884v1",
    "github_url": "https://github.com/AIGeeksGroup/Nav-R1",
    "published": "2025-09-13T16:31:03+00:00",
    "updated": "2025-09-13T16:31:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.10813v2",
    "title": "InternScenes: A Large-scale Simulatable Indoor Scene Dataset with Realistic Layouts",
    "authors": [
      "Zhong",
      "Cao",
      "Jin"
    ],
    "summary": "The advancement of Embodied AI heavily relies on large-scale, simulatable 3D scene datasets characterized by scene diversity and realistic layouts. However, existing datasets typically suffer from limitations in data scale or diversity, sanitized layouts lacking small items, and severe object collisions. To address these shortcomings, we introduce \\textbf{InternScenes}, a novel large-scale simulatable indoor scene dataset comprising approximately 40,000 diverse scenes by integrating three disparate scene sources, real-world scans, procedurally generated scenes, and designer-created scenes, including 1.96M 3D objects and covering 15 common scene types and 288 object classes. We particularly preserve massive small items in the scenes, resulting in realistic and complex layouts with an average of 41.5 objects per region. Our comprehensive data processing pipeline ensures simulatability by creating real-to-sim replicas for real-world scans, enhances interactivity by incorporating interactive objects into these scenes, and resolves object collisions by physical simulations. We demonstrate the value of InternScenes with two benchmark applications: scene layout generation and point-goal navigation. Both show the new challenges posed by the complex and realistic layouts. More importantly, InternScenes paves the way for scaling up the model training for both tasks, making the generation and navigation in such complex scenes possible. We commit to open-sourcing the data, models, and benchmarks to benefit the whole community.",
    "pdf_url": "https://arxiv.org/pdf/2509.10813v2",
    "github_url": null,
    "published": "2025-09-13T14:25:17+00:00",
    "updated": "2025-10-14T05:26:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.10454v1",
    "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation",
    "authors": [
      "Yin",
      "Wei",
      "Xu"
    ],
    "summary": "In this paper, we propose a training-free framework for vision-and-language navigation (VLN). Existing zero-shot VLN methods are mainly designed for discrete environments or involve unsupervised training in continuous simulator environments, which makes it challenging to generalize and deploy them in real-world scenarios. To achieve a training-free framework in continuous environments, our framework formulates navigation guidance as graph constraint optimization by decomposing instructions into explicit spatial constraints. The constraint-driven paradigm decodes spatial semantics through constraint solving, enabling zero-shot adaptation to unseen environments. Specifically, we construct a spatial constraint library covering all types of spatial relationship mentioned in VLN instructions. The human instruction is decomposed into a directed acyclic graph, with waypoint nodes, object nodes and edges, which are used as queries to retrieve the library to build the graph constraints. The graph constraint optimization is solved by the constraint solver to determine the positions of waypoints, obtaining the robot's navigation path and final goal. To handle cases of no solution or multiple solutions, we construct a navigation tree and the backtracking mechanism. Extensive experiments on standard benchmarks demonstrate significant improvements in success rate and navigation efficiency compared to state-of-the-art zero-shot VLN methods. We further conduct real-world experiments to show that our framework can effectively generalize to new environments and instruction sets, paving the way for a more robust and autonomous navigation framework.",
    "pdf_url": "https://arxiv.org/pdf/2509.10454v1",
    "github_url": null,
    "published": "2025-09-12T17:59:58+00:00",
    "updated": "2025-09-12T17:59:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.09594v1",
    "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation",
    "authors": [
      "Garg",
      "Craggs",
      "Bhat"
    ],
    "summary": "Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an \"image-relative\" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning \"object-relative\" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a \"relative\" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed \"ObjectReact\", conditioned directly on a high-level \"WayObject Costmap\" representation that eliminates the need for an explicit RGB input. We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments. Code and supplementary material are accessible via project page: https://object-react.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2509.09594v1",
    "github_url": null,
    "published": "2025-09-11T16:34:17+00:00",
    "updated": "2025-09-11T16:34:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.08757v1",
    "title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation",
    "authors": [
      "Munje",
      "Tang",
      "Liu"
    ],
    "summary": "Robot navigation in dynamic, human-centered environments requires socially-compliant decisions grounded in robust scene understanding. Recent Vision-Language Models (VLMs) exhibit promising capabilities such as object recognition, common-sense reasoning, and contextual understanding-capabilities that align with the nuanced requirements of social robot navigation. However, it remains unclear whether VLMs can accurately understand complex social navigation scenes (e.g., inferring the spatial-temporal relations among agents and human intentions), which is essential for safe and socially compliant robot navigation. While some recent works have explored the use of VLMs in social robot navigation, no existing work systematically evaluates their ability to meet these necessary conditions. In this paper, we introduce the Social Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene understanding in real-world social robot navigation scenarios. SocialNav-SUB provides a unified framework for evaluating VLMs against human and rule-based baselines across VQA tasks requiring spatial, spatiotemporal, and social reasoning in social robot navigation. Through experiments with state-of-the-art VLMs, we find that while the best-performing VLM achieves an encouraging probability of agreeing with human answers, it still underperforms simpler rule-based approach and human consensus baselines, indicating critical gaps in social scene understanding of current VLMs. Our benchmark sets the stage for further research on foundation models for social robot navigation, offering a framework to explore how VLMs can be tailored to meet real-world social robot navigation needs. An overview of this paper along with the code and data can be found at https://larg.github.io/socialnav-sub .",
    "pdf_url": "https://arxiv.org/pdf/2509.08757v1",
    "github_url": null,
    "published": "2025-09-10T16:47:00+00:00",
    "updated": "2025-09-10T16:47:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.08699v1",
    "title": "TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals",
    "authors": [
      "Podgorski",
      "Garg",
      "Hosseinzadeh"
    ],
    "summary": "Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and incorporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO.",
    "pdf_url": "https://arxiv.org/pdf/2509.08699v1",
    "github_url": "https://github.com/podgorki/TANGO",
    "published": "2025-09-10T15:43:32+00:00",
    "updated": "2025-09-10T15:43:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.08871v1",
    "title": "Relativistic Time Modeling for Lunar Positioning Navigation and Timing",
    "authors": [
      "Seyffert"
    ],
    "summary": "Future lunar missions will depend on an internationally agreed upon timescale that remains accurate under the Moon's unique gravitational environment and its orbital dynamics. This thesis investigates the proposed Lunar Coordinate Time (TCL), derived analogously to Geocentric Coordinate Time (TCG) and thus aligned with current IAU proposals. We first formalise the TCL transformation and quantify its characteristics from solar system simulations. Next, we compute stationary surface-clock drifts caused by gravitational redshift and the Moon's changing orientation parameters, evaluating how accurate atomic clocks deployed on the surface of the Moon (much like for ESA's proposed NovaMoon mission) would have to be to measure these effects. Finally, we simulate relativistic proper time for ESA's Moonlight navigation satellites, identifying average drift and harmonic variations, to better understand the system that will comprise and enable a Lunar PNT (Positioning, Navigation and Timing) architecture. These kinds of investigations are an essential step toward a sustained internationally cooperative operation at the lunar south pole and beyond.",
    "pdf_url": "https://arxiv.org/pdf/2509.08871v1",
    "github_url": null,
    "published": "2025-09-10T13:59:35+00:00",
    "updated": "2025-09-10T13:59:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.08435v1",
    "title": "PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot Diffusion Planner Flow Matching",
    "authors": [
      "Ye",
      "Gao",
      "Xu"
    ],
    "summary": "Diffusion models offer powerful generative capabilities for robot trajectory planning, yet their practical deployment on robots is hindered by a critical bottleneck: a reliance on imitation learning from expert demonstrations. This paradigm is often impractical for specialized robots where data is scarce and creates an inefficient, theoretically suboptimal training pipeline. To overcome this, we introduce PegasusFlow, a hierarchical rolling-denoising framework that enables direct and parallel sampling of trajectory score gradients from environmental interaction, completely bypassing the need for expert data. Our core innovation is a novel sampling algorithm, Weighted Basis Function Optimization (WBFO), which leverages spline basis representations to achieve superior sample efficiency and faster convergence compared to traditional methods like MPPI. The framework is embedded within a scalable, asynchronous parallel simulation architecture that supports massively parallel rollouts for efficient data collection. Extensive experiments on trajectory optimization and robotic navigation tasks demonstrate that our approach, particularly Action-Value WBFO (AVWBFO) combined with a reinforcement learning warm-start, significantly outperforms baselines. In a challenging barrier-crossing task, our method achieved a 100% success rate and was 18% faster than the next-best method, validating its effectiveness for complex terrain locomotion planning. https://masteryip.github.io/pegasusflow.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2509.08435v1",
    "github_url": null,
    "published": "2025-09-10T09:31:17+00:00",
    "updated": "2025-09-10T09:31:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.08177v1",
    "title": "Quadrotor Navigation using Reinforcement Learning with Privileged Information",
    "authors": [
      "Lee",
      "Rathod",
      "Goel"
    ],
    "summary": "This paper presents a reinforcement learning-based quadrotor navigation method that leverages efficient differentiable simulation, novel loss functions, and privileged information to navigate around large obstacles. Prior learning-based methods perform well in scenes that exhibit narrow obstacles, but struggle when the goal location is blocked by large walls or terrain. In contrast, the proposed method utilizes time-of-arrival (ToA) maps as privileged information and a yaw alignment loss to guide the robot around large obstacles. The policy is evaluated in photo-realistic simulation environments containing large obstacles, sharp corners, and dead-ends. Our approach achieves an 86% success rate and outperforms baseline strategies by 34%. We deploy the policy onboard a custom quadrotor in outdoor cluttered environments both during the day and night. The policy is validated across 20 flights, covering 589 meters without collisions at speeds up to 4 m/s.",
    "pdf_url": "https://arxiv.org/pdf/2509.08177v1",
    "github_url": null,
    "published": "2025-09-09T22:56:35+00:00",
    "updated": "2025-09-09T22:56:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.08159v1",
    "title": "Zero-Shot Metric Depth Estimation via Monocular Visual-Inertial Rescaling for Autonomous Aerial Navigation",
    "authors": [
      "Yang",
      "Tian",
      "Goel"
    ],
    "summary": "This paper presents a methodology to predict metric depth from monocular RGB images and an inertial measurement unit (IMU). To enable collision avoidance during autonomous flight, prior works either leverage heavy sensors (e.g., LiDARs or stereo cameras) or data-intensive and domain-specific fine-tuning of monocular metric depth estimation methods. In contrast, we propose several lightweight zero-shot rescaling strategies to obtain metric depth from relative depth estimates via the sparse 3D feature map created using a visual-inertial navigation system. These strategies are compared for their accuracy in diverse simulation environments. The best performing approach, which leverages monotonic spline fitting, is deployed in the real-world on a compute-constrained quadrotor. We obtain on-board metric depth estimates at 15 Hz and demonstrate successful collision avoidance after integrating the proposed method with a motion primitives-based planner.",
    "pdf_url": "https://arxiv.org/pdf/2509.08159v1",
    "github_url": null,
    "published": "2025-09-09T21:39:13+00:00",
    "updated": "2025-09-09T21:39:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.08157v2",
    "title": "Risk-Bounded Multi-Agent Visual Navigation via Iterative Risk Allocation",
    "authors": [
      "Parimi",
      "Williams"
    ],
    "summary": "Safe navigation is essential for autonomous systems operating in hazardous environments, especially when multiple agents must coordinate using only high-dimensional visual observations. While recent approaches successfully combine Goal-Conditioned RL (GCRL) for graph construction with Conflict-Based Search (CBS) for planning, they typically rely on static edge pruning to enforce safety. This binary strategy is overly conservative, precluding feasible missions that require traversing high-risk regions, even when the aggregate risk is acceptable. To address this, we introduce a framework for Risk-Bounded Multi-Agent Path Finding (\\problem{}), where agents share a user-specified global risk budget ($Δ$). Rather than permanently discarding edges, our framework dynamically distributes per-agent risk budgets ($δ_i$) during search via an Iterative Risk Allocation (IRA) layer that integrates with a standard CBS planner. We investigate two distribution strategies: a greedy surplus-deficit scheme for rapid feasibility repair, and a market-inspired mechanism that treats risk as a priced resource to guide improved allocation. This yields a tunable trade-off wherein agents exploit available risk to secure shorter, more efficient paths, but revert to longer, safer detours under tighter budgets. Experiments in complex visual environments show that, our dynamic allocation framework achieves higher success rates than baselines and effectively leverages the available safety budget to reduce travel time.",
    "pdf_url": "https://arxiv.org/pdf/2509.08157v2",
    "github_url": null,
    "published": "2025-09-09T21:35:55+00:00",
    "updated": "2025-12-10T21:59:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.07488v1",
    "title": "Fine-Tuning Vision-Language Models for Visual Navigation Assistance",
    "authors": [
      "Li",
      "Gandhi",
      "Zhan"
    ],
    "summary": "We address vision-language-driven indoor navigation to assist visually impaired individuals in reaching a target location using images and natural language guidance. Traditional navigation systems are ineffective indoors due to the lack of precise location data. Our approach integrates vision and language models to generate step-by-step navigational instructions, enhancing accessibility and independence. We fine-tune the BLIP-2 model with Low Rank Adaptation (LoRA) on a manually annotated indoor navigation dataset. We propose an evaluation metric that refines the BERT F1 score by emphasizing directional and sequential variables, providing a more comprehensive measure of navigational performance. After applying LoRA, the model significantly improved in generating directional instructions, overcoming limitations in the original BLIP-2 model.",
    "pdf_url": "https://arxiv.org/pdf/2509.07488v1",
    "github_url": null,
    "published": "2025-09-09T08:08:35+00:00",
    "updated": "2025-09-09T08:08:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.07220v1",
    "title": "OmniAcc: Personalized Accessibility Assistant Using Generative AI",
    "authors": [
      "Karki",
      "Han",
      "Mahmud"
    ],
    "summary": "Individuals with ambulatory disabilities often encounter significant barriers when navigating urban environments due to the lack of accessible information and tools. This paper presents OmniAcc, an AI-powered interactive navigation system that utilizes GPT-4, satellite imagery, and OpenStreetMap data to identify, classify, and map wheelchair-accessible features such as ramps and crosswalks in the built environment. OmniAcc offers personalized route planning, real-time hands-free navigation, and instant query responses regarding physical accessibility. By using zero-shot learning and customized prompts, the system ensures precise detection of accessibility features, while supporting validation through structured workflows. This paper introduces OmniAcc and explores its potential to assist urban planners and mobility-aid users, demonstrated through a case study on crosswalk detection. With a crosswalk detection accuracy of 97.5%, OmniAcc highlights the transformative potential of AI in improving navigation and fostering more inclusive urban spaces.",
    "pdf_url": "https://arxiv.org/pdf/2509.07220v1",
    "github_url": null,
    "published": "2025-09-08T21:03:48+00:00",
    "updated": "2025-09-08T21:03:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.06644v4",
    "title": "T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation",
    "authors": [
      "Zhao",
      "Lyu",
      "Li"
    ],
    "summary": "Agricultural robotic agents have been becoming powerful helpers in a wide range of agricultural tasks, however, still heavily rely on manual operation or fixed railways for movement. To address this limitation, the AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling agents to navigate to the target positions following the natural language instructions. AgriVLN effectively understands the simple instructions, but often misunderstands the complex ones. To bridge this gap, we propose the method of Translator for Agricultural Robotic Agents on Vision-and-Language Navigation (T-araVLN), in which the Instruction Translator module translates the original instruction to be more refined and precise. When evaluated on the A2A benchmark, our T-araVLN effectively improves Success Rate from 0.47 to 0.63 and reduces Navigation Error from 2.91m to 2.28m, demonstrating the state-of-the-art performance in the agricultural domain. Code: https://github.com/AlexTraveling/T-araVLN.",
    "pdf_url": "https://arxiv.org/pdf/2509.06644v4",
    "github_url": "https://github.com/AlexTraveling/T-araVLN",
    "published": "2025-09-08T12:59:36+00:00",
    "updated": "2025-09-18T12:15:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.06317v1",
    "title": "$\\mathcal{H}_\\infty$ Optimal Navigation in the Cislunar Space with LFT Models",
    "authors": [
      "Kumar",
      "Bhattacharya"
    ],
    "summary": "Navigation in the cislunar domain presents significant challenges due to chaotic and unmodeled dynamics, as well as state-dependent sensor errors. This paper develops a robust estimation framework based on Linear Fractional Transformation (LFT) models, and state estimation in $\\mathcal{H}_\\infty$ and $μ$ synthesis framework to address these challenges. The cislunar dynamics are embedded into an LFT form that captures nonlinearities in the gravitational model and state-dependent sensor errors as structured uncertainty. A nonlinear estimator is then synthesized in the $\\mathcal{H}_\\infty$ sense to ensure robust performance guarantees in the presence of the stated uncertainties. Simulation results demonstrate the effectiveness of the estimator for navigation in a surveillance constellation.",
    "pdf_url": "https://arxiv.org/pdf/2509.06317v1",
    "github_url": null,
    "published": "2025-09-08T03:47:28+00:00",
    "updated": "2025-09-08T03:47:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.05903v1",
    "title": "Optimal Anchor Deployment and Topology Design for Large-Scale AUV Navigation",
    "authors": [
      "Huang",
      "Lu",
      "Xu"
    ],
    "summary": "Seafloor acoustic anchors are an important component of AUV navigation, providing absolute updates that correct inertial dead-reckoning. Unlike terrestrial positioning systems, the deployment of underwater anchor nodes is usually sparse due to the uneven distribution of underwater users, as well as the high economic cost and difficult maintenance of underwater equipment. These anchor nodes lack satellite coverage and cannot form ubiquitous backhaul as terrestrial nodes do. In this paper, we investigate the optimal anchor deployment topology to provide high-quality AUV navigation and positioning services. We first analyze the possible deployment mode in large-scale underwater navigation system, and formulate a topology optimization for underwater anchor node deployment. Then, we derive a scaling law about the influence of anchors in each cluster on the navigation performance within a given area and demonstrate a service area coverage condition with a high probability of reaching the destination. Finally, the optimization performance is evaluated through experimental results.",
    "pdf_url": "https://arxiv.org/pdf/2509.05903v1",
    "github_url": null,
    "published": "2025-09-07T02:59:44+00:00",
    "updated": "2025-09-07T02:59:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.05672v1",
    "title": "Sharing but Not Caring: Similar Outcomes for Shared Control and Switching Control in Telepresence-Robot Navigation",
    "authors": [
      "Kalliokoski",
      "Center",
      "LaValle"
    ],
    "summary": "Telepresence robots enable users to interact with remote environments, but efficient and intuitive navigation remains a challenge. In this work, we developed and evaluated a shared control method, in which the robot navigates autonomously while allowing users to affect the path generation to better suit their needs. We compared this with control switching, where users toggle between direct and automated control. We hypothesized that shared control would maintain efficiency comparable to control switching while potentially reducing user workload. The results of two consecutive user studies (each with final sample of n=20) showed that shared control does not degrade navigation efficiency, but did not show a significant reduction in task load compared to control switching. Further research is needed to explore the underlying factors that influence user preference and performance in these control systems.",
    "pdf_url": "https://arxiv.org/pdf/2509.05672v1",
    "github_url": null,
    "published": "2025-09-06T10:27:05+00:00",
    "updated": "2025-09-06T10:27:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.05645v1",
    "title": "Stereovision Image Processing for Planetary Navigation Maps with Semi-Global Matching and Superpixel Segmentation",
    "authors": [
      "Lu",
      "Arana-Catania",
      "Upadhyay"
    ],
    "summary": "Mars exploration requires precise and reliable terrain models to ensure safe rover navigation across its unpredictable and often hazardous landscapes. Stereoscopic vision serves a critical role in the rover's perception, allowing scene reconstruction by generating precise depth maps through stereo matching. State-of-the-art Martian planetary exploration uses traditional local block-matching, aggregates cost over square windows, and refines disparities via smoothness constraints. However, this method often struggles with low-texture images, occlusion, and repetitive patterns because it considers only limited neighbouring pixels and lacks a wider understanding of scene context. This paper uses Semi-Global Matching (SGM) with superpixel-based refinement to mitigate the inherent block artefacts and recover lost details. The approach balances the efficiency and accuracy of SGM and adds context-aware segmentation to support more coherent depth inference. The proposed method has been evaluated in three datasets with successful results: In a Mars analogue, the terrain maps obtained show improved structural consistency, particularly in sloped or occlusion-prone regions. Large gaps behind rocks, which are common in raw disparity outputs, are reduced, and surface details like small rocks and edges are captured more accurately. Another two datasets, evaluated to test the method's general robustness and adaptability, show more precise disparity maps and more consistent terrain models, better suited for the demands of autonomous navigation on Mars, and competitive accuracy across both non-occluded and full-image error metrics. This paper outlines the entire terrain modelling process, from finding corresponding features to generating the final 2D navigation maps, offering a complete pipeline suitable for integration in future planetary exploration missions.",
    "pdf_url": "https://arxiv.org/pdf/2509.05645v1",
    "github_url": null,
    "published": "2025-09-06T08:53:10+00:00",
    "updated": "2025-09-06T08:53:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.04950v1",
    "title": "Ground-Aware Octree-A* Hybrid Path Planning for Memory-Efficient 3D Navigation of Ground Vehicles",
    "authors": [
      "Ham",
      "Kim",
      "Kim"
    ],
    "summary": "In this paper, we propose a 3D path planning method that integrates the A* algorithm with the octree structure. Unmanned Ground Vehicles (UGVs) and legged robots have been extensively studied, enabling locomotion across a variety of terrains. Advances in mobility have enabled obstacles to be regarded not only as hindrances to be avoided, but also as navigational aids when beneficial. A modified 3D A* algorithm generates an optimal path by leveraging obstacles during the planning process. By incorporating a height-based penalty into the cost function, the algorithm enables the use of traversable obstacles to aid locomotion while avoiding those that are impassable, resulting in more efficient and realistic path generation. The octree-based 3D grid map achieves compression by merging high-resolution nodes into larger blocks, especially in obstacle-free or sparsely populated areas. This reduces the number of nodes explored by the A* algorithm, thereby improving computational efficiency and memory usage, and supporting real-time path planning in practical environments. Benchmark results demonstrate that the use of octree structure ensures an optimal path while significantly reducing memory usage and computation time.",
    "pdf_url": "https://arxiv.org/pdf/2509.04950v1",
    "github_url": null,
    "published": "2025-09-05T09:15:20+00:00",
    "updated": "2025-09-05T09:15:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.04258v2",
    "title": "Safe Navigation in the Presence of Range-Limited Pursuers",
    "authors": [
      "Chapman",
      "Moll",
      "Weintraub"
    ],
    "summary": "This paper examines the degree to which an evader seeking a safe and efficient path to a target location can benefit from increasing levels of knowledge regarding one or more range-limited pursuers seeking to intercept it. Unlike previous work, this research considers the time of flight of the pursuers actively attempting interception. It is shown that additional knowledge allows the evader to safely steer closer to the threats, shortening paths without accepting additional risk of capture. A control heuristic is presented, suitable for real-time implementation, which capitalizes on all knowledge available to the evader.",
    "pdf_url": "https://arxiv.org/pdf/2509.04258v2",
    "github_url": null,
    "published": "2025-09-04T14:36:32+00:00",
    "updated": "2025-12-15T15:34:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.03199v1",
    "title": "Finding My Way: Influence of Different Audio Augmented Reality Navigation Cues on User Experience and Subjective Usefulness",
    "authors": [
      "Hinzmann",
      "Vona",
      "Henning"
    ],
    "summary": "As augmented reality (AR) becomes increasingly prevalent in mobile and context-aware applications, the role of auditory cues in guiding users through physical environments is becoming critical. This study investigates the effectiveness and user experience of various categories of audio cues, including fully non-verbal sounds and speech-derived Spearcons, during outdoor navigation tasks using the Meta Quest 3 headset. Twenty participants navigated five outdoor routes using audio-only cue types: Artificial Sounds, Nature Sounds, Spearcons, Musical Instruments, and Auditory Icons. Subjective evaluations were collected to assess the perceived effectiveness and user experience of each sound type. Results revealed significant differences in perceived novelty and stimulation across sound types. Artificial Sounds and Musical Instruments were rated higher than Spearcons in novelty, while Artificial Sounds were also rated higher than Spearcons in stimulation. Overall preference was evenly split between Nature Sounds and Artificial Sounds. These findings suggest that incorporating aspects of novelty and user engagement in auditory feedback design may enhance the effectiveness of AR navigation systems.",
    "pdf_url": "https://arxiv.org/pdf/2509.03199v1",
    "github_url": null,
    "published": "2025-09-03T10:35:39+00:00",
    "updated": "2025-09-03T10:35:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.02983v1",
    "title": "DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features",
    "authors": [
      "Yang",
      "Le",
      "Gong"
    ],
    "summary": "Autonomous underwater navigation remains a challenging problem due to limited sensing capabilities and the difficulty of constructing accurate maps in underwater environments. In this paper, we propose a Diffusion-based Underwater Visual Navigation policy via knowledge-transferred depth features, named DUViN, which enables vision-based end-to-end 4-DoF motion control for underwater vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles and maintain a safe and perception awareness altitude relative to the terrain without relying on pre-built maps. To address the difficulty of collecting large-scale underwater navigation datasets, we propose a method that ensures robust generalization under domain shifts from in-air to underwater environments by leveraging depth features and introducing a novel model transfer strategy. Specifically, our training framework consists of two phases: we first train the diffusion-based visual navigation policy on in-air datasets using a pre-trained depth feature extractor. Secondly, we retrain the extractor on an underwater depth estimation task and integrate the adapted extractor into the trained navigation policy from the first step. Experiments in both simulated and real-world underwater environments demonstrate the effectiveness and generalization of our approach. The experimental videos are available at https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.",
    "pdf_url": "https://arxiv.org/pdf/2509.02983v1",
    "github_url": null,
    "published": "2025-09-03T03:43:12+00:00",
    "updated": "2025-09-03T03:43:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.02808v1",
    "title": "Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers",
    "authors": [
      "Ward",
      "Paral",
      "Riordan"
    ],
    "summary": "Autonomously controlling quadrotors in large-scale subterranean environments is applicable to many areas such as environmental surveying, mining operations, and search and rescue. Learning-based controllers represent an appealing approach to autonomy, but are known to not generalize well to `out-of-distribution' environments not encountered during training. In this work, we train a normalizing flow-based prior over the environment, which provides a measure of how far out-of-distribution the quadrotor is at any given time. We use this measure as a runtime monitor, allowing us to switch between a learning-based controller and a safe controller when we are sufficiently out-of-distribution. Our methods are benchmarked on a point-to-point navigation task in a simulated 3D cave environment based on real-world point cloud data from the DARPA Subterranean Challenge Final Event Dataset. Our experimental results show that our combined controller simultaneously possesses the liveness of the learning-based controller (completing the task quickly) and the safety of the safety controller (avoiding collision).",
    "pdf_url": "https://arxiv.org/pdf/2509.02808v1",
    "github_url": null,
    "published": "2025-09-02T20:22:54+00:00",
    "updated": "2025-09-02T20:22:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.02761v3",
    "title": "Plan Verification for LLM-Based Embodied Task Completion Agents",
    "authors": [
      "Hariharan",
      "Dongre",
      "Hakkani-Tür"
    ],
    "summary": "Large language model (LLM) based task plans and corresponding human demonstrations for embodied AI may be noisy, with unnecessary actions, redundant navigation, and logical errors that reduce policy quality. We propose an iterative verification framework in which a Judge LLM critiques action sequences and a Planner LLM applies the revisions, yielding progressively cleaner and more spatially coherent trajectories. Unlike rule-based approaches, our method relies on natural language prompting, enabling broad generalization across error types including irrelevant actions, contradictions, and missing steps. On a set of manually annotated actions from the TEACh embodied AI dataset, our framework achieves up to 90% recall and 100% precision across four state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout). The refinement loop converges quickly, with 96.5% of sequences requiring at most three iterations, while improving both temporal efficiency and spatial action organization. Crucially, the method preserves human error-recovery patterns rather than collapsing them, supporting future work on robust corrective behavior. By establishing plan verification as a reliable LLM capability for spatial planning and action refinement, we provide a scalable path to higher-quality training data for imitation learning in embodied AI.",
    "pdf_url": "https://arxiv.org/pdf/2509.02761v3",
    "github_url": null,
    "published": "2025-09-02T19:06:56+00:00",
    "updated": "2025-09-24T03:01:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.02204v1",
    "title": "Adaptive Navigation Strategy for Low-Thrust Proximity Operations in Circular Relative Orbit",
    "authors": [
      "Ruggiero",
      "Mancini",
      "Capello"
    ],
    "summary": "This paper presents an adaptive observer-based navigation strategy for spacecraft in Circular Relative Orbit (CRO) scenarios, addressing challenges in proximity operations like formation flight and uncooperative target inspection. The proposed method adjusts observer gains based on the estimated state to achieve fast convergence and low noise sensitivity in state estimation. A Lyapunov-based analysis ensures stability and accuracy, while simulations using vision-based sensor data validate the approach under realistic conditions. Compared to classical observers with time-invariant gains, the proposed method enhances trajectory tracking precision and reduces control input switching, making it a promising solution for autonomous spacecraft localization and control.",
    "pdf_url": "https://arxiv.org/pdf/2509.02204v1",
    "github_url": null,
    "published": "2025-09-02T11:18:16+00:00",
    "updated": "2025-09-02T11:18:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.02134v1",
    "title": "Learning Social Heuristics for Human-Aware Path Planning",
    "authors": [
      "Eirale",
      "Leonetti",
      "Chiaberge"
    ],
    "summary": "Social robotic navigation has been at the center of numerous studies in recent years. Most of the research has focused on driving the robotic agent along obstacle-free trajectories, respecting social distances from humans, and predicting their movements to optimize navigation. However, in order to really be socially accepted, the robots must be able to attain certain social norms that cannot arise from conventional navigation, but require a dedicated learning process. We propose Heuristic Planning with Learned Social Value (HPLSV), a method to learn a value function encapsulating the cost of social navigation, and use it as an additional heuristic in heuristic-search path planning. In this preliminary work, we apply the methodology to the common social scenario of joining a queue of people, with the intention of generalizing to further human activities.",
    "pdf_url": "https://arxiv.org/pdf/2509.02134v1",
    "github_url": null,
    "published": "2025-09-02T09:36:11+00:00",
    "updated": "2025-09-02T09:36:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.01838v1",
    "title": "Goal-Conditioned Reinforcement Learning for Data-Driven Maritime Navigation",
    "authors": [
      "Vaidheeswaran",
      "Jayakody",
      "Mulay"
    ],
    "summary": "Routing vessels through narrow and dynamic waterways is challenging due to changing environmental conditions and operational constraints. Existing vessel-routing studies typically fail to generalize across multiple origin-destination pairs and do not exploit large-scale, data-driven traffic graphs. In this paper, we propose a reinforcement learning solution for big maritime data that can learn to find a route across multiple origin-destination pairs while adapting to different hexagonal grid resolutions. Agents learn to select direction and speed under continuous observations in a multi-discrete action space. A reward function balances fuel efficiency, travel time, wind resistance, and route diversity, using an Automatic Identification System (AIS)-derived traffic graph with ERA5 wind fields. The approach is demonstrated in the Gulf of St. Lawrence, one of the largest estuaries in the world. We evaluate configurations that combine Proximal Policy Optimization with recurrent networks, invalid-action masking, and exploration strategies. Our experiments demonstrate that action masking yields a clear improvement in policy performance and that supplementing penalty-only feedback with positive shaping rewards produces additional gains.",
    "pdf_url": "https://arxiv.org/pdf/2509.01838v1",
    "github_url": null,
    "published": "2025-09-01T23:42:16+00:00",
    "updated": "2025-09-01T23:42:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.01836v1",
    "title": "Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk Assessment",
    "authors": [
      "Alam",
      "Rodrigues-Jr",
      "Spadon"
    ],
    "summary": "Accurate vessel trajectory prediction is essential for enhancing situational awareness and preventing collisions. Still, existing data-driven models are constrained mainly to single-vessel forecasting, overlooking vessel interactions, navigation rules, and explicit collision risk assessment. We present a transformer-based framework for multi-vessel trajectory prediction with integrated collision risk analysis. For a given target vessel, the framework identifies nearby vessels. It jointly predicts their future trajectories through parallel streams encoding kinematic and derived physical features, causal convolutions for temporal locality, spatial transformations for positional encoding, and hybrid positional embeddings that capture both local motion patterns and long-range dependencies. Evaluated on large-scale real-world AIS data using joint multi-vessel metrics, the model demonstrates superior forecasting capabilities beyond traditional single-vessel displacement errors. By simulating interactions among predicted trajectories, the framework further quantifies potential collision risks, offering actionable insights to strengthen maritime safety and decision support.",
    "pdf_url": "https://arxiv.org/pdf/2509.01836v1",
    "github_url": null,
    "published": "2025-09-01T23:38:01+00:00",
    "updated": "2025-09-01T23:38:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.01658v1",
    "title": "MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation",
    "authors": [
      "Wu",
      "Ma",
      "Xu"
    ],
    "summary": "Mobile manipulation stands as a core challenge in robotics, enabling robots to assist humans across varied tasks and dynamic daily environments. Conventional mobile manipulation approaches often struggle to generalize across different tasks and environments due to the lack of large-scale training. However, recent advances in manipulation foundation models demonstrate impressive generalization capability on a wide range of fixed-base manipulation tasks, which are still limited to a fixed setting. Therefore, we devise a plug-in module named MoTo, which can be combined with any off-the-shelf manipulation foundation model to empower them with mobile manipulation ability. Specifically, we propose an interaction-aware navigation policy to generate robot docking points for generalized mobile manipulation. To enable zero-shot ability, we propose an interaction keypoints framework via vision-language models (VLM) under multi-view consistency for both target object and robotic arm following instructions, where fixed-base manipulation foundation models can be employed. We further propose motion planning objectives for the mobile base and robot arm, which minimize the distance between the two keypoints and maintain the physical feasibility of trajectories. In this way, MoTo guides the robot to move to the docking points where fixed-base manipulation can be successfully performed, and leverages VLM generation and trajectory optimization to achieve mobile manipulation in a zero-shot manner, without any requirement on mobile manipulation expert data. Extensive experimental results on OVMM and real-world demonstrate that MoTo achieves success rates of 2.68% and 16.67% higher than the state-of-the-art mobile manipulation methods, respectively, without requiring additional training data.",
    "pdf_url": "https://arxiv.org/pdf/2509.01658v1",
    "github_url": null,
    "published": "2025-09-01T17:59:03+00:00",
    "updated": "2025-09-01T17:59:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.01364v1",
    "title": "TopoNav: Topological Graphs as a Key Enabler for Advanced Object Navigation",
    "authors": [
      "Liu",
      "Zhang",
      "Peng"
    ],
    "summary": "Object Navigation (ObjectNav) has made great progress with large language models (LLMs), but still faces challenges in memory management, especially in long-horizon tasks and dynamic scenes. To address this, we propose TopoNav, a new framework that leverages topological structures as spatial memory. By building and updating a topological graph that captures scene connections, adjacency, and semantic meaning, TopoNav helps agents accumulate spatial knowledge over time, retrieve key information, and reason effectively toward distant goals. Our experiments show that TopoNav achieves state-of-the-art performance on benchmark ObjectNav datasets, with higher success rates and more efficient paths. It particularly excels in diverse and complex environments, as it connects temporary visual inputs with lasting spatial understanding.",
    "pdf_url": "https://arxiv.org/pdf/2509.01364v1",
    "github_url": null,
    "published": "2025-09-01T11:05:38+00:00",
    "updated": "2025-09-01T11:05:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.01251v1",
    "title": "Towards Data-Driven Metrics for Social Robot Navigation Benchmarking",
    "authors": [
      "Bachiller-Burgos",
      "Bernardet",
      "Calderita"
    ],
    "summary": "This paper presents a joint effort towards the development of a data-driven Social Robot Navigation metric to facilitate benchmarking and policy optimization. We provide our motivations for our approach and describe our proposal for storing rated social navigation trajectory datasets. Following these guidelines, we compiled a dataset with 4427 trajectories -- 182 real and 4245 simulated -- and presented it to human raters, yielding a total of 4402 rated trajectories after data quality assurance. We also trained an RNN-based baseline metric on the dataset and present quantitative and qualitative results. All data, software, and model weights are publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2509.01251v1",
    "github_url": null,
    "published": "2025-09-01T08:42:28+00:00",
    "updated": "2025-09-01T08:42:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.00329v1",
    "title": "Jacobian Exploratory Dual-Phase Reinforcement Learning for Dynamic Endoluminal Navigation of Deformable Continuum Robots",
    "authors": [
      "Tian",
      "Ng",
      "Ren"
    ],
    "summary": "Deformable continuum robots (DCRs) present unique planning challenges due to nonlinear deformation mechanics and partial state observability, violating the Markov assumptions of conventional reinforcement learning (RL) methods. While Jacobian-based approaches offer theoretical foundations for rigid manipulators, their direct application to DCRs remains limited by time-varying kinematics and underactuated deformation dynamics. This paper proposes Jacobian Exploratory Dual-Phase RL (JEDP-RL), a framework that decomposes planning into phased Jacobian estimation and policy execution. During each training step, we first perform small-scale local exploratory actions to estimate the deformation Jacobian matrix, then augment the state representation with Jacobian features to restore approximate Markovianity. Extensive SOFA surgical dynamic simulations demonstrate JEDP-RL's three key advantages over proximal policy optimization (PPO) baselines: 1) Convergence speed: 3.2x faster policy convergence, 2) Navigation efficiency: requires 25% fewer steps to reach the target, and 3) Generalization ability: achieve 92% success rate under material property variations and achieve 83% (33% higher than PPO) success rate in the unseen tissue environment.",
    "pdf_url": "https://arxiv.org/pdf/2509.00329v1",
    "github_url": null,
    "published": "2025-08-30T03:04:35+00:00",
    "updated": "2025-08-30T03:04:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.00319v1",
    "title": "Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep Reinforcement Learning in Dynamic Stomach",
    "authors": [
      "Ng",
      "Gao",
      "Ren"
    ],
    "summary": "Navigating a flexible robotic endoscope (FRE) through the gastrointestinal tract is critical for surgical diagnosis and treatment. However, navigation in the dynamic stomach is particularly challenging because the FRE must learn to effectively use contact with the deformable stomach walls to reach target locations. To address this, we introduce a deep reinforcement learning (DRL) based Contact-Aided Navigation (CAN) strategy for FREs, leveraging contact force feedback to enhance motion stability and navigation precision. The training environment is established using a physics-based finite element method (FEM) simulation of a deformable stomach. Trained with the Proximal Policy Optimization (PPO) algorithm, our approach achieves high navigation success rates (within 3 mm error between the FRE's end-effector and target) and significantly outperforms baseline policies. In both static and dynamic stomach environments, the CAN agent achieved a 100% success rate with 1.6 mm average error, and it maintained an 85% success rate in challenging unseen scenarios with stronger external disturbances. These results validate that the DRL-based CAN strategy substantially enhances FRE navigation performance over prior methods.",
    "pdf_url": "https://arxiv.org/pdf/2509.00319v1",
    "github_url": null,
    "published": "2025-08-30T02:42:06+00:00",
    "updated": "2025-08-30T02:42:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2509.00210v1",
    "title": "Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment",
    "authors": [
      "Tang",
      "zhang",
      "Liu"
    ],
    "summary": "Achieving human-like reasoning in deep learning models for complex tasks in unknown environments remains a critical challenge in embodied intelligence. While advanced vision-language models (VLMs) excel in static scene understanding, their limitations in spatio-temporal reasoning and adaptation to dynamic, open-set tasks like task-oriented navigation and embodied question answering (EQA) persist due to inadequate modeling of fine-grained spatio-temporal cues and physical world comprehension. To address this, we propose VEME, a novel cross-modal alignment method that enhances generalization in unseen scenes by learning an ego-centric, experience-centered world model. Our framework integrates three key components: (1) a cross-modal alignment framework bridging objects, spatial representations, and visual semantics with spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic, implicit cognitive map activated by world embedding to enable task-relevant geometric-semantic memory recall; and (3) an instruction-based navigation and reasoning framework leveraging embodied priors for long-term planning and efficient exploration. By embedding geometry-aware spatio-temporal episodic experiences, our method significantly improves reasoning and planning in dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate 1%-3% accuracy and exploration efficiency improvement compared to traditional approaches.",
    "pdf_url": "https://arxiv.org/pdf/2509.00210v1",
    "github_url": null,
    "published": "2025-08-29T19:47:25+00:00",
    "updated": "2025-08-29T19:47:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.21455v1",
    "title": "Assessing Human Cooperation for Enhancing Social Robot Navigation",
    "authors": [
      "Arunachalam",
      "Singamaneni",
      "Alami"
    ],
    "summary": "Socially aware robot navigation is a planning paradigm where the robot navigates in human environments and tries to adhere to social constraints while interacting with the humans in the scene. These navigation strategies were further improved using human prediction models, where the robot takes the potential future trajectory of humans while computing its own. Though these strategies significantly improve the robot's behavior, it faces difficulties from time to time when the human behaves in an unexpected manner. This happens as the robot fails to understand human intentions and cooperativeness, and the human does not have a clear idea of what the robot is planning to do. In this paper, we aim to address this gap through effective communication at an appropriate time based on a geometric analysis of the context and human cooperativeness in head-on crossing scenarios. We provide an assessment methodology and propose some evaluation metrics that could distinguish a cooperative human from a non-cooperative one. Further, we also show how geometric reasoning can be used to generate appropriate verbal responses or robot actions.",
    "pdf_url": "https://arxiv.org/pdf/2508.21455v1",
    "github_url": null,
    "published": "2025-08-29T09:38:21+00:00",
    "updated": "2025-08-29T09:38:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.21361v1",
    "title": "QUAV: Quantum-Assisted Path Planning and Optimization for UAV Navigation with Obstacle Avoidance",
    "authors": [
      "Innan",
      "Kashif",
      "Marchisio"
    ],
    "summary": "The growing demand for drone navigation in urban and restricted airspaces requires real-time path planning that is both safe and scalable. Classical methods often struggle with the computational load of high-dimensional optimization under dynamic constraints like obstacle avoidance and no-fly zones. This work introduces QUAV, a quantum-assisted UAV path planning framework based on the Quantum Approximate Optimization Algorithm (QAOA), to the best of our knowledge, this is one of the first applications of QAOA for drone trajectory optimization. QUAV models pathfinding as a quantum optimization problem, allowing efficient exploration of multiple paths while incorporating obstacle constraints and geospatial accuracy through UTM coordinate transformation. A theoretical analysis shows that QUAV achieves linear scaling in circuit depth relative to the number of edges, under fixed optimization settings. Extensive simulations and a real-hardware implementation on IBM's ibm_kyiv backend validate its performance and robustness under noise. Despite hardware constraints, results demonstrate that QUAV generates feasible, efficient trajectories, highlighting the promise of quantum approaches for future drone navigation systems.",
    "pdf_url": "https://arxiv.org/pdf/2508.21361v1",
    "github_url": null,
    "published": "2025-08-29T06:59:21+00:00",
    "updated": "2025-08-29T06:59:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.20822v1",
    "title": "Activated Backstepping with Control Barrier Functions for the Safe Navigation of Automated Vehicles",
    "authors": [
      "Gacsi",
      "Cohen",
      "Molnar"
    ],
    "summary": "This paper introduces a novel safety-critical control method through the synthesis of control barrier functions (CBFs) for systems with high-relative-degree safety constraints. By extending the procedure of CBF backstepping, we propose activated backstepping - a constructive method to synthesize valid CBFs. The novelty of our method is the incorporation of an activation function into the CBF, which offers less conservative safe sets in the state space than standard CBF backstepping. We demonstrate the proposed method on an inverted pendulum example, where we explain the underlying geometric meaning in the state space and provide comparisons with existing CBF synthesis techniques. Finally, we implement our method to achieve collision-free navigation for automated vehicles using a kinematic bicycle model in simulation.",
    "pdf_url": "https://arxiv.org/pdf/2508.20822v1",
    "github_url": null,
    "published": "2025-08-28T14:19:44+00:00",
    "updated": "2025-08-28T14:19:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.20762v1",
    "title": "SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer",
    "authors": [
      "Kartiman",
      "Rasim",
      "Wihardi"
    ],
    "summary": "Focusing on the development of an end-to-end autonomous vehicle model with pixel-to-pixel context awareness, this research proposes the SKGE-Swin architecture. This architecture utilizes the Swin Transformer with a skip-stage mechanism to broaden feature representation globally and at various network levels. This approach enables the model to extract information from distant pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head Self-Attention (SW-MSA) mechanism and to retain critical information from the initial to the final stages of feature extraction, thereby enhancing its capability to comprehend complex patterns in the vehicle's surroundings. The model is evaluated on the CARLA platform using adversarial scenarios to simulate real-world conditions. Experimental results demonstrate that the SKGE-Swin architecture achieves a superior Driving Score compared to previous methods. Furthermore, an ablation study will be conducted to evaluate the contribution of each architectural component, including the influence of skip connections and the use of the Swin Transformer, in improving model performance.",
    "pdf_url": "https://arxiv.org/pdf/2508.20762v1",
    "github_url": null,
    "published": "2025-08-28T13:17:35+00:00",
    "updated": "2025-08-28T13:17:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.20479v1",
    "title": "Joint Contact Planning for Navigation and Communication in GNSS-Libration Point Systems",
    "authors": [
      "Yan",
      "Fraire",
      "Yang"
    ],
    "summary": "Deploying satellites at Earth-Moon Libration Points (LPs) addresses the inherent deep-space coverage gaps of low-altitude GNSS constellations. Integrating LP satellites with GNSS into a joint constellation enables a more robust and comprehensive Positioning, Navigation, and Timing (PNT) system, while also extending navigation and communication services to spacecraft operating in cislunar space (i.e., users). However, the long propagation delays between LP satellites, users, and GNSS satellites result in significantly different link durations compared to those within the GNSS constellation. Scheduling inter-satellite links (ISLs) is a core task of Contact Plan Design (CPD). Existing CPD approaches focus exclusively on GNSS constellations, assuming uniform link durations, and thus cannot accommodate the heterogeneous link timescales present in a joint GNSS-LP system. To overcome this limitation, we introduce a Joint CPD (J-CPD) scheme tailored to handle ISLs with differing duration units across integrated constellations. The key contributions of J-CPD are: (i):introduction of LongSlots (Earth-Moon scale links) and ShortSlots (GNSS-scale links); (ii):a hierarchical and crossed CPD process for scheduling LongSlots and ShortSlots ISLs; (iii):an energy-driven link scheduling algorithm adapted to the CPD process. Simulations on a joint BeiDou-LP constellation demonstrate that J-CPD surpasses the baseline FCP method in both delay and ranging coverage, while maintaining high user satisfaction and enabling tunable trade-offs through adjustable potential-energy parameters. To our knowledge, this is the first CPD framework to jointly optimize navigation and communication in GNSS-LP systems, representing a key step toward unified and resilient deep-space PNT architectures.",
    "pdf_url": "https://arxiv.org/pdf/2508.20479v1",
    "github_url": null,
    "published": "2025-08-28T06:55:31+00:00",
    "updated": "2025-08-28T06:55:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.19595v1",
    "title": "A Lightweight Crowd Model for Robot Social Navigation",
    "authors": [
      "Eskeri",
      "Wiedemann",
      "Kyrki"
    ],
    "summary": "Robots operating in human-populated environments must navigate safely and efficiently while minimizing social disruption. Achieving this requires estimating crowd movement to avoid congested areas in real-time. Traditional microscopic models struggle to scale in dense crowds due to high computational cost, while existing macroscopic crowd prediction models tend to be either overly simplistic or computationally intensive. In this work, we propose a lightweight, real-time macroscopic crowd prediction model tailored for human motion, which balances prediction accuracy and computational efficiency. Our approach simplifies both spatial and temporal processing based on the inherent characteristics of pedestrian flow, enabling robust generalization without the overhead of complex architectures. We demonstrate a 3.6 times reduction in inference time, while improving prediction accuracy by 3.1 %. Integrated into a socially aware planning framework, the model enables efficient and socially compliant robot navigation in dynamic environments. This work highlights that efficient human crowd modeling enables robots to navigate dense environments without costly computations.",
    "pdf_url": "https://arxiv.org/pdf/2508.19595v1",
    "github_url": null,
    "published": "2025-08-27T06:13:43+00:00",
    "updated": "2025-08-27T06:13:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.19159v1",
    "title": "Safe Navigation under State Uncertainty: Online Adaptation for Robust Control Barrier Functions",
    "authors": [
      "Das",
      "Nanayakkara",
      "Tan"
    ],
    "summary": "Measurements and state estimates are often imperfect in control practice, posing challenges for safety-critical applications, where safety guarantees rely on accurate state information. In the presence of estimation errors, several prior robust control barrier function (R-CBF) formulations have imposed strict conditions on the input. These methods can be overly conservative and can introduce issues such as infeasibility, high control effort, etc. This work proposes a systematic method to improve R-CBFs, and demonstrates its advantages on a tracked vehicle that navigates among multiple obstacles. A primary contribution is a new optimization-based online parameter adaptation scheme that reduces the conservativeness of existing R-CBFs. In order to reduce the complexity of the parameter optimization, we merge several safety constraints into one unified numerical CBF via Poisson's equation. We further address the dual relative degree issue that typically causes difficulty in vehicle tracking. Experimental trials demonstrate the overall performance improvement of our approach over existing formulations.",
    "pdf_url": "https://arxiv.org/pdf/2508.19159v1",
    "github_url": null,
    "published": "2025-08-26T16:09:14+00:00",
    "updated": "2025-08-26T16:09:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.19131v2",
    "title": "ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments",
    "authors": [
      "Gummadi",
      "Gasparino",
      "Capezzuto"
    ],
    "summary": "The advancement of robotics and autonomous navigation systems hinges on the ability to accurately predict terrain traversability. Traditional methods for generating datasets to train these prediction models often involve putting robots into potentially hazardous environments, posing risks to equipment and safety. To solve this problem, we present ZeST, a novel approach leveraging visual reasoning capabilities of Large Language Models (LLMs) to create a traversability map in real-time without exposing robots to danger. Our approach not only performs zero-shot traversability and mitigates the risks associated with real-world data collection but also accelerates the development of advanced navigation systems, offering a cost-effective and scalable solution. To support our findings, we present navigation results, in both controlled indoor and unstructured outdoor environments. As shown in the experiments, our method provides safer navigation when compared to other state-of-the-art methods, constantly reaching the final goal.",
    "pdf_url": "https://arxiv.org/pdf/2508.19131v2",
    "github_url": null,
    "published": "2025-08-26T15:30:19+00:00",
    "updated": "2025-10-17T19:27:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.18606v2",
    "title": "SignLoc: Robust Localization using Navigation Signs and Public Maps",
    "authors": [
      "Zimmerman",
      "Loo",
      "Agrawal"
    ],
    "summary": "Navigation signs and maps, such as floor plans and street maps, are widely available and serve as ubiquitous aids for way-finding in human environments. Yet, they are rarely used by robot systems. This paper presents SignLoc, a global localization method that leverages navigation signs to localize the robot on publicly available maps -- specifically floor plans and OpenStreetMap (OSM) graphs -- without prior sensor-based mapping. SignLoc first extracts a navigation graph from the input map. It then employs a probabilistic observation model to match directional and locational cues from the detected signs to the graph, enabling robust topo-semantic localization within a Monte Carlo framework. We evaluated SignLoc in diverse large-scale environments: part of a university campus, a shopping mall, and a hospital complex. Experimental results show that SignLoc reliably localizes the robot after observing only one to two signs.",
    "pdf_url": "https://arxiv.org/pdf/2508.18606v2",
    "github_url": null,
    "published": "2025-08-26T02:24:04+00:00",
    "updated": "2025-08-29T05:59:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.18539v2",
    "title": "Adaptive Visual Navigation Assistant in 3D RPGs",
    "authors": [
      "Xu",
      "Verbrugge"
    ],
    "summary": "In complex 3D game environments, players rely on visual affordances to spot map transition points. Efficient identification of such points is important to client-side auto-mapping, and provides an objective basis for evaluating map cue presentation. In this work, we formalize the task of detecting traversable Spatial Transition Points (STPs)-connectors between two sub regions-and selecting the singular Main STP (MSTP), the unique STP that lies on the designer-intended critical path toward the player's current macro-objective, from a single game frame, proposing this as a new research focus. We introduce a two-stage deep-learning pipeline that first detects potential STPs using Faster R-CNN and then ranks them with a lightweight MSTP selector that fuses local and global visual features. Both stages benefit from parameter-efficient adapters, and we further introduce an optional retrieval-augmented fusion step. Our primary goal is to establish the feasibility of this problem and set baseline performance metrics. We validate our approach on a custom-built, diverse dataset collected from five Action RPG titles. Our experiments reveal a key trade-off: while full-network fine-tuning produces superior STP detection with sufficient data, adapter-only transfer is significantly more robust and effective in low-data scenarios and for the MSTP selection task. By defining this novel problem, providing a baseline pipeline and dataset, and offering initial insights into efficient model adaptation, we aim to contribute to future AI-driven navigation aids and data-informed level-design tools.",
    "pdf_url": "https://arxiv.org/pdf/2508.18539v2",
    "github_url": null,
    "published": "2025-08-25T22:20:37+00:00",
    "updated": "2025-08-29T13:05:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.18246v3",
    "title": "Flight-Ready Precise and Robust Carrier-Phase GNSS Navigation Software for Distributed Space Systems",
    "authors": [
      "Low",
      "Bell",
      "D'Amico"
    ],
    "summary": "This paper presents the full requirements analysis, design, development, and testing of high-precision navigation flight software for Distributed Space Systems (DSS) using Carrier Phase Differential GNSS (CDGNSS). Five main contributions are made. First, a survey of flown and upcoming DSS missions with stringent precision requirements is conducted, from which a thorough requirements analysis is distilled to guide development and testing. Second, a real-time navigation functional architecture is designed, and adopts a sparse and regularized Consider Kalman Filter with options for numerical stability in-flight. The filter rigorously accounts for uncertainties in process noise, measurement noise, and biases. It tracks float ambiguities with integer resolution where possible. The covariance correlation structure is preserved under all navigation modes, including contingencies and outages. Third, a lightweight, memoryless Fault Detection, Isolation, and Recovery (FDIR) module is developed to guard against anomalous measurements, providing statistical screening and ensuring robust navigation. Fourth, the software architecture is proposed for ease of integration, with strategies presented for modularity and computational efficiency tailored to constrained flight systems. Fifth, a comprehensive test campaign is conducted, mapped to a requirements verification matrix, spanning unit, interface, software-in-the-loop, and real-time hardware-in-the-loop tests, emphasizing gradual test fidelity for efficient fault isolation. Finally, flight-like results are demonstrated using the VISORS mission, due to the generalizability of the VISORS navigation operations, and the stringency which demands sub-centimeter relative position and sub-millimeter-per-second velocity accuracy. This architecture aims to serve as a reference for next-generation DSS missions adopting CDGNSS.",
    "pdf_url": "https://arxiv.org/pdf/2508.18246v3",
    "github_url": null,
    "published": "2025-08-25T17:37:02+00:00",
    "updated": "2025-12-04T09:03:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.17643v1",
    "title": "SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation",
    "authors": [
      "Vinod",
      "Ramesh",
      "N"
    ],
    "summary": "Event cameras offer microsecond latency, high dynamic range, and low power consumption, making them ideal for real-time robotic perception under challenging conditions such as motion blur, occlusion, and illumination changes. However, despite their advantages, synthetic event-based vision remains largely unexplored in mainstream robotics simulators. This lack of simulation setup hinders the evaluation of event-driven approaches for robotic manipulation and navigation tasks. This work presents an open-source, user-friendly v2e robotics operating system (ROS) package for Gazebo simulation that enables seamless event stream generation from RGB camera feeds. The package is used to investigate event-based robotic policies (ERP) for real-time navigation and manipulation. Two representative scenarios are evaluated: (1) object following with a mobile robot and (2) object detection and grasping with a robotic manipulator. Transformer-based ERPs are trained by behavior cloning and compared to RGB-based counterparts under various operating conditions. Experimental results show that event-guided policies consistently deliver competitive advantages. The results highlight the potential of event-driven perception to improve real-time robotic navigation and manipulation, providing a foundation for broader integration of event cameras into robotic policy learning. The GitHub repo for the dataset and code: https://eventbasedvision.github.io/SEBVS/",
    "pdf_url": "https://arxiv.org/pdf/2508.17643v1",
    "github_url": null,
    "published": "2025-08-25T04:14:04+00:00",
    "updated": "2025-08-25T04:14:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.16913v1",
    "title": "Chat-Driven Reconfiguration of Model Predictive Control",
    "authors": [
      "Miyaoka",
      "Inoue",
      "Maestre"
    ],
    "summary": "Traditional control personalization requires users to understand optimization parameters and provide repetitive numerical feedback, creating significant barriers for non-expert users. To deal with this issue, we propose ChatMPC, a model predictive control framework that enables users to personalize control systems and adapt to environmental changes through natural language interaction. The framework operates in two modes: personalization, where users iteratively adjust control behavior to their preferences, and co-development, where users provide real-time environmental information that complements sensor data. We establish convergence guarantees under different user behavior models, demonstrating exponential convergence for consistent feedback and finite-time convergence with logarithmic interaction complexity for tolerance-based users. We validate ChatMPC through experiments in robot navigation with personalized obstacle avoidance and semi-autonomous driving with conversational obstacle reporting. Both experiments achieve real-time performance and demonstrate effective adaptation to user preferences and environmental changes.",
    "pdf_url": "https://arxiv.org/pdf/2508.16913v1",
    "github_url": null,
    "published": "2025-08-23T06:03:01+00:00",
    "updated": "2025-08-23T06:03:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.16901v1",
    "title": "Relative Navigation and Dynamic Target Tracking for Autonomous Underwater Proximity Operations",
    "authors": [
      "Baxter",
      "Espinoza",
      "Espinoza"
    ],
    "summary": "Estimating a target's 6-DoF motion in underwater proximity operations is difficult because the chaser lacks target-side proprioception and the available relative observations are sparse, noisy, and often partial (e.g., Ultra-Short Baseline (USBL) positions). Without a motion prior, factor-graph maximum a posteriori estimation is underconstrained: consecutive target states are weakly linked and orientation can drift. We propose a generalized constant-twist motion prior defined on the tangent space of Lie groups that enforces temporally consistent trajectories across all degrees of freedom; in SE(3) it couples translation and rotation in the body frame. We present a ternary factor and derive its closed-form Jacobians based on standard Lie group operations, enabling drop-in use for trajectories on arbitrary Lie groups. We evaluate two deployment modes: (A) an SE(3)-only representation that regularizes orientation even when only position is measured, and (B) a mode with boundary factors that switches the target representation between SE(3) and 3D position while applying the same generalized constant-twist prior across representation changes. Validation on a real-world dynamic docking scenario dataset shows consistent ego-target trajectory estimation through USBL-only and optical relative measurement segments with an improved relative tracking accuracy compared to the noisy measurements to the target. Because the construction relies on standard Lie group primitives, it is portable across state manifolds and sensing modalities.",
    "pdf_url": "https://arxiv.org/pdf/2508.16901v1",
    "github_url": null,
    "published": "2025-08-23T05:19:50+00:00",
    "updated": "2025-08-23T05:19:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.16807v2",
    "title": "Autonomous UAV Flight Navigation in Confined Spaces: A Reinforcement Learning Approach",
    "authors": [
      "Tayar",
      "Oliveira",
      "Tommaselli"
    ],
    "summary": "Autonomous UAV inspection of confined industrial infrastructure, such as ventilation ducts, demands robust navigation policies where collisions are unacceptable. While Deep Reinforcement Learning (DRL) offers a powerful paradigm for developing such policies, it presents a critical trade-off between on-policy and off-policy algorithms. Off-policy methods promise high sample efficiency, a vital trait for minimizing costly and unsafe real-world fine-tuning. In contrast, on-policy methods often exhibit greater training stability, which is essential for reliable convergence in hazard-dense environments. This paper directly investigates this trade-off by comparing a leading on-policy algorithm, Proximal Policy Optimization (PPO), against an off-policy counterpart, Soft Actor-Critic (SAC), for precision flight in procedurally generated ducts within a high-fidelity simulator. Our results show that PPO consistently learned a stable, collision-free policy that completed the entire course. In contrast, SAC failed to find a complete solution, converging to a suboptimal policy that navigated only the initial segments before failure. This work provides evidence that for high-precision, safety-critical navigation tasks, the reliable convergence of a well-established on-policy method can be more decisive than the nominal sample efficiency of an off-policy algorithm.",
    "pdf_url": "https://arxiv.org/pdf/2508.16807v2",
    "github_url": null,
    "published": "2025-08-22T21:29:59+00:00",
    "updated": "2025-10-11T16:30:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.16574v1",
    "title": "Hierarchical Decision-Making for Autonomous Navigation: Integrating Deep Reinforcement Learning and Fuzzy Logic in Four-Wheel Independent Steering and Driving Systems",
    "authors": [
      "Wang",
      "Xu",
      "Xie"
    ],
    "summary": "This paper presents a hierarchical decision-making framework for autonomous navigation in four-wheel independent steering and driving (4WISD) systems. The proposed approach integrates deep reinforcement learning (DRL) for high-level navigation with fuzzy logic for low-level control to ensure both task performance and physical feasibility. The DRL agent generates global motion commands, while the fuzzy logic controller enforces kinematic constraints to prevent mechanical strain and wheel slippage. Simulation experiments demonstrate that the proposed framework outperforms traditional navigation methods, offering enhanced training efficiency and stability and mitigating erratic behaviors compared to purely DRL-based solutions. Real-world validations further confirm the framework's ability to navigate safely and effectively in dynamic industrial settings. Overall, this work provides a scalable and reliable solution for deploying 4WISD mobile robots in complex, real-world scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2508.16574v1",
    "github_url": null,
    "published": "2025-08-22T17:57:56+00:00",
    "updated": "2025-08-22T17:57:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.16515v2",
    "title": "Comparative Analysis of UAV Path Planning Algorithms for Efficient Navigation in Urban 3D Environments",
    "authors": [
      "Cheriet",
      "Badra",
      "Samira"
    ],
    "summary": "The most crucial challenges for UAVs are planning paths and avoiding obstacles in their way. In recent years, a wide variety of path-planning algorithms have been developed. These algorithms have successfully solved path-planning problems; however, they suffer from multiple challenges and limitations. To test the effectiveness and efficiency of three widely used algorithms, namely A*, RRT*, and Particle Swarm Optimization (PSO), this paper conducts extensive experiments in 3D urban city environments cluttered with obstacles. Three experiments were designed with two scenarios each to test the aforementioned algorithms. These experiments consider different city map sizes, different altitudes, and varying obstacle densities and sizes in the environment. According to the experimental results, the A* algorithm outperforms the others in both computation efficiency and path quality. PSO is especially suitable for tight turns and dense environments, and RRT* offers a balance and works well across all experiments due to its randomized approach to finding solutions.",
    "pdf_url": "https://arxiv.org/pdf/2508.16515v2",
    "github_url": null,
    "published": "2025-08-22T16:37:59+00:00",
    "updated": "2025-08-26T08:33:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.16698v1",
    "title": "A Computer Vision and Depth Sensor-Powered Smart Cane for Real-Time Obstacle Detection and Navigation Assistance for the Visually Impaired",
    "authors": [
      "Chandra",
      "Sharma",
      "Khilnani"
    ],
    "summary": "Visual impairment impacts more than 2.2 billion people worldwide, and it greatly restricts independent mobility and access. Conventional mobility aids - white canes and ultrasound-based intelligent canes - are inherently limited in the feedback they can offer and generally will not be able to differentiate among types of obstacles in dense or complex environments. Here, we introduce the IoT Cane, an internet of things assistive navigation tool that integrates real-time computer vision with a transformer-based RT-DETRv3-R50 model alongside depth sensing through the Intel RealSense camera. Our prototype records a mAP of 53.4% and an AP50 of 71.7% when tested on difficult datasets with low Intersection over Union (IoU) boundaries, outperforming similar ultrasound-based systems. Latency in end-to-end mode is around 150 ms per frame, accounting for preprocessing (1-3 ms), inference (50-70 ms), and post-processing (0.5-1.0 ms per object detected). Feedback is provided through haptic vibration motors and audio notifications driven by a LiPo battery, which controls power using a PowerBoost module. Future directions involve iOS integration to tap into more compute, hardware redesign to minimize cost, and mobile companion app support over Bluetooth. This effort offers a strong, extensible prototype toward large-scale vision-based assistive technology for the visually impaired.",
    "pdf_url": "https://arxiv.org/pdf/2508.16698v1",
    "github_url": null,
    "published": "2025-08-22T02:31:00+00:00",
    "updated": "2025-08-22T02:31:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.15354v1",
    "title": "Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey",
    "authors": [
      "Xiong",
      "Huang",
      "Yu"
    ],
    "summary": "Embodied navigation (EN) advances traditional navigation by enabling robots to perform complex egocentric tasks through sensing, social, and motion intelligence. In contrast to classic methodologies that rely on explicit localization and pre-defined maps, EN leverages egocentric perception and human-like interaction strategies. This survey introduces a comprehensive EN formulation structured into five stages: Transition, Observation, Fusion, Reward-policy construction, and Action (TOFRA). The TOFRA framework serves to synthesize the current state of the art, provide a critical review of relevant platforms and evaluation metrics, and identify critical open research challenges. A list of studies is available at https://github.com/Franky-X/Awesome-Embodied-Navigation.",
    "pdf_url": "https://arxiv.org/pdf/2508.15354v1",
    "github_url": "https://github.com/Franky-X/Awesome-Embodied-Navigation",
    "published": "2025-08-21T08:33:51+00:00",
    "updated": "2025-08-21T08:33:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.15232v1",
    "title": "AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation",
    "authors": [
      "Wu",
      "Zhang",
      "Chen"
    ],
    "summary": "Aerial Vision-and-Language Navigation (VLN) is an emerging task that enables Unmanned Aerial Vehicles (UAVs) to navigate outdoor environments using natural language instructions and visual cues. However, due to the extended trajectories and complex maneuverability of UAVs, achieving reliable UAV-VLN performance is challenging and often requires human intervention or overly detailed instructions. To harness the advantages of UAVs' high mobility, which could provide multi-grained perspectives, while maintaining a manageable motion space for learning, we introduce a novel task called Dual-Altitude UAV Collaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinct altitudes: a high-altitude UAV responsible for broad environmental reasoning, and a low-altitude UAV tasked with precise navigation. To support the training and evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising 13,838 collaborative high-low UAV demonstration trajectories, each paired with target-oriented language instructions. This dataset includes both unseen maps and an unseen object validation set to systematically evaluate the model's generalization capabilities across novel environments and unfamiliar targets. To consolidate their complementary strengths, we propose a dual-UAV collaborative VLN framework, AeroDuo, where the high-altitude UAV integrates a multimodal large language model (Pilot-LLM) for target reasoning, while the low-altitude UAV employs a lightweight multi-stage policy for navigation and target grounding. The two UAVs work collaboratively and only exchange minimal coordinate information to ensure efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2508.15232v1",
    "github_url": null,
    "published": "2025-08-21T04:43:35+00:00",
    "updated": "2025-08-21T04:43:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.14466v1",
    "title": "LookOut: Real-World Humanoid Egocentric Navigation",
    "authors": [
      "Pan",
      "Harley",
      "Liu"
    ],
    "summary": "The ability to predict collision-free future trajectories from egocentric observations is crucial in applications such as humanoid robotics, VR / AR, and assistive navigation. In this work, we introduce the challenging problem of predicting a sequence of future 6D head poses from an egocentric video. In particular, we predict both head translations and rotations to learn the active information-gathering behavior expressed through head-turning events. To solve this task, we propose a framework that reasons over temporally aggregated 3D latent features, which models the geometric and semantic constraints for both the static and dynamic parts of the environment. Motivated by the lack of training data in this space, we further contribute a data collection pipeline using the Project Aria glasses, and present a dataset collected through this approach. Our dataset, dubbed Aria Navigation Dataset (AND), consists of 4 hours of recording of users navigating in real-world scenarios. It includes diverse situations and navigation behaviors, providing a valuable resource for learning real-world egocentric navigation policies. Extensive experiments show that our model learns human-like navigation behaviors such as waiting / slowing down, rerouting, and looking around for traffic while generalizing to unseen environments. Check out our project webpage at https://sites.google.com/stanford.edu/lookout.",
    "pdf_url": "https://arxiv.org/pdf/2508.14466v1",
    "github_url": null,
    "published": "2025-08-20T06:43:36+00:00",
    "updated": "2025-08-20T06:43:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.16654v3",
    "title": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning",
    "authors": [
      "Liu",
      "Zhou",
      "Zhang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural language instructions and navigate complex environments. Current approaches often adopt a \"black-box\" paradigm, where a single Large Language Model (LLM) makes end-to-end decisions. However, it is plagued by critical vulnerabilities, including poor spatial reasoning, weak cross-modal grounding, and memory overload in long-horizon tasks. To systematically address these issues, we propose Memory Spatial Navigation(MSNav), a framework that fuses three modules into a synergistic architecture, which transforms fragile inference into a robust, integrated intelligence. MSNav integrates three modules: Memory Module, a dynamic map memory module that tackles memory overload through selective node pruning, enhancing long-range exploration; Spatial Module, a module for spatial reasoning and object relationship inference that improves endpoint recognition; and Decision Module, a module using LLM-based path planning to execute robust actions. Powering Spatial Module, we also introduce an Instruction-Object-Space (I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp), which outperforms leading commercial LLMs in object list extraction, achieving higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art performance with significant improvements in Success Rate (SR) and Success weighted by Path Length (SPL).",
    "pdf_url": "https://arxiv.org/pdf/2508.16654v3",
    "github_url": null,
    "published": "2025-08-20T05:41:22+00:00",
    "updated": "2025-09-10T11:47:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.13785v2",
    "title": "Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot",
    "authors": [
      "Liu",
      "Mihankhah",
      "Wallace"
    ],
    "summary": "In open-pit mining, holes are drilled into the surface of the excavation site and detonated with explosives to facilitate digging. These blast holes need to be inspected internally to assess subsurface material types and drill quality, in order to significantly reduce downstream material handling costs. Manual hole inspection is slow and expensive, limited in its ability to capture the geometric and geological characteristics of holes. This has been the motivation for the development of our autonomous mine-site inspection robot - \"DIPPeR\". In this paper, the automation aspect of the project is explained. We present a robust perception and navigation framework that provides streamlined blasthole seeking, tracking and accurate down-hole sensor positioning. To address challenges in the surface mining environment, where GPS and odometry data are noisy without RTK correction, we adopt a proximity-based adaptive navigation approach, enabling the vehicle to dynamically adjust its operations based on detected target availability and localisation accuracy. For perception, we process LiDAR data to extract the cone-shaped volume of drill-waste above ground, then project the 3D cone points into a virtual depth image to form accurate 2D segmentation of hole regions. To ensure continuous target-tracking as the robot approaches the goal, our system automatically adjusts projection parameters to preserve consistent hole image appearance. At the vicinity of the hole, we apply least squares circle fitting with non-maximum candidate suppression to achieve accurate hole detection and collision-free down-hole sensor placement. We demonstrate the effectiveness of our navigation and perception system in both high-fidelity simulation environments and on-site field trials. A demonstration video is available at https://www.youtube.com/watch?v=fRNbcBcaSqE.",
    "pdf_url": "https://arxiv.org/pdf/2508.13785v2",
    "github_url": null,
    "published": "2025-08-19T12:40:20+00:00",
    "updated": "2025-09-29T02:09:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.13459v3",
    "title": "Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms",
    "authors": [
      "Chandra",
      "Singh",
      "Luo"
    ],
    "summary": "The ``Last Mile Challenge'' has long been considered an important, yet unsolved, challenge for autonomous vehicles, public service robots, and delivery robots. A central issue in this challenge is the ability of robots to navigate constrained and cluttered environments that have high agency (e.g., doorways, hallways, corridor intersections), often while competing for space with other robots and humans. We refer to these environments as ``Social Mini-Games'' (SMGs). Traditional navigation approaches designed for MRN do not perform well in SMGs, which has led to focused research on dedicated SMG solvers. However, publications on SMG navigation research make different assumptions (on centralized versus decentralized, observability, communication, cooperation, etc.), and have different objective functions (safety versus liveness). These assumptions and objectives are sometimes implicitly assumed or described informally. This makes it difficult to establish appropriate baselines for comparison in research papers, as well as making it difficult for practitioners to find the papers relevant to their concrete application. Such ad-hoc representation of the field also presents a barrier to new researchers wanting to start research in this area. SMG navigation research requires its own taxonomy, definitions, and evaluation protocols to guide effective research moving forward. This survey is the first to catalog SMG solvers using a well-defined and unified taxonomy and to classify existing methods accordingly. It also discusses the essential properties of SMG solvers, defines what SMGs are and how they appear in practice, outlines how to evaluate SMG solvers, and highlights the differences between SMG solvers and general navigation systems. The survey concludes with an overview of future directions and open challenges in the field. Our project is open-sourced at https://socialminigames.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2508.13459v3",
    "github_url": null,
    "published": "2025-08-19T02:33:15+00:00",
    "updated": "2025-09-11T04:17:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.13421v1",
    "title": "Virtuous Machines: Towards Artificial General Science",
    "authors": [
      "Wehr",
      "Rideaux",
      "Fox"
    ],
    "summary": "Artificial intelligence systems are transforming scientific discovery by accelerating specific research tasks, from protein structure prediction to materials design, yet remain confined to narrow domains requiring substantial human oversight. The exponential growth of scientific literature and increasing domain specialisation constrain researchers' capacity to synthesise knowledge across disciplines and develop unifying theories, motivating exploration of more general-purpose AI systems for science. Here we show that a domain-agnostic, agentic AI system can independently navigate the scientific workflow - from hypothesis generation through data collection to manuscript preparation. The system autonomously designed and executed three psychological studies on visual working memory, mental rotation, and imagery vividness, executed one new online data collection with 288 participants, developed analysis pipelines through 8-hour+ continuous coding sessions, and produced completed manuscripts. The results demonstrate the capability of AI scientific discovery pipelines to conduct non-trivial research with theoretical reasoning and methodological rigour comparable to experienced researchers, though with limitations in conceptual nuance and theoretical interpretation. This is a step toward embodied AI that can test hypotheses through real-world experiments, accelerating discovery by autonomously exploring regions of scientific space that human cognitive and resource constraints might otherwise leave unexplored. It raises important questions about the nature of scientific understanding and the attribution of scientific credit.",
    "pdf_url": "https://arxiv.org/pdf/2508.13421v1",
    "github_url": null,
    "published": "2025-08-19T00:35:56+00:00",
    "updated": "2025-08-19T00:35:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.13407v2",
    "title": "Accelerating Signal-Temporal-Logic-Based Task and Motion Planning of Bipedal Navigation using Benders Decomposition",
    "authors": [
      "Ren",
      "Lin",
      "Mineyev"
    ],
    "summary": "Task and motion planning under Signal Temporal Logic constraints is known to be NP-hard. A common class of approaches formulates these hybrid problems, which involve discrete task scheduling and continuous motion planning, as mixed-integer programs (MIP). However, in applications for bipedal locomotion, introduction of non-convex constraints such as kinematic reachability and footstep rotation exacerbates the computational complexity of MIPs. In this work, we present a method based on Benders Decomposition to address scenarios where solving the entire monolithic optimization problem is prohibitively intractable. Benders Decomposition proposes an iterative cutting-plane technique that partitions the problem into a master problem to prototype a plan that meets the task specification, and a series of subproblems for kinematics and dynamics feasibility checks. Our experiments demonstrate that this method achieves faster planning compared to alternative algorithms for solving the resulting optimization program with nonlinear constraints.",
    "pdf_url": "https://arxiv.org/pdf/2508.13407v2",
    "github_url": null,
    "published": "2025-08-18T23:58:22+00:00",
    "updated": "2025-08-20T14:21:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.14105v1",
    "title": "Efficient Environment Design for Multi-Robot Navigation via Continuous Control",
    "authors": [
      "Choton",
      "Woods",
      "Hsu"
    ],
    "summary": "Multi-robot navigation and path planning in continuous state and action spaces with uncertain environments remains an open challenge. Deep Reinforcement Learning (RL) is one of the most popular paradigms for solving this task, but its real-world application has been limited due to sample inefficiency and long training periods. Moreover, the existing works using RL for multi-robot navigation lack formal guarantees while designing the environment. In this paper, we introduce an efficient and highly customizable environment for continuous-control multi-robot navigation, where the robots must visit a set of regions of interest (ROIs) by following the shortest paths. The task is formally modeled as a Markov Decision Process (MDP). We describe the multi-robot navigation task as an optimization problem and relate it to finding an optimal policy for the MDP. We crafted several variations of the environment and measured the performance using both gradient and non-gradient based RL methods: A2C, PPO, TRPO, TQC, CrossQ and ARS. To show real-world applicability, we deployed our environment to a 3-D agricultural field with uncertainties using the CoppeliaSim robot simulator and measured the robustness by running inference on the learned models. We believe our work will guide the researchers on how to develop MDP-based environments that are applicable to real-world systems and solve them using the existing state-of-the-art RL methods with limited resources and within reasonable time periods.",
    "pdf_url": "https://arxiv.org/pdf/2508.14105v1",
    "github_url": null,
    "published": "2025-08-17T16:18:07+00:00",
    "updated": "2025-08-17T16:18:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.12394v2",
    "title": "SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning",
    "authors": [
      "Yan",
      "Huang",
      "He"
    ],
    "summary": "Image-goal navigation (ImageNav) tasks a robot with autonomously exploring an unknown environment and reaching a location that visually matches a given target image. While prior works primarily study ImageNav for ground robots, enabling this capability for autonomous drones is substantially more challenging due to their need for high-frequency feedback control and global localization for stable flight. In this paper, we propose a novel sim-to-real framework that leverages reinforcement learning (RL) to achieve ImageNav for drones. To enhance visual representation ability, our approach trains the vision backbone with auxiliary tasks, including image perturbations and future transition prediction, which results in more effective policy training. The proposed algorithm enables end-to-end ImageNav with direct velocity control, eliminating the need for external localization. Furthermore, we integrate a depth-based safety module for real-time obstacle avoidance, allowing the drone to safely navigate in cluttered environments. Unlike most existing drone navigation methods that focus solely on reference tracking or obstacle avoidance, our framework supports comprehensive navigation behaviors, including autonomous exploration, obstacle avoidance, and image-goal seeking, without requiring explicit global mapping. Code and model checkpoints are available at https://github.com/Zichen-Yan/SIGN.",
    "pdf_url": "https://arxiv.org/pdf/2508.12394v2",
    "github_url": "https://github.com/Zichen-Yan/SIGN",
    "published": "2025-08-17T15:14:32+00:00",
    "updated": "2025-12-06T04:56:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.12087v2",
    "title": "MAPF-World: Action World Model for Multi-Agent Path Finding",
    "authors": [
      "Yang",
      "Shen",
      "Li"
    ],
    "summary": "Multi-agent path finding (MAPF) is the problem of planning conflict-free paths from the designated start locations to goal positions for multiple agents. It underlies a variety of real-world tasks, including multi-robot coordination, robot-assisted logistics, and social navigation. Recent decentralized learnable solvers have shown great promise for large-scale MAPF, especially when leveraging foundation models and large datasets. However, these agents are reactive policy models and exhibit limited modeling of environmental temporal dynamics and inter-agent dependencies, resulting in performance degradation in complex, long-term planning scenarios. To address these limitations, we propose MAPF-World, an autoregressive action world model for MAPF that unifies situation understanding and action generation, guiding decisions beyond immediate local observations. It improves situational awareness by explicitly modeling environmental dynamics, including spatial features and temporal dependencies, through future state and actions prediction. By incorporating these predicted futures, MAPF-World enables more informed, coordinated, and far-sighted decision-making, especially in complex multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an automatic map generator grounded in real-world scenarios, capturing practical map layouts for training and evaluating MAPF solvers. Extensive experiments demonstrate that MAPF-World outperforms state-of-the-art learnable solvers, showcasing superior zero-shot generalization to out-of-distribution cases. Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced data.",
    "pdf_url": "https://arxiv.org/pdf/2508.12087v2",
    "github_url": null,
    "published": "2025-08-16T15:50:26+00:00",
    "updated": "2025-09-07T04:05:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.11485v2",
    "title": "i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping",
    "authors": [
      "Tang",
      "Zhang",
      "Wang"
    ],
    "summary": "Accurate and reliable navigation is crucial for autonomous unmanned ground vehicle (UGV). However, current UGV datasets fall short in meeting the demands for advancing navigation and mapping techniques due to limitations in sensor configuration, time synchronization, ground truth, and scenario diversity. To address these challenges, we present i2Nav-Robot, a large-scale dataset designed for multi-sensor fusion navigation and mapping in indoor-outdoor environments. We integrate multi-modal sensors, including the newest front-view and 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras, odometer, global navigation satellite system (GNSS) receiver, and inertial measurement units (IMU) on an omnidirectional wheeled robot. Accurate timestamps are obtained through both online hardware synchronization and offline calibration for all sensors. The dataset includes ten larger-scale sequences covering diverse UGV operating scenarios, such as outdoor streets, and indoor parking lots, with a total length of about 17060 meters. High-frequency ground truth, with centimeter-level accuracy for position, is derived from post-processing integrated navigation methods using a navigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more than ten open-sourced multi-sensor fusion systems, and it has proven to have superior data quality.",
    "pdf_url": "https://arxiv.org/pdf/2508.11485v2",
    "github_url": null,
    "published": "2025-08-15T13:59:08+00:00",
    "updated": "2025-08-27T12:00:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.11479v1",
    "title": "OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation",
    "authors": [
      "Zemskova",
      "Staroverov",
      "Yudin"
    ],
    "summary": "Open-vocabulary Object Goal Navigation requires an embodied agent to reach objects described by free-form language, including categories never seen during training. Existing end-to-end policies overfit small simulator datasets, achieving high success on training scenes but failing to generalize and exhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a lightweight transformer policy that tackles these issues with two synergistic components. The first component is the semantic branch, which includes an encoder for the target binary mask and an auxiliary segmentation loss function, grounding the textual goal and providing precise spatial cues. The second component consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample scheduler that continuously balances imitation and reinforcement signals according to the policy entropy, eliminating brittle manual phase switches. These additions cut the sample complexity of training by 33%, and reduce collision count in two times while keeping inference cost low (130M parameters, RGB-only input). On HM3D-OVON, our model matches the performance on unseen categories to that on seen ones and establishes state-of-the-art results (40.1% SR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language models. Code is available at https://github.com/CognitiveAISystems/OVSegDT.",
    "pdf_url": "https://arxiv.org/pdf/2508.11479v1",
    "github_url": "https://github.com/CognitiveAISystems/OVSegDT",
    "published": "2025-08-15T13:48:15+00:00",
    "updated": "2025-08-15T13:48:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.11446v1",
    "title": "Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation",
    "authors": [
      "Airinei",
      "Burceanu",
      "Leordeanu"
    ],
    "summary": "Indoor navigation is a difficult task, as it generally comes with poor GPS access, forcing solutions to rely on other sources of information. While significant progress continues to be made in this area, deployment to production applications is still lacking, given the complexity and additional requirements of current solutions. Here, we introduce an efficient, real-time and easily deployable deep learning approach, based on visual input only, that can predict the direction towards a target from images captured by a mobile device. Our technical approach, based on a novel graph-based path generation method, combined with explainable data augmentation and curriculum learning, includes contributions that make the process of data collection, annotation and training, as automatic as possible, efficient and robust. On the practical side, we introduce a novel largescale dataset, with video footage inside a relatively large shopping mall, in which each frame is annotated with the correct next direction towards different specific target destinations. Different from current methods, ours relies solely on vision, avoiding the need of special sensors, additional markers placed along the path, knowledge of the scene map or internet access. We also created an easy to use application for Android, which we plan to make publicly available. We make all our data and code available along with visual demos on our project site",
    "pdf_url": "https://arxiv.org/pdf/2508.11446v1",
    "github_url": null,
    "published": "2025-08-15T12:54:13+00:00",
    "updated": "2025-08-15T12:54:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.11304v1",
    "title": "GulliVR: A Walking-Oriented Technique for Navigation in Virtual Reality Games Based on Virtual Body Resizing",
    "authors": [
      "Krekhov",
      "Cmentowski",
      "Emmerich"
    ],
    "summary": "Virtual reality games are often centered around our feeling of \"being there\". That presence can be significantly enhanced by supporting physical walking. Although modern virtual reality systems enable room-scale motions, the size of our living rooms is not enough to explore vast virtual environments. Developers bypass that limitation by adding virtual navigation such as teleportation. Although such techniques are intended (or designed) to extend but not replace natural walking, what we often observe are nonmoving players beaming to a location that is one real step ahead. Our navigation metaphor emphasizes physical walking by promoting players into giants on demand to cover large distances. In contrast to flying, our technique proportionally increases the modeled eye distance, preventing cybersickness and creating the feeling of being in a miniature world. Our evaluations underpin a significantly increased presence and walking distance compared to the teleportation approach. Finally, we derive a set of game design implications related to the integration of our technique.",
    "pdf_url": "https://arxiv.org/pdf/2508.11304v1",
    "github_url": null,
    "published": "2025-08-15T08:19:43+00:00",
    "updated": "2025-08-15T08:19:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.11144v1",
    "title": "CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets",
    "authors": [
      "Jain",
      "Rothenhäusler",
      "Bansak"
    ],
    "summary": "Machine learning (ML) tasks often utilize large-scale data that is drawn from several distinct sources, such as different locations, treatment arms, or groups. In such settings, practitioners often desire predictions that not only exhibit good overall accuracy, but also remain reliable within each source and preserve the differences that matter across sources. For instance, several asylum and refugee resettlement programs now use ML-based employment predictions to guide where newly arriving families are placed within a host country, which requires generating informative and differentiated predictions for many and often small source locations. However, this task is made challenging by several common characteristics of the data in these settings: the presence of numerous distinct data sources, distributional shifts between them, and substantial variation in sample sizes across sources. This paper introduces Clustered Transfer Residual Learning (CTRL), a meta-learning method that combines the strengths of cross-domain residual learning and adaptive pooling/clustering in order to simultaneously improve overall accuracy and preserve source-level heterogeneity. We provide theoretical results that clarify how our objective navigates the trade-off between data quantity and data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5 large-scale datasets. This includes a dataset from the national asylum program in Switzerland, where the algorithmic geographic assignment of asylum seekers is currently being piloted. CTRL consistently outperforms the benchmarks across several key metrics and when using a range of different base learners.",
    "pdf_url": "https://arxiv.org/pdf/2508.11144v1",
    "github_url": null,
    "published": "2025-08-15T01:27:17+00:00",
    "updated": "2025-08-15T01:27:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.10999v1",
    "title": "Robust Online Calibration for UWB-Aided Visual-Inertial Navigation with Bias Correction",
    "authors": [
      "Zhou",
      "Xu",
      "Xia"
    ],
    "summary": "This paper presents a novel robust online calibration framework for Ultra-Wideband (UWB) anchors in UWB-aided Visual-Inertial Navigation Systems (VINS). Accurate anchor positioning, a process known as calibration, is crucial for integrating UWB ranging measurements into state estimation. While several prior works have demonstrated satisfactory results by using robot-aided systems to autonomously calibrate UWB systems, there are still some limitations: 1) these approaches assume accurate robot localization during the initialization step, ignoring localization errors that can compromise calibration robustness, and 2) the calibration results are highly sensitive to the initial guess of the UWB anchors' positions, reducing the practical applicability of these methods in real-world scenarios. Our approach addresses these challenges by explicitly incorporating the impact of robot localization uncertainties into the calibration process, ensuring robust initialization. To further enhance the robustness of the calibration results against initialization errors, we propose a tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method, making the system suitable for practical applications. Simulations and real-world experiments validate the improved accuracy and robustness of our approach.",
    "pdf_url": "https://arxiv.org/pdf/2508.10999v1",
    "github_url": null,
    "published": "2025-08-14T18:06:05+00:00",
    "updated": "2025-08-14T18:06:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.10554v2",
    "title": "AR Surgical Navigation with Surface Tracing: Comparing In-Situ Visualization with Tool-Tracking Guidance for Neurosurgical Applications",
    "authors": [
      "Fischer",
      "Potts",
      "Urreola"
    ],
    "summary": "Augmented Reality (AR) surgical navigation systems are emerging as the next generation of intraoperative surgical guidance, promising to overcome limitations of traditional navigation systems. However, known issues with AR depth perception due to vergence-accommodation conflict and occlusion handling limitations of the currently commercially available display technology present acute challenges in surgical settings where precision is paramount. This study presents a novel methodology for utilizing AR guidance to register anatomical targets and provide real-time instrument navigation using placement of simulated external ventricular drain catheters on a phantom model as the clinical scenario. The system registers target positions to the patient through a novel surface tracing method and uses real-time infrared tool tracking to aid in catheter placement, relying only on the onboard sensors of the Microsoft HoloLens 2. A group of intended users performed the procedure of simulated insertions under two AR guidance conditions: static in-situ visualization, where planned trajectories are overlaid directly onto the patient anatomy, and real-time tool-tracking guidance, where live feedback of the catheter's pose is provided relative to the plan. Following the insertion tests, computed tomography scans of the phantom models were acquired, allowing for evaluation of insertion accuracy, target deviation, angular error, and depth precision. System Usability Scale surveys assessed user experience and cognitive workload. Tool-tracking guidance improved performance metrics across all accuracy measures and was preferred by users in subjective evaluations. A free copy of this paper and all supplemental materials are available at https://bit.ly/45l89Hq.",
    "pdf_url": "https://arxiv.org/pdf/2508.10554v2",
    "github_url": null,
    "published": "2025-08-14T11:46:30+00:00",
    "updated": "2025-08-17T16:36:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.10416v1",
    "title": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model",
    "authors": [
      "Yu",
      "Long",
      "Yang"
    ],
    "summary": "Existing vision-and-language navigation models often deviate from the correct trajectory when executing instructions. However, these models lack effective error correction capability, hindering their recovery from errors. To address this challenge, we propose Self-correction Flywheel, a novel post-training paradigm. Instead of considering the model's error trajectories on the training set as a drawback, our paradigm emphasizes their significance as a valuable data source. We have developed a method to identify deviations in these error trajectories and devised innovative techniques to automatically generate self-correction data for perception and action. These self-correction data serve as fuel to power the model's continued training. The brilliance of our paradigm is revealed when we re-evaluate the model on the training set, uncovering new error trajectories. At this time, the self-correction flywheel begins to spin. Through multiple flywheel iterations, we progressively enhance our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2% and 16.4%. Real robot tests in various indoor and outdoor environments demonstrate \\method's superior capability of error correction, dynamic obstacle avoidance, and long instruction following.",
    "pdf_url": "https://arxiv.org/pdf/2508.10416v1",
    "github_url": null,
    "published": "2025-08-14T07:39:26+00:00",
    "updated": "2025-08-14T07:39:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.09547v1",
    "title": "GoViG: Goal-Conditioned Visual Navigation Instruction Generation",
    "authors": [
      "Wu",
      "Dong",
      "Cheng"
    ],
    "summary": "We introduce Goal-Conditioned Visual Navigation Instruction Generation (GoViG), a new task that aims to autonomously generate precise and contextually coherent navigation instructions solely from egocentric visual observations of initial and goal states. Unlike conventional approaches that rely on structured inputs such as semantic annotations or environmental maps, GoViG exclusively leverages raw egocentric visual data, substantially improving its adaptability to unseen and unstructured environments. Our method addresses this task by decomposing it into two interconnected subtasks: (1) visual forecasting, which predicts intermediate visual states bridging the initial and goal views; and (2) instruction generation, which synthesizes linguistically coherent instructions grounded in both observed and anticipated visuals. These subtasks are integrated within an autoregressive multimodal large language model trained with tailored objectives to ensure spatial accuracy and linguistic clarity. Furthermore, we introduce two complementary multimodal reasoning strategies, one-pass and interleaved reasoning, to mimic incremental human cognitive processes during navigation. To evaluate our method, we propose the R2R-Goal dataset, combining diverse synthetic and real-world trajectories. Empirical results demonstrate significant improvements over state-of-the-art methods, achieving superior BLEU-4 and CIDEr scores along with robust cross-domain generalization.",
    "pdf_url": "https://arxiv.org/pdf/2508.09547v1",
    "github_url": null,
    "published": "2025-08-13T07:05:17+00:00",
    "updated": "2025-08-13T07:05:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.09444v1",
    "title": "DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation",
    "authors": [
      "Shi",
      "Deng",
      "Li"
    ],
    "summary": "Vision-Language Navigation in Continuous Environments (VLN-CE) requires agents to follow natural language instructions through free-form 3D spaces. Existing VLN-CE approaches typically use a two-stage waypoint planning framework, where a high-level waypoint predictor generates the navigable waypoints, and then a navigation planner suggests the intermediate goals in the high-level action space. However, this two-stage decomposition framework suffers from: (1) global sub-optimization due to the proxy objective in each stage, and (2) a performance bottleneck caused by the strong reliance on the quality of the first-stage predicted waypoints. To address these limitations, we propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE policy that unifies the traditional two stages, i.e. waypoint generation and planning, into a single diffusion policy. Notably, DifNav employs a conditional diffusion policy to directly model multi-modal action distributions over future actions in continuous navigation space, eliminating the need for a waypoint predictor while enabling the agent to capture multiple possible instruction-following behaviors. To address the issues of compounding error in imitation learning and enhance spatial reasoning in long-horizon navigation tasks, we employ DAgger for online policy training and expert trajectory augmentation, and use the aggregated data to further fine-tune the policy. This approach significantly improves the policy's robustness and its ability to recover from error states. Extensive experiments on benchmark datasets demonstrate that, even without a waypoint predictor, the proposed method substantially outperforms previous state-of-the-art two-stage waypoint-based models in terms of navigation performance. Our code is available at: https://github.com/Tokishx/DifNav.",
    "pdf_url": "https://arxiv.org/pdf/2508.09444v1",
    "github_url": "https://github.com/Tokishx/DifNav",
    "published": "2025-08-13T02:51:43+00:00",
    "updated": "2025-08-13T02:51:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.09423v2",
    "title": "Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation",
    "authors": [
      "Li",
      "Lu",
      "Zhou"
    ],
    "summary": "The Object Goal Navigation (ObjectNav) task challenges agents to locate a specified object in an unseen environment by imagining unobserved regions of the scene. Prior approaches rely on deterministic and discriminative models to complete semantic maps, overlooking the inherent uncertainty in indoor layouts and limiting their ability to generalize to unseen environments. In this work, we propose GOAL, a generative flow-based framework that models the semantic distribution of indoor environments by bridging observed regions with LLM-enriched full-scene semantic maps. During training, spatial priors inferred from large language models (LLMs) are encoded as two-dimensional Gaussian fields and injected into target maps, distilling rich contextual knowledge into the flow model and enabling more generalizable completions. Extensive experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D and Gibson, and shows strong generalization in transfer settings to HM3D. Codes and pretrained models are available at https://github.com/Badi-Li/GOAL.",
    "pdf_url": "https://arxiv.org/pdf/2508.09423v2",
    "github_url": "https://github.com/Badi-Li/GOAL",
    "published": "2025-08-13T01:57:48+00:00",
    "updated": "2025-10-21T03:50:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.09262v1",
    "title": "Harnessing Input-Adaptive Inference for Efficient VLN",
    "authors": [
      "Kang",
      "Perincherry",
      "Coalson"
    ],
    "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of history-aware multi-modal transformer models. Given a language instruction, these models process observation and navigation history to predict the most appropriate action for an agent. While they have significantly improved performance, the scale of these models can be a bottleneck in practical settings with limited computational resources. In this work, we propose a novel input-adaptive navigation method to enhance VLN model efficiency. We first show that existing input-adaptive mechanisms fail to reduce computations without substantial performance degradation. To address this, we introduce three adaptive algorithms, each deployed at a different level: (1) To improve spatial efficiency, we selectively process panoramic views at each observation of an agent. (2) To improve intra-model efficiency, we propose importance-based adaptive thresholding for the early-exit methods. (3) To improve temporal efficiency, we implement a caching mechanism that prevents reprocessing of views previously seen by the agent. In evaluations on seven VLN benchmarks, we demonstrate over a 2$\\times$ reduction in computation across three off-the-shelf agents in both standard and continuous environments. Our code is publicly available at https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.",
    "pdf_url": "https://arxiv.org/pdf/2508.09262v1",
    "github_url": "https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation",
    "published": "2025-08-12T18:05:33+00:00",
    "updated": "2025-08-12T18:05:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.08831v1",
    "title": "DiffPhysCam: Differentiable Physics-Based Camera Simulation for Inverse Rendering and Embodied AI",
    "authors": [
      "Chen",
      "Batagoda",
      "Negrut"
    ],
    "summary": "We introduce DiffPhysCam, a differentiable camera simulator designed to support robotics and embodied AI applications by enabling gradient-based optimization in visual perception pipelines. Generating synthetic images that closely mimic those from real cameras is essential for training visual models and enabling end-to-end visuomotor learning. Moreover, differentiable rendering allows inverse reconstruction of real-world scenes as digital twins, facilitating simulation-based robotics training. However, existing virtual cameras offer limited control over intrinsic settings, poorly capture optical artifacts, and lack tunable calibration parameters -- hindering sim-to-real transfer. DiffPhysCam addresses these limitations through a multi-stage pipeline that provides fine-grained control over camera settings, models key optical effects such as defocus blur, and supports calibration with real-world data. It enables both forward rendering for image synthesis and inverse rendering for 3D scene reconstruction, including mesh and material texture optimization. We show that DiffPhysCam enhances robotic perception performance in synthetic image tasks. As an illustrative example, we create a digital twin of a real-world scene using inverse rendering, simulate it in a multi-physics environment, and demonstrate navigation of an autonomous ground vehicle using images generated by DiffPhysCam.",
    "pdf_url": "https://arxiv.org/pdf/2508.08831v1",
    "github_url": null,
    "published": "2025-08-12T10:38:20+00:00",
    "updated": "2025-08-12T10:38:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.08120v1",
    "title": "Vision-Based Localization and LLM-based Navigation for Indoor Environments",
    "authors": [
      "Rahimi",
      "Haque",
      "Dasgupta"
    ],
    "summary": "Indoor navigation remains a complex challenge due to the absence of reliable GPS signals and the architectural intricacies of large enclosed environments. This study presents an indoor localization and navigation approach that integrates vision-based localization with large language model (LLM)-based navigation. The localization system utilizes a ResNet-50 convolutional neural network fine-tuned through a two-stage process to identify the user's position using smartphone camera input. To complement localization, the navigation module employs an LLM, guided by a carefully crafted system prompt, to interpret preprocessed floor plan images and generate step-by-step directions. Experimental evaluation was conducted in a realistic office corridor with repetitive features and limited visibility to test localization robustness. The model achieved high confidence and an accuracy of 96% across all tested waypoints, even under constrained viewing conditions and short-duration queries. Navigation tests using ChatGPT on real building floor maps yielded an average instruction accuracy of 75%, with observed limitations in zero-shot reasoning and inference time. This research demonstrates the potential for scalable, infrastructure-free indoor navigation using off-the-shelf cameras and publicly available floor plans, particularly in resource-constrained settings like hospitals, airports, and educational institutions.",
    "pdf_url": "https://arxiv.org/pdf/2508.08120v1",
    "github_url": null,
    "published": "2025-08-11T15:59:09+00:00",
    "updated": "2025-08-11T15:59:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.08108v1",
    "title": "Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain",
    "authors": [
      "Zhang",
      "Wang",
      "Lu"
    ],
    "summary": "It is a challenging task for ground robots to autonomously navigate in harsh environments due to the presence of non-trivial obstacles and uneven terrain. This requires trajectory planning that balances safety and efficiency. The primary challenge is to generate a feasible trajectory that prevents robot from tip-over while ensuring effective navigation. In this paper, we propose a capsizing-aware trajectory planner (CAP) to achieve trajectory planning on the uneven terrain. The tip-over stability of the robot on rough terrain is analyzed. Based on the tip-over stability, we define the traversable orientation, which indicates the safe range of robot orientations. This orientation is then incorporated into a capsizing-safety constraint for trajectory optimization. We employ a graph-based solver to compute a robust and feasible trajectory while adhering to the capsizing-safety constraint. Extensive simulation and real-world experiments validate the effectiveness and robustness of the proposed method. The results demonstrate that CAP outperforms existing state-of-the-art approaches, providing enhanced navigation performance on uneven terrains.",
    "pdf_url": "https://arxiv.org/pdf/2508.08108v1",
    "github_url": null,
    "published": "2025-08-11T15:47:24+00:00",
    "updated": "2025-08-11T15:47:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.08100v2",
    "title": "Grid2Guide: A* Enabled Small Language Model for Indoor Navigation",
    "authors": [
      "Haque",
      "Dasgupta",
      "Rahman"
    ],
    "summary": "Reliable indoor navigation remains a significant challenge in complex environments, particularly where external positioning signals and dedicated infrastructures are unavailable. This research presents Grid2Guide, a hybrid navigation framework that combines the A* search algorithm with a Small Language Model (SLM) to generate clear, human-readable route instructions. The framework first conducts a binary occupancy matrix from a given indoor map. Using this matrix, the A* algorithm computes the optimal path between origin and destination, producing concise textual navigation steps. These steps are then transformed into natural language instructions by the SLM, enhancing interpretability for end users. Experimental evaluations across various indoor scenarios demonstrate the method's effectiveness in producing accurate and timely navigation guidance. The results validate the proposed approach as a lightweight, infrastructure-free solution for real-time indoor navigation support.",
    "pdf_url": "https://arxiv.org/pdf/2508.08100v2",
    "github_url": null,
    "published": "2025-08-11T15:39:27+00:00",
    "updated": "2025-08-29T20:09:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.07885v1",
    "title": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning",
    "authors": [
      "Ahmmad",
      "Aditto",
      "Hossain"
    ],
    "summary": "This paper introduces an advanced AI-driven perception system for autonomous quadcopter navigation in GPS-denied indoor environments. The proposed framework leverages cloud computing to offload computationally intensive tasks and incorporates a custom-designed printed circuit board (PCB) for efficient sensor data acquisition, enabling robust navigation in confined spaces. The system integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for context-aware decision-making. A virtual safety envelope, enforced by calibrated sensor offsets, ensures collision avoidance, while a multithreaded architecture achieves low-latency processing. Enhanced spatial awareness is facilitated by 3D bounding box estimation with Kalman filtering. Experimental results in an indoor testbed demonstrate strong performance, with object detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42 trials over approximately 11 minutes, and end-to-end system latency below 1 second. This cloud-supported, high-intelligence framework serves as an auxiliary perception and navigation system, complementing state-of-the-art drone autonomy for GPS-denied confined spaces.",
    "pdf_url": "https://arxiv.org/pdf/2508.07885v1",
    "github_url": null,
    "published": "2025-08-11T12:00:03+00:00",
    "updated": "2025-08-11T12:00:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.07814v1",
    "title": "SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing",
    "authors": [
      "Zafar",
      "Khan",
      "Batool"
    ],
    "summary": "With the growing demand for efficient logistics, unmanned aerial vehicles (UAVs) are increasingly being paired with automated guided vehicles (AGVs). While UAVs offer the ability to navigate through dense environments and varying altitudes, they are limited by battery life, payload capacity, and flight duration, necessitating coordinated ground support.   Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by enabling semantic collaboration between UAVs and ground robots through impedance control. The system leverages the Vision Language Model (VLM) and the Retrieval-Augmented Generation (RAG) to adjust impedance control parameters in response to environmental changes. In this framework, the UAV acts as a leader using Artificial Potential Field (APF) planning for real-time navigation, while the ground robot follows via virtual impedance links with adaptive link topology to avoid collisions with short obstacles.   The system demonstrated a 92% success rate across 12 real-world trials. Under optimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in object detection and selection of impedance parameters. The mobile robot prioritized short obstacle avoidance, occasionally resulting in a lateral deviation of up to 50 cm from the UAV path, which showcases safe navigation in a cluttered setting.",
    "pdf_url": "https://arxiv.org/pdf/2508.07814v1",
    "github_url": null,
    "published": "2025-08-11T09:56:33+00:00",
    "updated": "2025-08-11T09:56:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.07642v2",
    "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents",
    "authors": [
      "Ma",
      "Zhang",
      "Wang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) poses significant challenges for agents to interpret natural language instructions and navigate complex 3D environments. While recent progress has been driven by large-scale pre-training and data augmentation, current methods still struggle to generalize to unseen scenarios, particularly when complex spatial and temporal reasoning is required. In this work, we propose SkillNav, a modular framework that introduces structured, skill-based reasoning into Transformer-based VLN agents. Our method decomposes navigation into a set of interpretable atomic skills (e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each handled by a specialized agent. To support targeted skill training without manual data annotation, we construct a synthetic dataset pipeline that generates diverse, linguistically natural, skill-specific instruction-trajectory pairs. We then introduce a novel training-free Vision-Language Model (VLM)-based router, which dynamically selects the most suitable agent at each time step by aligning sub-goals with visual observations and historical actions. SkillNav obtains competitive results on commonly used benchmarks and establishes state-of-the-art generalization to the GSA-R2R, a benchmark with novel instruction styles and unseen environments.",
    "pdf_url": "https://arxiv.org/pdf/2508.07642v2",
    "github_url": null,
    "published": "2025-08-11T05:50:30+00:00",
    "updated": "2025-10-01T00:48:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.07406v1",
    "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots",
    "authors": [
      "Zhao",
      "Lyu",
      "Li"
    ],
    "summary": "Agricultural robots have emerged as powerful members in agricultural tasks, nevertheless, still heavily rely on manual operation or untransportable railway for movement, resulting in limited mobility and poor adaptability. Vision-and-Language Navigation (VLN) enables robots to navigate to the target destinations following natural language instructions, demonstrating strong performance on several domains. However, none of the existing benchmarks or methods is specifically designed for agricultural scenes. To bridge this gap, we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560 episodes across six diverse agricultural scenes, in which all realistic RGB videos are captured by front-facing camera on a quadruped robot at a height of 0.38 meters, aligning with the practical deployment conditions. Meanwhile, we propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN) baseline based on Vision-Language Model (VLM) prompted with carefully crafted templates, which can understand both given instructions and agricultural environments to generate appropriate low-level actions for robot control. When evaluated on A2A, AgriVLN performs well on short instructions but struggles with long instructions, because it often fails to track which part of the instruction is currently being executed. To address this, we further propose Subtask List (STL) instruction decomposition module and integrate it into AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare AgriVLN with several existing VLN methods, demonstrating the state-of-the-art performance in the agricultural domain.",
    "pdf_url": "https://arxiv.org/pdf/2508.07406v1",
    "github_url": null,
    "published": "2025-08-10T16:07:23+00:00",
    "updated": "2025-08-10T16:07:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.07387v3",
    "title": "MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control",
    "authors": [
      "Sharma",
      "Jadhav",
      "Paul"
    ],
    "summary": "Navigating unknown environments with a single RGB camera is challenging, as the lack of depth information prevents reliable collision-checking. While some methods use estimated depth to build collision maps, we found that depth estimates from vision foundation models are too noisy for zero-shot navigation in cluttered environments. We propose an alternative approach: instead of using noisy estimated depth for direct collision-checking, we use it as a rich context input to a learned collision model. This model predicts the distribution of minimum obstacle clearance that the robot can expect for a given control sequence. At inference, these predictions inform a risk-aware MPC planner that minimizes estimated collision risk. We proposed a joint learning pipeline that co-trains the collision model and risk metric using both safe and unsafe trajectories. Crucially, our joint-training ensures well calibrated uncertainty in our collision model that improves navigation in highly cluttered environments. Consequently, real-world experiments show reductions in collision-rate and improvements in goal reaching and speed over several strong baselines.",
    "pdf_url": "https://arxiv.org/pdf/2508.07387v3",
    "github_url": null,
    "published": "2025-08-10T15:27:23+00:00",
    "updated": "2025-11-26T14:06:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.16602v1",
    "title": "An Embodied AR Navigation Agent: Integrating BIM with Retrieval-Augmented Generation for Language Guidance",
    "authors": [
      "Yang",
      "Hsiao",
      "Oka"
    ],
    "summary": "Delivering intelligent and adaptive navigation assistance in augmented reality (AR) requires more than visual cues, as it demands systems capable of interpreting flexible user intent and reasoning over both spatial and semantic context. Prior AR navigation systems often rely on rigid input schemes or predefined commands, which limit the utility of rich building data and hinder natural interaction. In this work, we propose an embodied AR navigation system that integrates Building Information Modeling (BIM) with a multi-agent retrieval-augmented generation (RAG) framework to support flexible, language-driven goal retrieval and route planning. The system orchestrates three language agents, Triage, Search, and Response, built on large language models (LLMs), which enables robust interpretation of open-ended queries and spatial reasoning using BIM data. Navigation guidance is delivered through an embodied AR agent, equipped with voice interaction and locomotion, to enhance user experience. A real-world user study yields a System Usability Scale (SUS) score of 80.5, indicating excellent usability, and comparative evaluations show that the embodied interface can significantly improves users' perception of system intelligence. These results underscore the importance and potential of language-grounded reasoning and embodiment in the design of user-centered AR navigation systems.",
    "pdf_url": "https://arxiv.org/pdf/2508.16602v1",
    "github_url": null,
    "published": "2025-08-10T15:13:23+00:00",
    "updated": "2025-08-10T15:13:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.07274v1",
    "title": "Time-dependent Zermelo navigation with tacking",
    "authors": [
      "Markvorsen",
      "Pendás-Recondo",
      "Rygaard"
    ],
    "summary": "We address the time- and position-dependent Zermelo navigation problem within the framework of Lorentz-Finsler geometry. Since the initial work of E. Zermelo, the task is to find the time-minimizing trajectory between two regions for a moving object whose speed profile depends on time, position and direction. We give a step-by-step review of the classical formulation of the problem, where the geometric shape generated by the velocity vectors -- the speed profile indicatrix -- is strongly convex at each point. We derive new global results for the cases where the indicatrix field is only time-dependent. In such (meso-scale realistic) cases, Zermelo navigation exhibits particularly favorable properties that have not been previously explored, making them especially appealing for both theoretical and numerical investigations. Moreover, motivated by real-world phenomena and examples, we obtain novel results for non-convex (or multi-convex) navigation, i.e. when the indicatrices fail to be convex. In this new setting -- which is not unlike the corresponding Finsler setting for Snell's law -- optimal paths may involve so-called tacking, which stems from discontinuous shifts of direction of motion. The tacking behaviour thus results in zig-zag trajectories, as observed in time-optimal sailboat navigation and surprisingly also in the flight paths of far-ranging seabirds. Finally, we provide new efficient computational algorithms and illustrate the use of them to solve the Zermelo navigation (boundary value) problem, i.e. to find numerically the time-minimizing trajectory between two fixed points in the general non-convex setting.",
    "pdf_url": "https://arxiv.org/pdf/2508.07274v1",
    "github_url": null,
    "published": "2025-08-10T10:08:02+00:00",
    "updated": "2025-08-10T10:08:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.07269v2",
    "title": "Navigation and Exploration with Active Inference: from Biology to Industry",
    "authors": [
      "Tinguy",
      "Verbelen",
      "Dhoedt"
    ],
    "summary": "By building and updating internal cognitive maps, animals exhibit extraordinary navigation abilities in complex, dynamic environments. Inspired by these biological mechanisms, we present a real time robotic navigation system grounded in the Active Inference Framework (AIF). Our model incrementally constructs a topological map, infers the agent's location, and plans actions by minimising expected uncertainty and fulfilling perceptual goals without any prior training. Integrated into the ROS2 ecosystem, we validate its adaptability and efficiency across both 2D and 3D environments (simulated and real world), demonstrating competitive performance with traditional and state of the art exploration approaches while offering a biologically inspired navigation approach.",
    "pdf_url": "https://arxiv.org/pdf/2508.07269v2",
    "github_url": null,
    "published": "2025-08-10T09:51:27+00:00",
    "updated": "2025-10-10T17:20:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.07267v1",
    "title": "Bio-Inspired Topological Autonomous Navigation with Active Inference in Robotics",
    "authors": [
      "Tinguy",
      "Verbelen",
      "Gamba"
    ],
    "summary": "Achieving fully autonomous exploration and navigation remains a critical challenge in robotics, requiring integrated solutions for localisation, mapping, decision-making and motion planning. Existing approaches either rely on strict navigation rules lacking adaptability or on pre-training, which requires large datasets. These AI methods are often computationally intensive or based on static assumptions, limiting their adaptability in dynamic or unknown environments. This paper introduces a bio-inspired agent based on the Active Inference Framework (AIF), which unifies mapping, localisation, and adaptive decision-making for autonomous navigation, including exploration and goal-reaching. Our model creates and updates a topological map of the environment in real-time, planning goal-directed trajectories to explore or reach objectives without requiring pre-training. Key contributions include a probabilistic reasoning framework for interpretable navigation, robust adaptability to dynamic changes, and a modular ROS2 architecture compatible with existing navigation systems. Our method was tested in simulated and real-world environments. The agent successfully explores large-scale simulated environments and adapts to dynamic obstacles and drift, proving to be comparable to other exploration strategies such as Gbplanner, FAEL and Frontiers. This approach offers a scalable and transparent approach for navigating complex, unstructured environments.",
    "pdf_url": "https://arxiv.org/pdf/2508.07267v1",
    "github_url": null,
    "published": "2025-08-10T09:42:13+00:00",
    "updated": "2025-08-10T09:42:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.07079v1",
    "title": "Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction",
    "authors": [
      "Aslam",
      "Derajic",
      "Bouzidi"
    ],
    "summary": "Safe navigation in pedestrian-rich environments remains a key challenge for autonomous robots. This work evaluates the integration of a deep learning-based Social-Implicit (SI) pedestrian trajectory predictor within a Model Predictive Control (MPC) framework on the physical Continental Corriere robot. Tested across varied pedestrian densities, the SI-MPC system is compared to a traditional Constant Velocity (CV) model in both open-loop prediction and closed-loop navigation. Results show that SI improves trajectory prediction - reducing errors by up to 76% in low-density settings - and enhances safety and motion smoothness in crowded scenes. Moreover, real-world deployment reveals discrepancies between open-loop metrics and closed-loop performance, as the SI model yields broader, more cautious predictions. These findings emphasize the importance of system-level evaluation and highlight the SI-MPC framework's promise for safer, more adaptive navigation in dynamic, human-populated environments.",
    "pdf_url": "https://arxiv.org/pdf/2508.07079v1",
    "github_url": null,
    "published": "2025-08-09T19:11:28+00:00",
    "updated": "2025-08-09T19:11:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.07045v1",
    "title": "From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline",
    "authors": [
      "Benders",
      "Köhler",
      "Babuška"
    ],
    "summary": "Model predictive control (MPC) is a powerful strategy for planning and control in autonomous mobile robot navigation. However, ensuring safety in real-world deployments remains challenging due to the presence of disturbances and measurement noise. Existing approaches often rely on idealized assumptions, neglect the impact of noisy measurements, and simply heuristically guess unrealistic bounds. In this work, we present an efficient and modular robust MPC design pipeline that systematically addresses these limitations. The pipeline consists of an iterative procedure that leverages closed-loop experimental data to estimate disturbance bounds and synthesize a robust output-feedback MPC scheme. We provide the pipeline in the form of deterministic and reproducible code to synthesize the robust output-feedback MPC from data. We empirically demonstrate robust constraint satisfaction and recursive feasibility in quadrotor simulations using Gazebo.",
    "pdf_url": "https://arxiv.org/pdf/2508.07045v1",
    "github_url": null,
    "published": "2025-08-09T17:06:22+00:00",
    "updated": "2025-08-09T17:06:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.06990v1",
    "title": "Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation",
    "authors": [
      "Hu",
      "Wu",
      "Xu"
    ],
    "summary": "Semantic navigation requires an agent to navigate toward a specified target in an unseen environment. Employing an imaginative navigation strategy that predicts future scenes before taking action, can empower the agent to find target faster. Inspired by this idea, we propose SGImagineNav, a novel imaginative navigation framework that leverages symbolic world modeling to proactively build a global environmental representation. SGImagineNav maintains an evolving hierarchical scene graphs and uses large language models to predict and explore unseen parts of the environment. While existing methods solely relying on past observations, this imaginative scene graph provides richer semantic context, enabling the agent to proactively estimate target locations. Building upon this, SGImagineNav adopts an adaptive navigation strategy that exploits semantic shortcuts when promising and explores unknown areas otherwise to gather additional context. This strategy continuously expands the known environment and accumulates valuable semantic contexts, ultimately guiding the agent toward the target. SGImagineNav is evaluated in both real-world scenarios and simulation benchmarks. SGImagineNav consistently outperforms previous methods, improving success rate to 65.4 and 66.8 on HM3D and HSSD, and demonstrating cross-floor and cross-room navigation in real-world environments, underscoring its effectiveness and generalizability.",
    "pdf_url": "https://arxiv.org/pdf/2508.06990v1",
    "github_url": null,
    "published": "2025-08-09T14:01:08+00:00",
    "updated": "2025-08-09T14:01:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.06823v1",
    "title": "Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation",
    "authors": [
      "Zhao",
      "Tao"
    ],
    "summary": "Exploring volumetric data is crucial for interpreting scientific datasets. However, selecting optimal viewpoints for effective navigation can be challenging, particularly for users without extensive domain expertise or familiarity with 3D navigation. In this paper, we propose a novel framework that leverages natural language interaction to enhance volumetric data exploration. Our approach encodes volumetric blocks to capture and differentiate underlying structures. It further incorporates a CLIP Score mechanism, which provides semantic information to the blocks to guide navigation. The navigation is empowered by a reinforcement learning framework that leverage these semantic cues to efficiently search for and identify desired viewpoints that align with the user's intent. The selected viewpoints are evaluated using CLIP Score to ensure that they best reflect the user queries. By automating viewpoint selection, our method improves the efficiency of volumetric data navigation and enhances the interpretability of complex scientific phenomena.",
    "pdf_url": "https://arxiv.org/pdf/2508.06823v1",
    "github_url": null,
    "published": "2025-08-09T04:44:59+00:00",
    "updated": "2025-08-09T04:44:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.05855v1",
    "title": "Safety of Embodied Navigation: A Survey",
    "authors": [
      "Wang",
      "Hu",
      "Mu"
    ],
    "summary": "As large language models (LLMs) continue to advance and gain influence, the development of embodied AI has accelerated, drawing significant attention, particularly in navigation scenarios. Embodied navigation requires an agent to perceive, interact with, and adapt to its environment while moving toward a specified target in unfamiliar settings. However, the integration of embodied navigation into critical applications raises substantial safety concerns. Given their deployment in dynamic, real-world environments, ensuring the safety of such systems is critical. This survey provides a comprehensive analysis of safety in embodied navigation from multiple perspectives, encompassing attack strategies, defense mechanisms, and evaluation methodologies. Beyond conducting a comprehensive examination of existing safety challenges, mitigation technologies, and various datasets and metrics that assess effectiveness and robustness, we explore unresolved issues and future research directions in embodied navigation safety. These include potential attack methods, mitigation strategies, more reliable evaluation techniques, and the implementation of verification frameworks. By addressing these critical gaps, this survey aims to provide valuable insights that can guide future research toward the development of safer and more reliable embodied navigation systems. Furthermore, the findings of this study have broader implications for enhancing societal safety and increasing industrial efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2508.05855v1",
    "github_url": null,
    "published": "2025-08-07T21:09:48+00:00",
    "updated": "2025-08-07T21:09:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.05634v1",
    "title": "Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling",
    "authors": [
      "Yao",
      "Zhang",
      "Xia"
    ],
    "summary": "Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent's behavior through constrained reinforcement learning. The system helps regulate the agent's actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on https://gen-safe-nav.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2508.05634v1",
    "github_url": null,
    "published": "2025-08-07T17:59:43+00:00",
    "updated": "2025-08-07T17:59:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.05543v1",
    "title": "CleanUpBench: Embodied Sweeping and Grasping Benchmark",
    "authors": [
      "Li",
      "Chen",
      "Zhao"
    ],
    "summary": "Embodied AI benchmarks have advanced navigation, manipulation, and reasoning, but most target complex humanoid agents or large-scale simulations that are far from real-world deployment. In contrast, mobile cleaning robots with dual mode capabilities, such as sweeping and grasping, are rapidly emerging as realistic and commercially viable platforms. However, no benchmark currently exists that systematically evaluates these agents in structured, multi-target cleaning tasks, revealing a critical gap between academic research and real-world applications. We introduce CleanUpBench, a reproducible and extensible benchmark for evaluating embodied agents in realistic indoor cleaning scenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service robot equipped with a sweeping mechanism and a six-degree-of-freedom robotic arm, enabling interaction with heterogeneous objects. The benchmark includes manually designed environments and one procedurally generated layout to assess generalization, along with a comprehensive evaluation suite covering task completion, spatial efficiency, motion quality, and control performance. To support comparative studies, we provide baseline agents based on heuristic strategies and map-based planning. CleanUpBench bridges the gap between low-level skill evaluation and full-scene testing, offering a scalable testbed for grounded, embodied intelligence in everyday settings.",
    "pdf_url": "https://arxiv.org/pdf/2508.05543v1",
    "github_url": null,
    "published": "2025-08-07T16:20:31+00:00",
    "updated": "2025-08-07T16:20:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.05253v3",
    "title": "Congestion Mitigation Path Planning for Large-Scale Multi-Agent Navigation in Dense Environments",
    "authors": [
      "Kato",
      "Okumura",
      "Sasaki"
    ],
    "summary": "In high-density environments where numerous autonomous agents move simultaneously in a distributed manner, streamlining global flows to mitigate local congestion is crucial to maintain overall navigation efficiency. This paper introduces a novel path-planning problem, congestion mitigation path planning (CMPP), which embeds congestion directly into the cost function, defined by the usage of incoming edges along agents' paths. CMPP assigns a flow-based multiplicative penalty to each vertex of a sparse graph, which grows steeply where frequently-traversed paths intersect, capturing the intuition that congestion intensifies where many agents enter the same area from different directions. Minimizing the total cost yields a set of coarse-level, time-independent routes that autonomous agents can follow while applying their own local collision avoidance. We formulate the problem and develop two solvers: (i) an exact mixed-integer nonlinear programming solver for small instances, and (ii) a scalable two-layer search algorithm, A-CMTS, which quickly finds suboptimal solutions for large-scale instances and iteratively refines them toward the optimum. Empirical studies show that augmenting state-of-the-art collision-avoidance planners with CMPP significantly reduces local congestion and enhances system throughput in both discrete- and continuous-space scenarios. These results indicate that CMPP improves the performance of multi-agent systems in real-world applications such as logistics and autonomous-vehicle operations.",
    "pdf_url": "https://arxiv.org/pdf/2508.05253v3",
    "github_url": null,
    "published": "2025-08-07T10:43:19+00:00",
    "updated": "2025-08-19T23:20:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.05021v1",
    "title": "MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding",
    "authors": [
      "Zhang",
      "Li",
      "Liu"
    ],
    "summary": "Visual navigation in unknown environments based solely on natural language descriptions is a key capability for intelligent robots. In this work, we propose a navigation framework built upon off-the-shelf Visual Language Models (VLMs), enhanced with two human-inspired mechanisms: perspective-based active grounding, which dynamically adjusts the robot's viewpoint for improved visual inspection, and historical memory backtracking, which enables the system to retain and re-evaluate uncertain observations over time. Unlike existing approaches that passively rely on incidental visual inputs, our method actively optimizes perception and leverages memory to resolve ambiguity, significantly improving vision-language grounding in complex, unseen environments. Our framework operates in a zero-shot manner, achieving strong generalization to diverse and open-ended language descriptions without requiring labeled data or model fine-tuning. Experimental results on Habitat-Matterport 3D (HM3D) show that our method outperforms state-of-the-art approaches in language-driven object navigation. We further demonstrate its practicality through real-world deployment on a quadruped robot, achieving robust and effective navigation performance.",
    "pdf_url": "https://arxiv.org/pdf/2508.05021v1",
    "github_url": null,
    "published": "2025-08-07T04:15:08+00:00",
    "updated": "2025-08-07T04:15:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.04994v1",
    "title": "Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots",
    "authors": [
      "Hu",
      "Zhou",
      "Ho"
    ],
    "summary": "Maze navigation is a fundamental challenge in robotics, requiring agents to traverse complex environments efficiently. While the Deep Deterministic Policy Gradient (DDPG) algorithm excels in control tasks, its performance in maze navigation suffers from sparse rewards, inefficient exploration, and long-horizon planning difficulties, often leading to low success rates and average rewards, sometimes even failing to achieve effective navigation. To address these limitations, this paper proposes an efficient Hierarchical DDPG (HDDPG) algorithm, which includes high-level and low-level policies. The high-level policy employs an advanced DDPG framework to generate intermediate subgoals from a long-term perspective and on a higher temporal scale. The low-level policy, also powered by the improved DDPG algorithm, generates primitive actions by observing current states and following the subgoal assigned by the high-level policy. The proposed method enhances stability with off-policy correction, refining subgoal assignments by relabeling historical experiences. Additionally, adaptive parameter space noise is utilized to improve exploration, and a reshaped intrinsic-extrinsic reward function is employed to boost learning efficiency. Further optimizations, including gradient clipping and Xavier initialization, are employed to improve robustness. The proposed algorithm is rigorously evaluated through numerical simulation experiments executed using the Robot Operating System (ROS) and Gazebo. Regarding the three distinct final targets in autonomous maze navigation tasks, HDDPG significantly overcomes the limitations of standard DDPG and its variants, improving the success rate by at least 56.59% and boosting the average reward by a minimum of 519.03 compared to baseline algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2508.04994v1",
    "github_url": null,
    "published": "2025-08-07T03:06:22+00:00",
    "updated": "2025-08-07T03:06:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.04678v1",
    "title": "Open Scene Graphs for Open-World Object-Goal Navigation",
    "authors": [
      "Loo",
      "Wu",
      "Hsu"
    ],
    "summary": "How can we build general-purpose robot systems for open-world semantic navigation, e.g., searching a novel environment for a target object specified in natural language? To tackle this challenge, we introduce OSG Navigator, a modular system composed of foundation models, for open-world Object-Goal Navigation (ObjectNav). Foundation models provide enormous semantic knowledge about the world, but struggle to organise and maintain spatial information effectively at scale. Key to OSG Navigator is the Open Scene Graph representation, which acts as spatial memory for OSG Navigator. It organises spatial information hierarchically using OSG schemas, which are templates, each describing the common structure of a class of environments. OSG schemas can be automatically generated from simple semantic labels of a given environment, e.g., \"home\" or \"supermarket\". They enable OSG Navigator to adapt zero-shot to new environment types. We conducted experiments using both Fetch and Spot robots in simulation and in the real world, showing that OSG Navigator achieves state-of-the-art performance on ObjectNav benchmarks and generalises zero-shot over diverse goals, environments, and robot embodiments.",
    "pdf_url": "https://arxiv.org/pdf/2508.04678v1",
    "github_url": null,
    "published": "2025-08-06T17:43:29+00:00",
    "updated": "2025-08-06T17:43:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.04598v1",
    "title": "$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything",
    "authors": [
      "Zhang",
      "Hao",
      "Tang"
    ],
    "summary": "Embodied navigation is a fundamental capability of embodied intelligence, enabling robots to move and interact within physical environments. However, existing navigation tasks primarily focus on predefined object navigation or instruction following, which significantly differs from human needs in real-world scenarios involving complex, open-ended scenes. To bridge this gap, we introduce a challenging long-horizon navigation task that requires understanding high-level human instructions and performing spatial-aware object navigation in real-world environments. Existing embodied navigation methods struggle with such tasks due to their limitations in comprehending high-level human instructions and localizing objects with an open vocabulary. In this paper, we propose $NavA^3$, a hierarchical framework divided into two stages: global and local policies. In the global policy, we leverage the reasoning capabilities of Reasoning-VLM to parse high-level human instructions and integrate them with global 3D scene views. This allows us to reason and navigate to regions most likely to contain the goal object. In the local policy, we have collected a dataset of 1.0 million samples of spatial-aware object affordances to train the NaviAfford model (PointingVLM), which provides robust open-vocabulary object localization and spatial awareness for precise goal identification and navigation in complex environments. Extensive experiments demonstrate that $NavA^3$ achieves SOTA results in navigation performance and can successfully complete longhorizon navigation tasks across different robot embodiments in real-world settings, paving the way for universal embodied navigation. The dataset and code will be made available. Project website: https://NavigationA3.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2508.04598v1",
    "github_url": null,
    "published": "2025-08-06T16:17:34+00:00",
    "updated": "2025-08-06T16:17:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.10012v1",
    "title": "Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs",
    "authors": [
      "Tao",
      "Liu",
      "Weizheng"
    ],
    "summary": "While Large Language Models (LLMs) exhibit strong linguistic capabilities, their reliance on static knowledge and opaque reasoning processes limits their performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a promising solution, but current exploration methods face a fundamental trade off: question guided approaches incur redundant exploration due to granularity mismatches, while clue guided methods fail to effectively leverage contextual information for complex scenarios. To address these limitations, we propose Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework that introduces an intermediate Guidance Graph to bridge unstructured queries and structured knowledge retrieval. The Guidance Graph defines the retrieval space by abstracting the target knowledge' s structure while preserving broader semantic context, enabling precise and efficient exploration. Building upon the Guidance Graph, we develop: (1) Structural Alignment that filters incompatible candidates without LLM overhead, and (2) Context Aware Pruning that enforces semantic consistency with graph constraints. Extensive experiments show our method achieves superior efficiency and outperforms SOTA, especially on complex tasks, while maintaining strong performance with smaller LLMs, demonstrating practical value.",
    "pdf_url": "https://arxiv.org/pdf/2508.10012v1",
    "github_url": null,
    "published": "2025-08-06T08:47:57+00:00",
    "updated": "2025-08-06T08:47:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.03890v2",
    "title": "Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes",
    "authors": [
      "Jung",
      "Gwak",
      "Boots"
    ],
    "summary": "Terrain elevation modeling for off-road navigation aims to accurately estimate changes in terrain geometry in real-time and quantify the corresponding uncertainties. Having precise estimations and uncertainties plays a crucial role in planning and control algorithms to explore safe and reliable maneuver strategies. However, existing approaches, such as Gaussian Processes (GPs) and neural network-based methods, often fail to meet these needs. They are either unable to perform in real-time due to high computational demands, underestimating sharp geometry changes, or harming elevation accuracy when learned with uncertainties. Recently, Neural Processes (NPs) have emerged as a promising approach that integrates the Bayesian uncertainty estimation of GPs with the efficiency and flexibility of neural networks. Inspired by NPs, we propose an effective NP-based method that precisely estimates sharp elevation changes and quantifies the corresponding predictive uncertainty without losing elevation accuracy. Our method leverages semantic features from LiDAR and camera sensors to improve interpolation and extrapolation accuracy in unobserved regions. Also, we introduce a local ball-query attention mechanism to effectively reduce the computational complexity of global attention by 17\\% while preserving crucial local and spatial information. We evaluate our method on off-road datasets having interesting geometric features, collected from trails, deserts, and hills. Our results demonstrate superior performance over baselines and showcase the potential of neural processes for effective and expressive terrain modeling in complex off-road environments.",
    "pdf_url": "https://arxiv.org/pdf/2508.03890v2",
    "github_url": null,
    "published": "2025-08-05T20:19:02+00:00",
    "updated": "2025-08-07T19:56:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.03672v2",
    "title": "Inland-LOAM: Voxel-Based Structural Semantic LiDAR Odometry and Mapping for Inland Waterway Navigation",
    "authors": [
      "Luo",
      "Wang",
      "Swevers"
    ],
    "summary": "Accurate geospatial information is crucial for safe, autonomous Inland Waterway Transport (IWT), as existing charts (IENC) lack real-time detail and conventional LiDAR SLAM fails in waterway environments. These challenges lead to vertical drift and non-semantic maps, hindering autonomous navigation.   This paper introduces Inland-LOAM, a LiDAR SLAM framework for waterways. It uses an improved feature extraction and a water surface planar constraint to mitigate vertical drift. A novel pipeline transforms 3D point clouds into structured 2D semantic maps using voxel-based geometric analysis, enabling real-time computation of navigational parameters like bridge clearances. An automated module extracts shorelines and exports them into a lightweight, IENC-compatible format.   Evaluations on a real-world dataset show Inland-LOAM achieves superior localization accuracy over state-of-the-art methods. The generated semantic maps and shorelines align with real-world conditions, providing reliable data for enhanced situational awareness. The code and dataset will be publicly available",
    "pdf_url": "https://arxiv.org/pdf/2508.03672v2",
    "github_url": null,
    "published": "2025-08-05T17:37:43+00:00",
    "updated": "2025-10-15T01:31:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.03410v1",
    "title": "VisAug: Facilitating Speech-Rich Web Video Navigation and Engagement with Auto-Generated Visual Augmentations",
    "authors": [
      "Zhao",
      "Ma",
      "Pang"
    ],
    "summary": "The widespread adoption of digital technology has ushered in a new era of digital transformation across all aspects of our lives. Online learning, social, and work activities, such as distance education, videoconferencing, interviews, and talks, have led to a dramatic increase in speech-rich video content. In contrast to other video types, such as surveillance footage, which typically contain abundant visual cues, speech-rich videos convey most of their meaningful information through the audio channel. This poses challenges for improving content consumption using existing visual-based video summarization, navigation, and exploration systems. In this paper, we present VisAug, a novel interactive system designed to enhance speech-rich video navigation and engagement by automatically generating informative and expressive visual augmentations based on the speech content of videos. Our findings suggest that this system has the potential to significantly enhance the consumption and engagement of information in an increasingly video-driven digital landscape.",
    "pdf_url": "https://arxiv.org/pdf/2508.03410v1",
    "github_url": null,
    "published": "2025-08-05T12:55:53+00:00",
    "updated": "2025-08-05T12:55:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.03350v1",
    "title": "Investigation of Air Fluidization during Intruder Penetration in Sand",
    "authors": [
      "Wang",
      "Peng",
      "Vergara"
    ],
    "summary": "Self-burrowing robots navigating through granular media benefit from airflow-assisted burrowing, which reduces penetration resistance. However, the mechanisms underlying airflow-granular interactions remain poorly understood. To address this knowledge gap, we employ a coupled computational fluid dynamics and discrete element method (CFD-DEM) approach, supplemented by experimental cone penetration tests (CPT) under varying airflow conditions, to investigate the effects of aeration on penetration resistance. Experimental results reveal a nonlinear relationship between penetration resistance reduction and depth, wherein resistance approaches near-zero values up to a critical depth, beyond which the effectiveness of fluidization diminishes. Simulations demonstrate that higher airflow rates enhance the mobilization of overlying grains, increasing the critical depth. A detailed meso- and micro-scale analysis of particle motion, contact forces, and fluid pressure fields reveals four distinct penetration stages: particle ejection and channel formation, channel sealing, channel refill, and final compaction. These findings contribute to a deeper understanding of granular aeration mechanisms and their implications for geotechnical engineering, excavation technologies, and the development of self-burrowing robotic systems.",
    "pdf_url": "https://arxiv.org/pdf/2508.03350v1",
    "github_url": null,
    "published": "2025-08-05T11:55:20+00:00",
    "updated": "2025-08-05T11:55:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.03246v1",
    "title": "Force-Compliance MPC and Robot-User CBFs for Interactive Navigation and User-Robot Safety in Hexapod Guide Robots",
    "authors": [
      "Fan",
      "Gao",
      "Chen"
    ],
    "summary": "Guiding the visually impaired in complex environments requires real-time two-way interaction and safety assurance. We propose a Force-Compliance Model Predictive Control (FC-MPC) and Robot-User Control Barrier Functions (CBFs) for force-compliant navigation and obstacle avoidance in Hexapod guide robots. FC-MPC enables two-way interaction by estimating user-applied forces and moments using the robot's dynamic model and the recursive least squares (RLS) method, and then adjusting the robot's movements accordingly, while Robot-User CBFs ensure the safety of both the user and the robot by handling static and dynamic obstacles, and employ weighted slack variables to overcome feasibility issues in complex dynamic environments. We also adopt an Eight-Way Connected DBSCAN method for obstacle clustering, reducing computational complexity from O(n2) to approximately O(n), enabling real-time local perception on resource-limited on-board robot computers. Obstacles are modeled using Minimum Bounding Ellipses (MBEs), and their trajectories are predicted through Kalman filtering. Implemented on the HexGuide robot, the system seamlessly integrates force compliance, autonomous navigation, and obstacle avoidance. Experimental results demonstrate the system's ability to adapt to user force commands while guaranteeing user and robot safety simultaneously during navigation in complex environments.",
    "pdf_url": "https://arxiv.org/pdf/2508.03246v1",
    "github_url": null,
    "published": "2025-08-05T09:24:13+00:00",
    "updated": "2025-08-05T09:24:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.03216v1",
    "title": "Navigation Pixie: Implementation and Empirical Study Toward On-demand Navigation Agents in Commercial Metaverse",
    "authors": [
      "Yanagawa",
      "Hiroi",
      "Tokida"
    ],
    "summary": "While commercial metaverse platforms offer diverse user-generated content, they lack effective navigation assistance that can dynamically adapt to users' interests and intentions. Although previous research has investigated on-demand agents in controlled environments, implementation in commercial settings with diverse world configurations and platform constraints remains challenging.   We present Navigation Pixie, an on-demand navigation agent employing a loosely coupled architecture that integrates structured spatial metadata with LLM-based natural language processing while minimizing platform dependencies, which enables experiments on the extensive user base of commercial metaverse platforms. Our cross-platform experiments on commercial metaverse platform Cluster with 99 PC client and 94 VR-HMD participants demonstrated that Navigation Pixie significantly increased dwell time and free exploration compared to fixed-route and no-agent conditions across both platforms. Subjective evaluations revealed consistent on-demand preferences in PC environments versus context-dependent social perception advantages in VR-HMD. This research contributes to advancing VR interaction design through conversational spatial navigation agents, establishes cross-platform evaluation methodologies revealing environment-dependent effectiveness, and demonstrates empirical experimentation frameworks for commercial metaverse platforms.",
    "pdf_url": "https://arxiv.org/pdf/2508.03216v1",
    "github_url": null,
    "published": "2025-08-05T08:45:34+00:00",
    "updated": "2025-08-05T08:45:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.03138v1",
    "title": "Language as Cost: Proactive Hazard Mapping using VLM for Robot Navigation",
    "authors": [
      "Oh",
      "Kim",
      "Seo"
    ],
    "summary": "Robots operating in human-centric or hazardous environments must proactively anticipate and mitigate dangers beyond basic obstacle detection. Traditional navigation systems often depend on static maps, which struggle to account for dynamic risks, such as a person emerging from a suddenly opening door. As a result, these systems tend to be reactive rather than anticipatory when handling dynamic hazards. Recent advancements in pre-trained large language models and vision-language models (VLMs) create new opportunities for proactive hazard avoidance. In this work, we propose a zero-shot language-as-cost mapping framework that leverages VLMs to interpret visual scenes, assess potential dynamic risks, and assign risk-aware navigation costs preemptively, enabling robots to anticipate hazards before they materialize. By integrating this language-based cost map with a geometric obstacle map, the robot not only identifies existing obstacles but also anticipates and proactively plans around potential hazards arising from environmental dynamics. Experiments in simulated and diverse dynamic environments demonstrate that the proposed method significantly improves navigation success rates and reduces hazard encounters, compared to reactive baseline planners. Code and supplementary materials are available at https://github.com/Taekmino/LaC.",
    "pdf_url": "https://arxiv.org/pdf/2508.03138v1",
    "github_url": "https://github.com/Taekmino/LaC",
    "published": "2025-08-05T06:35:37+00:00",
    "updated": "2025-08-05T06:35:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.03068v2",
    "title": "Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching",
    "authors": [
      "Chen",
      "Ye",
      "Cao"
    ],
    "summary": "We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns navigation, locomotion, and reaching skills for humanoids, directly from human motion and vision perception data. We take a modular approach where the high-level planner commands the target position and orientation of the hands and eyes of the humanoid, delivered by the low-level policy that controls the whole-body movements. Specifically, the low-level whole-body controller learns to track the three points (eyes, left hand, and right hand) from existing large-scale human motion capture data while high-level policy learns from human data collected by Aria glasses. Our modular approach decouples the ego-centric vision perception from physical actions, promoting efficient learning and scalability to novel scenes. We evaluate our method both in simulation and in the real-world, demonstrating humanoid's capabilities to navigate and reach in complex environments designed for humans.",
    "pdf_url": "https://arxiv.org/pdf/2508.03068v2",
    "github_url": null,
    "published": "2025-08-05T04:30:02+00:00",
    "updated": "2025-08-07T19:18:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.03053v1",
    "title": "SkeNa: Learning to Navigate Unseen Environments Based on Abstract Hand-Drawn Maps",
    "authors": [
      "Xu",
      "Xiang",
      "Wei"
    ],
    "summary": "A typical human strategy for giving navigation guidance is to sketch route maps based on the environmental layout. Inspired by this, we introduce Sketch map-based visual Navigation (SkeNa), an embodied navigation task in which an agent must reach a goal in an unseen environment using only a hand-drawn sketch map as guidance. To support research for SkeNa, we present a large-scale dataset named SoR, comprising 54k trajectory and sketch map pairs across 71 indoor scenes. In SoR, we introduce two navigation validation sets with varying levels of abstraction in hand-drawn sketches, categorized based on their preservation of spatial scales in the environment, to facilitate future research. To construct SoR, we develop an automated sketch-generation pipeline that efficiently converts floor plans into hand-drawn representations. To solve SkeNa, we propose SkeNavigator, a navigation framework that aligns visual observations with hand-drawn maps to estimate navigation targets. It employs a Ray-based Map Descriptor (RMD) to enhance sketch map valid feature representation using equidistant sampling points and boundary distances. To improve alignment with visual observations, a Dual-Map Aligned Goal Predictor (DAGP) leverages the correspondence between sketch map features and on-site constructed exploration map features to predict goal position and guide navigation. SkeNavigator outperforms prior floor plan navigation methods by a large margin, improving SPL on the high-abstract validation set by 105% relatively. Our code and dataset will be released.",
    "pdf_url": "https://arxiv.org/pdf/2508.03053v1",
    "github_url": null,
    "published": "2025-08-05T03:56:32+00:00",
    "updated": "2025-08-05T03:56:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.02917v1",
    "title": "Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces",
    "authors": [
      "Kåsene",
      "Lison"
    ],
    "summary": "Vision-and-Language Navigation (VLN) refers to the task of enabling autonomous robots to navigate unfamiliar environments by following natural language instructions. While recent Large Vision-Language Models (LVLMs) have shown promise in this task, most current VLM systems rely on models specifically designed and optimized for navigation, leaving the potential of off-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used low-level action spaces with egocentric views and atomic actions (such as \"turn left\" or \"move forward\"), newer models tend to favor panoramic action spaces with discrete navigable viewpoints. This paper investigates (1) whether off-the-shelf LVLMs (fine-tuned without architectural modifications or simulator-based training) can effectively support VLN tasks and (2) whether such models can support both low-level and panoramic action paradigms. To this end, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the Room-to-Room (R2R) dataset and evaluate its empirical performance across both low-level and panoramic action spaces. The best resulting model achieves a 41% success rate on the R2R test set, demonstrating that while off-the-shelf LVLMs can learn to perform Vision-and-Language Navigation, they still lag behind models specifically designed for this task.",
    "pdf_url": "https://arxiv.org/pdf/2508.02917v1",
    "github_url": null,
    "published": "2025-08-04T21:45:21+00:00",
    "updated": "2025-08-04T21:45:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.02617v1",
    "title": "Vision-based Navigation of Unmanned Aerial Vehicles in Orchards: An Imitation Learning Approach",
    "authors": [
      "Wei",
      "Ragbir",
      "Vougioukas"
    ],
    "summary": "Autonomous unmanned aerial vehicle (UAV) navigation in orchards presents significant challenges due to obstacles and GPS-deprived environments. In this work, we introduce a learning-based approach to achieve vision-based navigation of UAVs within orchard rows. Our method employs a variational autoencoder (VAE)-based controller, trained with an intervention-based learning framework that allows the UAV to learn a visuomotor policy from human experience. We validate our approach in real orchard environments with a custom-built quadrotor platform. Field experiments demonstrate that after only a few iterations of training, the proposed VAE-based controller can autonomously navigate the UAV based on a front-mounted camera stream. The controller exhibits strong obstacle avoidance performance, achieves longer flying distances with less human assistance, and outperforms existing algorithms. Furthermore, we show that the policy generalizes effectively to novel environments and maintains competitive performance across varying conditions and speeds. This research not only advances UAV autonomy but also holds significant potential for precision agriculture, improving efficiency in orchard monitoring and management.",
    "pdf_url": "https://arxiv.org/pdf/2508.02617v1",
    "github_url": null,
    "published": "2025-08-04T17:06:04+00:00",
    "updated": "2025-08-04T17:06:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.02549v4",
    "title": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming",
    "authors": [
      "Wang",
      "Wang",
      "Fan"
    ],
    "summary": "Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth inputs to provide rich spatial cues for action planning, but these sensors can be costly or less accessible in real-world deployments. Recent approaches based on Vision-Language Action (VLA) models achieve strong results with monocular input, yet they still lag behind methods using panoramic RGB-D information. We present MonoDream, a lightweight VLA framework that enables monocular agents to learn a Unified Navigation Representation (UNR). This shared feature representation jointly aligns navigation-relevant visual semantics (e.g., global layout, depth, and future cues) and language-grounded action intent, enabling more reliable action prediction. MonoDream further introduces Latent Panoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to predict latent features of panoramic RGB and depth observations at both current and future steps based on only monocular input. Experiments on multiple VLN benchmarks show that MonoDream consistently improves monocular navigation performance and significantly narrows the gap with panoramic-based agents.",
    "pdf_url": "https://arxiv.org/pdf/2508.02549v4",
    "github_url": null,
    "published": "2025-08-04T16:01:30+00:00",
    "updated": "2025-11-27T10:49:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.02046v2",
    "title": "NaviMaster: Learning a Unified Policy for GUI and Embodied Navigation Tasks",
    "authors": [
      "Luo",
      "Yan",
      "Gong"
    ],
    "summary": "Recent advances in Graphical User Interface (GUI) and embodied navigation have driven progress, yet these domains have largely evolved in isolation, with disparate datasets and training paradigms. In this paper, we observe that both tasks can be formulated as Markov Decision Processes (MDP), suggesting a foundational principle for their unification. Hence, we present NaviMaster, the first unified agent capable of unifying GUI navigation and embodied navigation within a single framework. Specifically, NaviMaster (i) proposes a visual-target trajectory collection pipeline that generates trajectories for both GUI and embodied tasks using a single formulation. (ii) employs a unified reinforcement learning framework on the mix data to improve generalization. (iii) designs a novel distance-aware reward to ensure efficient learning from the trajectories. Through extensive experiments on out-of-domain benchmarks, NaviMaster is shown to outperform state-of-the-art agents in GUI navigation, spatial affordance prediction, and embodied navigation. Ablation studies further demonstrate the efficacy of our unified training strategy, data mixing strategy, and reward design.",
    "pdf_url": "https://arxiv.org/pdf/2508.02046v2",
    "github_url": null,
    "published": "2025-08-04T04:28:18+00:00",
    "updated": "2025-10-11T08:21:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.01766v6",
    "title": "VPN: Visual Prompt Navigation",
    "authors": [
      "Feng",
      "Wang",
      "Li"
    ],
    "summary": "While natural language is commonly used to guide embodied agents, the inherent ambiguity and verbosity of language often hinder the effectiveness of language-guided navigation in complex environments. To this end, we propose Visual Prompt Navigation (VPN), a novel paradigm that guides agents to navigate using only user-provided visual prompts within 2D top-view maps. This visual prompt primarily focuses on marking the visual navigation trajectory on a top-down view of a scene, offering intuitive and spatially grounded guidance without relying on language instructions. It is more friendly for non-expert users and reduces interpretive ambiguity. We build VPN tasks in both discrete and continuous navigation settings, constructing two new datasets, R2R-VP and R2R-CE-VP, by extending existing R2R and R2R-CE episodes with corresponding visual prompts. Furthermore, we introduce VPNet, a dedicated baseline network to handle the VPN tasks, with two data augmentation strategies: view-level augmentation (altering initial headings and prompt orientations) and trajectory-level augmentation (incorporating diverse trajectories from large-scale 3D scenes), to enhance navigation performance. Extensive experiments evaluate how visual prompt forms, top-view map formats, and data augmentation strategies affect the performance of visual prompt navigation. The code is available at https://github.com/farlit/VPN.",
    "pdf_url": "https://arxiv.org/pdf/2508.01766v6",
    "github_url": "https://github.com/farlit/VPN",
    "published": "2025-08-03T14:07:45+00:00",
    "updated": "2025-11-23T15:42:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.01765v2",
    "title": "HeadZoom: Hands-Free Zooming and Panning for 2D Image Navigation Using Head Motion",
    "authors": [
      "Zhang",
      "Moreira",
      "Belchior"
    ],
    "summary": "We introduce \\textit{HeadZoom}, a hands-free interaction technique for navigating two-dimensional visual content using head movements. HeadZoom enables fluid zooming and panning using only real-time head tracking. It supports natural control in applications such as map exploration, radiograph inspection, and image browsing, where physical interaction is limited. We evaluated HeadZoom in a within-subjects study comparing three interaction techniques-Static, Tilt Zoom, and Parallel Zoom-across spatial, error, and subjective metrics. Parallel Zoom significantly reduced total head movement compared to Static and Tilt modes. Users reported significantly lower perceived exertion for Parallel Zoom, confirming its suitability for prolonged or precision-based tasks. By minimizing movement demands while maintaining task effectiveness, HeadZoom advances the design of head-based 2D interaction in VR and creates new opportunities for accessible hands-free systems for image exploration.",
    "pdf_url": "https://arxiv.org/pdf/2508.01765v2",
    "github_url": null,
    "published": "2025-08-03T14:06:17+00:00",
    "updated": "2025-08-11T13:15:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.01723v1",
    "title": "OpenMap: Instruction Grounding via Open-Vocabulary Visual-Language Mapping",
    "authors": [
      "Li",
      "Yang",
      "Qi"
    ],
    "summary": "Grounding natural language instructions to visual observations is fundamental for embodied agents operating in open-world environments. Recent advances in visual-language mapping have enabled generalizable semantic representations by leveraging vision-language models (VLMs). However, these methods often fall short in aligning free-form language commands with specific scene instances, due to limitations in both instance-level semantic consistency and instruction interpretation. We present OpenMap, a zero-shot open-vocabulary visual-language map designed for accurate instruction grounding in navigation tasks. To address semantic inconsistencies across views, we introduce a Structural-Semantic Consensus constraint that jointly considers global geometric structure and vision-language similarity to guide robust 3D instance-level aggregation. To improve instruction interpretation, we propose an LLM-assisted Instruction-to-Instance Grounding module that enables fine-grained instance selection by incorporating spatial context and expressive target descriptions. We evaluate OpenMap on ScanNet200 and Matterport3D, covering both semantic mapping and instruction-to-target retrieval tasks. Experimental results show that OpenMap outperforms state-of-the-art baselines in zero-shot settings, demonstrating the effectiveness of our method in bridging free-form language and 3D perception for embodied navigation.",
    "pdf_url": "https://arxiv.org/pdf/2508.01723v1",
    "github_url": null,
    "published": "2025-08-03T11:25:52+00:00",
    "updated": "2025-08-03T11:25:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.01539v1",
    "title": "HALO: Human Preference Aligned Offline Reward Learning for Robot Navigation",
    "authors": [
      "Seneviratne",
      "An",
      "Ellahy"
    ],
    "summary": "In this paper, we introduce HALO, a novel Offline Reward Learning algorithm that quantifies human intuition in navigation into a vision-based reward function for robot navigation. HALO learns a reward model from offline data, leveraging expert trajectories collected from mobile robots. During training, actions are uniformly sampled around a reference action and ranked using preference scores derived from a Boltzmann distribution centered on the preferred action, and shaped based on binary user feedback to intuitive navigation queries. The reward model is trained via the Plackett-Luce loss to align with these ranked preferences. To demonstrate the effectiveness of HALO, we deploy its reward model in two downstream applications: (i) an offline learned policy trained directly on the HALO-derived rewards, and (ii) a model-predictive-control (MPC) based planner that incorporates the HALO reward as an additional cost term. This showcases the versatility of HALO across both learning-based and classical navigation frameworks. Our real-world deployments on a Clearpath Husky across diverse scenarios demonstrate that policies trained with HALO generalize effectively to unseen environments and hardware setups not present in the training data. HALO outperforms state-of-the-art vision-based navigation methods, achieving at least a 33.3% improvement in success rate, a 12.9% reduction in normalized trajectory length, and a 26.6% reduction in Frechet distance compared to human expert trajectories.",
    "pdf_url": "https://arxiv.org/pdf/2508.01539v1",
    "github_url": null,
    "published": "2025-08-03T01:46:10+00:00",
    "updated": "2025-08-03T01:46:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.01192v2",
    "title": "Unified Generation-Refinement Planning: Bridging Guided Flow Matching and Sampling-Based MPC for Social Navigation",
    "authors": [
      "Mizuta",
      "Leung"
    ],
    "summary": "Planning safe and effective robot behavior in dynamic, human-centric environments remains a core challenge due to the need to handle multimodal uncertainty, adapt in real-time, and ensure safety. Optimization-based planners offer explicit constraint handling but performance relies on initialization quality. Learning-based planners better capture multimodal possible solutions but struggle to enforce constraints such as safety. In this paper, we introduce a unified generation-refinement framework bridging learning and optimization with a novel reward-guided conditional flow matching (CFM) model and model predictive path integral (MPPI) control. Our key innovation is in the incorporation of a bidirectional information exchange: samples from a reward-guided CFM model provide informed priors for MPPI refinement, while the optimal trajectory from MPPI warm-starts the next CFM generation. Using autonomous social navigation as a motivating application, we demonstrate that our approach can flexibly adapt to dynamic environments to satisfy safety requirements in real-time.",
    "pdf_url": "https://arxiv.org/pdf/2508.01192v2",
    "github_url": null,
    "published": "2025-08-02T04:42:34+00:00",
    "updated": "2025-11-22T21:47:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.00823v1",
    "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
    "authors": [
      "Guo",
      "Xu",
      "Yin"
    ],
    "summary": "Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/.",
    "pdf_url": "https://arxiv.org/pdf/2508.00823v1",
    "github_url": null,
    "published": "2025-08-01T17:59:56+00:00",
    "updated": "2025-08-01T17:59:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.00400v1",
    "title": "Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents",
    "authors": [
      "Gajo",
      "Merales",
      "Escarcha"
    ],
    "summary": "We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store simulation for benchmarking embodied agents against human performance in shopping tasks. Addressing a gap in retail-specific sim environments for embodied agent training, Sari Sandbox features over 250 interactive grocery items across three store configurations, controlled via an API. It supports both virtual reality (VR) for human interaction and a vision language model (VLM)-powered embodied agent. We also introduce SariBench, a dataset of annotated human demonstrations across varied task difficulties. Our sandbox enables embodied agents to navigate, inspect, and manipulate retail items, providing baselines against human performance. We conclude with benchmarks, performance analysis, and recommendations for enhancing realism and scalability. The source code can be accessed via https://github.com/upeee/sari-sandbox-env.",
    "pdf_url": "https://arxiv.org/pdf/2508.00400v1",
    "github_url": "https://github.com/upeee/sari-sandbox-env",
    "published": "2025-08-01T08:01:38+00:00",
    "updated": "2025-08-01T08:01:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.00390v1",
    "title": "SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation",
    "authors": [
      "Cai",
      "Dong",
      "Rao"
    ],
    "summary": "Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable agents to accurately localize targets and plan flight paths in complex environments based on natural language instructions, with broad applications in intelligent inspection, disaster rescue, and urban monitoring. Recent progress in Vision-Language Models (VLMs) has provided strong semantic understanding for this task, while reinforcement learning (RL) has emerged as a promising post-training strategy to further improve generalization. However, existing RL methods often suffer from inefficient use of training data, slow convergence, and insufficient consideration of the difficulty variation among training samples, which limits further performance improvement. To address these challenges, we propose \\textbf{Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS)}, a novel training framework that systematically integrates Curriculum Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator (SA-DE) to quantify the complexity of training samples and a Gaussian Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution, enabling a smooth progression from easy to challenging tasks. This design significantly improves training efficiency, accelerates convergence, and enhances overall model performance. Extensive experiments on the CityNav benchmark demonstrate that SA-GCS consistently outperforms strong baselines across all metrics, achieves faster and more stable convergence, and generalizes well across models of different scales, highlighting its robustness and scalability. The implementation of our approach is publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2508.00390v1",
    "github_url": null,
    "published": "2025-08-01T07:35:48+00:00",
    "updated": "2025-08-01T07:35:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.00288v4",
    "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents",
    "authors": [
      "Xiao",
      "Sun",
      "Shao"
    ],
    "summary": "Aerial navigation is a fundamental yet underexplored capability in embodied intelligence, enabling agents to operate in large-scale, unstructured environments where traditional navigation paradigms fall short. However, most existing research follows the Vision-and-Language Navigation (VLN) paradigm, which heavily depends on sequential linguistic instructions, limiting its scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark for large-scale Object Goal Navigation (ObjectNav) by aerial agents in open-world environments, where agents operate based on high-level semantic goals without relying on detailed instructional guidance as in VLN. UAV-ON comprises 14 high-fidelity Unreal Engine environments with diverse semantic regions and complex spatial layouts, covering urban, natural, and mixed-use settings. It defines 1270 annotated target objects, each characterized by an instance-level instruction that encodes category, physical footprint, and visual descriptors, allowing grounded reasoning. These instructions serve as semantic goals, introducing realistic ambiguity and complex reasoning challenges for aerial agents. To evaluate the benchmark, we implement several baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that integrates instruction semantics with egocentric observations for long-horizon, goal-directed exploration. Empirical results show that all baselines struggle in this setting, highlighting the compounded challenges of aerial navigation and semantic goal grounding. UAV-ON aims to advance research on scalable UAV autonomy driven by semantic goal descriptions in complex real-world environments.",
    "pdf_url": "https://arxiv.org/pdf/2508.00288v4",
    "github_url": null,
    "published": "2025-08-01T03:23:06+00:00",
    "updated": "2025-08-22T00:25:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2508.00191v1",
    "title": "State-switching navigation strategies in C. elegans are beneficial for chemotaxis",
    "authors": [
      "Chen",
      "Leifer",
      "Pillow"
    ],
    "summary": "Animals employ different strategies for relating sensory input to behavioral output to navigate sensory environments, but what strategy to use, when to switch and why remain unclear. In C. elegans, navigation is composed of 'steering' and 'turns', corresponding to small heading changes and large reorientation events, respectively. It is unclear whether transitions between these elements are driven solely by sensory input or are influenced by internal states that persist over time. It also remains unknown how worms accomplish seemingly surprising feats of navigation--for example, worms appear to exit turns correctly oriented toward a goal, despite their presumed lack of spatial awareness during the turn. Here, we resolve these questions using detailed measurements of sensory-guided navigation and a novel statistical model of state-dependent navigation. We show that the worm's navigation is well described by a sensory-driven state-switching model with two distinct states, each persisting over many seconds and producing different mixtures of sensorimotor relations. One state is enriched for steering, while the other is enriched for turning. This hierarchical, temporal organization of strategies challenges the previous assumption that strategies are static over time and driven solely by immediate sensory input. Sensory input causally drives transitions between these persistent internal states, and creates the appearance of 'directed turns.' Genetic perturbations and a data-constrained reinforcement learning model demonstrate that state-switching enhances gradient-climbing performance. By combining measurement, perturbation, and modeling, we show that state-switching plays a functionally beneficial role in organizing behavior over time--a principle likely to generalize across species and contexts.",
    "pdf_url": "https://arxiv.org/pdf/2508.00191v1",
    "github_url": null,
    "published": "2025-07-31T22:18:08+00:00",
    "updated": "2025-07-31T22:18:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.23719v1",
    "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation",
    "authors": [
      "McDonnell",
      "Meng",
      "Hariprasad"
    ],
    "summary": "The American cockroach (Periplaneta americana) uses its soft antennae to guide decision making by extracting rich tactile information from tens of thousands of distributed mechanosensors. Although tactile sensors enable robust, autonomous perception and navigation in natural systems, replicating these capabilities in insect-scale robots remains challenging due to stringent size, weight, and power constraints that limit existing sensor technologies. To overcome these limitations, we introduce CITRAS (Cockroach Inspired Tactile Robotic Antenna Sensor), a bioinspired, multi-segmented, compliant laminate sensor with embedded capacitive angle sensors. CITRAS is compact (73.7x15.6x2.1 mm), lightweight (491 mg), and low-power (32 mW), enabling seamless integration with miniature robotic platforms. The segmented compliant structure passively bends in response to environmental stimuli, achieving accurate hinge angle measurements with maximum errors of just 0.79 degree (quasistatic bending) and 3.58 degree (dynamic bending). Experimental evaluations demonstrate CITRAS' multifunctional tactile perception capabilities: predicting base-to-tip distances with 7.75 % error, estimating environmental gap widths with 6.73 % error, and distinguishing surface textures through differential sensor response. The future integration of this bioinspired tactile antenna in insect-scale robots addresses critical sensing gaps, promising enhanced autonomous exploration, obstacle avoidance, and environmental mapping in complex, confined environments.",
    "pdf_url": "https://arxiv.org/pdf/2507.23719v1",
    "github_url": null,
    "published": "2025-07-31T16:53:54+00:00",
    "updated": "2025-07-31T16:53:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.23042v2",
    "title": "Goal-Based Vision-Language Driving",
    "authors": [
      "Patapati",
      "Srinivasan"
    ],
    "summary": "Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations. We introduce NovaDrive, a single-branch vision-language architecture that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a single branch. A lightweight, two-stage cross-attention block first aligns waypoint tokens with the HD map, then refines attention over fine-grained image and depth patches. Coupled with a novel smoothness loss that discourages abrupt steering and speed changes, this design eliminates the need for recurrent memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language backbone, enabling real-time inference. On the nuScenes / Waymo subset of the MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from 2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention fusion each contribute the most to these gains. Beyond safety, NovaDrive's shorter routes (resulting from the novel smoothness loss) translate to lower fuel or battery usage, pointing toward leaner, more easily updated driving stacks. NovaDrive can be extended to other embodied-AI domains as well.",
    "pdf_url": "https://arxiv.org/pdf/2507.23042v2",
    "github_url": null,
    "published": "2025-07-30T19:12:42+00:00",
    "updated": "2025-10-13T04:53:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.22473v1",
    "title": "A Two-Stage Lightweight Framework for Efficient Land-Air Bimodal Robot Autonomous Navigation",
    "authors": [
      "Li",
      "Liu",
      "Yu"
    ],
    "summary": "Land-air bimodal robots (LABR) are gaining attention for autonomous navigation, combining high mobility from aerial vehicles with long endurance from ground vehicles. However, existing LABR navigation methods are limited by suboptimal trajectories from mapping-based approaches and the excessive computational demands of learning-based methods. To address this, we propose a two-stage lightweight framework that integrates global key points prediction with local trajectory refinement to generate efficient and reachable trajectories. In the first stage, the Global Key points Prediction Network (GKPN) was used to generate a hybrid land-air keypoint path. The GKPN includes a Sobel Perception Network (SPN) for improved obstacle detection and a Lightweight Attention Planning Network (LAPN) to improves predictive ability by capturing contextual information. In the second stage, the global path is segmented based on predicted key points and refined using a mapping-based planner to create smooth, collision-free trajectories. Experiments conducted on our LABR platform show that our framework reduces network parameters by 14\\% and energy consumption during land-air transitions by 35\\% compared to existing approaches. The framework achieves real-time navigation without GPU acceleration and enables zero-shot transfer from simulation to reality during",
    "pdf_url": "https://arxiv.org/pdf/2507.22473v1",
    "github_url": null,
    "published": "2025-07-30T08:23:26+00:00",
    "updated": "2025-07-30T08:23:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.21450v1",
    "title": "Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation",
    "authors": [
      "Chen",
      "Kang",
      "Wang"
    ],
    "summary": "Vision Language Navigation (VLN) typically requires agents to navigate to specified objects or remote regions in unknown scenes by obeying linguistic commands. Such tasks require organizing historical visual observations for linguistic grounding, which is critical for long-sequence navigational decisions. However, current agents suffer from overly detailed scene representation and ambiguous vision-language alignment, which weaken their comprehension of navigation-friendly high-level scene priors and easily lead to behaviors that violate linguistic commands. To tackle these issues, we propose a navigation policy by recursively summarizing along-the-way visual perceptions, which are adaptively aligned with commands to enhance linguistic grounding. In particular, by structurally modeling historical trajectories as compact neural grids, several Recursive Visual Imagination (RVI) techniques are proposed to motivate agents to focus on the regularity of visual transitions and semantic scene layouts, instead of dealing with misleading geometric details. Then, an Adaptive Linguistic Grounding (ALG) technique is proposed to align the learned situational memories with different linguistic components purposefully. Such fine-grained semantic matching facilitates the accurate anticipation of navigation actions and progress. Our navigation policy outperforms the state-of-the-art methods on the challenging VLN-CE and ObjectNav tasks, showing the superiority of our RVI and ALG techniques for VLN.",
    "pdf_url": "https://arxiv.org/pdf/2507.21450v1",
    "github_url": null,
    "published": "2025-07-29T02:40:07+00:00",
    "updated": "2025-07-29T02:40:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.20395v1",
    "title": "MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models",
    "authors": [
      "Einarsson"
    ],
    "summary": "As Large Language Models (LLMs) increasingly power autonomous agents in robotics and embodied AI, understanding their spatial reasoning capabilities becomes crucial for ensuring reliable real-world deployment. Despite advances in language understanding, current research lacks evaluation of how LLMs perform spatial navigation without visual cues, a fundamental requirement for agents operating with limited sensory information. This paper addresses this gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our methodology employs a function-calling interface where models navigate mazes of varying complexity ($5\\times 5$ to $15\\times 15$ grids) using only coordinate feedback and distance-to-wall information, excluding visual input to test fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across identical mazes in both English and Icelandic to assess cross-linguistic transfer of spatial abilities. Our findings reveal striking disparities: while OpenAI's O3 achieves perfect navigation for mazes up to size $30\\times 30$, other models exhibit catastrophic failure beyond $9\\times 9$ mazes, with 100% of failures attributed to excessive looping behavior where models revisit a cell at least 10 times. We document a significant performance degradation in Icelandic, with models solving mazes 3-4 sizes smaller than in English, suggesting spatial reasoning in LLMs emerges from linguistic patterns rather than language-agnostic mechanisms. These results have important implications for global deployment of LLM-powered autonomous systems, showing spatial intelligence remains fundamentally constrained by training data availability and highlighting the need for architectural innovations to achieve reliable navigation across linguistic contexts.",
    "pdf_url": "https://arxiv.org/pdf/2507.20395v1",
    "github_url": null,
    "published": "2025-07-27T19:33:45+00:00",
    "updated": "2025-07-27T19:33:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.18033v1",
    "title": "OpenNav: Open-World Navigation with Multimodal Large Language Models",
    "authors": [
      "Yuan",
      "Wang",
      "Waslander"
    ],
    "summary": "Pre-trained large language models (LLMs) have demonstrated strong common-sense reasoning abilities, making them promising for robotic navigation and planning tasks. However, despite recent progress, bridging the gap between language descriptions and actual robot actions in the open-world, beyond merely invoking limited predefined motion primitives, remains an open challenge. In this work, we aim to enable robots to interpret and decompose complex language instructions, ultimately synthesizing a sequence of trajectory points to complete diverse navigation tasks given open-set instructions and open-set objects. We observe that multi-modal large language models (MLLMs) exhibit strong cross-modal understanding when processing free-form language instructions, demonstrating robust scene comprehension. More importantly, leveraging their code-generation capability, MLLMs can interact with vision-language perception models to generate compositional 2D bird-eye-view value maps, effectively integrating semantic knowledge from MLLMs with spatial information from maps to reinforce the robot's spatial understanding. To further validate our approach, we effectively leverage large-scale autonomous vehicle datasets (AVDs) to validate our proposed zero-shot vision-language navigation framework in outdoor navigation tasks, demonstrating its capability to execute a diverse range of free-form natural language navigation instructions while maintaining robustness against object detection errors and linguistic ambiguities. Furthermore, we validate our system on a Husky robot in both indoor and outdoor scenes, demonstrating its real-world robustness and applicability. Supplementary videos are available at https://trailab.github.io/OpenNav-website/",
    "pdf_url": "https://arxiv.org/pdf/2507.18033v1",
    "github_url": null,
    "published": "2025-07-24T02:05:28+00:00",
    "updated": "2025-07-24T02:05:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.14731v2",
    "title": "X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots",
    "authors": [
      "Wang",
      "Tan",
      "Fung"
    ],
    "summary": "Existing navigation methods are primarily designed for specific robot embodiments, limiting their generalizability across diverse robot platforms. In this paper, we introduce X-Nav, a novel framework for end-to-end cross-embodiment navigation where a single unified policy can be deployed across various embodiments for both wheeled and quadrupedal robots. X-Nav consists of two learning stages: 1) multiple expert policies are trained using deep reinforcement learning with privileged observations on a wide range of randomly generated robot embodiments; and 2) a single general policy is distilled from the expert policies via navigation action chunking with transformer (Nav-ACT). The general policy directly maps visual and proprioceptive observations to low-level control commands, enabling generalization to novel robot embodiments. Simulated experiments demonstrated that X-Nav achieved zero-shot transfer to both unseen embodiments and photorealistic environments. A scalability study showed that the performance of X-Nav improves when trained with an increasing number of randomly generated embodiments. An ablation study confirmed the design choices of X-Nav. Furthermore, real-world experiments were conducted to validate the generalizability of X-Nav in real-world environments.",
    "pdf_url": "https://arxiv.org/pdf/2507.14731v2",
    "github_url": null,
    "published": "2025-07-19T19:40:53+00:00",
    "updated": "2025-11-26T17:05:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.13152v3",
    "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models",
    "authors": [
      "Dong",
      "Zhao",
      "Gao"
    ],
    "summary": "Recent advances in vision-language navigation (VLN) were mainly attributed to emerging large language models (LLMs). These methods exhibited excellent generalization capabilities in instruction understanding and task reasoning. However, they were constrained by the fixed knowledge bases and reasoning abilities of LLMs, preventing fully incorporating experiential knowledge and thus resulting in a lack of efficient evolutionary capacity. To address this, we drew inspiration from the evolution capabilities of natural agents, and proposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the ability to continuously evolve during testing. To the best of our knowledge, it was the first time that an multimodal LLM-powered self-evolving VLN framework was proposed. Specifically, SE-VLN comprised three core modules, i.e., a hierarchical memory module to transfer successful and failure cases into reusable knowledge, a retrieval-augmented thought-based reasoning module to retrieve experience and enable multi-step decision-making, and a reflection module to realize continual evolution. Comprehensive tests illustrated that the SE-VLN achieved navigation success rates of 57% and 35.2% in unseen environments, representing absolute performance improvements of 23.9% and 15.0% over current state-of-the-art methods on R2R and REVERSE datasets, respectively. Moreover, the SE-VLN showed performance improvement with increasing experience repository, elucidating its great potential as a self-evolving agent framework for VLN.",
    "pdf_url": "https://arxiv.org/pdf/2507.13152v3",
    "github_url": null,
    "published": "2025-07-17T14:13:50+00:00",
    "updated": "2025-08-26T08:52:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.13019v2",
    "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities",
    "authors": [
      "Wang",
      "Xia",
      "Zhao"
    ],
    "summary": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but their idealized assumptions about robot movement and control fail to reflect physically embodied deployment challenges. To bridge this gap, we introduce VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and wheeled robots. For the first time, we systematically evaluate several ego-centric VLN methods in physical robotic settings across different technical pipelines, including classification models for single-step discrete action prediction, a diffusion model for dense waypoint prediction, and a train-free, map-based large language model (LLM) integrated with path planning. Our results reveal significant performance degradation due to limited robot observation space, environmental lighting variations, and physical challenges like collisions and falls. This also exposes locomotion constraints for legged robots in complex environments. VLN-PE is highly extensible, allowing seamless integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN evaluation. Despite the weak generalization of current models in physical deployment, VLN-PE provides a new pathway for improving cross-embodiment's overall adaptability. We hope our findings and tools inspire the community to rethink VLN limitations and advance robust, practical VLN models. The code is available at https://crystalsixone.github.io/vln_pe.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2507.13019v2",
    "github_url": null,
    "published": "2025-07-17T11:46:00+00:00",
    "updated": "2025-09-26T07:50:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.12753v1",
    "title": "osmAG-LLM: Zero-Shot Open-Vocabulary Object Navigation via Semantic Maps and Large Language Models Reasoning",
    "authors": [
      "Xie",
      "Schwertfeger",
      "Blum"
    ],
    "summary": "Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features, achieving a high level of detail and guiding robots to find objects specified by open-vocabulary language queries. While the issue of scalability for such approaches has received some attention, another fundamental problem is that high-detail object mapping quickly becomes outdated, as objects get moved around a lot. In this work, we develop a mapping and navigation system for object-goal navigation that, from the ground up, considers the possibilities that a queried object can have moved, or may not be mapped at all. Instead of striving for high-fidelity mapping detail, we consider that the main purpose of a map is to provide environment grounding and context, which we combine with the semantic priors of LLMs to reason about object locations and deploy an active, online approach to navigate to the objects. Through simulated and real-world experiments we find that our approach tends to have higher retrieval success at shorter path lengths for static objects and by far outperforms prior approaches in cases of dynamic or unmapped object queries. We provide our code and dataset at: https://anonymous.4open.science/r/osmAG-LLM.",
    "pdf_url": "https://arxiv.org/pdf/2507.12753v1",
    "github_url": null,
    "published": "2025-07-17T03:14:37+00:00",
    "updated": "2025-07-17T03:14:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.11464v1",
    "title": "LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control",
    "authors": [
      "Shankar",
      "Okumura",
      "Prorok"
    ],
    "summary": "We propose a multi-robot control paradigm to solve point-to-point navigation tasks for a team of holonomic robots with access to the full environment information. The framework invokes two processes asynchronously at high frequency: (i) a centralized, discrete, and full-horizon planner for computing collision- and deadlock-free paths rapidly, leveraging recent advances in multi-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal trajectory controllers that ensure all robots independently follow their assigned paths reliably. This hierarchical shift in planning representation from (i) discrete and coupled to (ii) continuous and decoupled domains enables the framework to maintain long-term scalable motion synthesis. As an instantiation of this idea, we present LF, which combines a fast state-of-the-art MAPF solver (LaCAM), and a robust feedback control stack (Freyja) for executing agile robot maneuvers. LF provides a robust and versatile mechanism for lifelong multi-robot navigation even under asynchronous and partial goal updates, and adapts to dynamic workspaces simply by quick replanning. We present various multirotor and ground robot demonstrations, including the deployment of 15 real multirotors with random, consecutive target updates while a person walks through the operational workspace.",
    "pdf_url": "https://arxiv.org/pdf/2507.11464v1",
    "github_url": null,
    "published": "2025-07-15T16:35:37+00:00",
    "updated": "2025-07-15T16:35:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.10894v1",
    "title": "NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization",
    "authors": [
      "He",
      "Wang",
      "Chen"
    ],
    "summary": "Language-guided navigation is a cornerstone of embodied AI, enabling agents to interpret language instructions and navigate complex environments. However, expert-provided instructions are limited in quantity, while synthesized annotations often lack quality, making them insufficient for large-scale research. To address this, we propose NavComposer, a novel framework for automatically generating high-quality navigation instructions. NavComposer explicitly decomposes semantic entities such as actions, scenes, and objects, and recomposes them into natural language instructions. Its modular architecture allows flexible integration of state-of-the-art techniques, while the explicit use of semantic entities enhances both the richness and accuracy of instructions. Moreover, it operates in a data-agnostic manner, supporting adaptation to diverse navigation trajectories without domain-specific training. Complementing NavComposer, we introduce NavInstrCritic, a comprehensive annotation-free evaluation system that assesses navigation instructions on three dimensions: contrastive matching, semantic consistency, and linguistic diversity. NavInstrCritic provides a holistic evaluation of instruction quality, addressing limitations of traditional metrics that rely heavily on expert annotations. By decoupling instruction generation and evaluation from specific navigation agents, our method enables more scalable and generalizable research. Extensive experiments provide direct and practical evidence for the effectiveness of our method.",
    "pdf_url": "https://arxiv.org/pdf/2507.10894v1",
    "github_url": null,
    "published": "2025-07-15T01:20:22+00:00",
    "updated": "2025-07-15T01:20:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.07299v2",
    "title": "MLFM: Multi-Layered Feature Maps for Richer Language Understanding in Zero-Shot Semantic Navigation",
    "authors": [
      "Raychaudhuri",
      "Cancelli",
      "Campari"
    ],
    "summary": "Recent progress in large vision-language models has driven improvements in language-based semantic navigation, where an embodied agent must reach a target object described in natural language. Yet we still lack a clear, language-focused evaluation framework to test how well agents ground the words in their instructions. We address this gap by proposing LangNav, an open-vocabulary multi-object navigation dataset with natural language goal descriptions (e.g. 'go to the red short candle on the table') and corresponding fine-grained linguistic annotations (e.g., attributes: color=red, size=short; relations: support=on). These labels enable systematic evaluation of language understanding. To evaluate on this setting, we extend multi-object navigation task setting to Language-guided Multi-Object Navigation (LaMoN), where the agent must find a sequence of goals specified using language. Furthermore, we propose Multi-Layered Feature Map (MLFM), a novel method that builds a queryable, multi-layered semantic map from pretrained vision-language features and proves effective for reasoning over fine-grained attributes and spatial relations in goal descriptions. Experiments on LangNav show that MLFM outperforms state-of-the-art zero-shot mapping-based navigation baselines.",
    "pdf_url": "https://arxiv.org/pdf/2507.07299v2",
    "github_url": null,
    "published": "2025-07-09T21:46:43+00:00",
    "updated": "2025-10-17T00:58:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.06747v1",
    "title": "LOVON: Legged Open-Vocabulary Object Navigator",
    "authors": [
      "Peng",
      "Cao",
      "Zhang"
    ],
    "summary": "Object navigation in open-world environments remains a formidable and pervasive challenge for robotic systems, particularly when it comes to executing long-horizon tasks that require both open-world object detection and high-level task planning. Traditional methods often struggle to integrate these components effectively, and this limits their capability to deal with complex, long-range navigation missions. In this paper, we propose LOVON, a novel framework that integrates large language models (LLMs) for hierarchical task planning with open-vocabulary visual detection models, tailored for effective long-range object navigation in dynamic, unstructured environments. To tackle real-world challenges including visual jittering, blind zones, and temporary target loss, we design dedicated solutions such as Laplacian Variance Filtering for visual stabilization. We also develop a functional execution logic for the robot that guarantees LOVON's capabilities in autonomous navigation, task adaptation, and robust task completion. Extensive evaluations demonstrate the successful completion of long-sequence tasks involving real-time detection, search, and navigation toward open-vocabulary dynamic targets. Furthermore, real-world experiments across different legged robots (Unitree Go2, B2, and H1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.",
    "pdf_url": "https://arxiv.org/pdf/2507.06747v1",
    "github_url": null,
    "published": "2025-07-09T11:02:46+00:00",
    "updated": "2025-07-09T11:02:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.06719v1",
    "title": "A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding",
    "authors": [
      "Liu",
      "Zheng",
      "Chen"
    ],
    "summary": "Open-vocabulary 3D visual grounding aims to localize target objects based on free-form language queries, which is crucial for embodied AI applications such as autonomous navigation, robotics, and augmented reality. Learning 3D language fields through neural representations enables accurate understanding of 3D scenes from limited viewpoints and facilitates the localization of target objects in complex environments. However, existing language field methods struggle to accurately localize instances using spatial relations in language queries, such as ``the book on the chair.'' This limitation mainly arises from inadequate reasoning about spatial relations in both language queries and 3D scenes. In this work, we propose SpatialReasoner, a novel neural representation-based framework with large language model (LLM)-driven spatial reasoning that constructs a visual properties-enhanced hierarchical feature field for open-vocabulary 3D visual grounding. To enable spatial reasoning in language queries, SpatialReasoner fine-tunes an LLM to capture spatial relations and explicitly infer instructions for the target, anchor, and spatial relation. To enable spatial reasoning in 3D scenes, SpatialReasoner incorporates visual properties (opacity and color) to construct a hierarchical feature field. This field represents language and instance features using distilled CLIP features and masks extracted via the Segment Anything Model (SAM). The field is then queried using the inferred instructions in a hierarchical manner to localize the target 3D instance based on the spatial relation in the language query. Extensive experiments show that our framework can be seamlessly integrated into different neural representations, outperforming baseline models in 3D visual grounding while empowering their spatial reasoning capability.",
    "pdf_url": "https://arxiv.org/pdf/2507.06719v1",
    "github_url": null,
    "published": "2025-07-09T10:20:38+00:00",
    "updated": "2025-07-09T10:20:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.06564v1",
    "title": "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments",
    "authors": [
      "Li",
      "Huai",
      "Li"
    ],
    "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across various sectors, driven by their mobility and adaptability. This paper introduces SkyVLN, a novel framework integrating vision-and-language navigation (VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in complex urban environments. Unlike traditional navigation methods, SkyVLN leverages Large Language Models (LLMs) to interpret natural language instructions and visual observations, enabling UAVs to navigate through dynamic 3D spaces with improved accuracy and robustness. We present a multimodal navigation agent equipped with a fine-grained spatial verbalizer and a history path memory mechanism. These components allow the UAV to disambiguate spatial contexts, handle ambiguous instructions, and backtrack when necessary. The framework also incorporates an NMPC module for dynamic obstacle avoidance, ensuring precise trajectory tracking and collision prevention. To validate our approach, we developed a high-fidelity 3D urban simulation environment using AirSim, featuring realistic imagery and dynamic urban elements. Extensive experiments demonstrate that SkyVLN significantly improves navigation success rates and efficiency, particularly in new and unseen environments.",
    "pdf_url": "https://arxiv.org/pdf/2507.06564v1",
    "github_url": null,
    "published": "2025-07-09T05:38:32+00:00",
    "updated": "2025-07-09T05:38:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.06208v3",
    "title": "Ontological differentiation as a measure of semantic accuracy",
    "authors": [
      "Garcia-Cuadrillero",
      "Revuelta",
      "Capitan"
    ],
    "summary": "Understanding semantic relationships within complex networks derived from lexical resources is fundamental for network science and language modeling. While network embedding methods capture contextual similarity, quantifying semantic distance based directly on explicit definitional structure remains challenging. Accurate measures of semantic similarity allow for navigation on lexical networks based on maximizing semantic similarity in each navigation jump (Semantic Navigation, SN). This work introduces Ontological Differentiation (OD), a formal method for measuring divergence between concepts by analyzing overlap during recursive definition expansion. The methodology is applied to networks extracted from the Simple English Wiktionary, comparing OD scores with other measures of semantic similarity proposed in the literature (cosine similarity based on random-walk network exploration). We find weak correlations between direct pairwise OD scores and cosine similarities across $\\sim$~2 million word pairs, sampled from a pool representing over 50\\% of the entries in the Wiktionary lexicon. This establishes OD as a largely independent, definition-based semantic metric, whose orthogonality to cosine similarity becomes more pronounced when low-semantic-content terms were removed from the dataset. Additionally, we use cumulative OD scores to evaluate paths generated by vector-based SN and structurally optimal Shortest Paths (SP) across networks. We find SN paths consistently exhibit significantly lower cumulative OD scores than shortest paths, suggesting that SN produces trajectories more coherent with the dictionary's definitional structure, as measured by OD. Ontological Differentiation thus provides a novel, definition-grounded tool for analyzing, validating, and potentially constructing navigation processes in lexical networks.",
    "pdf_url": "https://arxiv.org/pdf/2507.06208v3",
    "github_url": null,
    "published": "2025-07-08T17:37:29+00:00",
    "updated": "2025-11-03T20:07:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.05240v1",
    "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling",
    "authors": [
      "Wei",
      "Wan",
      "Yu"
    ],
    "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
    "pdf_url": "https://arxiv.org/pdf/2507.05240v1",
    "github_url": null,
    "published": "2025-07-07T17:49:41+00:00",
    "updated": "2025-07-07T17:49:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.04430v1",
    "title": "\"Hi AirStar, Guide Me to the Badminton Court.\"",
    "authors": [
      "Wang",
      "Chen",
      "Zheng"
    ],
    "summary": "Unmanned Aerial Vehicles, operating in environments with relatively few obstacles, offer high maneuverability and full three-dimensional mobility. This allows them to rapidly approach objects and perform a wide range of tasks often challenging for ground robots, making them ideal for exploration, inspection, aerial imaging, and everyday assistance. In this paper, we introduce AirStar, a UAV-centric embodied platform that turns a UAV into an intelligent aerial assistant: a large language model acts as the cognitive core for environmental understanding, contextual reasoning, and task planning. AirStar accepts natural interaction through voice commands and gestures, removing the need for a remote controller and significantly broadening its user base. It combines geospatial knowledge-driven long-distance navigation with contextual reasoning for fine-grained short-range control, resulting in an efficient and accurate vision-and-language navigation (VLN) capability.Furthermore, the system also offers built-in capabilities such as cross-modal question answering, intelligent filming, and target tracking. With a highly extensible framework, it supports seamless integration of new functionalities, paving the way toward a general-purpose, instruction-driven intelligent UAV agent. The supplementary PPT is available at \\href{https://buaa-colalab.github.io/airstar.github.io}{https://buaa-colalab.github.io/airstar.github.io}.",
    "pdf_url": "https://arxiv.org/pdf/2507.04430v1",
    "github_url": null,
    "published": "2025-07-06T15:29:07+00:00",
    "updated": "2025-07-06T15:29:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.08831v2",
    "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments",
    "authors": [
      "Sun",
      "Xing",
      "Weng"
    ],
    "summary": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent follows instructions and moves freely to reach a destination, is a key research problem in embodied AI. However, most navigation policies are sensitive to viewpoint changes, i.e., variations in camera height and viewing angle that alter the agent's observation. In this paper, we introduce a generalized scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View Invariant Learning), a view-invariant post-training strategy that enhances the robustness of existing navigation policies to changes in camera viewpoint. VIL employs a contrastive learning framework to learn sparse and view-invariant features. Additionally, we introduce a teacher-student framework for the Waypoint Predictor Module, a core component of most VLNCE baselines, where a view-dependent teacher model distills knowledge into a view-invariant student model. We employ an end-to-end training paradigm to jointly optimize these components, thus eliminating the cost for individual module training. Empirical results show that our method outperforms state-of-the-art approaches on V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE setting and find that, despite being trained for varied viewpoints, it often still improves performance. On the more challenging RxR-CE dataset, our method also achieved state-of-the-art performance across all metrics when compared to other map-free methods. This suggests that adding VIL does not diminish the standard viewpoint performance and can serve as a plug-and-play post-training method.",
    "pdf_url": "https://arxiv.org/pdf/2507.08831v2",
    "github_url": null,
    "published": "2025-07-05T18:04:35+00:00",
    "updated": "2025-07-15T01:49:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.08831v3",
    "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments",
    "authors": [
      "Sun",
      "Xing",
      "Weng"
    ],
    "summary": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent follows instructions and moves freely to reach a destination, is a key research problem in embodied AI. However, most navigation policies are sensitive to viewpoint changes, i.e., variations in camera height and viewing angle that alter the agent's observation. In this paper, we introduce a generalized scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View Invariant Learning), a view-invariant post-training strategy that enhances the robustness of existing navigation policies to changes in camera viewpoint. VIL employs a contrastive learning framework to learn sparse and view-invariant features. Additionally, we introduce a teacher-student framework for the Waypoint Predictor Module, a core component of most VLNCE baselines, where a view-dependent teacher model distills knowledge into a view-invariant student model. We employ an end-to-end training paradigm to jointly optimize these components, thus eliminating the cost for individual module training. Empirical results show that our method outperforms state-of-the-art approaches on V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE setting and find that, despite being trained for varied viewpoints, it often still improves performance. On the more challenging RxR-CE dataset, our method also achieved state-of-the-art performance across all metrics when compared to other map-free methods. This suggests that adding VIL does not diminish the standard viewpoint performance and can serve as a plug-and-play post-training method.",
    "pdf_url": "https://arxiv.org/pdf/2507.08831v3",
    "github_url": null,
    "published": "2025-07-05T18:04:35+00:00",
    "updated": "2026-02-18T17:20:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.04086v1",
    "title": "Are Learning-Based Approaches Ready for Real-World Indoor Navigation? A Case for Imitation Learning",
    "authors": [
      "Selvaraj",
      "Mitrevski",
      "Houben"
    ],
    "summary": "Traditional indoor robot navigation methods provide a reliable solution when adapted to constrained scenarios, but lack flexibility or require manual re-tuning when deployed in more complex settings. In contrast, learning-based approaches learn directly from sensor data and environmental interactions, enabling easier adaptability. While significant work has been presented in the context of learning navigation policies, learning-based methods are rarely compared to traditional navigation methods directly, which is a problem for their ultimate acceptance in general navigation contexts. In this work, we explore the viability of imitation learning (IL) for indoor navigation, using expert (joystick) demonstrations to train various navigation policy networks based on RGB images, LiDAR, and a combination of both, and we compare our IL approach to a traditional potential field-based navigation method. We evaluate the approach on a physical mobile robot platform equipped with a 2D LiDAR and a camera in an indoor university environment. Our multimodal model demonstrates superior navigation capabilities in most scenarios, but faces challenges in dynamic environments, likely due to limited diversity in the demonstrations. Nevertheless, the ability to learn directly from data and generalise across layouts suggests that IL can be a practical navigation approach, and potentially a useful initialisation strategy for subsequent lifelong learning.",
    "pdf_url": "https://arxiv.org/pdf/2507.04086v1",
    "github_url": null,
    "published": "2025-07-05T16:18:48+00:00",
    "updated": "2025-07-05T16:18:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.04047v2",
    "title": "Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation",
    "authors": [
      "Zhu",
      "Wang",
      "Li"
    ],
    "summary": "Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce \\underline{\\textbf{M}}ove \\underline{\\textbf{t}}o \\underline{\\textbf{U}}nderstand (\\textbf{\\model}), a unified framework that integrates active perception with \\underline{\\textbf{3D}} vision-language learning, enabling embodied agents to effectively explore and understand their environment. This is achieved by three key innovations: 1) Online query-based representation learning, enabling direct spatial memory construction from RGB-D frames, eliminating the need for explicit 3D reconstruction. 2) A unified objective for grounding and exploring, which represents unexplored locations as frontier queries and jointly optimizes object grounding and frontier selection. 3) End-to-end trajectory learning that combines \\textbf{V}ision-\\textbf{L}anguage-\\textbf{E}xploration pre-training over a million diverse trajectories collected from both simulated and real-world RGB-D sequences. Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14\\%, 23\\%, 9\\%, and 2\\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. \\model's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.",
    "pdf_url": "https://arxiv.org/pdf/2507.04047v2",
    "github_url": null,
    "published": "2025-07-05T14:15:52+00:00",
    "updated": "2025-07-30T11:32:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.01667v1",
    "title": "What does really matter in image goal navigation?",
    "authors": [
      "Monaci",
      "Weinzaepfel",
      "Wolf"
    ],
    "summary": "Image goal navigation requires two different skills: firstly, core navigation skills, including the detection of free space and obstacles, and taking decisions based on an internal representation; and secondly, computing directional information by comparing visual observations to the goal image. Current state-of-the-art methods either rely on dedicated image-matching, or pre-training of computer vision modules on relative pose estimation. In this paper, we study whether this task can be efficiently solved with end-to-end training of full agents with RL, as has been claimed by recent work. A positive answer would have impact beyond Embodied AI and allow training of relative pose estimation from reward for navigation alone. In a large study we investigate the effect of architectural choices like late fusion, channel stacking, space-to-depth projections and cross-attention, and their role in the emergence of relative pose estimators from navigation training. We show that the success of recent methods is influenced up to a certain extent by simulator settings, leading to shortcuts in simulation. However, we also show that these capabilities can be transferred to more realistic setting, up to some extend. We also find evidence for correlations between navigation performance and probed (emerging) relative pose estimation performance, an important sub skill.",
    "pdf_url": "https://arxiv.org/pdf/2507.01667v1",
    "github_url": null,
    "published": "2025-07-02T12:50:26+00:00",
    "updated": "2025-07-02T12:50:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2507.01125v1",
    "title": "VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online Semantic Gaussian Splatting",
    "authors": [
      "Nagami",
      "Chen",
      "Yu"
    ],
    "summary": "We present VISTA (Viewpoint-based Image selection with Semantic Task Awareness), an active exploration method for robots to plan informative trajectories that improve 3D map quality in areas most relevant for task completion. Given an open-vocabulary search instruction (e.g., \"find a person\"), VISTA enables a robot to explore its environment to search for the object of interest, while simultaneously building a real-time semantic 3D Gaussian Splatting reconstruction of the scene. The robot navigates its environment by planning receding-horizon trajectories that prioritize semantic similarity to the query and exploration of unseen regions of the environment. To evaluate trajectories, VISTA introduces a novel, efficient viewpoint-semantic coverage metric that quantifies both the geometric view diversity and task relevance in the 3D scene. On static datasets, our coverage metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in computation speed and reconstruction quality. In quadrotor hardware experiments, VISTA achieves 6x higher success rates in challenging maps, compared to baseline methods, while matching baseline performance in less challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying it on a quadrotor drone and a Spot quadruped robot. Open-source code will be released upon acceptance of the paper.",
    "pdf_url": "https://arxiv.org/pdf/2507.01125v1",
    "github_url": null,
    "published": "2025-07-01T18:35:05+00:00",
    "updated": "2025-07-01T18:35:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.23468v2",
    "title": "NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments",
    "authors": [
      "Yao",
      "Gao",
      "Xu"
    ],
    "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to execute sequential navigation actions in complex environments guided by natural language instructions. Current approaches often struggle with generalizing to novel environments and adapting to ongoing changes during navigation. Inspired by human cognition, we present NavMorph, a self-evolving world model framework that enhances environmental understanding and decision-making in VLN-CE tasks. NavMorph employs compact latent representations to model environmental dynamics, equipping agents with foresight for adaptive planning and policy refinement. By integrating a novel Contextual Evolution Memory, NavMorph leverages scene-contextual information to support effective navigation while maintaining online adaptability. Extensive experiments demonstrate that our method achieves notable performance improvements on popular VLN-CE benchmarks. Code is available at https://github.com/Feliciaxyao/NavMorph.",
    "pdf_url": "https://arxiv.org/pdf/2506.23468v2",
    "github_url": "https://github.com/Feliciaxyao/NavMorph",
    "published": "2025-06-30T02:20:00+00:00",
    "updated": "2025-07-22T03:03:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.20566v1",
    "title": "HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction",
    "authors": [
      "Shi",
      "Zhao",
      "Dennler"
    ],
    "summary": "Real-time human perception is crucial for effective human-robot interaction (HRI). Large vision-language models (VLMs) offer promising generalizable perceptual capabilities but often suffer from high latency, which negatively impacts user experience and limits VLM applicability in real-world scenarios. To systematically study VLM capabilities in human perception for HRI and performance-latency trade-offs, we introduce HRIBench, a visual question-answering (VQA) benchmark designed to evaluate VLMs across a diverse set of human perceptual tasks critical for HRI. HRIBench covers five key domains: (1) non-verbal cue understanding, (2) verbal instruction understanding, (3) human-robot object relationship understanding, (4) social navigation, and (5) person identification. To construct HRIBench, we collected data from real-world HRI environments to curate questions for non-verbal cue understanding, and leveraged publicly available datasets for the remaining four domains. We curated 200 VQA questions for each domain, resulting in a total of 1000 questions for HRIBench. We then conducted a comprehensive evaluation of both state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench. Our results show that, despite their generalizability, current VLMs still struggle with core perceptual capabilities essential for HRI. Moreover, none of the models within our experiments demonstrated a satisfactory performance-latency trade-off suitable for real-time deployment, underscoring the need for future research on developing smaller, low-latency VLMs with improved human perception capabilities. HRIBench and our results can be found in this Github repository: https://github.com/interaction-lab/HRIBench.",
    "pdf_url": "https://arxiv.org/pdf/2506.20566v1",
    "github_url": "https://github.com/interaction-lab/HRIBench",
    "published": "2025-06-25T16:01:38+00:00",
    "updated": "2025-06-25T16:01:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.20376v1",
    "title": "Enhanced Robotic Navigation in Deformable Environments using Learning from Demonstration and Dynamic Modulation",
    "authors": [
      "Chen",
      "Zhao",
      "Campanha"
    ],
    "summary": "This paper presents a novel approach for robot navigation in environments containing deformable obstacles. By integrating Learning from Demonstration (LfD) with Dynamical Systems (DS), we enable adaptive and efficient navigation in complex environments where obstacles consist of both soft and hard regions. We introduce a dynamic modulation matrix within the DS framework, allowing the system to distinguish between traversable soft regions and impassable hard areas in real-time, ensuring safe and flexible trajectory planning. We validate our method through extensive simulations and robot experiments, demonstrating its ability to navigate deformable environments. Additionally, the approach provides control over both trajectory and velocity when interacting with deformable objects, including at intersections, while maintaining adherence to the original DS trajectory and dynamically adapting to obstacles for smooth and reliable navigation.",
    "pdf_url": "https://arxiv.org/pdf/2506.20376v1",
    "github_url": null,
    "published": "2025-06-25T12:40:27+00:00",
    "updated": "2025-06-25T12:40:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.20320v2",
    "title": "Finding the Easy Way Through -- the Probabilistic Gap Planner for Social Robot Navigation",
    "authors": [
      "Probst",
      "Wenzel",
      "Puphal"
    ],
    "summary": "In Social Robot Navigation, autonomous agents need to resolve many sequential interactions with other agents. State-of-the art planners can efficiently resolve the next, imminent interaction cooperatively and do not focus on longer planning horizons. This makes it hard to maneuver scenarios where the agent needs to select a good strategy to find gaps or channels in the crowd. We propose to decompose trajectory planning into two separate steps: Conflict avoidance for finding good, macroscopic trajectories, and cooperative collision avoidance (CCA) for resolving the next interaction optimally. We propose the Probabilistic Gap Planner (PGP) as a conflict avoidance planner. PGP modifies an established probabilistic collision risk model to include a general assumption of cooperativity. PGP biases the short-term CCA planner to head towards gaps in the crowd. In extensive simulations with crowds of varying density, we show that using PGP in addition to state-of-the-art CCA planners improves the agents' performance: On average, agents keep more space to others, create less tension, and cause fewer collisions. This typically comes at the expense of slightly longer paths. PGP runs in real-time on WaPOCHI mobile robot by Honda R&D.",
    "pdf_url": "https://arxiv.org/pdf/2506.20320v2",
    "github_url": null,
    "published": "2025-06-25T11:01:51+00:00",
    "updated": "2025-06-26T06:26:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.19433v2",
    "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System",
    "authors": [
      "He",
      "Dong",
      "Chen"
    ],
    "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.",
    "pdf_url": "https://arxiv.org/pdf/2506.19433v2",
    "github_url": "https://github.com/tsinghua-fib-lab/Mem4Nav",
    "published": "2025-06-24T09:00:43+00:00",
    "updated": "2025-10-10T13:08:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.17960v2",
    "title": "GeNIE: A Generalizable Navigation System for In-the-Wild Environments",
    "authors": [
      "Wang",
      "Liu",
      "Chen"
    ],
    "summary": "Reliable navigation in unstructured, real-world environments remains a significant challenge for embodied agents, especially when operating across diverse terrains, weather conditions, and sensor configurations. In this paper, we introduce GeNIE (Generalizable Navigation System for In-the-Wild Environments), a robust navigation framework designed for global deployment. GeNIE integrates a generalizable traversability prediction model built on SAM2 with a novel path fusion strategy that enhances planning stability in noisy and ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at ICRA 2025, where it was evaluated across six countries spanning three continents. GeNIE took first place and achieved 79% of the maximum possible score, outperforming the second-best team by 17%, and completed the entire competition without a single human intervention. These results set a new benchmark for robust, generalizable outdoor robot navigation. We will release the codebase, pretrained model weights, and newly curated datasets to support future research in real-world navigation.",
    "pdf_url": "https://arxiv.org/pdf/2506.17960v2",
    "github_url": null,
    "published": "2025-06-22T09:36:05+00:00",
    "updated": "2025-10-18T11:13:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.17221v2",
    "title": "VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning",
    "authors": [
      "Qi",
      "Zhang",
      "Yu"
    ],
    "summary": "Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation systems operate on discrete topological graphs, limiting path planning to predefined node connections. We propose VLN-R1, an end-to-end framework that leverages Large Vision-Language Models (LVLM) to directly translate egocentric video streams into continuous navigation actions, adopting GRPO-based training inspired by DeepSeek-R1. To enable effective training, we first construct the VLN-Ego dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling to balance historical and current observations. While large language models can supervise complete textual instructions, they lack fine-grained action-level control. Our framework employs a two-stage training approach: a) Supervised fine-tuning (SFT) to align the model's action sequence text predictions with expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced with a Time-Decayed Reward (TDR) mechanism that strategically weights multi-step future actions. Experimental results show VLN-R1 achieves strong performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied navigation and enhance task-specific reasoning through data-efficient, reward-driven post-training.",
    "pdf_url": "https://arxiv.org/pdf/2506.17221v2",
    "github_url": null,
    "published": "2025-06-20T17:59:59+00:00",
    "updated": "2025-06-25T06:03:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.16623v1",
    "title": "History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation",
    "authors": [
      "Habibpour",
      "Afghah"
    ],
    "summary": "Object Goal Navigation (ObjectNav) challenges robots to find objects in unseen environments, demanding sophisticated reasoning. While Vision-Language Models (VLMs) show potential, current ObjectNav methods often employ them superficially, primarily using vision-language embeddings for object-scene similarity checks rather than leveraging deeper reasoning. This limits contextual understanding and leads to practical issues like repetitive navigation behaviors. This paper introduces a novel zero-shot ObjectNav framework that pioneers the use of dynamic, history-aware prompting to more deeply integrate VLM reasoning into frontier-based exploration. Our core innovation lies in providing the VLM with action history context, enabling it to generate semantic guidance scores for navigation actions while actively avoiding decision loops. We also introduce a VLM-assisted waypoint generation mechanism for refining the final approach to detected objects. Evaluated on the HM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and 24.8% Success weighted by Path Length (SPL). These results are comparable to state-of-the-art zero-shot methods, demonstrating the significant potential of our history-augmented VLM prompting strategy for more robust and context-aware robotic navigation.",
    "pdf_url": "https://arxiv.org/pdf/2506.16623v1",
    "github_url": null,
    "published": "2025-06-19T21:50:16+00:00",
    "updated": "2025-06-19T21:50:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.15377v1",
    "title": "Efficient and Generalizable Environmental Understanding for Visual Navigation",
    "authors": [
      "Wang",
      "Li",
      "Wang"
    ],
    "summary": "Visual Navigation is a core task in Embodied AI, enabling agents to navigate complex environments toward given objectives. Across diverse settings within Navigation tasks, many necessitate the modelling of sequential data accumulated from preceding time steps. While existing methods perform well, they typically process all historical observations simultaneously, overlooking the internal association structure within the data, which may limit the potential for further improvements in task performance. We address this by examining the unique characteristics of Navigation tasks through the lens of causality, introducing a causal framework to highlight the limitations of conventional sequential methods. Leveraging this insight, we propose Causality-Aware Navigation (CAN), which incorporates a Causal Understanding Module to enhance the agent's environmental understanding capability. Empirical evaluations show that our approach consistently outperforms baselines across various tasks and simulation environments. Extensive ablations studies attribute these gains to the Causal Understanding Module, which generalizes effectively in both Reinforcement and Supervised Learning settings without computational overhead.",
    "pdf_url": "https://arxiv.org/pdf/2506.15377v1",
    "github_url": null,
    "published": "2025-06-18T11:47:02+00:00",
    "updated": "2025-06-18T11:47:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.15757v1",
    "title": "Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation",
    "authors": [
      "Wang",
      "Yu",
      "Wu"
    ],
    "summary": "Visual Language Navigation (VLN) is a fundamental task within the field of Embodied AI, focusing on the ability of agents to navigate complex environments based on natural language instructions. Despite the progress made by existing methods, these methods often present some common challenges. First, they rely on pre-trained backbone models for visual perception, which struggle with the dynamic viewpoints in VLN scenarios. Second, the performance is limited when using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results, their computational costs are higher than those without fine-tuning. To address these limitations, we propose Weakly-supervised Partial Contrastive Learning (WPCL), a method that enhances an agent's ability to identify objects from dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM knowledge into the perception process, without requiring VLM fine-tuning. Our method enhances the agent's ability to interpret and respond to environmental cues while ensuring computational efficiency. Experimental results have shown that our method outperforms the baseline methods on multiple benchmarks, which validate the effectiveness, robustness and generalizability of our method.",
    "pdf_url": "https://arxiv.org/pdf/2506.15757v1",
    "github_url": null,
    "published": "2025-06-18T11:43:50+00:00",
    "updated": "2025-06-18T11:43:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.15096v1",
    "title": "DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory",
    "authors": [
      "Ji",
      "Lin",
      "Gao"
    ],
    "summary": "We present DyNaVLM, an end-to-end vision-language navigation framework using Vision-Language Models (VLM). In contrast to prior methods constrained by fixed angular or distance intervals, our system empowers agents to freely select navigation targets via visual-language reasoning. At its core lies a self-refining graph memory that 1) stores object locations as executable topological relations, 2) enables cross-robot memory sharing through distributed graph updates, and 3) enhances VLM's decision-making via retrieval augmentation. Operating without task-specific training or fine-tuning, DyNaVLM demonstrates high performance on GOAT and ObjectNav benchmarks. Real-world tests further validate its robustness and generalization. The system's three innovations: dynamic action space formulation, collaborative graph memory, and training-free deployment, establish a new paradigm for scalable embodied robot, bridging the gap between discrete VLN tasks and continuous real-world navigation.",
    "pdf_url": "https://arxiv.org/pdf/2506.15096v1",
    "github_url": null,
    "published": "2025-06-18T03:06:01+00:00",
    "updated": "2025-06-18T03:06:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.14507v1",
    "title": "Can Pretrained Vision-Language Embeddings Alone Guide Robot Navigation?",
    "authors": [
      "Subedi",
      "Haroon",
      "Ganguly"
    ],
    "summary": "Foundation models have revolutionized robotics by providing rich semantic representations without task-specific training. While many approaches integrate pretrained vision-language models (VLMs) with specialized navigation architectures, the fundamental question remains: can these pretrained embeddings alone successfully guide navigation without additional fine-tuning or specialized modules? We present a minimalist framework that decouples this question by training a behavior cloning policy directly on frozen vision-language embeddings from demonstrations collected by a privileged expert. Our approach achieves a 74% success rate in navigation to language-specified targets, compared to 100% for the state-aware expert, though requiring 3.2 times more steps on average. This performance gap reveals that pretrained embeddings effectively support basic language grounding but struggle with long-horizon planning and spatial reasoning. By providing this empirical baseline, we highlight both the capabilities and limitations of using foundation models as drop-in representations for embodied tasks, offering critical insights for robotics researchers facing practical design tradeoffs between system complexity and performance in resource-constrained scenarios. Our code is available at https://github.com/oadamharoon/text2nav",
    "pdf_url": "https://arxiv.org/pdf/2506.14507v1",
    "github_url": "https://github.com/oadamharoon/text2nav",
    "published": "2025-06-17T13:31:05+00:00",
    "updated": "2025-06-17T13:31:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.13953v1",
    "title": "Socially-aware Object Transportation by a Mobile Manipulator in Static Planar Environments with Obstacles",
    "authors": [
      "Ribeiro",
      "Paes",
      "Macharet"
    ],
    "summary": "Socially-aware robotic navigation is essential in environments where humans and robots coexist, ensuring both safety and comfort. However, most existing approaches have been primarily developed for mobile robots, leaving a significant gap in research that addresses the unique challenges posed by mobile manipulators. In this paper, we tackle the challenge of navigating a robotic mobile manipulator, carrying a non-negligible load, within a static human-populated environment while adhering to social norms. Our goal is to develop a method that enables the robot to simultaneously manipulate an object and navigate between locations in a socially-aware manner. We propose an approach based on the Risk-RRT* framework that enables the coordinated actuation of both the mobile base and manipulator. This approach ensures collision-free navigation while adhering to human social preferences. We compared our approach in a simulated environment to socially-aware mobile-only methods applied to a mobile manipulator. The results highlight the necessity for mobile manipulator-specific techniques, with our method outperforming mobile-only approaches. Our method enabled the robot to navigate, transport an object, avoid collisions, and minimize social discomfort effectively.",
    "pdf_url": "https://arxiv.org/pdf/2506.13953v1",
    "github_url": null,
    "published": "2025-06-16T19:45:30+00:00",
    "updated": "2025-06-16T19:45:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.14831v2",
    "title": "Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review",
    "authors": [
      "Finet",
      "Martins",
      "Hayet"
    ],
    "summary": "With the emergence of powerful data-driven methods in human trajectory prediction (HTP), gaining a finer understanding of multi-agent interactions lies within hand's reach, with important implications in areas such as social robot navigation, autonomous navigation, and crowd modeling. This survey reviews some of the most recent advancements in deep learning-based multi-agent trajectory prediction, focusing on studies published between 2020 and 2025. We categorize the existing methods based on their architectural design, their input representations, and their overall prediction strategies, placing a particular emphasis on models evaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges and future research directions in the field of multi-agent HTP.",
    "pdf_url": "https://arxiv.org/pdf/2506.14831v2",
    "github_url": null,
    "published": "2025-06-13T23:03:43+00:00",
    "updated": "2025-12-16T10:00:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.10875v1",
    "title": "Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material",
    "authors": [
      "Wang",
      "Zhao",
      "Azarm"
    ],
    "summary": "An alternative data-driven modeling approach has been proposed and employed to gain fundamental insights into robot motion interaction with granular terrain at certain length scales. The approach is based on an integration of dimension reduction (Sequentially Truncated Higher-Order Singular Value Decomposition), surrogate modeling (Gaussian Process), and data assimilation techniques (Reduced Order Particle Filter). This approach can be used online and is based on offline data, obtained from the offline collection of high-fidelity simulation data and a set of sparse experimental data. The results have shown that orders of magnitude reduction in computational time can be obtained from the proposed data-driven modeling approach compared with physics-based high-fidelity simulations. With only simulation data as input, the data-driven prediction technique can generate predictions that have comparable accuracy as simulations. With both simulation data and sparse physical experimental measurement as input, the data-driven approach with its embedded data assimilation techniques has the potential in outperforming only high-fidelity simulations for the long-horizon predictions. In addition, it is demonstrated that the data-driven modeling approach can also reproduce the scaling relationship recovered by physics-based simulations for maximum resistive forces, which may indicate its general predictability beyond a case-by-case basis. The results are expected to help robot navigation and exploration in unknown and complex terrains during both online and offline phases.",
    "pdf_url": "https://arxiv.org/pdf/2506.10875v1",
    "github_url": null,
    "published": "2025-06-12T16:43:21+00:00",
    "updated": "2025-06-12T16:43:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.10756v1",
    "title": "Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding",
    "authors": [
      "Zhang",
      "Yu",
      "Xiao"
    ],
    "summary": "Vision-and-language navigation (VLN) is a long-standing challenge in autonomous robotics, aiming to empower agents with the ability to follow human instructions while navigating complex environments. Two key bottlenecks remain in this field: generalization to out-of-distribution environments and reliance on fixed discrete action spaces. To address these challenges, we propose Vision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles (UAVs) to execute language-guided flight. Without the requirement for localization or active ranging sensors, VLFly outputs continuous velocity commands purely from egocentric observations captured by an onboard monocular camera. The VLFly integrates three modules: an instruction encoder based on a large language model (LLM) that reformulates high-level language into structured prompts, a goal retriever powered by a vision-language model (VLM) that matches these prompts to goal images via vision-language similarity, and a waypoint planner that generates executable trajectories for real-time UAV control. VLFly is evaluated across diverse simulation environments without additional fine-tuning and consistently outperforms all baselines. Moreover, real-world VLN tasks in indoor and outdoor environments under direct and indirect instructions demonstrate that VLFly achieves robust open-vocabulary goal understanding and generalized navigation capabilities, even in the presence of abstract language input.",
    "pdf_url": "https://arxiv.org/pdf/2506.10756v1",
    "github_url": null,
    "published": "2025-06-12T14:40:50+00:00",
    "updated": "2025-06-12T14:40:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.10383v1",
    "title": "RICE: Reactive Interaction Controller for Cluttered Canopy Environment",
    "authors": [
      "Parayil",
      "Peynot",
      "Lehnert"
    ],
    "summary": "Robotic navigation in dense, cluttered environments such as agricultural canopies presents significant challenges due to physical and visual occlusion caused by leaves and branches. Traditional vision-based or model-dependent approaches often fail in these settings, where physical interaction without damaging foliage and branches is necessary to reach a target. We present a novel reactive controller that enables safe navigation for a robotic arm in a contact-rich, cluttered, deformable environment using end-effector position and real-time tactile feedback. Our proposed framework's interaction strategy is based on a trade-off between minimizing disturbance by maneuvering around obstacles and pushing through them to move towards the target. We show that over 35 trials in 3 experimental plant setups with an occluded target, the proposed controller successfully reached the target in all trials without breaking any branch and outperformed the state-of-the-art model-free controller in robustness and adaptability. This work lays the foundation for safe, adaptive interaction in cluttered, contact-rich deformable environments, enabling future agricultural tasks such as pruning and harvesting in plant canopies.",
    "pdf_url": "https://arxiv.org/pdf/2506.10383v1",
    "github_url": null,
    "published": "2025-06-12T06:19:22+00:00",
    "updated": "2025-06-12T06:19:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.10172v1",
    "title": "A Navigation Framework Utilizing Vision-Language Models",
    "authors": [
      "Duan",
      "tang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) presents a complex challenge in embodied AI, requiring agents to interpret natural language instructions and navigate through visually rich, unfamiliar environments. Recent advances in large vision-language models (LVLMs), such as CLIP and Flamingo, have significantly improved multimodal understanding but introduced new challenges related to computational cost and real-time deployment. In this project, we propose a modular, plug-and-play navigation framework that decouples vision-language understanding from action planning. By integrating a frozen vision-language model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to achieve flexible, fast, and adaptable navigation without extensive model fine-tuning. Our framework leverages prompt engineering, structured history management, and a two-frame visual input strategy to enhance decision-making continuity across navigation steps. We evaluate our system on the Room-to-Room benchmark within the VLN-CE setting using the Matterport3D dataset and Habitat-Lab simulation environment. Although our initial results reveal challenges in generalizing to unseen environments under strict evaluation settings, our modular approach lays a foundation for scalable and efficient navigation systems, highlighting promising directions for future improvement through enhanced environmental priors and expanded multimodal input integration.",
    "pdf_url": "https://arxiv.org/pdf/2506.10172v1",
    "github_url": null,
    "published": "2025-06-11T20:51:58+00:00",
    "updated": "2025-06-11T20:51:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.09839v1",
    "title": "OctoNav: Towards Generalist Embodied Navigation",
    "authors": [
      "Gao",
      "Jin",
      "Peng"
    ],
    "summary": "Embodied navigation stands as a foundation pillar within the broader pursuit of embodied AI. However, previous navigation research is divided into different tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task objectives and modalities, making datasets and methods are designed individually. In this work, we take steps toward generalist navigation agents, which can follow free-form instructions that include arbitrary compounds of multi-modal and multi-capability. To achieve this, we propose a large-scale benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1. Specifically, OctoNav-Bench features continuous environments and is constructed via a designed annotation pipeline. We thoroughly craft instruction-trajectory pairs, where instructions are diverse in free-form with arbitrary modality and capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1, we build it upon MLLMs and adapt it to a VLA-type model, which can produce low-level actions solely based on 2D visual observations. Moreover, we design a Hybrid Training Paradigm (HTP) that consists of three stages, i.e., Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains specifically designed learning policies and rewards. Importantly, for TBA-SFT and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which show impressive reasoning ability via thinking-before-answer. Thus, we aim to investigate how to achieve thinking-before-action in the embodied navigation field, to improve model's reasoning ability toward generalists. Specifically, we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a cold-start phrase and then leverage Nav-GPRO to improve its thinking ability. Finally, OctoNav-R1 shows superior performance compared with previous methods.",
    "pdf_url": "https://arxiv.org/pdf/2506.09839v1",
    "github_url": null,
    "published": "2025-06-11T15:15:17+00:00",
    "updated": "2025-06-11T15:15:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.09581v1",
    "title": "Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities",
    "authors": [
      "González-Santamarta",
      "Rodríguez-Lera",
      "Sobrín-Hidalgo"
    ],
    "summary": "Large Language Models (LLMs) have experienced great advancements in the last year resulting in an increase of these models in several fields to face natural language tasks. The integration of these models in robotics can also help to improve several aspects such as human-robot interaction, navigation, planning and decision-making. Therefore, this paper introduces llama\\_ros, a tool designed to integrate quantized Large Language Models (LLMs) into robotic systems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine, llama\\_ros enables the efficient execution of quantized LLMs as edge artificial intelligence (AI) in robotics systems with resource-constrained environments, addressing the challenges of computational efficiency and memory limitations. By deploying quantized LLMs, llama\\_ros empowers robots to leverage the natural language understanding and generation for enhanced decision-making and interaction which can be paired with prompt engineering, knowledge graphs, ontologies or other tools to improve the capabilities of autonomous robots. Additionally, this paper provides insights into some use cases of using llama\\_ros for planning and explainability in robotics.",
    "pdf_url": "https://arxiv.org/pdf/2506.09581v1",
    "github_url": null,
    "published": "2025-06-11T10:19:49+00:00",
    "updated": "2025-06-11T10:19:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.08566v1",
    "title": "Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations",
    "authors": [
      "Cui",
      "Xie",
      "Zhao"
    ],
    "summary": "Vision-Language Navigation (VLN) enables intelligent agents to navigate environments by integrating visual perception and natural language instructions, yet faces significant challenges due to the scarcity of fine-grained cross-modal alignment annotations. Existing datasets primarily focus on global instruction-trajectory matching, neglecting sub-instruction-level and entity-level alignments critical for accurate navigation action decision-making. To address this limitation, we propose FCA-NIG, a generative framework that automatically constructs navigation instructions with dual-level fine-grained cross-modal annotations. In this framework, an augmented trajectory is first divided into sub-trajectories, which are then processed through GLIP-based landmark detection, crafted instruction construction, OFA-Speaker based R2R-like instruction generation, and CLIP-powered entity selection, generating sub-instruction-trajectory pairs with entity-landmark annotations. Finally, these sub-pairs are aggregated to form a complete instruction-trajectory pair. The framework generates the FCA-R2R dataset, the first large-scale augmentation dataset featuring precise sub-instruction-sub-trajectory and entity-landmark alignments. Extensive experiments demonstrate that training with FCA-R2R significantly improves the performance of multiple state-of-the-art VLN agents, including SF, EnvDrop, RecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances agents' state awareness and decision accuracy, while entity-landmark alignment further boosts navigation performance and generalization. These results highlight the effectiveness of FCA-NIG in generating high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in complex navigation tasks.",
    "pdf_url": "https://arxiv.org/pdf/2506.08566v1",
    "github_url": null,
    "published": "2025-06-10T08:36:51+00:00",
    "updated": "2025-06-10T08:36:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.08002v1",
    "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
    "authors": [
      "Sahoo",
      "Tibrewal",
      "Gkioxari"
    ],
    "summary": "Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/",
    "pdf_url": "https://arxiv.org/pdf/2506.08002v1",
    "github_url": null,
    "published": "2025-06-09T17:59:37+00:00",
    "updated": "2025-06-09T17:59:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.07570v2",
    "title": "OptiScene: LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization",
    "authors": [
      "Yang",
      "Luo",
      "Ding"
    ],
    "summary": "Automatic indoor layout generation has attracted increasing attention due to its potential in interior design, virtual environment construction, and embodied AI. Existing methods fall into two categories: prompt-driven approaches that leverage proprietary LLM services (e.g., GPT APIs) and learning-based methods trained on layout data upon diffusion-based models. Prompt-driven methods often suffer from spatial inconsistency and high computational costs, while learning-based methods are typically constrained by coarse relational graphs and limited datasets, restricting their generalization to diverse room categories. In this paper, we revisit LLM-based indoor layout generation and present 3D-SynthPlace, a large-scale dataset that combines synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline, upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000 scenes, covering four common room types -- bedroom, living room, kitchen, and bathroom -- enriched with diverse objects and high-level spatial annotations. We further introduce OptiScene, a strong open-source LLM optimized for indoor layout generation, fine-tuned based on our 3D-SynthPlace dataset through our two-stage training. For the warum-up stage I, we adopt supervised fine-tuning (SFT), which is taught to first generate high-level spatial descriptions then conditionally predict concrete object placements. For the reinforcing stage II, to better align the generated layouts with human design preferences, we apply multi-turn direct preference optimization (DPO), which significantly improving layout quality and generation success rates. Extensive experiments demonstrate that OptiScene outperforms traditional prompt-driven and learning-based baselines. Moreover, OptiScene shows promising potential in interactive tasks such as scene editing and robot navigation.",
    "pdf_url": "https://arxiv.org/pdf/2506.07570v2",
    "github_url": null,
    "published": "2025-06-09T09:13:06+00:00",
    "updated": "2025-09-19T09:25:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.07164v1",
    "title": "Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs",
    "authors": [
      "Chang",
      "Chen",
      "Li"
    ],
    "summary": "The visual-based SLAM (Simultaneous Localization and Mapping) is a technology widely used in applications such as robotic navigation and virtual reality, which primarily focuses on detecting feature points from visual images to construct an unknown environmental map and simultaneously determines its own location. It usually imposes stringent requirements on hardware power consumption, processing speed and accuracy. Currently, the ORB (Oriented FAST and Rotated BRIEF)-based SLAM systems have exhibited superior performance in terms of processing speed and robustness. However, they still fall short of meeting the demands for real-time processing on mobile platforms. This limitation is primarily due to the time-consuming Oriented FAST calculations accounting for approximately half of the entire SLAM system. This paper presents two methods to accelerate the Oriented FAST feature detection on low-end embedded GPUs. These methods optimize the most time-consuming steps in Oriented FAST feature detection: FAST feature point detection and Harris corner detection, which is achieved by implementing a binary-level encoding strategy to determine candidate points quickly and a separable Harris detection strategy with efficient low-level GPU hardware-specific instructions. Extensive experiments on a Jetson TX2 embedded GPU demonstrate an average speedup of over 7.3 times compared to widely used OpenCV with GPU support. This significant improvement highlights its effectiveness and potential for real-time applications in mobile and resource-constrained environments.",
    "pdf_url": "https://arxiv.org/pdf/2506.07164v1",
    "github_url": null,
    "published": "2025-06-08T14:30:30+00:00",
    "updated": "2025-06-08T14:30:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.06862v1",
    "title": "Multimodal Spatial Language Maps for Robot Navigation and Manipulation",
    "authors": [
      "Huang",
      "Mees",
      "Zeng"
    ],
    "summary": "Grounding language to a navigating agent's observations can leverage pretrained multimodal foundation models to match perceptions to object or event descriptions. However, previous approaches remain disconnected from environment mapping, lack the spatial precision of geometric maps, or neglect additional modality information beyond vision. To address this, we propose multimodal spatial language maps as a spatial map representation that fuses pretrained multimodal features with a 3D reconstruction of the environment. We build these maps autonomously using standard exploration. We present two instances of our maps, which are visual-language maps (VLMaps) and their extension to audio-visual-language maps (AVLMaps) obtained by adding audio information. When combined with large language models (LLMs), VLMaps can (i) translate natural language commands into open-vocabulary spatial goals (e.g., \"in between the sofa and TV\") directly localized in the map, and (ii) be shared across different robot embodiments to generate tailored obstacle maps on demand. Building upon the capabilities above, AVLMaps extend VLMaps by introducing a unified 3D spatial representation integrating audio, visual, and language cues through the fusion of features from pretrained multimodal foundation models. This enables robots to ground multimodal goal queries (e.g., text, images, or audio snippets) to spatial locations for navigation. Additionally, the incorporation of diverse sensory inputs significantly enhances goal disambiguation in ambiguous environments. Experiments in simulation and real-world settings demonstrate that our multimodal spatial language maps enable zero-shot spatial and multimodal goal navigation and improve recall by 50% in ambiguous scenarios. These capabilities extend to mobile robots and tabletop manipulators, supporting navigation and interaction guided by visual, audio, and spatial cues.",
    "pdf_url": "https://arxiv.org/pdf/2506.06862v1",
    "github_url": null,
    "published": "2025-06-07T17:02:13+00:00",
    "updated": "2025-06-07T17:02:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.06804v1",
    "title": "IRS: Instance-Level 3D Scene Graphs via Room Prior Guided LiDAR-Camera Fusion",
    "authors": [
      "Chen",
      "Lin",
      "Li"
    ],
    "summary": "Indoor scene understanding remains a fundamental challenge in robotics, with direct implications for downstream tasks such as navigation and manipulation. Traditional approaches often rely on closed-set recognition or loop closure, limiting their adaptability in open-world environments. With the advent of visual foundation models (VFMs), open-vocabulary recognition and natural language querying have become feasible, unlocking new possibilities for 3D scene graph construction.   In this paper, we propose a robust and efficient framework for instance-level 3D scene graph construction via LiDAR-camera fusion. Leveraging LiDAR's wide field of view (FOV) and long-range sensing capabilities, we rapidly acquire room-level geometric priors. Multi-level VFMs are employed to improve the accuracy and consistency of semantic extraction. During instance fusion, room-based segmentation enables parallel processing, while the integration of geometric and semantic cues significantly enhances fusion accuracy and robustness. Compared to state-of-the-art methods, our approach achieves up to an order-of-magnitude improvement in construction speed while maintaining high semantic precision.   Extensive experiments in both simulated and real-world environments validate the effectiveness of our approach. We further demonstrate its practical value through a language-guided semantic navigation task, highlighting its potential for real-world robotic applications.",
    "pdf_url": "https://arxiv.org/pdf/2506.06804v1",
    "github_url": null,
    "published": "2025-06-07T13:55:34+00:00",
    "updated": "2025-06-07T13:55:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.06630v1",
    "title": "Active Test-time Vision-Language Navigation",
    "authors": [
      "Ko",
      "Kim",
      "Oh"
    ],
    "summary": "Vision-Language Navigation (VLN) policies trained on offline datasets often exhibit degraded task performance when deployed in unfamiliar navigation environments at test time, where agents are typically evaluated without access to external interaction or feedback. Entropy minimization has emerged as a practical solution for reducing prediction uncertainty at test time; however, it can suffer from accumulated errors, as agents may become overconfident in incorrect actions without sufficient contextual grounding. To tackle these challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time active learning framework that enables a practical human-robot interaction via episodic feedback on uncertain navigation outcomes. In particular, ATENA learns to increase certainty in successful episodes and decrease it in failed ones, improving uncertainty calibration. Here, we propose mixture entropy optimization, where entropy is obtained from a combination of the action and pseudo-expert distributions-a hypothetical action distribution assuming the agent's selected action to be optimal-controlling both prediction confidence and action preference. In addition, we propose a self-active learning strategy that enables an agent to evaluate its navigation outcomes based on confident predictions. As a result, the agent stays actively engaged throughout all iterations, leading to well-grounded and adaptive decision-making. Extensive evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate that ATENA successfully overcomes distributional shifts at test time, outperforming the compared baseline methods across various settings.",
    "pdf_url": "https://arxiv.org/pdf/2506.06630v1",
    "github_url": null,
    "published": "2025-06-07T02:24:44+00:00",
    "updated": "2025-06-07T02:24:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.05896v1",
    "title": "Object Navigation with Structure-Semantic Reasoning-Based Multi-level Map and Multimodal Decision-Making LLM",
    "authors": [
      "Yan",
      "He",
      "Li"
    ],
    "summary": "The zero-shot object navigation (ZSON) in unknown open-ended environments coupled with semantically novel target often suffers from the significant decline in performance due to the neglect of high-dimensional implicit scene information and the long-range target searching task. To address this, we proposed an active object navigation framework with Environmental Attributes Map (EAM) and MLLM Hierarchical Reasoning module (MHR) to improve its success rate and efficiency. EAM is constructed by reasoning observed environments with SBERT and predicting unobserved ones with Diffusion, utilizing human space regularities that underlie object-room correlations and area adjacencies. MHR is inspired by EAM to perform frontier exploration decision-making, avoiding the circuitous trajectories in long-range scenarios to improve path efficiency. Experimental results demonstrate that the EAM module achieves 64.5\\% scene mapping accuracy on MP3D dataset, while the navigation task attains SPLs of 28.4\\% and 26.3\\% on HM3D and MP3D benchmarks respectively - representing absolute improvements of 21.4\\% and 46.0\\% over baseline methods.",
    "pdf_url": "https://arxiv.org/pdf/2506.05896v1",
    "github_url": null,
    "published": "2025-06-06T09:08:40+00:00",
    "updated": "2025-06-06T09:08:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.05790v1",
    "title": "Discrete Minds in a Continuous World: Do Language Models Know Time Passes?",
    "authors": [
      "Wang",
      "Bai",
      "Vu"
    ],
    "summary": "While Large Language Models (LLMs) excel at temporal reasoning tasks like event ordering and duration estimation, their ability to perceive the actual passage of time remains unexplored. We investigate whether LLMs perceive the passage of time and adapt their decision-making accordingly through three complementary experiments. First, we introduce the Token-Time Hypothesis, positing that LLMs can map discrete token counts to continuous wall-clock time, and validate this through a dialogue duration judgment task. Second, we demonstrate that LLMs could use this awareness to adapt their response length while maintaining accuracy when users express urgency in question answering tasks. Finally, we develop BombRush, an interactive navigation challenge that examines how LLMs modify behavior under progressive time pressure in dynamic environments. Our findings indicate that LLMs possess certain awareness of time passage, enabling them to bridge discrete linguistic tokens and continuous physical time, though this capability varies with model size and reasoning abilities. This work establishes a theoretical foundation for enhancing temporal awareness in LLMs for time-sensitive applications.",
    "pdf_url": "https://arxiv.org/pdf/2506.05790v1",
    "github_url": null,
    "published": "2025-06-06T06:37:01+00:00",
    "updated": "2025-06-06T06:37:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.05020v3",
    "title": "Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System",
    "authors": [
      "Liu",
      "Ma",
      "Li"
    ],
    "summary": "Heterogeneous multirobot systems show great potential in complex tasks requiring coordinated hybrid cooperation. However, existing methods that rely on static or task-specific models often lack generalizability across diverse tasks and dynamic environments. This highlights the need for generalizable intelligence that can bridge high-level reasoning with low-level execution across heterogeneous agents. To address this, we propose a hierarchical multimodal framework that integrates a prompted large language model (LLM) with a fine-tuned vision-language model (VLM). At the system level, the LLM performs hierarchical task decomposition and constructs a global semantic map, while the VLM provides semantic perception and object localization, where the proposed GridMask significantly enhances the VLM's spatial accuracy for reliable fine-grained manipulation. The aerial robot leverages this global map to generate semantic paths and guide the ground robot's local navigation and manipulation, ensuring robust coordination even in target-absent or ambiguous scenarios. We validate the framework through extensive simulation and real-world experiments on long-horizon object arrangement tasks, demonstrating zero-shot adaptability, robust semantic navigation, and reliable manipulation in dynamic environments. To the best of our knowledge, this work presents the first heterogeneous aerial-ground robotic system that integrates VLM-based perception with LLM-driven reasoning for global high-level task planning and execution.",
    "pdf_url": "https://arxiv.org/pdf/2506.05020v3",
    "github_url": null,
    "published": "2025-06-05T13:27:41+00:00",
    "updated": "2025-10-27T04:26:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.04500v1",
    "title": "\"Don't Do That!\": Guiding Embodied Systems through Large Language Model-based Constraint Generation",
    "authors": [
      "Djuhera",
      "Seffo",
      "Asai"
    ],
    "summary": "Recent advancements in large language models (LLMs) have spurred interest in robotic navigation that incorporates complex spatial, mathematical, and conditional constraints from natural language into the planning problem. Such constraints can be informal yet highly complex, making it challenging to translate into a formal description that can be passed on to a planning algorithm. In this paper, we propose STPR, a constraint generation framework that uses LLMs to translate constraints (expressed as instructions on ``what not to do'') into executable Python functions. STPR leverages the LLM's strong coding capabilities to shift the problem description from language into structured and transparent code, thus circumventing complex reasoning and avoiding potential hallucinations. We show that these LLM-generated functions accurately describe even complex mathematical constraints, and apply them to point cloud representations with traditional search algorithms. Experiments in a simulated Gazebo environment show that STPR ensures full compliance across several constraints and scenarios, while having short runtimes. We also verify that STPR can be used with smaller, code-specific LLMs, making it applicable to a wide range of compact models at low inference cost.",
    "pdf_url": "https://arxiv.org/pdf/2506.04500v1",
    "github_url": null,
    "published": "2025-06-04T22:47:53+00:00",
    "updated": "2025-06-04T22:47:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.03709v1",
    "title": "AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives",
    "authors": [
      "Sikdar",
      "Gandhamal",
      "Sundaram"
    ],
    "summary": "Open-vocabulary semantic segmentation (OVSS) involves assigning labels to each pixel in an image based on textual descriptions, leveraging world models like CLIP. However, they encounter significant challenges in cross-domain generalization, hindering their practical efficacy in real-world applications. Embodied AI systems are transforming autonomous navigation for ground vehicles and drones by enhancing their perception abilities, and in this study, we present AetherVision-Bench, a benchmark for multi-angle segmentation across aerial, and ground perspectives, which facilitates an extensive evaluation of performance across different viewing angles and sensor modalities. We assess state-of-the-art OVSS models on the proposed benchmark and investigate the key factors that impact the performance of zero-shot transfer models. Our work pioneers the creation of a robustness benchmark, offering valuable insights and establishing a foundation for future research.",
    "pdf_url": "https://arxiv.org/pdf/2506.03709v1",
    "github_url": null,
    "published": "2025-06-04T08:41:19+00:00",
    "updated": "2025-06-04T08:41:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.03642v2",
    "title": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data",
    "authors": [
      "Zhang",
      "Liu",
      "Li"
    ],
    "summary": "Visual-spatial understanding, the ability to infer object relationships and layouts from visual input, is fundamental to downstream tasks such as robotic navigation and embodied interaction. However, existing methods face spatial uncertainty and data scarcity, limiting the 3D spatial reasoning capability of pre-trained vision-language models (VLMs). To address these challenges, we present a unified framework for enhancing 3D spatial reasoning in pre-trained VLMs without modifying their architecture. This framework combines SpatialMind, a structured prompting strategy that decomposes complex scenes and questions into interpretable reasoning steps, with ScanForgeQA, a scalable question-answering dataset built from diverse 3D simulation scenes through an automated construction process designed for fine-tuning. Extensive experiments across multiple benchmarks demonstrate the individual and combined effectiveness of our prompting and fine-tuning strategies, and yield insights that may inspire future research on visual-spatial understanding.",
    "pdf_url": "https://arxiv.org/pdf/2506.03642v2",
    "github_url": null,
    "published": "2025-06-04T07:36:33+00:00",
    "updated": "2025-09-19T05:48:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.03516v1",
    "title": "SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models",
    "authors": [
      "Debnath",
      "Stein",
      "Kosecka"
    ],
    "summary": "Object goal navigation is a fundamental task in embodied AI, where an agent is instructed to locate a target object in an unexplored environment. Traditional learning-based methods rely heavily on large-scale annotated data or require extensive interaction with the environment in a reinforcement learning setting, often failing to generalize to novel environments and limiting scalability. To overcome these challenges, we explore a zero-shot setting where the agent operates without task-specific training, enabling more scalable and adaptable solution. Recent advances in Vision Foundation Models (VFMs) offer powerful capabilities for visual understanding and reasoning, making them ideal for agents to comprehend scenes, identify relevant regions, and infer the likely locations of objects. In this work, we present a zero-shot object goal navigation framework that integrates the perceptual strength of VFMs with a model-based planner that is capable of long-horizon decision making through frontier exploration. We evaluate our approach on the HM3D dataset using the Habitat simulator and demonstrate that our method achieves state-of-the-art performance in terms of success weighted by path length for zero-shot object goal navigation.",
    "pdf_url": "https://arxiv.org/pdf/2506.03516v1",
    "github_url": null,
    "published": "2025-06-04T03:04:54+00:00",
    "updated": "2025-06-04T03:04:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.02917v1",
    "title": "Text-guided Generation of Efficient Personalized Inspection Plans",
    "authors": [
      "Sun",
      "Pan",
      "Gao"
    ],
    "summary": "We propose a training-free, Vision-Language Model (VLM)-guided approach for efficiently generating trajectories to facilitate target inspection planning based on text descriptions. Unlike existing Vision-and-Language Navigation (VLN) methods designed for general agents in unknown environments, our approach specifically targets the efficient inspection of known scenes, with widespread applications in fields such as medical, marine, and civil engineering. Leveraging VLMs, our method first extracts points of interest (POIs) from the text description, then identifies a set of waypoints from which POIs are both salient and align with the spatial constraints defined in the prompt. Next, we interact with the VLM to iteratively refine the trajectory, preserving the visibility and prominence of the POIs. Further, we solve a Traveling Salesman Problem (TSP) to find the most efficient visitation order that satisfies the order constraint implied in the text description. Finally, we apply trajectory optimization to generate smooth, executable inspection paths for aerial and underwater vehicles. We have evaluated our method across a series of both handcrafted and real-world scanned environments. The results demonstrate that our approach effectively generates inspection planning trajectories that adhere to user instructions.",
    "pdf_url": "https://arxiv.org/pdf/2506.02917v1",
    "github_url": null,
    "published": "2025-06-03T14:18:37+00:00",
    "updated": "2025-06-03T14:18:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.02593v1",
    "title": "A Hybrid Approach to Indoor Social Navigation: Integrating Reactive Local Planning and Proactive Global Planning",
    "authors": [
      "Debnath",
      "Stein",
      "Kosecka"
    ],
    "summary": "We consider the problem of indoor building-scale social navigation, where the robot must reach a point goal as quickly as possible without colliding with humans who are freely moving around. Factors such as varying crowd densities, unpredictable human behavior, and the constraints of indoor spaces add significant complexity to the navigation task, necessitating a more advanced approach. We propose a modular navigation framework that leverages the strengths of both classical methods and deep reinforcement learning (DRL). Our approach employs a global planner to generate waypoints, assigning soft costs around anticipated pedestrian locations, encouraging caution around potential future positions of humans. Simultaneously, the local planner, powered by DRL, follows these waypoints while avoiding collisions. The combination of these planners enables the agent to perform complex maneuvers and effectively navigate crowded and constrained environments while improving reliability. Many existing studies on social navigation are conducted in simplistic or open environments, limiting the ability of trained models to perform well in complex, real-world settings. To advance research in this area, we introduce a new 2D benchmark designed to facilitate development and testing of social navigation strategies in indoor environments. We benchmark our method against traditional and RL-based navigation strategies, demonstrating that our approach outperforms both.",
    "pdf_url": "https://arxiv.org/pdf/2506.02593v1",
    "github_url": null,
    "published": "2025-06-03T08:12:55+00:00",
    "updated": "2025-06-03T08:12:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.02354v1",
    "title": "RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models",
    "authors": [
      "Li",
      "Zhang",
      "Qu"
    ],
    "summary": "Object Navigation (ObjectNav) is a fundamental task in embodied artificial intelligence. Although significant progress has been made in semantic map construction and target direction prediction in current research, redundant exploration and exploration failures remain inevitable. A critical but underexplored direction is the timely termination of exploration to overcome these challenges. We observe a diminishing marginal effect between exploration steps and exploration rates and analyze the cost-benefit relationship of exploration. Inspired by this, we propose RATE-Nav, a Region-Aware Termination-Enhanced method. It includes a geometric predictive region segmentation algorithm and region-Based exploration estimation algorithm for exploration rate calculation. By leveraging the visual question answering capabilities of visual language models (VLMs) and exploration rates enables efficient termination.RATE-Nav achieves a success rate of 67.8% and an SPL of 31.3% on the HM3D dataset. And on the more challenging MP3D dataset, RATE-Nav shows approximately 10% improvement over previous zero-shot methods.",
    "pdf_url": "https://arxiv.org/pdf/2506.02354v1",
    "github_url": null,
    "published": "2025-06-03T01:15:00+00:00",
    "updated": "2025-06-03T01:15:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.01551v3",
    "title": "EvolveNav: Empowering LLM-Based Vision-Language Navigation via Self-Improving Embodied Reasoning",
    "authors": [
      "Lin",
      "Nie",
      "Zai"
    ],
    "summary": "Recent studies have revealed the potential of training open-source Large Language Models (LLMs) to unleash LLMs' reasoning ability for enhancing vision-language navigation (VLN) performance, and simultaneously mitigate the domain gap between LLMs' training corpus and the VLN task. However, these approaches predominantly adopt straightforward input-output mapping paradigms, causing the mapping learning difficult and the navigational decisions unexplainable. Chain-of-Thought (CoT) training is a promising way to improve both navigational decision accuracy and interpretability, while the complexity of the navigation task makes the perfect CoT labels unavailable and may lead to overfitting through pure CoT supervised fine-tuning. To address these issues, we propose EvolveNav, a novel sElf-improving embodied reasoning paradigm that realizes adaptable and generalizable navigational reasoning for boosting LLM-based vision-language Navigation. Specifically, EvolveNav involves a two-stage training process: (1) Formalized CoT Supervised Fine-Tuning, where we train the model with curated formalized CoT labels to first activate the model's navigational reasoning capabilities, and simultaneously increase the reasoning speed; (2) Self-Reflective Post-Training, where the model is iteratively trained with its own reasoning outputs as self-enriched CoT labels to enhance the supervision diversity. A self-reflective auxiliary task is also designed to encourage the model to learn correct reasoning patterns by contrasting with wrong ones. Experimental results under both task-specific and cross-task training paradigms demonstrate the consistent superiority of EvolveNav over previous LLM-based VLN approaches on various popular benchmarks, including R2R, REVERIE, CVDN, and SOON. Code is available at https://github.com/expectorlin/EvolveNav.",
    "pdf_url": "https://arxiv.org/pdf/2506.01551v3",
    "github_url": "https://github.com/expectorlin/EvolveNav",
    "published": "2025-06-02T11:28:32+00:00",
    "updated": "2025-10-14T02:26:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.01418v1",
    "title": "SEMNAV: A Semantic Segmentation-Driven Approach to Visual Semantic Navigation",
    "authors": [
      "Flor-Rodríguez",
      "Gutiérrez-Álvarez",
      "Acevedo-Rodríguez"
    ],
    "summary": "Visual Semantic Navigation (VSN) is a fundamental problem in robotics, where an agent must navigate toward a target object in an unknown environment, mainly using visual information. Most state-of-the-art VSN models are trained in simulation environments, where rendered scenes of the real world are used, at best. These approaches typically rely on raw RGB data from the virtual scenes, which limits their ability to generalize to real-world environments due to domain adaptation issues. To tackle this problem, in this work, we propose SEMNAV, a novel approach that leverages semantic segmentation as the main visual input representation of the environment to enhance the agent's perception and decision-making capabilities. By explicitly incorporating high-level semantic information, our model learns robust navigation policies that improve generalization across unseen environments, both in simulated and real world settings. We also introduce a newly curated dataset, i.e. the SEMNAV dataset, designed for training semantic segmentation-aware navigation models like SEMNAV. Our approach is evaluated extensively in both simulated environments and with real-world robotic platforms. Experimental results demonstrate that SEMNAV outperforms existing state-of-the-art VSN models, achieving higher success rates in the Habitat 2.0 simulation environment, using the HM3D dataset. Furthermore, our real-world experiments highlight the effectiveness of semantic segmentation in mitigating the sim-to-real gap, making our model a promising solution for practical VSN-based robotic applications. We release SEMNAV dataset, code and trained models at https://github.com/gramuah/semnav",
    "pdf_url": "https://arxiv.org/pdf/2506.01418v1",
    "github_url": "https://github.com/gramuah/semnav",
    "published": "2025-06-02T08:19:41+00:00",
    "updated": "2025-06-02T08:19:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.01031v1",
    "title": "NavBench: Probing Multimodal Large Language Models for Embodied Navigation",
    "authors": [
      "Qiao",
      "Hong",
      "Lyu"
    ],
    "summary": "Multimodal Large Language Models (MLLMs) have demonstrated strong generalization in vision-language tasks, yet their ability to understand and act within embodied environments remains underexplored. We present NavBench, a benchmark to evaluate the embodied navigation capabilities of MLLMs under zero-shot settings. NavBench consists of two components: (1) navigation comprehension, assessed through three cognitively grounded tasks including global instruction alignment, temporal progress estimation, and local observation-action reasoning, covering 3,200 question-answer pairs; and (2) step-by-step execution in 432 episodes across 72 indoor scenes, stratified by spatial, cognitive, and execution complexity. To support real-world deployment, we introduce a pipeline that converts MLLMs' outputs into robotic actions. We evaluate both proprietary and open-source models, finding that GPT-4o performs well across tasks, while lighter open-source models succeed in simpler cases. Results also show that models with higher comprehension scores tend to achieve better execution performance. Providing map-based context improves decision accuracy, especially in medium-difficulty scenarios. However, most models struggle with temporal understanding, particularly in estimating progress during navigation, which may pose a key challenge.",
    "pdf_url": "https://arxiv.org/pdf/2506.01031v1",
    "github_url": null,
    "published": "2025-06-01T14:21:02+00:00",
    "updated": "2025-06-01T14:21:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.00075v1",
    "title": "Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation",
    "authors": [
      "Pollini",
      "Guterres",
      "Guerra"
    ],
    "summary": "The integration of Large Language Models (LLMs), such as GPT, in industrial robotics enhances operational efficiency and human-robot collaboration. However, the computational complexity and size of these models often provide latency problems in request and response times. This study explores the integration of the ChatGPT natural language model with the Robot Operating System 2 (ROS 2) to mitigate interaction latency and improve robotic system control within a simulated Gazebo environment. We present an architecture that integrates these technologies without requiring a middleware transport platform, detailing how a simulated mobile robot responds to text and voice commands. Experimental results demonstrate that this integration improves execution speed, usability, and accessibility of the human-robot interaction by decreasing the communication latency by 7.01\\% on average. Such improvements facilitate smoother, real-time robot operations, which are crucial for industrial automation and precision tasks.",
    "pdf_url": "https://arxiv.org/pdf/2506.00075v1",
    "github_url": null,
    "published": "2025-05-29T21:16:14+00:00",
    "updated": "2025-05-29T21:16:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.23266v1",
    "title": "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion",
    "authors": [
      "Xie",
      "He",
      "Guo"
    ],
    "summary": "We present Adversarial Object Fusion (AdvOF), a novel attack framework targeting vision-and-language navigation (VLN) agents in service-oriented environments by generating adversarial 3D objects. While foundational models like Large Language Models (LLMs) and Vision Language Models (VLMs) have enhanced service-oriented navigation systems through improved perception and decision-making, their integration introduces vulnerabilities in mission-critical service workflows. Existing adversarial attacks fail to address service computing contexts, where reliability and quality-of-service (QoS) are paramount. We utilize AdvOF to investigate and explore the impact of adversarial environments on the VLM-based perception module of VLN agents. In particular, AdvOF first precisely aggregates and aligns the victim object positions in both 2D and 3D space, defining and rendering adversarial objects. Then, we collaboratively optimize the adversarial object with regularization between the adversarial and victim object across physical properties and VLM perceptions. Through assigning importance weights to varying views, the optimization is processed stably and multi-viewedly by iterative fusions from local updates and justifications. Our extensive evaluations demonstrate AdvOF can effectively degrade agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks. This work advances the understanding of service security in VLM-powered navigation systems, providing computational foundations for robust service composition in physical-world deployments.",
    "pdf_url": "https://arxiv.org/pdf/2505.23266v1",
    "github_url": null,
    "published": "2025-05-29T09:14:50+00:00",
    "updated": "2025-05-29T09:14:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.23153v2",
    "title": "Conceptual Framework Toward Embodied Collective Adaptive Intelligence",
    "authors": [
      "Wang",
      "Liu"
    ],
    "summary": "Collective Adaptive Intelligence (CAI) represent a transformative approach in embodied AI, wherein numerous autonomous agents collaborate, adapt, and self-organize to navigate complex, dynamic environments. By enabling systems to reconfigure themselves in response to unforeseen challenges, CAI facilitate robust performance in real-world scenarios. This article introduces a conceptual framework for designing and analyzing CAI. It delineates key attributes including task generalization, resilience, scalability, and self-assembly, aiming to bridge theoretical foundations with practical methodologies for engineering adaptive, emergent intelligence. By providing a structured foundation for understanding and implementing CAI, this work seeks to guide researchers and practitioners in developing more resilient, scalable, and adaptable AI systems across various domains.",
    "pdf_url": "https://arxiv.org/pdf/2505.23153v2",
    "github_url": null,
    "published": "2025-05-29T06:43:14+00:00",
    "updated": "2025-07-01T03:22:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.20897v2",
    "title": "Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation",
    "authors": [
      "Zhang",
      "Su",
      "Wu"
    ],
    "summary": "Vision-and-Language Navigation (VLN) requires the agent to navigate by following natural instructions under partial observability, making it difficult to align perception with language. Recent methods mitigate this by imagining future scenes, yet they rely on vision-based synthesis, leading to high computational cost and redundant details. To this end, we propose to adaptively imagine key environmental semantics via \\textit{language} form, enabling a more reliable and efficient strategy. Specifically, we introduce a novel Adaptive Text Dreamer (ATD), a dual-branch self-guided imagination policy built upon a large language model (LLM). ATD is designed with a human-like left-right brain architecture, where the left brain focuses on logical integration, and the right brain is responsible for imaginative prediction of future scenes. To achieve this, we fine-tune only the Q-former within both brains to efficiently activate domain-specific knowledge in the LLM, enabling dynamic updates of logical reasoning and imagination during navigation. Furthermore, we introduce a cross-interaction mechanism to regularize the imagined outputs and inject them into a navigation expert module, allowing ATD to jointly exploit both the reasoning capacity of the LLM and the expertise of the navigation model. We conduct extensive experiments on the R2R benchmark, where ATD achieves state-of-the-art performance with fewer parameters. The code is \\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}.",
    "pdf_url": "https://arxiv.org/pdf/2505.20897v2",
    "github_url": "https://github.com/zhangpingrui/Adaptive-Text-Dreamer",
    "published": "2025-05-27T08:40:20+00:00",
    "updated": "2025-06-22T13:53:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2506.06487v1",
    "title": "BeliefMapNav: 3D Voxel-Based Belief Map for Zero-Shot Object Navigation",
    "authors": [
      "Zhou",
      "Hu",
      "Zhang"
    ],
    "summary": "Zero-shot object navigation (ZSON) allows robots to find target objects in unfamiliar environments using natural language instructions, without relying on pre-built maps or task-specific training. Recent general-purpose models, such as large language models (LLMs) and vision-language models (VLMs), equip agents with semantic reasoning abilities to estimate target object locations in a zero-shot manner. However, these models often greedily select the next goal without maintaining a global understanding of the environment and are fundamentally limited in the spatial reasoning necessary for effective navigation. To overcome these limitations, we propose a novel 3D voxel-based belief map that estimates the target's prior presence distribution within a voxelized 3D space. This approach enables agents to integrate semantic priors from LLMs and visual embeddings with hierarchical spatial structure, alongside real-time observations, to build a comprehensive 3D global posterior belief of the target's location. Building on this 3D voxel map, we introduce BeliefMapNav, an efficient navigation system with two key advantages: i) grounding LLM semantic reasoning within the 3D hierarchical semantics voxel space for precise target position estimation, and ii) integrating sequential path planning to enable efficient global navigation decisions. Experiments on HM3D, MP3D, and HSSD benchmarks show that BeliefMapNav achieves state-of-the-art (SOTA) Success Rate (SR) and Success weighted by Path Length (SPL), with a notable 46.4% SPL improvement over the previous best SR method, validating its effectiveness and efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2506.06487v1",
    "github_url": null,
    "published": "2025-05-27T07:28:27+00:00",
    "updated": "2025-05-27T07:28:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.18881v1",
    "title": "SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes",
    "authors": [
      "Qiu",
      "You",
      "Gong"
    ],
    "summary": "We present the Semantics-aware Dataset and Benchmark Generation Pipeline for Open-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes pretraining multimodal foundation models to generate infinite unique photo-realistic scene variants that adhere to real-world semantics and daily commonsense for the training and the evaluation of navigation agents, accompanied with a plugin for generating object navigation task episodes compatible to the Habitat simulator. In addition, we offer two pre-generated object navigation task datasets, SD-OVON-3k and SD-OVON-10k, comprising respectively about 3k and 10k episodes of the open-vocabulary object navigation task, derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans of real-world environments and the SD-OVON-Objects dataset with 0.9k manually inspected scanned and artist-created manipulatable object models. Unlike prior datasets limited to static environments, SD-OVON covers dynamic scenes and manipulatable objects, facilitating both real-to-sim and sim-to-real robotic applications. This approach enhances the realism of navigation tasks, the training and the evaluation of open-vocabulary object navigation agents in complex settings. To demonstrate the effectiveness of our pipeline and datasets, we propose two baselines and evaluate them along with state-of-the-art baselines on SD-OVON-3k. The datasets, benchmark and source code are publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2505.18881v1",
    "github_url": null,
    "published": "2025-05-24T21:37:06+00:00",
    "updated": "2025-05-24T21:37:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.16663v1",
    "title": "CoNav: Collaborative Cross-Modal Reasoning for Embodied Navigation",
    "authors": [
      "Hao",
      "Han",
      "Li"
    ],
    "summary": "Embodied navigation demands comprehensive scene understanding and precise spatial reasoning. While image-text models excel at interpreting pixel-level color and lighting cues, 3D-text models capture volumetric structure and spatial relationships. However, unified fusion approaches that jointly fuse 2D images, 3D point clouds, and textual instructions face challenges in limited availability of triple-modality data and difficulty resolving conflicting beliefs among modalities. In this work, we introduce CoNav, a collaborative cross-modal reasoning framework where a pretrained 3D-text model explicitly guides an image-text navigation agent by providing structured spatial-semantic knowledge to resolve ambiguities during navigation. Specifically, we introduce Cross-Modal Belief Alignment, which operationalizes this cross-modal guidance by simply sharing textual hypotheses from the 3D-text model to the navigation agent. Through lightweight fine-tuning on a small 2D-3D-text corpus, the navigation agent learns to integrate visual cues with spatial-semantic knowledge derived from the 3D-text model, enabling effective reasoning in embodied navigation. CoNav achieves significant improvements on four standard embodied navigation benchmarks (R2R, CVDN, REVERIE, SOON) and two spatial reasoning benchmarks (ScanQA, SQA3D). Moreover, under close navigation Success Rate, CoNav often generates shorter paths compared to other methods (as measured by SPL), showcasing the potential and challenges of fusing data from different modalities in embodied navigation. Project Page: https://oceanhao.github.io/CoNav/",
    "pdf_url": "https://arxiv.org/pdf/2505.16663v1",
    "github_url": null,
    "published": "2025-05-22T13:27:54+00:00",
    "updated": "2025-05-22T13:27:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.16498v1",
    "title": "Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models",
    "authors": [
      "Ballardini",
      "Sotelo"
    ],
    "summary": "Achieving full automation in self-driving vehicles remains a challenge, especially in dynamic urban environments where navigation requires real-time adaptability. Existing systems struggle to handle navigation plans when faced with unpredictable changes in road layouts, spontaneous detours, or missing map data, due to their heavy reliance on predefined cartographic information. In this work, we explore the use of Large Language Models to generate Answer Set Programming rules by translating informal navigation instructions into structured, logic-based reasoning. ASP provides non-monotonic reasoning, allowing autonomous vehicles to adapt to evolving scenarios without relying on predefined maps. We present an experimental evaluation in which LLMs generate ASP constraints that encode real-world urban driving logic into a formal knowledge representation. By automating the translation of informal navigation instructions into logical rules, our method improves adaptability and explainability in autonomous navigation. Results show that LLM-driven ASP rule generation supports semantic-based decision-making, offering an explainable framework for dynamic navigation planning that aligns closely with how humans communicate navigational intent.",
    "pdf_url": "https://arxiv.org/pdf/2505.16498v1",
    "github_url": null,
    "published": "2025-05-22T10:32:43+00:00",
    "updated": "2025-05-22T10:32:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.15725v2",
    "title": "UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV Imitation Learning",
    "authors": [
      "Wang",
      "Yang",
      "Liao"
    ],
    "summary": "Unmanned Aerial Vehicles (UAVs) are evolving into language-interactive platforms, enabling more intuitive forms of human-drone interaction. While prior works have primarily focused on high-level planning and long-horizon navigation, we shift attention to language-guided fine-grained trajectory control, where UAVs execute short-range, reactive flight behaviors in response to language instructions. We formalize this problem as the Flying-on-a-Word (Flow) task and introduce UAV imitation learning as an effective approach. In this framework, UAVs learn fine-grained control policies by mimicking expert pilot trajectories paired with atomic language instructions. To support this paradigm, we present UAV-Flow, the first real-world benchmark for language-conditioned, fine-grained UAV control. It includes a task formulation, a large-scale dataset collected in diverse environments, a deployable control framework, and a simulation suite for systematic evaluation. Our design enables UAVs to closely imitate the precise, expert-level flight trajectories of human pilots and supports direct deployment without sim-to-real gap. We conduct extensive experiments on UAV-Flow, benchmarking VLN and VLA paradigms. Results show that VLA models are superior to VLN baselines and highlight the critical role of spatial grounding in the fine-grained Flow setting.",
    "pdf_url": "https://arxiv.org/pdf/2505.15725v2",
    "github_url": null,
    "published": "2025-05-21T16:31:28+00:00",
    "updated": "2025-05-26T11:15:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.14866v1",
    "title": "UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction",
    "authors": [
      "Nilavadi",
      "Rudenko",
      "Linder"
    ],
    "summary": "We introduce a unified approach to forecast the dynamics of human keypoints along with the motion trajectory based on a short sequence of input poses. While many studies address either full-body pose prediction or motion trajectory prediction, only a few attempt to merge them. We propose a motion transformation technique to simultaneously predict full-body pose and trajectory key-points in a global coordinate frame. We utilize an off-the-shelf 3D human pose estimation module, a graph attention network to encode the skeleton structure, and a compact, non-autoregressive transformer suitable for real-time motion prediction for human-robot interaction and human-aware navigation. We introduce a human navigation dataset ``DARKO'' with specific focus on navigational activities that are relevant for human-aware mobile robot navigation. We perform extensive evaluation on Human3.6M, CMU-Mocap, and our DARKO dataset. In comparison to prior work, we show that our approach is compact, real-time, and accurate in predicting human navigation motion across all datasets. Result animations, our dataset, and code will be available at https://nisarganc.github.io/UPTor-page/",
    "pdf_url": "https://arxiv.org/pdf/2505.14866v1",
    "github_url": null,
    "published": "2025-05-20T19:57:25+00:00",
    "updated": "2025-05-20T19:57:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.13729v1",
    "title": "SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation",
    "authors": [
      "Rajvanshi",
      "Sahu",
      "Shan"
    ],
    "summary": "Adaptive collaboration is critical to a team of autonomous robots to perform complicated navigation tasks in large-scale unknown environments. An effective collaboration strategy should be determined and adapted according to each robot's skills and current status to successfully achieve the shared goal. We present SayCoNav, a new approach that leverages large language models (LLMs) for automatically generating this collaboration strategy among a team of robots. Building on the collaboration strategy, each robot uses the LLM to generate its plans and actions in a decentralized way. By sharing information to each other during navigation, each robot also continuously updates its step-by-step plans accordingly. We evaluate SayCoNav on Multi-Object Navigation (MultiON) tasks, that require the team of the robots to utilize their complementary strengths to efficiently search multiple different objects in unknown environments. By validating SayCoNav with varied team compositions and conditions against baseline methods, our experimental results show that SayCoNav can improve search efficiency by at most 44.28% through effective collaboration among heterogeneous robots. It can also dynamically adapt to the changing conditions during task execution.",
    "pdf_url": "https://arxiv.org/pdf/2505.13729v1",
    "github_url": null,
    "published": "2025-05-19T20:58:06+00:00",
    "updated": "2025-05-19T20:58:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.12835v1",
    "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models",
    "authors": [
      "Cai",
      "Dong",
      "Tan"
    ],
    "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital for applications such as disaster response, logistics delivery, and urban inspection. However, existing methods often struggle with insufficient multimodal fusion, weak generalization, and poor interpretability. To address these challenges, we propose FlightGPT, a novel UAV VLN framework built upon Vision-Language Models (VLMs) with powerful multimodal perception capabilities. We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT) using high-quality demonstrations to improve initialization and structured reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by a composite reward that considers goal accuracy, reasoning quality, and format compliance, to enhance generalization and adaptability. Furthermore, FlightGPT introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve decision interpretability. Extensive experiments on the city-scale dataset CityNav demonstrate that FlightGPT achieves state-of-the-art performance across all scenarios, with a 9.22\\% higher success rate than the strongest baseline in unseen environments. Our implementation is publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2505.12835v1",
    "github_url": null,
    "published": "2025-05-19T08:21:20+00:00",
    "updated": "2025-05-19T08:21:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.12443v1",
    "title": "BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation",
    "authors": [
      "Lyu",
      "Li",
      "Qiao"
    ],
    "summary": "Multimodal large language models (MLLMs) have recently gained attention for their generalization and reasoning capabilities in Vision-and-Language Navigation (VLN) tasks, leading to the rise of MLLM-driven navigators. However, MLLMs are vulnerable to jailbreak attacks, where crafted prompts bypass safety mechanisms and trigger undesired outputs. In embodied scenarios, such vulnerabilities pose greater risks: unlike plain text models that generate toxic content, embodied agents may interpret malicious instructions as executable commands, potentially leading to real-world harm. In this paper, we present the first systematic jailbreak attack paradigm targeting MLLM-driven navigator. We propose a three-tiered attack framework and construct malicious queries across four intent categories, concatenated with standard navigation instructions. In the Matterport3D simulator, we evaluate navigation agents powered by five MLLMs and report an average attack success rate over 90%. To test real-world feasibility, we replicate the attack on a physical robot. Our results show that even well-crafted prompts can induce harmful actions and intents in MLLMs, posing risks beyond toxic output and potentially leading to physical harm.",
    "pdf_url": "https://arxiv.org/pdf/2505.12443v1",
    "github_url": null,
    "published": "2025-05-18T14:33:17+00:00",
    "updated": "2025-05-18T14:33:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.12084v1",
    "title": "Bench-NPIN: Benchmarking Non-prehensile Interactive Navigation",
    "authors": [
      "Zhong",
      "Caro",
      "Iskandar"
    ],
    "summary": "Mobile robots are increasingly deployed in unstructured environments where obstacles and objects are movable. Navigation in such environments is known as interactive navigation, where task completion requires not only avoiding obstacles but also strategic interactions with movable objects. Non-prehensile interactive navigation focuses on non-grasping interaction strategies, such as pushing, rather than relying on prehensile manipulation. Despite a growing body of research in this field, most solutions are evaluated using case-specific setups, limiting reproducibility and cross-comparison. In this paper, we present Bench-NPIN, the first comprehensive benchmark for non-prehensile interactive navigation. Bench-NPIN includes multiple components: 1) a comprehensive range of simulated environments for non-prehensile interactive navigation tasks, including navigating a maze with movable obstacles, autonomous ship navigation in icy waters, box delivery, and area clearing, each with varying levels of complexity; 2) a set of evaluation metrics that capture unique aspects of interactive navigation, such as efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-NPIN to evaluate example implementations of established baselines across environments. Bench-NPIN is an open-source Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.",
    "pdf_url": "https://arxiv.org/pdf/2505.12084v1",
    "github_url": "https://github.com/IvanIZ/BenchNPIN",
    "published": "2025-05-17T16:54:18+00:00",
    "updated": "2025-05-17T16:54:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.11886v4",
    "title": "Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation",
    "authors": [
      "Wang",
      "Wang",
      "Li"
    ],
    "summary": "Vision-Language Navigation (VLN) is a critical task for developing embodied agents that can follow natural language instructions to navigate in complex real-world environments. Recent advances in VLN by large pretrained models have significantly improved generalization and instruction grounding compared to traditional approaches. However, the role of reasoning strategies in navigation-an action-centric, long-horizon task-remains underexplored, despite Chain-of-Thought (CoT) reasoning's demonstrated success in static tasks like visual question answering. To address this gap, we conduct the first systematic evaluation of reasoning strategies for VLN, including No-Think (direct action prediction), Pre-Think (reason before action), and Post-Think (reason after action). Surprisingly, our findings reveal the Inference-time Reasoning Collapse issue, where inference-time reasoning degrades navigation accuracy, highlighting the challenges of integrating reasoning into VLN. Based on this insight, we propose Aux-Think, a framework that trains models to internalize structured reasoning patterns through CoT supervision, while inferring action directly without reasoning in online prediction. To support this framework, we release R2R-CoT-320k, the first Chain-of-Thought annotated dataset for VLN. Extensive experiments show that Aux-Think reduces training effort greatly and achieves the best performance under the same data scale.",
    "pdf_url": "https://arxiv.org/pdf/2505.11886v4",
    "github_url": null,
    "published": "2025-05-17T07:34:56+00:00",
    "updated": "2025-10-14T10:16:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.11383v1",
    "title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "Lee",
      "Lee"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is a core task where embodied agents leverage their spatial mobility to navigate in 3D environments toward designated destinations based on natural language instructions. Recently, video-language large models (Video-VLMs) with strong generalization capabilities and rich commonsense knowledge have shown remarkable performance when applied to VLN tasks. However, these models still encounter the following challenges when applied to real-world 3D navigation: 1) Insufficient understanding of 3D geometry and spatial semantics; 2) Limited capacity for large-scale exploration and long-term environmental memory; 3) Poor adaptability to dynamic and changing environments.To address these limitations, we propose Dynam3D, a dynamic layered 3D representation model that leverages language-aligned, generalizable, and hierarchical 3D representations as visual input to train 3D-VLM in navigation action prediction. Given posed RGB-D images, our Dynam3D projects 2D CLIP features into 3D space and constructs multi-level 3D patch-instance-zone representations for 3D geometric and semantic understanding with a dynamic and layer-wise update strategy. Our Dynam3D is capable of online encoding and localization of 3D instances, and dynamically updates them in changing environments to provide large-scale exploration and long-term memory capabilities for navigation. By leveraging large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D sets new state-of-the-art performance on VLN benchmarks including R2R-CE, REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for pre-exploration, lifelong memory, and real-world robot validate the effectiveness of practical deployment.",
    "pdf_url": "https://arxiv.org/pdf/2505.11383v1",
    "github_url": null,
    "published": "2025-05-16T15:46:27+00:00",
    "updated": "2025-05-16T15:46:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.10415v1",
    "title": "Internal State Estimation in Groups via Active Information Gathering",
    "authors": [
      "Ji",
      "Pan",
      "Gao"
    ],
    "summary": "Accurately estimating human internal states, such as personality traits or behavioral patterns, is critical for enhancing the effectiveness of human-robot interaction, particularly in group settings. These insights are key in applications ranging from social navigation to autism diagnosis. However, prior methods are limited by scalability and passive observation, making real-time estimation in complex, multi-human settings difficult. In this work, we propose a practical method for active human personality estimation in groups, with a focus on applications related to Autism Spectrum Disorder (ASD). Our method combines a personality-conditioned behavior model, based on the Eysenck 3-Factor theory, with an active robot information gathering policy that triggers human behaviors through a receding-horizon planner. The robot's belief about human personality is then updated via Bayesian inference. We demonstrate the effectiveness of our approach through simulations, user studies with typical adults, and preliminary experiments involving participants with ASD. Our results show that our method can scale to tens of humans and reduce personality prediction error by 29.2% and uncertainty by 79.9% in simulation. User studies with typical adults confirm the method's ability to generalize across complex personality distributions. Additionally, we explore its application in autism-related scenarios, demonstrating that the method can identify the difference between neurotypical and autistic behavior, highlighting its potential for diagnosing ASD. The results suggest that our framework could serve as a foundation for future ASD-specific interventions.",
    "pdf_url": "https://arxiv.org/pdf/2505.10415v1",
    "github_url": null,
    "published": "2025-05-15T15:35:00+00:00",
    "updated": "2025-05-15T15:35:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.09841v1",
    "title": "Hamilton's Rule for Enabling Altruism in Multi-Agent Systems",
    "authors": [
      "Butler",
      "Egerstedt"
    ],
    "summary": "This paper explores the application of Hamilton's rule to altruistic decision-making in multi-agent systems. Inspired by biological altruism, we introduce a framework that evaluates when individual agents should incur costs to benefit their neighbors. By adapting Hamilton's rule, we define agent ``fitness\" in terms of task productivity rather than genetic survival. We formalize altruistic decision-making through a graph-based model of multi-agent interactions and propose a solution using collaborative control Lyapunov functions. The approach ensures that altruistic behaviors contribute to the collective goal-reaching efficiency of the system. We illustrate this framework on a multi-agent way-point navigation problem, where we show through simulation how agent importance levels influence altruistic decision-making, leading to improved coordination in navigation tasks.",
    "pdf_url": "https://arxiv.org/pdf/2505.09841v1",
    "github_url": null,
    "published": "2025-05-14T22:54:42+00:00",
    "updated": "2025-05-14T22:54:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.08854v1",
    "title": "Generative AI for Autonomous Driving: Frontiers and Opportunities",
    "authors": [
      "Wang",
      "Xing",
      "Can"
    ],
    "summary": "Generative Artificial Intelligence (GenAI) constitutes a transformative technological wave that reconfigures industries through its unparalleled capabilities for content creation, reasoning, planning, and multimodal understanding. This revolutionary force offers the most promising path yet toward solving one of engineering's grandest challenges: achieving reliable, fully autonomous driving, particularly the pursuit of Level 5 autonomy. This survey delivers a comprehensive and critical synthesis of the emerging role of GenAI across the autonomous driving stack. We begin by distilling the principles and trade-offs of modern generative modeling, encompassing VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). We then map their frontier applications in image, LiDAR, trajectory, occupancy, video generation as well as LLM-guided reasoning and decision making. We categorize practical applications, such as synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI. We identify key obstacles and possibilities such as comprehensive generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects, while proposing research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence. By unifying these threads, the survey provides a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility. An actively maintained repository of cited works is available at https://github.com/taco-group/GenAI4AD.",
    "pdf_url": "https://arxiv.org/pdf/2505.08854v1",
    "github_url": "https://github.com/taco-group/GenAI4AD",
    "published": "2025-05-13T17:59:20+00:00",
    "updated": "2025-05-13T17:59:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.07634v3",
    "title": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents",
    "authors": [
      "Liu",
      "Shi",
      "Nguyen"
    ],
    "summary": "The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2505.07634v3",
    "github_url": null,
    "published": "2025-05-12T15:05:34+00:00",
    "updated": "2025-10-06T10:13:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.07254v1",
    "title": "Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking",
    "authors": [
      "Nagy",
      "Werghi",
      "Hassan"
    ],
    "summary": "This work addresses the critical lack of precision in state estimation in the Kalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of selecting the appropriate motion model. Existing literature commonly relies on constant motion models for estimating the states of objects, neglecting the complex motion dynamics unique to each object. Consequently, trajectory division and imprecise object localization arise, especially under occlusion conditions. The core of these challenges lies in the limitations of the current Kalman filter formulation, which fails to account for the variability of motion dynamics as objects navigate their environments. This work introduces a novel formulation of the Kalman filter that incorporates motion dynamics, allowing the motion model to adaptively adjust according to changes in the object's movement. The proposed Kalman filter substantially improves state estimation, localization, and trajectory prediction compared to the traditional Kalman filter. This is reflected in tracking performance that surpasses recent benchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\\% and 0.81\\% in higher order tracking accuracy (HOTA) and multi-object tracking accuracy (MOTA), respectively. Furthermore, the proposed Kalman filter consistently outperforms the baseline across various detectors. Additionally, it shows an enhanced capability in managing long occlusions compared to the baseline Kalman filter, achieving margins of 1.22\\% in higher order tracking accuracy (HOTA) and 1.55\\% in multi-object tracking accuracy (MOTA) on the KITTI dataset. The formulation's efficiency is evident, with an additional processing time of only approximately 0.078 ms per frame, ensuring its applicability in real-time applications.",
    "pdf_url": "https://arxiv.org/pdf/2505.07254v1",
    "github_url": null,
    "published": "2025-05-12T06:09:32+00:00",
    "updated": "2025-05-12T06:09:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.06729v2",
    "title": "STRIVE: Structured Representation Integrating VLM Reasoning for Efficient Object Navigation",
    "authors": [
      "Zhu",
      "Li",
      "Liu"
    ],
    "summary": "Vision-Language Models (VLMs) have been increasingly integrated into object navigation tasks for their rich prior knowledge and strong reasoning abilities. However, applying VLMs to navigation poses two key challenges: effectively representing complex environment information and determining \\textit{when and how} to query VLMs. Insufficient environment understanding and over-reliance on VLMs (e.g. querying at every step) can lead to unnecessary backtracking and reduced navigation efficiency, especially in continuous environments. To address these challenges, we propose a novel framework that constructs a multi-layer representation of the environment during navigation. This representation consists of viewpoint, object nodes, and room nodes. Viewpoints and object nodes facilitate intra-room exploration and accurate target localization, while room nodes support efficient inter-room planning. Building on this representation, we propose a novel two-stage navigation policy, integrating high-level planning guided by VLM reasoning with low-level VLM-assisted exploration to efficiently locate a goal object. We evaluated our approach on three simulated benchmarks (HM3D, RoboTHOR, and MP3D), and achieved state-of-the-art performance on both the success rate ($\\mathord{\\uparrow}\\, 7.1\\%$) and navigation efficiency ($\\mathord{\\uparrow}\\, 12.5\\%$). We further validate our method on a real robot platform, demonstrating strong robustness across 15 object navigation tasks in 10 different indoor environments. Project page is available at https://zwandering.github.io/STRIVE.github.io/ .",
    "pdf_url": "https://arxiv.org/pdf/2505.06729v2",
    "github_url": null,
    "published": "2025-05-10T18:38:41+00:00",
    "updated": "2025-09-15T21:34:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.06218v1",
    "title": "Let Humanoids Hike! Integrative Skill Development on Complex Trails",
    "authors": [
      "Lin",
      "Yu"
    ],
    "summary": "Hiking on complex trails demands balance, agility, and adaptive decision-making over unpredictable terrain. Current humanoid research remains fragmented and inadequate for hiking: locomotion focuses on motor skills without long-term goals or situational awareness, while semantic navigation overlooks real-world embodiment and local terrain variability. We propose training humanoids to hike on complex trails, driving integrative skill development across visual perception, decision making, and motor execution. We develop a learning framework, LEGO-H, that enables a vision-equipped humanoid robot to hike complex trails autonomously. We introduce two technical innovations: 1) A temporal vision transformer variant - tailored into Hierarchical Reinforcement Learning framework - anticipates future local goals to guide movement, seamlessly integrating locomotion with goal-directed navigation. 2) Latent representations of joint movement patterns, combined with hierarchical metric learning - enhance Privileged Learning scheme - enable smooth policy transfer from privileged training to onboard execution. These components allow LEGO-H to handle diverse physical and environmental challenges without relying on predefined motion patterns. Experiments across varied simulated trails and robot morphologies highlight LEGO-H's versatility and robustness, positioning hiking as a compelling testbed for embodied autonomy and LEGO-H as a baseline for future humanoid development.",
    "pdf_url": "https://arxiv.org/pdf/2505.06218v1",
    "github_url": null,
    "published": "2025-05-09T17:53:02+00:00",
    "updated": "2025-05-09T17:53:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.06131v3",
    "title": "LOG-Nav: Efficient Layout-Aware Object-Goal Navigation with Hierarchical Planning",
    "authors": [
      "Hou",
      "Xiao",
      "Xue"
    ],
    "summary": "We introduce LOG-Nav, an efficient layout-aware object-goal navigation approach designed for complex multi-room indoor environments. By planning hierarchically leveraging a global topologigal map with layout information and local imperative approach with detailed scene representation memory, LOG-Nav achieves both efficient and effective navigation. The process is managed by an LLM-powered agent, ensuring seamless effective planning and navigation, without the need for human interaction, complex rewards, or costly training. Our experimental results on the MP3D benchmark achieves 85\\% object navigation success rate (SR) and 79\\% success rate weighted by path length (SPL) (over 40\\% point improvement in SR and 60\\% improvement in SPL compared to exsisting methods). Furthermore, we validate the robustness of our approach through virtual agent and real-world robotic deployment, showcasing its capability in practical scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2505.06131v3",
    "github_url": null,
    "published": "2025-05-09T15:39:37+00:00",
    "updated": "2025-12-08T11:41:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.07868v2",
    "title": "VISTA: Generative Visual Imagination for Vision-and-Language Navigation",
    "authors": [
      "Huang",
      "Wu",
      "Li"
    ],
    "summary": "Vision-and-Language Navigation (VLN) tasks agents with locating specific objects in unseen environments using natural language instructions and visual cues. Many existing VLN approaches typically follow an 'observe-and-reason' schema, that is, agents observe the environment and decide on the next action to take based on the visual observations of their surroundings. They often face challenges in long-horizon scenarios due to limitations in immediate observation and vision-language modality gaps. To overcome this, we present VISTA, a novel framework that employs an 'imagine-and-align' navigation strategy. Specifically, we leverage the generative prior of pre-trained diffusion models for dynamic visual imagination conditioned on both local observations and high-level language instructions. A Perceptual Alignment Filter module then grounds these goal imaginations against current observations, guiding an interpretable and structured reasoning process for action selection. Experiments show that VISTA sets new state-of-the-art results on Room-to-Room (R2R) and RoboTHOR benchmarks, e.g.,+3.6% increase in Success Rate on R2R. Extensive ablation analysis underscores the value of integrating forward-looking imagination, perceptual alignment, and structured reasoning for robust navigation in long-horizon environments.",
    "pdf_url": "https://arxiv.org/pdf/2505.07868v2",
    "github_url": null,
    "published": "2025-05-09T09:07:10+00:00",
    "updated": "2025-05-17T01:06:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.05622v1",
    "title": "CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory",
    "authors": [
      "Zhang",
      "Gao",
      "Yu"
    ],
    "summary": "Aerial vision-and-language navigation (VLN), requiring drones to interpret natural language instructions and navigate complex urban environments, emerges as a critical embodied AI challenge that bridges human-robot interaction, 3D spatial reasoning, and real-world deployment. Although existing ground VLN agents achieved notable results in indoor and outdoor settings, they struggle in aerial VLN due to the absence of predefined navigation graphs and the exponentially expanding action space in long-horizon exploration. In this work, we propose \\textbf{CityNavAgent}, a large language model (LLM)-empowered agent that significantly reduces the navigation complexity for urban aerial VLN. Specifically, we design a hierarchical semantic planning module (HSPM) that decomposes the long-horizon task into sub-goals with different semantic levels. The agent reaches the target progressively by achieving sub-goals with different capacities of the LLM. Additionally, a global memory module storing historical trajectories into a topological graph is developed to simplify navigation for visited targets. Extensive benchmark experiments show that our method achieves state-of-the-art performance with significant improvement. Further experiments demonstrate the effectiveness of different modules of CityNavAgent for aerial VLN in continuous city environments. The code is available at \\href{https://github.com/VinceOuti/CityNavAgent}{link}.",
    "pdf_url": "https://arxiv.org/pdf/2505.05622v1",
    "github_url": "https://github.com/VinceOuti/CityNavAgent",
    "published": "2025-05-08T20:01:35+00:00",
    "updated": "2025-05-08T20:01:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.05108v2",
    "title": "Multi-agent Embodied AI: Advances and Future Directions",
    "authors": [
      "Feng",
      "Xue",
      "Yuan"
    ],
    "summary": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the application of advanced technologies in the intelligent era, where AI systems are integrated with physical bodies that enable them to perceive, reason, and interact with their environments. Through the use of sensors for input and actuators for action, these systems can learn and adapt based on real-world feedback, allowing them to perform tasks effectively in dynamic and unpredictable environments. As techniques such as deep learning (DL), reinforcement learning (RL), and large language models (LLMs) mature, embodied AI has become a leading field in both academia and industry, with applications spanning robotics, healthcare, transportation, and manufacturing. However, most research has focused on single-agent systems that often assume static, closed environments, whereas real-world embodied AI must navigate far more complex scenarios. In such settings, agents must not only interact with their surroundings but also collaborate with other agents, necessitating sophisticated mechanisms for adaptation, real-time learning, and collaborative problem-solving. Despite increasing interest in multi-agent systems, existing research remains narrow in scope, often relying on simplified models that fail to capture the full complexity of dynamic, open environments for multi-agent embodied AI. Moreover, no comprehensive survey has systematically reviewed the advancements in this area. As embodied AI rapidly evolves, it is crucial to deepen our understanding of multi-agent embodied AI to address the challenges presented by real-world applications. To fill this gap and foster further development in the field, this paper reviews the current state of research, analyzes key contributions, and identifies challenges and future directions, providing insights to guide innovation and progress in this field.",
    "pdf_url": "https://arxiv.org/pdf/2505.05108v2",
    "github_url": null,
    "published": "2025-05-08T10:13:53+00:00",
    "updated": "2025-06-21T18:06:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.03460v1",
    "title": "LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal Delivery Based on Agentic UAVs",
    "authors": [
      "Zhang",
      "Tian",
      "Lin"
    ],
    "summary": "The growing demand for intelligent logistics, particularly fine-grained terminal delivery, underscores the need for autonomous UAV (Unmanned Aerial Vehicle)-based delivery systems. However, most existing last-mile delivery studies rely on ground robots, while current UAV-based Vision-Language Navigation (VLN) tasks primarily focus on coarse-grained, long-range goals, making them unsuitable for precise terminal delivery. To bridge this gap, we propose LogisticsVLN, a scalable aerial delivery system built on multimodal large language models (MLLMs) for autonomous terminal delivery. LogisticsVLN integrates lightweight Large Language Models (LLMs) and Visual-Language Models (VLMs) in a modular pipeline for request understanding, floor localization, object detection, and action-decision making. To support research and evaluation in this new setting, we construct the Vision-Language Delivery (VLD) dataset within the CARLA simulator. Experimental results on the VLD dataset showcase the feasibility of the LogisticsVLN system. In addition, we conduct subtask-level evaluations of each module of our system, offering valuable insights for improving the robustness and real-world deployment of foundation model-based vision-language delivery systems.",
    "pdf_url": "https://arxiv.org/pdf/2505.03460v1",
    "github_url": null,
    "published": "2025-05-06T12:00:49+00:00",
    "updated": "2025-05-06T12:00:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.03174v1",
    "title": "Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets",
    "authors": [
      "Roque",
      "Maquiling",
      "Lopez"
    ],
    "summary": "Instruction-Action (IA) data pairs are valuable for training robotic systems, especially autonomous vehicles (AVs), but having humans manually annotate this data is costly and time-inefficient. This paper explores the potential of using mobile application Global Positioning System (GPS) references and Natural Language Processing (NLP) to automatically generate large volumes of IA commands and responses without having a human generate or retroactively tag the data. In our pilot data collection, by driving to various destinations and collecting voice instructions from GPS applications, we demonstrate a means to collect and categorize the diverse sets of instructions, further accompanied by video data to form complete vision-language-action triads. We provide details on our completely automated data collection prototype system, ADVLAT-Engine. We characterize collected GPS voice instructions into eight different classifications, highlighting the breadth of commands and referentialities available for curation from freely available mobile applications. Through research and exploration into the automation of IA data pairs using GPS references, the potential to increase the speed and volume at which high-quality IA datasets are created, while minimizing cost, can pave the way for robust vision-language-action (VLA) models to serve tasks in vision-language navigation (VLN) and human-interactive autonomous systems.",
    "pdf_url": "https://arxiv.org/pdf/2505.03174v1",
    "github_url": null,
    "published": "2025-05-06T04:38:41+00:00",
    "updated": "2025-05-06T04:38:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.02388v1",
    "title": "MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans",
    "authors": [
      "Yu",
      "Jia",
      "Chen"
    ],
    "summary": "Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale, simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research. Project website: https://meta-scenes.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2505.02388v1",
    "github_url": null,
    "published": "2025-05-05T06:13:25+00:00",
    "updated": "2025-05-05T06:13:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.02293v1",
    "title": "Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety",
    "authors": [
      "Choi",
      "Aloor",
      "Li"
    ],
    "summary": "Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination.   To overcome this challenge, we propose a method that integrates multiple layers of safety by combining MARL with safety filters. First, MARL is used to learn strategies that minimize multiple agent interactions, where multiple indicates more than two. Particularly, we focus on interactions likely to result in conflicting constraints within the engagement distance. Next, for agents that enter the engagement distance, we prioritize pairs requiring the most urgent corrective actions. Finally, a dedicated safety filter provides tactical corrective actions to resolve these conflicts. Crucially, the design decisions for all layers of this framework are grounded in reachability analysis and a control barrier-value function-based filtering mechanism.   We validate our Layered Safe MARL framework in 1) hardware experiments using Crazyflie drones and 2) high-density advanced aerial mobility (AAM) operation scenarios, where agents navigate to designated waypoints while avoiding collisions. The results show that our method significantly reduces conflict while maintaining safety without sacrificing much efficiency (i.e., shorter travel time and distance) compared to baselines that do not incorporate layered safety. The project website is available at https://dinamo-mit.github.io/Layered-Safe-MARL/",
    "pdf_url": "https://arxiv.org/pdf/2505.02293v1",
    "github_url": null,
    "published": "2025-05-04T23:42:52+00:00",
    "updated": "2025-05-04T23:42:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.01931v1",
    "title": "Semantic Intelligence: Integrating GPT-4 with A Planning in Low-Cost Robotics",
    "authors": [
      "Barkley",
      "George",
      "Farimani"
    ],
    "summary": "Classical robot navigation often relies on hardcoded state machines and purely geometric path planners, limiting a robot's ability to interpret high-level semantic instructions. In this paper, we first assess GPT-4's ability to act as a path planner compared to the A* algorithm, then present a hybrid planning framework that integrates GPT-4's semantic reasoning with A* on a low-cost robot platform operating on ROS2 Humble. Our approach eliminates explicit finite state machine (FSM) coding by using prompt-based GPT-4 reasoning to handle task logic while maintaining the accurate paths computed by A*. The GPT-4 module provides semantic understanding of instructions and environmental cues (e.g., recognizing toxic obstacles or crowded areas to avoid, or understanding low-battery situations requiring alternate route selection), and dynamically adjusts the robot's occupancy grid via obstacle buffering to enforce semantic constraints. We demonstrate multi-step reasoning for sequential tasks, such as first navigating to a resource goal and then reaching a final destination safely. Experiments on a Petoi Bittle robot with an overhead camera and Raspberry Pi Zero 2W compare classical A* against GPT-4-assisted planning. Results show that while A* is faster and more accurate for basic route generation and obstacle avoidance, the GPT-4-integrated system achieves high success rates (96-100%) on semantic tasks that are infeasible for pure geometric planners. This work highlights how affordable robots can exhibit intelligent, context-aware behaviors by leveraging large language model reasoning with minimal hardware and no fine-tuning.",
    "pdf_url": "https://arxiv.org/pdf/2505.01931v1",
    "github_url": null,
    "published": "2025-05-03T21:49:14+00:00",
    "updated": "2025-05-03T21:49:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.01881v3",
    "title": "PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications",
    "authors": [
      "Srinivasan",
      "Patapati"
    ],
    "summary": "Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-of-thought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded and clear. This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.",
    "pdf_url": "https://arxiv.org/pdf/2505.01881v3",
    "github_url": null,
    "published": "2025-05-03T17:59:26+00:00",
    "updated": "2025-06-13T03:36:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.01458v1",
    "title": "A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI",
    "authors": [
      "Wong",
      "Kang",
      "Bai"
    ],
    "summary": "Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. We also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, we offer a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods-such as world models and geometric equivariance-to help researchers select suitable tools while accounting for hardware constraints.",
    "pdf_url": "https://arxiv.org/pdf/2505.01458v1",
    "github_url": null,
    "published": "2025-05-01T09:22:23+00:00",
    "updated": "2025-05-01T09:22:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.21432v1",
    "title": "UAV-VLN: End-to-End Vision Language guided Navigation for UAVs",
    "authors": [
      "Saxena",
      "Raghuvanshi",
      "Goveas"
    ],
    "summary": "A core challenge in AI-guided autonomy is enabling agents to navigate realistically and effectively in previously unseen environments based on natural language commands. We propose UAV-VLN, a novel end-to-end Vision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs) that seamlessly integrates Large Language Models (LLMs) with visual perception to facilitate human-interactive navigation. Our system interprets free-form natural language instructions, grounds them into visual observations, and plans feasible aerial trajectories in diverse environments.   UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse high-level semantic goals, while a vision model detects and localizes semantically relevant objects in the environment. By fusing these modalities, the UAV can reason about spatial relationships, disambiguate references in human instructions, and plan context-aware behaviors with minimal task-specific supervision. To ensure robust and interpretable decision-making, the framework includes a cross-modal grounding mechanism that aligns linguistic intent with visual context.   We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios, demonstrating its ability to generalize to novel instructions and environments with minimal task-specific training. Our results show significant improvements in instruction-following accuracy and trajectory efficiency, highlighting the potential of LLM-driven vision-language interfaces for safe, intuitive, and generalizable UAV autonomy.",
    "pdf_url": "https://arxiv.org/pdf/2504.21432v1",
    "github_url": null,
    "published": "2025-04-30T08:40:47+00:00",
    "updated": "2025-04-30T08:40:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2505.00743v1",
    "title": "DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation",
    "authors": [
      "Yu",
      "Yang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is a challenging task where an agent must understand language instructions and navigate unfamiliar environments using visual cues. The agent must accurately locate the target based on visual information from the environment and complete tasks through interaction with the surroundings. Despite significant advancements in this field, two major limitations persist: (1) Many existing methods input complete language instructions directly into multi-layer Transformer networks without fully exploiting the detailed information within the instructions, thereby limiting the agent's language understanding capabilities during task execution; (2) Current approaches often overlook the modeling of object relationships across different modalities, failing to effectively utilize latent clues between objects, which affects the accuracy and robustness of navigation decisions. We propose a Dual Object Perception-Enhancement Network (DOPE) to address these issues to improve navigation performance. First, we design a Text Semantic Extraction (TSE) to extract relatively essential phrases from the text and input them into the Text Object Perception-Augmentation (TOPA) to fully leverage details such as objects and actions within the instructions. Second, we introduce an Image Object Perception-Augmentation (IOPA), which performs additional modeling of object information across different modalities, enabling the model to more effectively utilize latent clues between objects in images and text, enhancing decision-making accuracy. Extensive experiments on the R2R and REVERIE datasets validate the efficacy of the proposed approach.",
    "pdf_url": "https://arxiv.org/pdf/2505.00743v1",
    "github_url": null,
    "published": "2025-04-30T06:47:13+00:00",
    "updated": "2025-04-30T06:47:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.19322v2",
    "title": "Learned Perceptive Forward Dynamics Model for Safe and Platform-aware Robotic Navigation",
    "authors": [
      "Roth",
      "Frey",
      "Cadena"
    ],
    "summary": "Ensuring safe navigation in complex environments requires accurate real-time traversability assessment and understanding of environmental interactions relative to the robot`s capabilities. Traditional methods, which assume simplified dynamics, often require designing and tuning cost functions to safely guide paths or actions toward the goal. This process is tedious, environment-dependent, and not generalizable. To overcome these issues, we propose a novel learned perceptive Forward Dynamics Model (FDM) that predicts the robot`s future state conditioned on the surrounding geometry and history of proprioceptive measurements, proposing a more scalable, safer, and heuristic-free solution. The FDM is trained on multiple years of simulated navigation experience, including high-risk maneuvers, and real-world interactions to incorporate the full system dynamics beyond rigid body simulation. We integrate our perceptive FDM into a zero-shot Model Predictive Path Integral (MPPI) planning framework, leveraging the learned mapping between actions, future states, and failure probability. This allows for optimizing a simplified cost function, eliminating the need for extensive cost-tuning to ensure safety. On the legged robot ANYmal, the proposed perceptive FDM improves the position estimation by on average 41% over competitive baselines, which translates into a 27% higher navigation success rate in rough simulation environments. Moreover, we demonstrate effective sim-to-real transfer and showcase the benefit of training on synthetic and real data. Code and models are made publicly available under https://github.com/leggedrobotics/fdm.",
    "pdf_url": "https://arxiv.org/pdf/2504.19322v2",
    "github_url": "https://github.com/leggedrobotics/fdm",
    "published": "2025-04-27T18:27:28+00:00",
    "updated": "2025-04-29T09:26:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.16516v2",
    "title": "Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation",
    "authors": [
      "Yue",
      "Zhang",
      "Qin"
    ],
    "summary": "Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agent's ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation.",
    "pdf_url": "https://arxiv.org/pdf/2504.16516v2",
    "github_url": null,
    "published": "2025-04-23T08:41:27+00:00",
    "updated": "2025-04-24T19:36:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.14478v3",
    "title": "ApexNav: An Adaptive Exploration Strategy for Zero-Shot Object Navigation with Target-centric Semantic Fusion",
    "authors": [
      "Zhang",
      "Du",
      "Wu"
    ],
    "summary": "Navigating unknown environments to find a target object is a significant challenge. While semantic information is crucial for navigation, relying solely on it for decision-making may not always be efficient, especially in environments with weak semantic cues. Additionally, many methods are susceptible to misdetections, especially in environments with visually similar objects. To address these limitations, we propose ApexNav, a zero-shot object navigation framework that is both more efficient and reliable. For efficiency, ApexNav adaptively utilizes semantic information by analyzing its distribution in the environment, guiding exploration through semantic reasoning when cues are strong, and switching to geometry-based exploration when they are weak. For reliability, we propose a target-centric semantic fusion method that preserves long-term memory of the target and similar objects, enabling robust object identification even under noisy detections. We evaluate ApexNav on the HM3Dv1, HM3Dv2, and MP3D datasets, where it outperforms state-of-the-art methods in both SR and SPL metrics. Comprehensive ablation studies further demonstrate the effectiveness of each module. Furthermore, real-world experiments validate the practicality of ApexNav in physical environments. The code will be released at https://github.com/Robotics-STAR-Lab/ApexNav.",
    "pdf_url": "https://arxiv.org/pdf/2504.14478v3",
    "github_url": "https://github.com/Robotics-STAR-Lab/ApexNav",
    "published": "2025-04-20T04:03:29+00:00",
    "updated": "2025-09-05T15:20:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.09843v2",
    "title": "ST-Booster: An Iterative SpatioTemporal Perception Booster for Vision-and-Language Navigation in Continuous Environments",
    "authors": [
      "Yue",
      "Zhou",
      "Xie"
    ],
    "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to navigate unknown, continuous spaces based on natural language instructions. Compared to discrete settings, VLN-CE poses two core perception challenges. First, the absence of predefined observation points leads to heterogeneous visual memories and weakened global spatial correlations. Second, cumulative reconstruction errors in three-dimensional scenes introduce structural noise, impairing local feature perception. To address these challenges, this paper proposes ST-Booster, an iterative spatiotemporal booster that enhances navigation performance through multi-granularity perception and instruction-aware reasoning. ST-Booster consists of three key modules -- Hierarchical SpatioTemporal Encoding (HSTE), Multi-Granularity Aligned Fusion (MGAF), and ValueGuided Waypoint Generation (VGWG). HSTE encodes long-term global memory using topological graphs and captures shortterm local details via grid maps. MGAF aligns these dualmap representations with instructions through geometry-aware knowledge fusion. The resulting representations are iteratively refined through pretraining tasks. During reasoning, VGWG generates Guided Attention Heatmaps (GAHs) to explicitly model environment-instruction relevance and optimize waypoint selection. Extensive comparative experiments and performance analyses are conducted, demonstrating that ST-Booster outperforms existing state-of-the-art methods, particularly in complex, disturbance-prone environments.",
    "pdf_url": "https://arxiv.org/pdf/2504.09843v2",
    "github_url": null,
    "published": "2025-04-14T03:29:08+00:00",
    "updated": "2025-12-02T07:31:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.09587v3",
    "title": "GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation",
    "authors": [
      "Xu",
      "Hu",
      "Gao"
    ],
    "summary": "Language-goal aerial navigation is a critical challenge in embodied AI, requiring UAVs to localize targets in complex environments such as urban blocks based on textual specification. Existing methods, often adapted from indoor navigation, struggle to scale due to limited field of view, semantic ambiguity among objects, and lack of structured spatial reasoning. In this work, we propose GeoNav, a geospatially aware multimodal agent to enable long-range navigation. GeoNav operates in three phases-landmark navigation, target search, and precise localization-mimicking human coarse-to-fine spatial strategies. To support such reasoning, it dynamically builds two different types of spatial memory. The first is a global but schematic cognitive map, which fuses prior textual geographic knowledge and embodied visual cues into a top-down, annotated form for fast navigation to the landmark region. The second is a local but delicate scene graph representing hierarchical spatial relationships between blocks, landmarks, and objects, which is used for definite target localization. On top of this structured representation, GeoNav employs a spatially aware, multimodal chain-of-thought prompting mechanism to enable multimodal large language models with efficient and interpretable decision-making across stages. On the CityNav urban navigation benchmark, GeoNav surpasses the current state-of-the-art by up to 12.53% in success rate and significantly improves navigation efficiency, even in hard-level tasks. Ablation studies highlight the importance of each module, showcasing how geospatial representations and coarse-to-fine reasoning enhance UAV navigation.",
    "pdf_url": "https://arxiv.org/pdf/2504.09587v3",
    "github_url": null,
    "published": "2025-04-13T14:12:42+00:00",
    "updated": "2025-05-12T00:59:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.08581v1",
    "title": "FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level Interactive Agents",
    "authors": [
      "Tan",
      "Ji",
      "Zhu"
    ],
    "summary": "The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality when it comes to queries upon object components. In this work, we present FMLGS, an approach that supports part-level open-vocabulary query within 3D Gaussian Splatting (3DGS). We propose an efficient pipeline for building and querying consistent object- and part-level semantics based on Segment Anything Model 2 (SAM2). We designed a semantic deviation strategy to solve the problem of language ambiguity among object parts, which interpolates the semantic features of fine-grained targets for enriched information. Once trained, we can query both objects and their describable parts using natural language. Comparisons with other state-of-the-art methods prove that our method can not only better locate specified part-level targets, but also achieve first-place performance concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further integrate FMLGS as a virtual agent that can interactively navigate through 3D scenes, locate targets, and respond to user demands through a chat interface, which demonstrates the potential of our work to be further expanded and applied in the future.",
    "pdf_url": "https://arxiv.org/pdf/2504.08581v1",
    "github_url": null,
    "published": "2025-04-11T14:33:27+00:00",
    "updated": "2025-04-11T14:33:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.08806v1",
    "title": "Endowing Embodied Agents with Spatial Reasoning Capabilities for Vision-and-Language Navigation",
    "authors": [
      "Ling",
      "Qianqian"
    ],
    "summary": "Enhancing the spatial perception capabilities of mobile robots is crucial for achieving embodied Vision-and-Language Navigation (VLN). Although significant progress has been made in simulated environments, directly transferring these capabilities to real-world scenarios often results in severe hallucination phenomena, causing robots to lose effective spatial awareness. To address this issue, we propose BrainNav, a bio-inspired spatial cognitive navigation framework inspired by biological spatial cognition theories and cognitive map theory. BrainNav integrates dual-map (coordinate map and topological map) and dual-orientation (relative orientation and absolute orientation) strategies, enabling real-time navigation through dynamic scene capture and path planning. Its five core modules-Hippocampal Memory Hub, Visual Cortex Perception Engine, Parietal Spatial Constructor, Prefrontal Decision Center, and Cerebellar Motion Execution Unit-mimic biological cognitive functions to reduce spatial hallucinations and enhance adaptability. Validated in a zero-shot real-world lab environment using the Limo Pro robot, BrainNav, compatible with GPT-4, outperforms existing State-of-the-Art (SOTA) Vision-and-Language Navigation in Continuous Environments (VLN-CE) methods without fine-tuning.",
    "pdf_url": "https://arxiv.org/pdf/2504.08806v1",
    "github_url": null,
    "published": "2025-04-09T02:19:22+00:00",
    "updated": "2025-04-09T02:19:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.06513v5",
    "title": "Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive CVaR Barrier Functions",
    "authors": [
      "Wang",
      "Kim",
      "Hoxha"
    ],
    "summary": "Robot navigation in dynamic, crowded environments poses a significant challenge due to the inherent uncertainties in the obstacle model. In this work, we propose a risk-adaptive approach based on the Conditional Value-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically adjusted to accept the minimum necessary risk, achieving a good performance in terms of safety and optimization feasibility under uncertainty. Additionally, we introduce a dynamic zone-based barrier function which characterizes the collision likelihood by evaluating the relative state between the robot and the obstacle. By integrating risk adaptation with this new function, our approach adaptively expands the safety margin, enabling the robot to proactively avoid obstacles in highly dynamic environments. Comparisons and ablation studies demonstrate that our method outperforms existing social navigation approaches, and validate the effectiveness of our proposed framework.",
    "pdf_url": "https://arxiv.org/pdf/2504.06513v5",
    "github_url": null,
    "published": "2025-04-09T01:23:44+00:00",
    "updated": "2025-08-01T00:21:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.05477v1",
    "title": "Trust Through Transparency: Explainable Social Navigation for Autonomous Mobile Robots via Vision-Language Models",
    "authors": [
      "Sotomi",
      "Kodi",
      "Arab"
    ],
    "summary": "Service and assistive robots are increasingly being deployed in dynamic social environments; however, ensuring transparent and explainable interactions remains a significant challenge. This paper presents a multimodal explainability module that integrates vision language models and heat maps to improve transparency during navigation. The proposed system enables robots to perceive, analyze, and articulate their observations through natural language summaries. User studies (n=30) showed a preference of majority for real-time explanations, indicating improved trust and understanding. Our experiments were validated through confusion matrix analysis to assess the level of agreement with human expectations. Our experimental and simulation results emphasize the effectiveness of explainability in autonomous navigation, enhancing trust and interpretability.",
    "pdf_url": "https://arxiv.org/pdf/2504.05477v1",
    "github_url": null,
    "published": "2025-04-07T20:16:00+00:00",
    "updated": "2025-04-07T20:16:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.00848v1",
    "title": "Zero-Shot 4D Lidar Panoptic Segmentation",
    "authors": [
      "Zhang",
      "Ošep",
      "Leal-Taixé"
    ],
    "summary": "Zero-shot 4D segmentation and recognition of arbitrary objects in Lidar is crucial for embodied navigation, with applications ranging from streaming perception to semantic mapping and localization. However, the primary challenge in advancing research and developing generalized, versatile methods for spatio-temporal scene understanding in Lidar lies in the scarcity of datasets that provide the necessary diversity and scale of annotations.To overcome these challenges, we propose SAL-4D (Segment Anything in Lidar--4D), a method that utilizes multi-modal robotic sensor setups as a bridge to distill recent developments in Video Object Segmentation (VOS) in conjunction with off-the-shelf Vision-Language foundation models to Lidar. We utilize VOS models to pseudo-label tracklets in short video sequences, annotate these tracklets with sequence-level CLIP tokens, and lift them to the 4D Lidar space using calibrated multi-modal sensory setups to distill them to our SAL-4D model. Due to temporal consistent predictions, we outperform prior art in 3D Zero-Shot Lidar Panoptic Segmentation (LPS) over $5$ PQ, and unlock Zero-Shot 4D-LPS.",
    "pdf_url": "https://arxiv.org/pdf/2504.00848v1",
    "github_url": null,
    "published": "2025-04-01T14:36:12+00:00",
    "updated": "2025-04-01T14:36:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.00697v2",
    "title": "Auditory Localization and Assessment of Consequential Robot Sounds: A Multi-Method Study in Virtual Reality",
    "authors": [
      "Wessels",
      "Heuvel",
      "Müller"
    ],
    "summary": "Mobile robots increasingly operate alongside humans but are often out of sight, so that humans need to rely on the sounds of the robots to recognize their presence. For successful human-robot interaction (HRI), it is therefore crucial to understand how humans perceive robots by their consequential sounds, i.e., operating noise. Prior research suggests that the sound of a quadruped Go1 is more detectable than that of a wheeled Turtlebot. This study builds on this and examines the human ability to localize consequential sounds of three robots (quadruped Go1, wheeled Turtlebot 2i, wheeled HSR) in Virtual Reality. In a within-subjects design, we assessed participants' localization performance for the robots with and without an acoustic vehicle alerting system (AVAS) for two velocities (0.3, 0.8 m/s) and two trajectories (head-on, radial). In each trial, participants were presented with the sound of a moving robot for 3~s and were tasked to point at its final position (localization task). Localization errors were measured as the absolute angular difference between the participants' estimated and the actual robot position. Results showed that the robot type significantly influenced the localization accuracy and precision, with the sound of the wheeled HSR (especially without AVAS) performing worst under all experimental conditions. Surprisingly, participants rated the HSR sound as more positive, less annoying, and more trustworthy than the Turtlebot and Go1 sound. This reveals a tension between subjective evaluation and objective auditory localization performance. Our findings highlight consequential robot sounds as a critical factor for designing intuitive and effective HRI, with implications for human-centered robot design and social navigation.",
    "pdf_url": "https://arxiv.org/pdf/2504.00697v2",
    "github_url": null,
    "published": "2025-04-01T12:07:23+00:00",
    "updated": "2025-10-18T02:08:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.00682v2",
    "title": "Immersive Explainability: Visualizing Robot Navigation Decisions through XAI Semantic Scene Projections in Virtual Reality",
    "authors": [
      "Heuvel",
      "Müller",
      "Wessels"
    ],
    "summary": "End-to-end robot policies achieve high performance through neural networks trained via reinforcement learning (RL). Yet, their black box nature and abstract reasoning pose challenges for human-robot interaction (HRI), because humans may experience difficulty in understanding and predicting the robot's navigation decisions, hindering trust development. We present a virtual reality (VR) interface that visualizes explainable AI (XAI) outputs and the robot's lidar perception to support intuitive interpretation of RL-based navigation behavior. By visually highlighting objects based on their attribution scores, the interface grounds abstract policy explanations in the scene context. This XAI visualization bridges the gap between obscure numerical XAI attribution scores and a human-centric semantic level of explanation. A within-subjects study with 24 participants evaluated the effectiveness of our interface for four visualization conditions combining XAI and lidar. Participants ranked scene objects across navigation scenarios based on their importance to the robot, followed by a questionnaire assessing subjective understanding and predictability. Results show that semantic projection of attributions significantly enhances non-expert users' objective understanding and subjective awareness of robot behavior. In addition, lidar visualization further improves perceived predictability, underscoring the value of integrating XAI and sensor for transparent, trustworthy HRI.",
    "pdf_url": "https://arxiv.org/pdf/2504.00682v2",
    "github_url": null,
    "published": "2025-04-01T11:52:39+00:00",
    "updated": "2025-10-18T00:28:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2504.00580v2",
    "title": "MRHaD: Mixed Reality-based Hand-Drawn Map Editing Interface for Mobile Robot Navigation",
    "authors": [
      "Taki",
      "Kobayashi",
      "Iglesius"
    ],
    "summary": "Mobile robot navigation systems are increasingly relied upon in dynamic and complex environments, yet they often struggle with map inaccuracies and the resulting inefficient path planning. This paper presents MRHaD, a Mixed Reality-based Hand-drawn Map Editing Interface that enables intuitive, real-time map modifications through natural hand gestures. By integrating the MR head-mounted display with the robotic navigation system, operators can directly create hand-drawn restricted zones (HRZ), thereby bridging the gap between 2D map representations and the real-world environment. Comparative experiments against conventional 2D editing methods demonstrate that MRHaD significantly improves editing efficiency, map accuracy, and overall usability, contributing to safer and more efficient mobile robot operations. The proposed approach provides a robust technical foundation for advancing human-robot collaboration and establishing innovative interaction models that enhance the hybrid future of robotics and human society. For additional material, please check: https://mertcookimg.github.io/mrhad/",
    "pdf_url": "https://arxiv.org/pdf/2504.00580v2",
    "github_url": null,
    "published": "2025-04-01T09:34:02+00:00",
    "updated": "2025-07-28T01:14:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.24065v1",
    "title": "COSMO: Combination of Selective Memorization for Low-cost Vision-and-Language Navigation",
    "authors": [
      "Zhang",
      "Qiao",
      "Wang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) tasks have gained prominence within artificial intelligence research due to their potential application in fields like home assistants. Many contemporary VLN approaches, while based on transformer architectures, have increasingly incorporated additional components such as external knowledge bases or map information to enhance performance. These additions, while boosting performance, also lead to larger models and increased computational costs. In this paper, to achieve both high performance and low computational costs, we propose a novel architecture with the COmbination of Selective MemOrization (COSMO). Specifically, COSMO integrates state-space modules and transformer modules, and incorporates two VLN-customized selective state space modules: the Round Selective Scan (RSS) and the Cross-modal Selective State Space Module (CS3). RSS facilitates comprehensive inter-modal interactions within a single scan, while the CS3 module adapts the selective state space module into a dual-stream architecture, thereby enhancing the acquisition of cross-modal interactions. Experimental validations on three mainstream VLN benchmarks, REVERIE, R2R, and R2R-CE, not only demonstrate competitive navigation performance of our model but also show a significant reduction in computational costs.",
    "pdf_url": "https://arxiv.org/pdf/2503.24065v1",
    "github_url": null,
    "published": "2025-03-31T13:24:10+00:00",
    "updated": "2025-03-31T13:24:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.22942v1",
    "title": "Adaptive Interactive Navigation of Quadruped Robots using Large Language Models",
    "authors": [
      "Zhou",
      "Mu",
      "Song"
    ],
    "summary": "Robotic navigation in complex environments remains a critical research challenge. Traditional navigation methods focus on optimal trajectory generation within free space, struggling in environments lacking viable paths to the goal, such as disaster zones or cluttered warehouses. To address this gap, we propose an adaptive interactive navigation approach that proactively interacts with environments to create feasible paths to reach originally unavailable goals. Specifically, we present a primitive tree for task planning with large language models (LLMs), facilitating effective reasoning to determine interaction objects and sequences. To ensure robust subtask execution, we adopt reinforcement learning to pre-train a comprehensive skill library containing versatile locomotion and interaction behaviors for motion planning. Furthermore, we introduce an adaptive replanning method featuring two LLM-based modules: an advisor serving as a flexible replanning trigger and an arborist for autonomous plan adjustment. Integrated with the tree structure, the replanning mechanism allows for convenient node addition and pruning, enabling rapid plan modification in unknown environments. Comprehensive simulations and experiments have demonstrated our method's effectiveness and adaptivity in diverse scenarios. The supplementary video is available at page: https://youtu.be/W5ttPnSap2g.",
    "pdf_url": "https://arxiv.org/pdf/2503.22942v1",
    "github_url": null,
    "published": "2025-03-29T02:17:52+00:00",
    "updated": "2025-03-29T02:17:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.22201v1",
    "title": "Multi-modal Knowledge Distillation-based Human Trajectory Forecasting",
    "authors": [
      "Jeong",
      "Lee",
      "Park"
    ],
    "summary": "Pedestrian trajectory forecasting is crucial in various applications such as autonomous driving and mobile robot navigation. In such applications, camera-based perception enables the extraction of additional modalities (human pose, text) to enhance prediction accuracy. Indeed, we find that textual descriptions play a crucial role in integrating additional modalities into a unified understanding. However, online extraction of text requires the use of VLM, which may not be feasible for resource-constrained systems. To address this challenge, we propose a multi-modal knowledge distillation framework: a student model with limited modality is distilled from a teacher model trained with full range of modalities. The comprehensive knowledge of a teacher model trained with trajectory, human pose, and text is distilled into a student model using only trajectory or human pose as a sole supplement. In doing so, we separately distill the core locomotion insights from intra-agent multi-modality and inter-agent interaction. Our generalizable framework is validated with two state-of-the-art models across three datasets on both ego-view (JRDB, SIT) and BEV-view (ETH/UCY) setups, utilizing both annotated and VLM-generated text captions. Distilled student models show consistent improvement in all prediction metrics for both full and instantaneous observations, improving up to ~13%. The code is available at https://github.com/Jaewoo97/KDTF.",
    "pdf_url": "https://arxiv.org/pdf/2503.22201v1",
    "github_url": "https://github.com/Jaewoo97/KDTF",
    "published": "2025-03-28T07:32:51+00:00",
    "updated": "2025-03-28T07:32:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.21997v2",
    "title": "Beyond Omakase: Designing Shared Control for Navigation Robots with Blind People",
    "authors": [
      "Kamikubo",
      "Kayukawa",
      "Kaniwa"
    ],
    "summary": "Autonomous navigation robots can increase the independence of blind people but often limit user control, following what is called in Japanese an \"omakase\" approach where decisions are left to the robot. This research investigates ways to enhance user control in social robot navigation, based on two studies conducted with blind participants. The first study, involving structured interviews (N=14), identified crowded spaces as key areas with significant social challenges. The second study (N=13) explored navigation tasks with an autonomous robot in these environments and identified design strategies across different modes of autonomy. Participants preferred an active role, termed the \"boss\" mode, where they managed crowd interactions, while the \"monitor\" mode helped them assess the environment, negotiate movements, and interact with the robot. These findings highlight the importance of shared control and user involvement for blind users, offering valuable insights for designing future social navigation robots.",
    "pdf_url": "https://arxiv.org/pdf/2503.21997v2",
    "github_url": null,
    "published": "2025-03-27T21:32:24+00:00",
    "updated": "2025-03-31T05:46:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.21168v2",
    "title": "TAGA: A Tangent-Based Reactive Approach for Socially Compliant Robot Navigation Around Human Groups",
    "authors": [
      "Roy",
      "Rahman"
    ],
    "summary": "Robot navigation in densely populated environments presents significant challenges, particularly regarding the interplay between individual and group dynamics. Current navigation models predominantly address interactions with individual pedestrians while failing to account for human groups that naturally form in real-world settings. Conversely, the limited models implementing group-aware navigation typically prioritize group dynamics at the expense of individual interactions, both of which are essential for socially appropriate navigation. This research extends an existing simulation framework to incorporate both individual pedestrians and human groups. We present Tangent Action for Group Avoidance (TAGA), a modular reactive mechanism that can be integrated with existing navigation frameworks to enhance their group-awareness capabilities. TAGA dynamically modifies robot trajectories using tangent action-based avoidance strategies while preserving the underlying model's capacity to navigate around individuals. Additionally, we introduce Group Collision Rate (GCR), a novel metric to quantitatively assess how effectively robots maintain group integrity during navigation. Through comprehensive simulation-based benchmarking, we demonstrate that integrating TAGA with state-of-the-art navigation models (ORCA, Social Force, DS-RNN, and AG-RL) reduces group intrusions by 45.7-78.6% while maintaining comparable success rates and navigation efficiency. Future work will focus on real-world implementation and validation of this approach.",
    "pdf_url": "https://arxiv.org/pdf/2503.21168v2",
    "github_url": null,
    "published": "2025-03-27T05:37:16+00:00",
    "updated": "2025-08-22T16:18:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.20425v3",
    "title": "Perspective-Shifted Neuro-Symbolic World Models: A Framework for Socially-Aware Robot Navigation",
    "authors": [
      "Alcedo",
      "Lima",
      "Alami"
    ],
    "summary": "Navigating in environments alongside humans requires agents to reason under uncertainty and account for the beliefs and intentions of those around them. Under a sequential decision-making framework, egocentric navigation can naturally be represented as a Markov Decision Process (MDP). However, social navigation additionally requires reasoning about the hidden beliefs of others, inherently leading to a Partially Observable Markov Decision Process (POMDP), where agents lack direct access to others' mental states. Inspired by Theory of Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based reinforcement learning architecture for social navigation, addressing the challenge of belief tracking in partially observable environments; and (2) a perspective-shift operator for belief estimation, leveraging recent work on Influence-based Abstractions (IBA) in structured multi-agent settings.",
    "pdf_url": "https://arxiv.org/pdf/2503.20425v3",
    "github_url": null,
    "published": "2025-03-26T10:59:08+00:00",
    "updated": "2025-09-02T14:25:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.18525v4",
    "title": "RoboTron-Nav: A Unified Framework for Embodied Navigation Integrating Perception, Planning, and Prediction",
    "authors": [
      "Zhong",
      "Feng",
      "Yan"
    ],
    "summary": "In language-guided visual navigation, agents locate target objects in unseen environments using natural language instructions. For reliable navigation in unfamiliar scenes, agents should possess strong perception, planning, and prediction capabilities. Additionally, when agents revisit previously explored areas during long-term navigation, they may retain irrelevant and redundant historical perceptions, leading to suboptimal results. In this work, we propose RoboTron-Nav, a unified framework that integrates perception, planning, and prediction capabilities through multitask collaborations on navigation and embodied question answering tasks, thereby enhancing navigation performances. Furthermore, RoboTron-Nav employs an adaptive 3D-aware history sampling strategy to effectively and efficiently utilize historical observations. By leveraging large language model, RoboTron-Nav comprehends diverse commands and complex visual scenes, resulting in appropriate navigation actions. RoboTron-Nav achieves an 81.1% success rate in object goal navigation on the $\\mathrm{CHORES}$-$\\mathbb{S}$ benchmark, setting a new state-of-the-art performance. Project page: https://yvfengzhong.github.io/RoboTron-Nav",
    "pdf_url": "https://arxiv.org/pdf/2503.18525v4",
    "github_url": null,
    "published": "2025-03-24T10:29:47+00:00",
    "updated": "2025-08-08T11:19:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.18065v3",
    "title": "Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation",
    "authors": [
      "Wei",
      "Lin",
      "Nie"
    ],
    "summary": "Data scarcity is a long-standing challenge in the Vision-Language Navigation (VLN) field, which extremely hinders the generalization of agents to unseen environments. Previous works primarily rely on additional simulator data or web-collected images/videos to improve the generalization. However, the simulator environments still face limited diversity, and the web-collected data often requires extensive labor to remove the noise. In this paper, we propose a Rewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates the unseen observation-instruction pairs via rewriting human-annotated training data. Benefiting from our rewriting mechanism, new observation-instruction pairs can be obtained in both simulator-free and labor-saving manners to promote generalization. Specifically, we first introduce Object-Enriched Observation Rewriting, where we combine Vision-Language Models (VLMs) and Large Language Models (LLMs) to derive rewritten object-enriched scene descriptions, enabling observation synthesis with diverse objects and spatial layouts via Text-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast Instruction Rewriting, which generates observation-aligned rewritten instructions by requiring LLMs to reason the difference between original and new observations. We further develop a mixing-then-focusing training strategy with a random observation cropping scheme, effectively enhancing data distribution diversity while suppressing augmentation data noise during training. Experiments on both the discrete environments (R2R, REVERIE, and R4R datasets) and continuous environments (R2R-CE dataset) show the superior performance and impressive generalization ability of our method. Code is available at https://github.com/SaDil13/VLN-RAM.",
    "pdf_url": "https://arxiv.org/pdf/2503.18065v3",
    "github_url": "https://github.com/SaDil13/VLN-RAM",
    "published": "2025-03-23T13:18:17+00:00",
    "updated": "2025-11-04T08:39:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.17005v2",
    "title": "Sequential Autonomous Exploration-Based Precise Mapping for Mobile Robots through Stepwise and Consistent Motions",
    "authors": [
      "Zhang",
      "Ma",
      "Wu"
    ],
    "summary": "This paper proposes a 2-D autonomous exploration and mapping framework for LiDAR-based SLAM mobile robots, designed to address the major challenges on low-cost platforms, including process instability, map drift, and increased risks of collisions and deadlocks. For frontier search, the local-global sampling architecture based on Rapidly-exploring Random Trees (RRTs) is employed. For local exploration, the proposed Self-Convergent RRT (SC-RRT) efficiently covers the reachable space within a finite time while the robot remains stationary, without relying on motion-induced sampling diversity. In addition, traversability checks during RRT expansion and global RRT pruning upon map updates eliminate unreachable frontiers, reducing potential collisions and deadlocks. For frontier point navigation, a stepwise consistent motion strategy is employed to generate motion trajectories that are more amenable to stable scan matching. The resulting straight-segment and in-place-rotation pattern improves scan-matching robustness and effectively suppresses map drift on resource-constrained platforms. For the process control, the framework serializes frontier point selection and navigation, avoiding oscillations caused by frequent goal changes in conventional parallelized processes. The waypoint retracing mechanism is incorporated to generate repeated observations, triggering loop closure detection and backend optimization in graph-based SLAM, thereby improving map consistency. Experiments in challenging simulated and real-world environments validate the effectiveness of the framework. Compared with baseline methods, the proposed framework achieves higher mapping success rates and stronger robustness on resource-constrained robots and maintains consistent mapping quality across various LiDAR field-of-view (FoV) configurations.",
    "pdf_url": "https://arxiv.org/pdf/2503.17005v2",
    "github_url": null,
    "published": "2025-03-21T10:10:04+00:00",
    "updated": "2025-11-17T10:24:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.16394v1",
    "title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?",
    "authors": [
      "Perincherry",
      "Krantz",
      "Lee"
    ],
    "summary": "Vision-and-Language Navigation (VLN) agents are tasked with navigating an unseen environment using natural language instructions. In this work, we study if visual representations of sub-goals implied by the instructions can serve as navigational cues and lead to increased navigation performance. To synthesize these visual representations or imaginations, we leverage a text-to-image diffusion model on landmark references contained in segmented instructions. These imaginations are provided to VLN agents as an added modality to act as landmark cues and an auxiliary loss is added to explicitly encourage relating these with their corresponding referring expressions. Our findings reveal an increase in success rate (SR) of around 1 point and up to 0.5 points in success scaled by inverse path length (SPL) across agents. These results suggest that the proposed approach reinforces visual understanding compared to relying on language instructions alone. Code and data for our work can be found at https://www.akhilperincherry.com/VLN-Imagine-website/.",
    "pdf_url": "https://arxiv.org/pdf/2503.16394v1",
    "github_url": null,
    "published": "2025-03-20T17:53:12+00:00",
    "updated": "2025-03-20T17:53:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.17406v1",
    "title": "IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes",
    "authors": [
      "Zhang",
      "Zantout",
      "Kachana"
    ],
    "summary": "With the recent rise of large language models, vision-language models, and other general foundation models, there is growing potential for multimodal, multi-task robotics that can operate in diverse environments given natural language input. One such application is indoor navigation using natural language instructions. However, despite recent progress, this problem remains challenging due to the 3D spatial reasoning and semantic understanding required. Additionally, the language used may be imperfect or misaligned with the scene, further complicating the task. To address this challenge, we curate a benchmark dataset, IRef-VLA, for Interactive Referential Vision and Language-guided Action in 3D Scenes with imperfect references. IRef-VLA is the largest real-world dataset for the referential grounding task, consisting of over 11.5K scanned 3D rooms from existing datasets, 7.6M heuristically generated semantic relations, and 4.7M referential statements. Our dataset also contains semantic object and room annotations, scene graphs, navigable free space annotations, and is augmented with statements where the language has imperfections or ambiguities. We verify the generalizability of our dataset by evaluating with state-of-the-art models to obtain a performance baseline and also develop a graph-search baseline to demonstrate the performance bound and generation of alternatives using scene-graph knowledge. With this benchmark, we aim to provide a resource for 3D scene understanding that aids the development of robust, interactive navigation systems. The dataset and all source code is publicly released at https://github.com/HaochenZ11/IRef-VLA.",
    "pdf_url": "https://arxiv.org/pdf/2503.17406v1",
    "github_url": "https://github.com/HaochenZ11/IRef-VLA",
    "published": "2025-03-20T16:16:10+00:00",
    "updated": "2025-03-20T16:16:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.14931v1",
    "title": "Advancing a taxonomy for proxemics in robot social navigation",
    "authors": [
      "Nahum",
      "Edan",
      "Oron-Gilad"
    ],
    "summary": "Deploying robots in human environments requires effective social robot navigation. This article focuses on proxemics, proposing a new taxonomy and suggesting future directions through an analysis of state-of-the-art studies and the identification of research gaps. The various factors that affect the dynamic properties of proxemics patterns in human-robot interaction are thoroughly explored. To establish a coherent proxemics framework, we identified and organized the key parameters and attributes that shape proxemics behavior. Building on this framework, we introduce a novel approach to define proxemics in robot navigation, emphasizing the significant attributes that influence its structure and size. This leads to the development of a new taxonomy that serves as a foundation for guiding future research and development. Our findings underscore the complexity of defining personal distance, revealing it as a complex, multi-dimensional challenge. Furthermore, we highlight the flexible and dynamic nature of personal zone boundaries, which should be adaptable to different contexts and circumstances. Additionally, we propose a new layer for implementing proxemics in the navigation of social robots.",
    "pdf_url": "https://arxiv.org/pdf/2503.14931v1",
    "github_url": null,
    "published": "2025-03-19T06:33:03+00:00",
    "updated": "2025-03-19T06:33:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.14328v2",
    "title": "Risk-Sensitive Model Predictive Control for Interaction-Aware Planning -- A Sequential Convexification Algorithm",
    "authors": [
      "Wang",
      "Schuurmans",
      "Patrinos"
    ],
    "summary": "This paper considers risk-sensitive model predictive control for stochastic systems with a decision-dependent distribution. This class of systems is commonly found in human-robot interaction scenarios. We derive computationally tractable convex upper bounds to both the objective function, and to frequently used penalty terms for collision avoidance, allowing us to efficiently solve the generally nonconvex optimal control problem as a sequence of convex problems. Simulations of a robot navigating a corridor demonstrate the effectiveness and the computational advantage of the proposed approach.",
    "pdf_url": "https://arxiv.org/pdf/2503.14328v2",
    "github_url": null,
    "published": "2025-03-18T15:01:37+00:00",
    "updated": "2025-05-30T08:20:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.14229v3",
    "title": "HA-VLN 2.0: An Open Benchmark and Leaderboard for Human-Aware Navigation in Discrete and Continuous Environments with Dynamic Multi-Human Interactions",
    "authors": [
      "Dong",
      "Wu",
      "He"
    ],
    "summary": "Vision-and-Language Navigation (VLN) has been studied mainly in either discrete or continuous settings, with little attention to dynamic, crowded environments. We present HA-VLN 2.0, a unified benchmark introducing explicit social-awareness constraints. Our contributions are: (i) a standardized task and metrics capturing both goal accuracy and personal-space adherence; (ii) HAPS 2.0 dataset and simulators modeling multi-human interactions, outdoor contexts, and finer language-motion alignment; (iii) benchmarks on 16,844 socially grounded instructions, revealing sharp performance drops of leading agents under human dynamics and partial observability; and (iv) real-world robot experiments validating sim-to-real transfer, with an open leaderboard enabling transparent comparison. Results show that explicit social modeling improves navigation robustness and reduces collisions, underscoring the necessity of human-centric approaches. By releasing datasets, simulators, baselines, and protocols, HA-VLN 2.0 provides a strong foundation for safe, socially responsible navigation research.",
    "pdf_url": "https://arxiv.org/pdf/2503.14229v3",
    "github_url": null,
    "published": "2025-03-18T13:05:55+00:00",
    "updated": "2025-10-09T18:17:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.13966v1",
    "title": "FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation Tasks",
    "authors": [
      "Zhang",
      "Qiao",
      "Wang"
    ],
    "summary": "The aspiration of the Vision-and-Language Navigation (VLN) task has long been to develop an embodied agent with robust adaptability, capable of seamlessly transferring its navigation capabilities across various tasks. Despite remarkable advancements in recent years, most methods necessitate dataset-specific training, thereby lacking the capability to generalize across diverse datasets encompassing distinct types of instructions. Large language models (LLMs) have demonstrated exceptional reasoning and generalization abilities, exhibiting immense potential in robot action planning. In this paper, we propose FlexVLN, an innovative hierarchical approach to VLN that integrates the fundamental navigation ability of a supervised-learning-based Instruction Follower with the robust generalization ability of the LLM Planner, enabling effective generalization across diverse VLN datasets. Moreover, a verification mechanism and a multi-model integration mechanism are proposed to mitigate potential hallucinations by the LLM Planner and enhance execution accuracy of the Instruction Follower. We take REVERIE, SOON, and CVDN-target as out-of-domain datasets for assessing generalization ability. The generalization performance of FlexVLN surpasses that of all the previous methods to a large extent.",
    "pdf_url": "https://arxiv.org/pdf/2503.13966v1",
    "github_url": null,
    "published": "2025-03-18T06:58:41+00:00",
    "updated": "2025-03-18T06:58:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.13934v1",
    "title": "COLSON: Controllable Learning-Based Social Navigation via Diffusion-Based Reinforcement Learning",
    "authors": [
      "Tomita",
      "Matsumoto",
      "Hyodo"
    ],
    "summary": "Mobile robot navigation in dynamic environments with pedestrian traffic is a key challenge in the development of autonomous mobile service robots. Recently, deep reinforcement learning-based methods have been actively studied and have outperformed traditional rule-based approaches owing to their optimization capabilities. Among these, methods that assume a continuous action space typically rely on a Gaussian distribution assumption, which limits the flexibility of generated actions. Meanwhile, the application of diffusion models to reinforcement learning has advanced, allowing for more flexible action distributions compared with Gaussian distribution-based approaches. In this study, we applied a diffusion-based reinforcement learning approach to social navigation and validated its effectiveness. Furthermore, by leveraging the characteristics of diffusion models, we propose an extension that enables post-training action smoothing and adaptation to static obstacle scenarios not considered during the training steps.",
    "pdf_url": "https://arxiv.org/pdf/2503.13934v1",
    "github_url": null,
    "published": "2025-03-18T06:02:30+00:00",
    "updated": "2025-03-18T06:02:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.13098v1",
    "title": "LIVEPOINT: Fully Decentralized, Safe, Deadlock-Free Multi-Robot Control in Cluttered Environments with High-Dimensional Inputs",
    "authors": [
      "Chen",
      "Chandra"
    ],
    "summary": "Fully decentralized, safe, and deadlock-free multi-robot navigation in dynamic, cluttered environments is a critical challenge in robotics. Current methods require exact state measurements in order to enforce safety and liveness e.g. via control barrier functions (CBFs), which is challenging to achieve directly from onboard sensors like lidars and cameras. This work introduces LIVEPOINT, a decentralized control framework that synthesizes universal CBFs over point clouds to enable safe, deadlock-free real-time multi-robot navigation in dynamic, cluttered environments. Further, LIVEPOINT ensures minimally invasive deadlock avoidance behavior by dynamically adjusting agents' speeds based on a novel symmetric interaction metric. We validate our approach in simulation experiments across highly constrained multi-robot scenarios like doorways and intersections. Results demonstrate that LIVEPOINT achieves zero collisions or deadlocks and a 100% success rate in challenging settings compared to optimization-based baselines such as MPC and ORCA and neural methods such as MPNet, which fail in such environments. Despite prioritizing safety and liveness, LIVEPOINT is 35% smoother than baselines in the doorway environment, and maintains agility in constrained environments while still being safe and deadlock-free.",
    "pdf_url": "https://arxiv.org/pdf/2503.13098v1",
    "github_url": null,
    "published": "2025-03-17T12:07:25+00:00",
    "updated": "2025-03-17T12:07:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.12538v1",
    "title": "EmoBipedNav: Emotion-aware Social Navigation for Bipedal Robots with Deep Reinforcement Learning",
    "authors": [
      "Zhu",
      "Raju",
      "Shamsah"
    ],
    "summary": "This study presents an emotion-aware navigation framework -- EmoBipedNav -- using deep reinforcement learning (DRL) for bipedal robots walking in socially interactive environments. The inherent locomotion constraints of bipedal robots challenge their safe maneuvering capabilities in dynamic environments. When combined with the intricacies of social environments, including pedestrian interactions and social cues, such as emotions, these challenges become even more pronounced. To address these coupled problems, we propose a two-stage pipeline that considers both bipedal locomotion constraints and complex social environments. Specifically, social navigation scenarios are represented using sequential LiDAR grid maps (LGMs), from which we extract latent features, including collision regions, emotion-related discomfort zones, social interactions, and the spatio-temporal dynamics of evolving environments. The extracted features are directly mapped to the actions of reduced-order models (ROMs) through a DRL architecture. Furthermore, the proposed framework incorporates full-order dynamics and locomotion constraints during training, effectively accounting for tracking errors and restrictions of the locomotion controller while planning the trajectory with ROMs. Comprehensive experiments demonstrate that our approach exceeds both model-based planners and DRL-based baselines. The hardware videos and open-source code are available at https://gatech-lidar.github.io/emobipednav.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2503.12538v1",
    "github_url": null,
    "published": "2025-03-16T15:11:57+00:00",
    "updated": "2025-03-16T15:11:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.11091v1",
    "title": "Aerial Vision-and-Language Navigation with Grid-based View Selection and Map Construction",
    "authors": [
      "Zhao",
      "Li",
      "Pan"
    ],
    "summary": "Aerial Vision-and-Language Navigation (Aerial VLN) aims to obtain an unmanned aerial vehicle agent to navigate aerial 3D environments following human instruction. Compared to ground-based VLN, aerial VLN requires the agent to decide the next action in both horizontal and vertical directions based on the first-person view observations. Previous methods struggle to perform well due to the longer navigation path, more complicated 3D scenes, and the neglect of the interplay between vertical and horizontal actions. In this paper, we propose a novel grid-based view selection framework that formulates aerial VLN action prediction as a grid-based view selection task, incorporating vertical action prediction in a manner that accounts for the coupling with horizontal actions, thereby enabling effective altitude adjustments. We further introduce a grid-based bird's eye view map for aerial space to fuse the visual information in the navigation history, provide contextual scene information, and mitigate the impact of obstacles. Finally, a cross-modal transformer is adopted to explicitly align the long navigation history with the instruction. We demonstrate the superiority of our method in extensive experiments.",
    "pdf_url": "https://arxiv.org/pdf/2503.11091v1",
    "github_url": null,
    "published": "2025-03-14T05:20:43+00:00",
    "updated": "2025-03-14T05:20:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.11081v1",
    "title": "MoMa-Kitchen: A 100K+ Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation",
    "authors": [
      "Zhang",
      "Gao",
      "Wu"
    ],
    "summary": "In mobile manipulation, navigation and manipulation are often treated as separate problems, resulting in a significant gap between merely approaching an object and engaging with it effectively. Many navigation approaches primarily define success by proximity to the target, often overlooking the necessity for optimal positioning that facilitates subsequent manipulation. To address this, we introduce MoMa-Kitchen, a benchmark dataset comprising over 100k samples that provide training data for models to learn optimal final navigation positions for seamless transition to manipulation. Our dataset includes affordance-grounded floor labels collected from diverse kitchen environments, in which robotic mobile manipulators of different models attempt to grasp target objects amidst clutter. Using a fully automated pipeline, we simulate diverse real-world scenarios and generate affordance labels for optimal manipulation positions. Visual data are collected from RGB-D inputs captured by a first-person view camera mounted on the robotic arm, ensuring consistency in viewpoint during data collection. We also develop a lightweight baseline model, NavAff, for navigation affordance grounding that demonstrates promising performance on the MoMa-Kitchen benchmark. Our approach enables models to learn affordance-based final positioning that accommodates different arm types and platform heights, thereby paving the way for more robust and generalizable integration of navigation and manipulation in embodied AI. Project page: \\href{https://momakitchen.github.io/}{https://momakitchen.github.io/}.",
    "pdf_url": "https://arxiv.org/pdf/2503.11081v1",
    "github_url": null,
    "published": "2025-03-14T04:47:38+00:00",
    "updated": "2025-03-14T04:47:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.11006v1",
    "title": "Observation-Graph Interaction and Key-Detail Guidance for Vision and Language Navigation",
    "authors": [
      "Xie",
      "Ou",
      "Ma"
    ],
    "summary": "Vision and Language Navigation (VLN) requires an agent to navigate through environments following natural language instructions. However, existing methods often struggle with effectively integrating visual observations and instruction details during navigation, leading to suboptimal path planning and limited success rates. In this paper, we propose OIKG (Observation-graph Interaction and Key-detail Guidance), a novel framework that addresses these limitations through two key components: (1) an observation-graph interaction module that decouples angular and visual information while strengthening edge representations in the navigation space, and (2) a key-detail guidance module that dynamically extracts and utilizes fine-grained location and object information from instructions. By enabling more precise cross-modal alignment and dynamic instruction interpretation, our approach significantly improves the agent's ability to follow complex navigation instructions. Extensive experiments on the R2R and RxR datasets demonstrate that OIKG achieves state-of-the-art performance across multiple evaluation metrics, validating the effectiveness of our method in enhancing navigation precision through better observation-instruction alignment.",
    "pdf_url": "https://arxiv.org/pdf/2503.11006v1",
    "github_url": null,
    "published": "2025-03-14T02:05:16+00:00",
    "updated": "2025-03-14T02:05:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.10070v1",
    "title": "AhaRobot: A Low-Cost Open-Source Bimanual Mobile Manipulator for Embodied AI",
    "authors": [
      "Cui",
      "Yuan",
      "Zheng"
    ],
    "summary": "Navigation and manipulation in open-world environments remain unsolved challenges in the Embodied AI. The high cost of commercial mobile manipulation robots significantly limits research in real-world scenes. To address this issue, we propose AhaRobot, a low-cost and fully open-source dual-arm mobile manipulation robot system with a hardware cost of only $1,000 (excluding optional computational resources), which is less than 1/15 of the cost of popular mobile robots. The AhaRobot system consists of three components: (1) a novel low-cost hardware architecture primarily composed of off-the-shelf components, (2) an optimized control solution to enhance operational precision integrating dual-motor backlash control and static friction compensation, and (3) a simple remote teleoperation method RoboPilot. We use handles to control the dual arms and pedals for whole-body movement. The teleoperation process is low-burden and easy to operate, much like piloting. RoboPilot is designed for remote data collection in embodied scenarios. Experimental results demonstrate that RoboPilot significantly enhances data collection efficiency in complex manipulation tasks, achieving a 30% increase compared to methods using 3D mouse and leader-follower systems. It also excels at completing extremely long-horizon tasks in one go. Furthermore, AhaRobot can be used to learn end-to-end policies and autonomously perform complex manipulation tasks, such as pen insertion and cleaning up the floor. We aim to build an affordable yet powerful platform to promote the development of embodied tasks on real devices, advancing more robust and reliable embodied AI. All hardware and software systems are available at https://aha-robot.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2503.10070v1",
    "github_url": null,
    "published": "2025-03-13T05:34:43+00:00",
    "updated": "2025-03-13T05:34:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.10069v2",
    "title": "SmartWay: Enhanced Waypoint Prediction and Backtracking for Zero-Shot Vision-and-Language Navigation",
    "authors": [
      "Shi",
      "Li",
      "Lyu"
    ],
    "summary": "Vision-and-Language Navigation (VLN) in continuous environments requires agents to interpret natural language instructions while navigating unconstrained 3D spaces. Existing VLN-CE frameworks rely on a two-stage approach: a waypoint predictor to generate waypoints and a navigator to execute movements. However, current waypoint predictors struggle with spatial awareness, while navigators lack historical reasoning and backtracking capabilities, limiting adaptability. We propose a zero-shot VLN-CE framework integrating an enhanced waypoint predictor with a Multi-modal Large Language Model (MLLM)-based navigator. Our predictor employs a stronger vision encoder, masked cross-attention fusion, and an occupancy-aware loss for better waypoint quality. The navigator incorporates history-aware reasoning and adaptive path planning with backtracking, improving robustness. Experiments on R2R-CE and MP3D benchmarks show our method achieves state-of-the-art (SOTA) performance in zero-shot settings, demonstrating competitive results compared to fully supervised methods. Real-world validation on Turtlebot 4 further highlights its adaptability.",
    "pdf_url": "https://arxiv.org/pdf/2503.10069v2",
    "github_url": null,
    "published": "2025-03-13T05:32:57+00:00",
    "updated": "2025-06-17T05:47:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.09938v1",
    "title": "PanoGen++: Domain-Adapted Text-Guided Panoramic Environment Generation for Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "Zhou",
      "Xie"
    ],
    "summary": "Vision-and-language navigation (VLN) tasks require agents to navigate three-dimensional environments guided by natural language instructions, offering substantial potential for diverse applications. However, the scarcity of training data impedes progress in this field. This paper introduces PanoGen++, a novel framework that addresses this limitation by generating varied and pertinent panoramic environments for VLN tasks. PanoGen++ incorporates pre-trained diffusion models with domain-specific fine-tuning, employing parameter-efficient techniques such as low-rank adaptation to minimize computational costs. We investigate two settings for environment generation: masked image inpainting and recursive image outpainting. The former maximizes novel environment creation by inpainting masked regions based on textual descriptions, while the latter facilitates agents' learning of spatial relationships within panoramas. Empirical evaluations on room-to-room (R2R), room-for-room (R4R), and cooperative vision-and-dialog navigation (CVDN) datasets reveal significant performance enhancements: a 2.44% increase in success rate on the R2R test leaderboard, a 0.63% improvement on the R4R validation unseen set, and a 0.75-meter enhancement in goal progress on the CVDN validation unseen set. PanoGen++ augments the diversity and relevance of training environments, resulting in improved generalization and efficacy in VLN tasks.",
    "pdf_url": "https://arxiv.org/pdf/2503.09938v1",
    "github_url": null,
    "published": "2025-03-13T01:16:58+00:00",
    "updated": "2025-03-13T01:16:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.09758v1",
    "title": "Multi-Agent LLM Actor-Critic Framework for Social Robot Navigation",
    "authors": [
      "Wang",
      "Obi",
      "Min"
    ],
    "summary": "Recent advances in robotics and large language models (LLMs) have sparked growing interest in human-robot collaboration and embodied intelligence. To enable the broader deployment of robots in human-populated environments, socially-aware robot navigation (SAN) has become a key research area. While deep reinforcement learning approaches that integrate human-robot interaction (HRI) with path planning have demonstrated strong benchmark performance, they often struggle to adapt to new scenarios and environments. LLMs offer a promising avenue for zero-shot navigation through commonsense inference. However, most existing LLM-based frameworks rely on centralized decision-making, lack robust verification mechanisms, and face inconsistencies in translating macro-actions into precise low-level control signals. To address these challenges, we propose SAMALM, a decentralized multi-agent LLM actor-critic framework for multi-robot social navigation. In this framework, a set of parallel LLM actors, each reflecting distinct robot personalities or configurations, directly generate control signals. These actions undergo a two-tier verification process via a global critic that evaluates group-level behaviors and individual critics that assess each robot's context. An entropy-based score fusion mechanism further enhances self-verification and re-query, improving both robustness and coordination. Experimental results confirm that SAMALM effectively balances local autonomy with global oversight, yielding socially compliant behaviors and strong adaptability across diverse multi-robot scenarios. More details and videos about this work are available at: https://sites.google.com/view/SAMALM.",
    "pdf_url": "https://arxiv.org/pdf/2503.09758v1",
    "github_url": null,
    "published": "2025-03-12T18:59:53+00:00",
    "updated": "2025-03-12T18:59:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.08858v3",
    "title": "SICNav-Diffusion: Safe and Interactive Crowd Navigation with Diffusion Trajectory Predictions",
    "authors": [
      "Samavi",
      "Lem",
      "Sato"
    ],
    "summary": "To navigate crowds without collisions, robots must interact with humans by forecasting their future motion and reacting accordingly. While learning-based prediction models have shown success in generating likely human trajectory predictions, integrating these stochastic models into a robot controller presents several challenges. The controller needs to account for interactive coupling between planned robot motion and human predictions while ensuring both predictions and robot actions are safe (i.e. collision-free). To address these challenges, we present a receding horizon crowd navigation method for single-robot multi-human environments. We first propose a diffusion model to generate joint trajectory predictions for all humans in the scene. We then incorporate these multi-modal predictions into a SICNav Bilevel MPC problem that simultaneously solves for a robot plan (upper-level) and acts as a safety filter to refine the predictions for non-collision (lower-level). Combining planning and prediction refinement into one bilevel problem ensures that the robot plan and human predictions are coupled. We validate the open-loop trajectory prediction performance of our diffusion model on the commonly used ETH/UCY benchmark and evaluate the closed-loop performance of our robot navigation method in simulation and extensive real-robot experiments demonstrating safe, efficient, and reactive robot motion.",
    "pdf_url": "https://arxiv.org/pdf/2503.08858v3",
    "github_url": null,
    "published": "2025-03-11T19:54:50+00:00",
    "updated": "2025-06-30T16:22:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.08367v2",
    "title": "Embodied Crowd Counting",
    "authors": [
      "Long",
      "Wang",
      "Wan"
    ],
    "summary": "Occlusion is one of the fundamental challenges in crowd counting. In the community, various data-driven approaches have been developed to address this issue, yet their effectiveness is limited. This is mainly because most existing crowd counting datasets on which the methods are trained are based on passive cameras, restricting their ability to fully sense the environment. Recently, embodied navigation methods have shown significant potential in precise object detection in interactive scenes. These methods incorporate active camera settings, holding promise in addressing the fundamental issues in crowd counting. However, most existing methods are designed for indoor navigation, showing unknown performance in analyzing complex object distribution in large scale scenes, such as crowds. Besides, most existing embodied navigation datasets are indoor scenes with limited scale and object quantity, preventing them from being introduced into dense crowd analysis. Based on this, a novel task, Embodied Crowd Counting (ECC), is proposed. We first build up an interactive simulator, Embodied Crowd Counting Dataset (ECCD), which enables large scale scenes and large object quantity. A prior probability distribution that approximates realistic crowd distribution is introduced to generate crowds. Then, a zero-shot navigation method (ZECC) is proposed. This method contains a MLLM driven coarse-to-fine navigation mechanism, enabling active Z-axis exploration, and a normal-line-based crowd distribution analysis method for fine counting. Experimental results against baselines show that the proposed method achieves the best trade-off between counting accuracy and navigation cost.",
    "pdf_url": "https://arxiv.org/pdf/2503.08367v2",
    "github_url": null,
    "published": "2025-03-11T12:23:34+00:00",
    "updated": "2025-11-25T08:42:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.08306v4",
    "title": "Reasoning in visual navigation of end-to-end trained agents: a dynamical systems approach",
    "authors": [
      "Janny",
      "Poirier",
      "Antsfeld"
    ],
    "summary": "Progress in Embodied AI has made it possible for end-to-end-trained agents to navigate in photo-realistic environments with high-level reasoning and zero-shot or language-conditioned behavior, but benchmarks are still dominated by simulation. In this work, we focus on the fine-grained behavior of fast-moving real robots and present a large-scale experimental study involving \\numepisodes{} navigation episodes in a real environment with a physical robot, where we analyze the type of reasoning emerging from end-to-end training. In particular, we study the presence of realistic dynamics which the agent learned for open-loop forecasting, and their interplay with sensing. We analyze the way the agent uses latent memory to hold elements of the scene structure and information gathered during exploration. We probe the planning capabilities of the agent, and find in its memory evidence for somewhat precise plans over a limited horizon. Furthermore, we show in a post-hoc analysis that the value function learned by the agent relates to long-term planning. Put together, our experiments paint a new picture on how using tools from computer vision and sequential decision making have led to new capabilities in robotics and control. An interactive tool is available at europe.naverlabs.com/research/publications/reasoning-in-visual-navigation-of-end-to-end-trained-agents.",
    "pdf_url": "https://arxiv.org/pdf/2503.08306v4",
    "github_url": null,
    "published": "2025-03-11T11:16:47+00:00",
    "updated": "2025-04-15T08:24:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.08174v1",
    "title": "Investigating the Effectiveness of a Socratic Chain-of-Thoughts Reasoning Method for Task Planning in Robotics, A Case Study",
    "authors": [
      "Bot",
      "Xu"
    ],
    "summary": "Large language models (LLMs) have demonstrated unprecedented capability in reasoning with natural language. Coupled with this development is the emergence of embodied AI in robotics. Despite showing promise for verbal and written reasoning tasks, it remains unknown whether LLMs are capable of navigating complex spatial tasks with physical actions in the real world. To this end, it is of interest to investigate applying LLMs to robotics in zero-shot learning scenarios, and in the absence of fine-tuning - a feat which could significantly improve human-robot interaction, alleviate compute cost, and eliminate low-level programming tasks associated with robot tasks.   To explore this question, we apply GPT-4(Omni) with a simulated Tiago robot in Webots engine for an object search task. We evaluate the effectiveness of three reasoning strategies based on Chain-of-Thought (CoT) sub-task list generation with the Socratic method (SocraCoT) (in order of increasing rigor): (1) Non-CoT/Non-SocraCoT, (2) CoT only, and (3) SocraCoT. Performance was measured in terms of the proportion of tasks successfully completed and execution time (N = 20). Our preliminary results show that when combined with chain-of-thought reasoning, the Socratic method can be used for code generation for robotic tasks that require spatial awareness. In extension of this finding, we propose EVINCE-LoC; a modified EVINCE method that could further enhance performance in highly complex and or dynamic testing scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2503.08174v1",
    "github_url": null,
    "published": "2025-03-11T08:36:37+00:00",
    "updated": "2025-03-11T08:36:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.07902v2",
    "title": "LTLCodeGen: Code Generation of Syntactically Correct Temporal Logic for Robot Task Planning",
    "authors": [
      "Rabiei",
      "R.",
      "Dai"
    ],
    "summary": "This paper focuses on planning robot navigation tasks from natural language specifications. We develop a modular approach, where a large language model (LLM) translates the natural language instructions into a linear temporal logic (LTL) formula with propositions defined by object classes in a semantic occupancy map. The LTL formula and the semantic occupancy map are provided to a motion planning algorithm to generate a collision-free robot path that satisfies the natural language instructions. Our main contribution is LTLCodeGen, a method to translate natural language to syntactically correct LTL using code generation. We demonstrate the complete task planning method in real-world experiments involving human speech to provide navigation instructions to a mobile robot. We also thoroughly evaluate our approach in simulated and real-world experiments in comparison to end-to-end LLM task planning and state-of-the-art LLM-to-LTL translation methods.",
    "pdf_url": "https://arxiv.org/pdf/2503.07902v2",
    "github_url": null,
    "published": "2025-03-10T22:43:13+00:00",
    "updated": "2025-08-06T06:20:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.07557v1",
    "title": "AutoSpatial: Visual-Language Reasoning for Social Robot Navigation through Efficient Spatial Reasoning Learning",
    "authors": [
      "Kong",
      "Song",
      "Liang"
    ],
    "summary": "We present a novel method, AutoSpatial, an efficient approach with structured spatial grounding to enhance VLMs' spatial reasoning. By combining minimal manual supervision with large-scale Visual Question-Answering (VQA) pairs auto-labeling, our approach tackles the challenge of VLMs' limited spatial understanding in social navigation tasks. By applying a hierarchical two-round VQA strategy during training, AutoSpatial achieves both global and detailed understanding of scenarios, demonstrating more accurate spatial perception, movement prediction, Chain of Thought (CoT) reasoning, final action, and explanation compared to other SOTA approaches. These five components are essential for comprehensive social navigation reasoning. Our approach was evaluated using both expert systems (GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet) that provided cross-validation scores and human evaluators who assigned relative rankings to compare model performances across four key aspects. Augmented by the enhanced spatial reasoning capabilities, AutoSpatial demonstrates substantial improvements by averaged cross-validation score from expert systems in: perception & prediction (up to 10.71%), reasoning (up to 16.26%), action (up to 20.50%), and explanation (up to 18.73%) compared to baseline models trained only on manually annotated data.",
    "pdf_url": "https://arxiv.org/pdf/2503.07557v1",
    "github_url": null,
    "published": "2025-03-10T17:27:17+00:00",
    "updated": "2025-03-10T17:27:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.07323v2",
    "title": "Navigating Motion Agents in Dynamic and Cluttered Environments through LLM Reasoning",
    "authors": [
      "Zhao",
      "Wu",
      "Wang"
    ],
    "summary": "This paper advances motion agents empowered by large language models (LLMs) toward autonomous navigation in dynamic and cluttered environments, significantly surpassing first and recent seminal but limited studies on LLM's spatial reasoning, where movements are restricted in four directions in simple, static environments in the presence of only single agents much less multiple agents. Specifically, we investigate LLMs as spatial reasoners to overcome these limitations by uniformly encoding environments (e.g., real indoor floorplans), agents which can be dynamic obstacles and their paths as discrete tokens akin to language tokens. Our training-free framework supports multi-agent coordination, closed-loop replanning, and dynamic obstacle avoidance without retraining or fine-tuning. We show that LLMs can generalize across agents, tasks, and environments using only text-based interactions, opening new possibilities for semantically grounded, interactive navigation in both simulation and embodied systems.",
    "pdf_url": "https://arxiv.org/pdf/2503.07323v2",
    "github_url": null,
    "published": "2025-03-10T13:39:09+00:00",
    "updated": "2025-06-05T12:17:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.06937v2",
    "title": "Handle Object Navigation as Weighted Traveling Repairman Problem",
    "authors": [
      "Liu",
      "Xu",
      "Yuan"
    ],
    "summary": "Zero-Shot Object Navigation (ZSON) requires agents to navigate to objects specified via open-ended natural language without predefined categories or prior environmental knowledge. While recent methods leverage foundation models or multi-modal maps, they often rely on 2D representations and greedy strategies or require additional training or modules with high computation load, limiting performance in complex environments and real applications. We propose WTRP-Searcher, a novel framework that formulates ZSON as a Weighted Traveling Repairman Problem (WTRP), minimizing the weighted waiting time of viewpoints. Using a Vision-Language Model (VLM), we score viewpoints based on object-description similarity, projected onto a 2D map with depth information. An open-vocabulary detector identifies targets, dynamically updating goals, while a 3D embedding feature map enhances spatial awareness and environmental recall. WTRP-Searcher outperforms existing methods, offering efficient global planning and improved performance in complex ZSON tasks. Code and design will be open-sourced upon acceptance.",
    "pdf_url": "https://arxiv.org/pdf/2503.06937v2",
    "github_url": null,
    "published": "2025-03-10T05:32:45+00:00",
    "updated": "2025-09-18T15:28:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.16481v2",
    "title": "PeRoI: A Pedestrian-Robot Interaction Dataset for Learning Avoidance, Neutrality, and Attraction Behaviors in Social Navigation",
    "authors": [
      "Agrawal",
      "Ostermann-Myrau",
      "Dengler"
    ],
    "summary": "Robots are increasingly being deployed in public spaces such as shopping malls, sidewalks, and hospitals, where safe and socially aware navigation depends on anticipating how pedestrians respond to their presence. However, existing datasets rarely capture the full spectrum of robot-induced reactions, e.g., avoidance, neutrality, attraction, which limits progress in modeling these interactions. In this paper, we present the Pedestrian-Robot Interaction~(PeRoI) dataset that captures pedestrian motions categorized into attraction, neutrality, and repulsion across two outdoor sites under three controlled conditions: no robot present, with stationary robot, and with moving robot. This design explicitly reveals how pedestrian behavior varies across robot contexts, and we provide qualitative and quantitative comparisons to established state-of-the-art datasets. Building on these data, we propose the Neural Robot Social Force Model~(NeuRoSFM), an extension of the Social Force Model that integrates neural networks to augment inter-human dynamics with learned components and explicit robot-induced forces to better predict pedestrian motion in vicinity of robots. We evaluate NeuRoSFM by generating trajectories on multiple real-world datasets. The results demonstrate improved modeling of pedestrian-robot interactions, leading to better prediction accuracy, and highlight the value of our dataset and method for advancing socially aware navigation strategies in human-centered environments.",
    "pdf_url": "https://arxiv.org/pdf/2503.16481v2",
    "github_url": null,
    "published": "2025-03-05T17:02:29+00:00",
    "updated": "2025-10-10T14:58:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.03208v1",
    "title": "Embodied Escaping: End-to-End Reinforcement Learning for Robot Navigation in Narrow Environment",
    "authors": [
      "Zheng",
      "Zhang",
      "Jiang"
    ],
    "summary": "Autonomous navigation is a fundamental task for robot vacuum cleaners in indoor environments. Since their core function is to clean entire areas, robots inevitably encounter dead zones in cluttered and narrow scenarios. Existing planning methods often fail to escape due to complex environmental constraints, high-dimensional search spaces, and high difficulty maneuvers. To address these challenges, this paper proposes an embodied escaping model that leverages reinforcement learning-based policy with an efficient action mask for dead zone escaping. To alleviate the issue of the sparse reward in training, we introduce a hybrid training policy that improves learning efficiency. In handling redundant and ineffective action options, we design a novel action representation to reshape the discrete action space with a uniform turning radius. Furthermore, we develop an action mask strategy to select valid action quickly, balancing precision and efficiency. In real-world experiments, our robot is equipped with a Lidar, IMU, and two-wheel encoders. Extensive quantitative and qualitative experiments across varying difficulty levels demonstrate that our robot can consistently escape from challenging dead zones. Moreover, our approach significantly outperforms compared path planning and reinforcement learning methods in terms of success rate and collision avoidance.",
    "pdf_url": "https://arxiv.org/pdf/2503.03208v1",
    "github_url": null,
    "published": "2025-03-05T05:53:08+00:00",
    "updated": "2025-03-05T05:53:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.02247v5",
    "title": "WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation",
    "authors": [
      "Nie",
      "Guo",
      "Duan"
    ],
    "summary": "Object Goal Navigation-requiring an agent to locate a specific object in an unseen environment-remains a core challenge in embodied AI. Although recent progress in Vision-Language Model (VLM)-based agents has demonstrated promising perception and decision-making abilities through prompting, none has yet established a fully modular world model design that reduces risky and costly interactions with the environment by predicting the future state of the world. We introduce WMNav, a novel World Model-based Navigation framework powered by Vision-Language Models (VLMs). It predicts possible outcomes of decisions and builds memories to provide feedback to the policy module. To retain the predicted state of the environment, WMNav proposes the online maintained Curiosity Value Map as part of the world model memory to provide dynamic configuration for navigation policy. By decomposing according to a human-like thinking process, WMNav effectively alleviates the impact of model hallucination by making decisions based on the feedback difference between the world model plan and observation. To further boost efficiency, we implement a two-stage action proposer strategy: broad exploration followed by precise localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses existing zero-shot benchmarks in both success rate and exploration efficiency (absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.",
    "pdf_url": "https://arxiv.org/pdf/2503.02247v5",
    "github_url": null,
    "published": "2025-03-04T03:51:36+00:00",
    "updated": "2025-07-19T03:44:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.02223v1",
    "title": "DQO-MAP: Dual Quadrics Multi-Object mapping with Gaussian Splatting",
    "authors": [
      "Li",
      "Ye",
      "Hao"
    ],
    "summary": "Accurate object perception is essential for robotic applications such as object navigation. In this paper, we propose DQO-MAP, a novel object-SLAM system that seamlessly integrates object pose estimation and reconstruction. We employ 3D Gaussian Splatting for high-fidelity object reconstruction and leverage quadrics for precise object pose estimation. Both of them management is handled on the CPU, while optimization is performed on the GPU, significantly improving system efficiency. By associating objects with unique IDs, our system enables rapid object extraction from the scene. Extensive experimental results on object reconstruction and pose estimation demonstrate that DQO-MAP achieves outstanding performance in terms of precision, reconstruction quality, and computational efficiency. The code and dataset are available at: https://github.com/LiHaoy-ux/DQO-MAP.",
    "pdf_url": "https://arxiv.org/pdf/2503.02223v1",
    "github_url": "https://github.com/LiHaoy-ux/DQO-MAP",
    "published": "2025-03-04T02:55:07+00:00",
    "updated": "2025-03-04T02:55:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.01474v2",
    "title": "Interactive Navigation for Legged Manipulators with Learned Arm-Pushing Controller",
    "authors": [
      "Bi",
      "Chen",
      "Zheng"
    ],
    "summary": "Interactive navigation is crucial in scenarios where proactively interacting with objects can yield shorter paths, thus significantly improving traversal efficiency. Existing methods primarily focus on using the robot body to relocate large obstacles (which could be comparable to the size of a robot). However, they prove ineffective in narrow or constrained spaces where the robot's dimensions restrict its manipulation capabilities. This paper introduces a novel interactive navigation framework for legged manipulators, featuring an active arm-pushing mechanism that enables the robot to reposition movable obstacles in space-constrained environments. To this end, we develop a reinforcement learning-based arm-pushing controller with a two-stage reward strategy for large-object manipulation. Specifically, this strategy first directs the manipulator to a designated pushing zone to achieve a kinematically feasible contact configuration. Then, the end effector is guided to maintain its position at appropriate contact points for stable object displacement while preventing toppling. The simulations validate the robustness of the arm-pushing controller, showing that the two-stage reward strategy improves policy convergence and long-term performance. Real-world experiments further demonstrate the effectiveness of the proposed navigation framework, which achieves shorter paths and reduced traversal time. The open-source project can be found at https://github.com/Zhihaibi/Interactive-Navigation-for-legged-manipulator.git.",
    "pdf_url": "https://arxiv.org/pdf/2503.01474v2",
    "github_url": "https://github.com/Zhihaibi/Interactive-Navigation-for-legged-manipulator",
    "published": "2025-03-03T12:29:48+00:00",
    "updated": "2025-07-21T14:00:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.00727v1",
    "title": "From Understanding the World to Intervening in It: A Unified Multi-Scale Framework for Embodied Cognition",
    "authors": [
      "Wang"
    ],
    "summary": "In this paper, we propose AUKAI, an Adaptive Unified Knowledge-Action Intelligence for embodied cognition that seamlessly integrates perception, memory, and decision-making via multi-scale error feedback. Interpreting AUKAI as an embedded world model, our approach simultaneously predicts state transitions and evaluates intervention utility. The framework is underpinned by rigorous theoretical analysis drawn from convergence theory, optimal control, and Bayesian inference, which collectively establish conditions for convergence, stability, and near-optimal performance. Furthermore, we present a hybrid implementation that combines the strengths of neural networks with symbolic reasoning modules, thereby enhancing interpretability and robustness. Finally, we demonstrate the potential of AUKAI through a detailed application in robotic navigation and obstacle avoidance, and we outline comprehensive experimental plans to validate its effectiveness in both simulated and real-world environments.",
    "pdf_url": "https://arxiv.org/pdf/2503.00727v1",
    "github_url": null,
    "published": "2025-03-02T04:43:08+00:00",
    "updated": "2025-03-02T04:43:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.00397v3",
    "title": "Floorplan-SLAM: A Real-Time, High-Accuracy, and Long-Term Multi-Session Point-Plane SLAM for Efficient Floorplan Reconstruction",
    "authors": [
      "Wang",
      "Lv",
      "Wei"
    ],
    "summary": "Floorplan reconstruction provides structural priors essential for reliable indoor robot navigation and high-level scene understanding. However, existing approaches either require time-consuming offline processing with a complete map, or rely on expensive sensors and substantial computational resources. To address the problems, we propose Floorplan-SLAM, which incorporates floorplan reconstruction tightly into a multi-session SLAM system by seamlessly interacting with plane extraction, pose estimation, and back-end optimization, achieving real-time, high-accuracy, and long-term floorplan reconstruction using only a stereo camera. Specifically, we present a robust plane extraction algorithm that operates in a compact plane parameter space and leverages spatially complementary features to accurately detect planar structures, even in weakly textured scenes. Furthermore, we propose a floorplan reconstruction module tightly coupled with the SLAM system, which uses continuously optimized plane landmarks and poses to formulate and solve a novel optimization problem, thereby enabling real-time incremental floorplan reconstruction. Note that by leveraging the map merging capability of multi-session SLAM, our method supports long-term floorplan reconstruction across multiple sessions without redundant data collection. Experiments on the VECtor and the self-collected datasets indicate that Floorplan-SLAM significantly outperforms state-of-the-art methods in terms of plane extraction robustness, pose estimation accuracy, and floorplan reconstruction fidelity and speed, achieving real-time performance at 25-45 FPS without GPU acceleration, which reduces the floorplan reconstruction time for a 1000 square meters scene from over 10 hours to just 9.44 minutes.",
    "pdf_url": "https://arxiv.org/pdf/2503.00397v3",
    "github_url": null,
    "published": "2025-03-01T08:18:11+00:00",
    "updated": "2025-03-05T08:09:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.19024v1",
    "title": "Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments",
    "authors": [
      "Li",
      "Zhou",
      "Hong"
    ],
    "summary": "Vision-and-Language Navigation (VLN) empowers agents to associate time-sequenced visual observations with corresponding instructions to make sequential decisions. However, generalization remains a persistent challenge, particularly when dealing with visually diverse scenes or transitioning from simulated environments to real-world deployment. In this paper, we address the mismatch between human-centric instructions and quadruped robots with a low-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav) approach to mitigate this issue. This work represents the first attempt to highlight the generalization gap in VLN across varying heights of visual observation in realistic robot deployments. Our approach leverages weighted historical observations as enriched spatiotemporal contexts for instruction following, effectively managing feature collisions within cells by assigning appropriate weights to identical features across different viewpoints. This enables low-height robots to overcome challenges such as visual obstructions and perceptual mismatches. Additionally, we transfer the connectivity graph from the HM3D and Gibson datasets as an extra resource to enhance spatial priors and a more comprehensive representation of real-world scenarios, leading to improved performance and generalizability of the waypoint predictor in real-world environments. Extensive experiments demonstrate that our Ground-level Viewpoint Navigation (GVnav) approach significantly improves performance in both simulated environments and real-world deployments with quadruped robots.",
    "pdf_url": "https://arxiv.org/pdf/2502.19024v1",
    "github_url": null,
    "published": "2025-02-26T10:30:40+00:00",
    "updated": "2025-02-26T10:30:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.18041v6",
    "title": "OpenFly: A Comprehensive Platform for Aerial Vision-Language Navigation",
    "authors": [
      "Gao",
      "Li",
      "You"
    ],
    "summary": "Vision-Language Navigation (VLN) aims to guide agents by leveraging language instructions and visual cues, playing a pivotal role in embodied AI. Indoor VLN has been extensively studied, whereas outdoor aerial VLN remains underexplored. The potential reason is that outdoor aerial view encompasses vast areas, making data collection more challenging, which results in a lack of benchmarks. To address this problem, we propose OpenFly, a platform comprising various rendering engines, a versatile toolchain, and a large-scale benchmark for aerial VLN. Firstly, we integrate diverse rendering engines and advanced techniques for environment simulation, including Unreal Engine, GTA V, Google Earth, and 3D Gaussian Splatting (3D GS). Particularly, 3D GS supports real-to-sim rendering, further enhancing the realism of our environments. Secondly, we develop a highly automated toolchain for aerial VLN data collection, streamlining point cloud acquisition, scene semantic segmentation, flight trajectory creation, and instruction generation. Thirdly, based on the toolchain, we construct a large-scale aerial VLN dataset with 100k trajectories, covering diverse heights and lengths across 18 scenes. Moreover, we propose OpenFly-Agent, a keyframe-aware VLN model emphasizing key observations during flight. For benchmarking, extensive experiments and analyses are conducted, evaluating several recent VLN methods and showcasing the superiority of our OpenFly platform and agent. The toolchain, dataset, and codes will be open-sourced.",
    "pdf_url": "https://arxiv.org/pdf/2502.18041v6",
    "github_url": null,
    "published": "2025-02-25T09:57:18+00:00",
    "updated": "2025-07-31T07:55:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.17581v1",
    "title": "Intention Recognition in Real-Time Interactive Navigation Maps",
    "authors": [
      "Zhao",
      "Arefin",
      "Meneguzzi"
    ],
    "summary": "In this demonstration, we develop IntentRec4Maps, a system to recognise users' intentions in interactive maps for real-world navigation. IntentRec4Maps uses the Google Maps Platform as the real-world interactive map, and a very effective approach for recognising users' intentions in real-time. We showcase the recognition process of IntentRec4Maps using two different Path-Planners and a Large Language Model (LLM).   GitHub: https://github.com/PeijieZ/IntentRec4Maps",
    "pdf_url": "https://arxiv.org/pdf/2502.17581v1",
    "github_url": "https://github.com/PeijieZ/IntentRec4Maps",
    "published": "2025-02-24T19:04:18+00:00",
    "updated": "2025-02-24T19:04:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.16515v1",
    "title": "Path Planning using Instruction-Guided Probabilistic Roadmaps",
    "authors": [
      "Bao",
      "Yonetani"
    ],
    "summary": "This work presents a novel data-driven path planning algorithm named Instruction-Guided Probabilistic Roadmap (IG-PRM). Despite the recent development and widespread use of mobile robot navigation, the safe and effective travels of mobile robots still require significant engineering effort to take into account the constraints of robots and their tasks. With IG-PRM, we aim to address this problem by allowing robot operators to specify such constraints through natural language instructions, such as ``aim for wider paths'' or ``mind small gaps''. The key idea is to convert such instructions into embedding vectors using large-language models (LLMs) and use the vectors as a condition to predict instruction-guided cost maps from occupancy maps. By constructing a roadmap based on the predicted costs, we can find instruction-guided paths via the standard shortest path search. Experimental results demonstrate the effectiveness of our approach on both synthetic and real-world indoor navigation environments.",
    "pdf_url": "https://arxiv.org/pdf/2502.16515v1",
    "github_url": null,
    "published": "2025-02-23T09:26:20+00:00",
    "updated": "2025-02-23T09:26:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.14254v2",
    "title": "Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation",
    "authors": [
      "Zhang",
      "Liu",
      "Zhang"
    ],
    "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have made them powerful tools in embodied navigation, enabling agents to leverage commonsense and spatial reasoning for efficient exploration in unfamiliar environments. Existing LLM-based approaches convert global memory, such as semantic or topological maps, into language descriptions to guide navigation. While this improves efficiency and reduces redundant exploration, the loss of geometric information in language-based representations hinders spatial reasoning, especially in intricate environments. To address this, VLM-based approaches directly process ego-centric visual inputs to select optimal directions for exploration. However, relying solely on a first-person perspective makes navigation a partially observed decision-making problem, leading to suboptimal decisions in complex environments. In this paper, we present a novel vision-language model (VLM)-based navigation framework that addresses these challenges by adaptively retrieving task-relevant cues from a global memory module and integrating them with the agent's egocentric observations. By dynamically aligning global contextual information with local perception, our approach enhances spatial reasoning and decision-making in long-horizon tasks. Experimental results demonstrate that the proposed method surpasses previous state-of-the-art approaches in object navigation tasks, providing a more effective and scalable solution for embodied navigation.",
    "pdf_url": "https://arxiv.org/pdf/2502.14254v2",
    "github_url": null,
    "published": "2025-02-20T04:41:40+00:00",
    "updated": "2025-06-10T21:36:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.13637v1",
    "title": "Exploring Mutual Cross-Modal Attention for Context-Aware Human Affordance Generation",
    "authors": [
      "Roy",
      "Bhattacharya",
      "Ghosh"
    ],
    "summary": "Human affordance learning investigates contextually relevant novel pose prediction such that the estimated pose represents a valid human action within the scene. While the task is fundamental to machine perception and automated interactive navigation agents, the exponentially large number of probable pose and action variations make the problem challenging and non-trivial. However, the existing datasets and methods for human affordance prediction in 2D scenes are significantly limited in the literature. In this paper, we propose a novel cross-attention mechanism to encode the scene context for affordance prediction by mutually attending spatial feature maps from two different modalities. The proposed method is disentangled among individual subtasks to efficiently reduce the problem complexity. First, we sample a probable location for a person within the scene using a variational autoencoder (VAE) conditioned on the global scene context encoding. Next, we predict a potential pose template from a set of existing human pose candidates using a classifier on the local context encoding around the predicted location. In the subsequent steps, we use two VAEs to sample the scale and deformation parameters for the predicted pose template by conditioning on the local context and template class. Our experiments show significant improvements over the previous baseline of human affordance injection into complex 2D scenes.",
    "pdf_url": "https://arxiv.org/pdf/2502.13637v1",
    "github_url": null,
    "published": "2025-02-19T11:24:45+00:00",
    "updated": "2025-02-19T11:24:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.13451v4",
    "title": "MapNav: A Novel Memory Representation via Annotated Semantic Maps for Vision-and-Language Navigation",
    "authors": [
      "Zhang",
      "Hao",
      "Xu"
    ],
    "summary": "Vision-and-language navigation (VLN) is a key task in Embodied AI, requiring agents to navigate diverse and unseen environments while following natural language instructions. Traditional approaches rely heavily on historical observations as spatio-temporal contexts for decision making, leading to significant storage and computational overhead. In this paper, we introduce MapNav, a novel end-to-end VLN model that leverages Annotated Semantic Map (ASM) to replace historical frames. Specifically, our approach constructs a top-down semantic map at the start of each episode and update it at each timestep, allowing for precise object mapping and structured navigation information. Then, we enhance this map with explicit textual labels for key regions, transforming abstract semantics into clear navigation cues and generate our ASM. MapNav agent using the constructed ASM as input, and use the powerful end-to-end capabilities of VLM to empower VLN. Extensive experiments demonstrate that MapNav achieves state-of-the-art (SOTA) performance in both simulated and real-world environments, validating the effectiveness of our method. Moreover, we will release our ASM generation source code and dataset to ensure reproducibility, contributing valuable resources to the field. We believe that our proposed MapNav can be used as a new memory representation method in VLN, paving the way for future research in this field.",
    "pdf_url": "https://arxiv.org/pdf/2502.13451v4",
    "github_url": null,
    "published": "2025-02-19T05:52:34+00:00",
    "updated": "2025-07-10T02:53:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.11142v3",
    "title": "NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM",
    "authors": [
      "Wang",
      "Zhu",
      "Lee"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is an essential skill for embodied agents, allowing them to navigate in 3D environments following natural language instructions. High-performance navigation models require a large amount of training data, the high cost of manually annotating data has seriously hindered this field. Therefore, some previous methods translate trajectory videos into step-by-step instructions for expanding data, but such instructions do not match well with users' communication styles that briefly describe destinations or state specific needs. Moreover, local navigation trajectories overlook global context and high-level task planning. To address these issues, we propose NavRAG, a retrieval-augmented generation (RAG) framework that generates user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical scene description tree for 3D scene understanding from global layout to local details, then simulates various user roles with specific demands to retrieve from the scene tree, generating diverse instructions with LLM. We annotate over 2 million navigation instructions across 861 scenes and evaluate the data quality and navigation performance of trained models.",
    "pdf_url": "https://arxiv.org/pdf/2502.11142v3",
    "github_url": null,
    "published": "2025-02-16T14:17:36+00:00",
    "updated": "2025-03-07T06:06:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2503.16441v2",
    "title": "Safe and Efficient Social Navigation through Explainable Safety Regions Based on Topological Features",
    "authors": [
      "Toscano-Duran",
      "Narteni",
      "Carlevaro"
    ],
    "summary": "The recent adoption of artificial intelligence in robotics has driven the development of algorithms that enable autonomous systems to adapt to complex social environments. In particular, safe and efficient social navigation is a key challenge, requiring AI not only to avoid collisions and deadlocks but also to interact intuitively and predictably with its surroundings. Methods based on probabilistic models and the generation of conformal safety regions have shown promising results in defining safety regions with a controlled margin of error, primarily relying on classification approaches and explicit rules to describe collision-free navigation conditions. This work extends the existing perspective by investigating how topological features can contribute to the creation of explainable safety regions in social navigation scenarios, enabling the classification and characterization of different simulation behaviors. Rather than relying on behaviors parameters to generate safety regions, we leverage topological features through topological data analysis. We first utilize global rule-based classification to provide interpretable characterizations of different simulation behaviors, distinguishing between safe and unsafe scenarios based on topological properties. Next, we define safety regions, $S_\\varepsilon$, representing zones in the topological feature space where collisions are avoided with a maximum classification error of $\\varepsilon$. These regions are constructed using adjustable SVM classifiers and order statistics, ensuring a robust and scalable decision boundary. Our approach initially separates simulations with and without collisions, outperforming methods that not incorporate topological features. We further refine safety regions to ensure deadlock-free simulations and integrate both aspects to define a compliant simulation space that guarantees safe and efficient navigation.",
    "pdf_url": "https://arxiv.org/pdf/2503.16441v2",
    "github_url": null,
    "published": "2025-02-14T07:29:13+00:00",
    "updated": "2025-08-28T08:50:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.09238v1",
    "title": "OpenBench: A New Benchmark and Baseline for Semantic Navigation in Smart Logistics",
    "authors": [
      "Wang",
      "Huo",
      "Xu"
    ],
    "summary": "The increasing demand for efficient last-mile delivery in smart logistics underscores the role of autonomous robots in enhancing operational efficiency and reducing costs. Traditional navigation methods, which depend on high-precision maps, are resource-intensive, while learning-based approaches often struggle with generalization in real-world scenarios. To address these challenges, this work proposes the Openstreetmap-enhanced oPen-air sEmantic Navigation (OPEN) system that combines foundation models with classic algorithms for scalable outdoor navigation. The system uses off-the-shelf OpenStreetMap (OSM) for flexible map representation, thereby eliminating the need for extensive pre-mapping efforts. It also employs Large Language Models (LLMs) to comprehend delivery instructions and Vision-Language Models (VLMs) for global localization, map updates, and house number recognition. To compensate the limitations of existing benchmarks that are inadequate for assessing last-mile delivery, this work introduces a new benchmark specifically designed for outdoor navigation in residential areas, reflecting the real-world challenges faced by autonomous delivery systems. Extensive experiments in simulated and real-world environments demonstrate the proposed system's efficacy in enhancing navigation efficiency and reliability. To facilitate further research, our code and benchmark are publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2502.09238v1",
    "github_url": null,
    "published": "2025-02-13T11:55:33+00:00",
    "updated": "2025-02-13T11:55:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.08791v2",
    "title": "VL-Explore: Zero-shot Vision-Language Exploration and Target Discovery by Mobile Robots",
    "authors": [
      "Zhang",
      "Abdullah",
      "Koppal"
    ],
    "summary": "Vision-language navigation (VLN) has emerged as a promising paradigm, enabling mobile robots to perform zero-shot inference and execute tasks without specific pre-programming. However, current systems often separate map exploration and path planning, with exploration relying on inefficient algorithms due to limited (partially observed) environmental information. In this paper, we present a novel navigation pipeline named \"VL-Explore\" for simultaneous exploration and target discovery in unknown environments, leveraging the capabilities of a vision-language model named CLIP. Our approach requires only monocular vision and operates without any prior map or knowledge about the target. For comprehensive evaluations, we designed a functional prototype of a UGV (unmanned ground vehicle) system named \"Open Rover\", a customized platform for general-purpose VLN tasks. We integrated and deployed the VL-Explore pipeline on Open Rover to evaluate its throughput, obstacle avoidance capability, and trajectory performance across various real-world scenarios. Experimental results demonstrate that VL-Explore consistently outperforms traditional map-traversal algorithms and achieves performance comparable to path-planning methods that depend on prior map and target knowledge. Notably, VL-Explore offers real-time active navigation without requiring pre-captured candidate images or pre-built node graphs, addressing key limitations of existing VLN pipelines.",
    "pdf_url": "https://arxiv.org/pdf/2502.08791v2",
    "github_url": null,
    "published": "2025-02-12T21:07:10+00:00",
    "updated": "2025-07-22T21:17:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.07306v2",
    "title": "TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation",
    "authors": [
      "Rajabi",
      "Kosecka"
    ],
    "summary": "In this work, we propose a modular approach for the Vision-Language Navigation (VLN) task by decomposing the problem into four sub-modules that use state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs) in a zero-shot setting. Given navigation instruction in natural language, we first prompt LLM to extract the landmarks and the order in which they are visited. Assuming the known model of the environment, we retrieve the top-k locations of the last landmark and generate $k$ path hypotheses from the starting location to the last landmark using the shortest path algorithm on the topological map of the environment. Each path hypothesis is represented by a sequence of panoramas. We then use dynamic programming to compute the alignment score between the sequence of panoramas and the sequence of landmark names, which match scores obtained from VLM. Finally, we compute the nDTW metric between the hypothesis that yields the highest alignment score to evaluate the path fidelity. We demonstrate superior performance compared to other approaches that use joint semantic maps like VLMaps on the complex R2R-Habitat instruction dataset and quantify in detail the effect of visual grounding on navigation performance.",
    "pdf_url": "https://arxiv.org/pdf/2502.07306v2",
    "github_url": null,
    "published": "2025-02-11T07:09:37+00:00",
    "updated": "2025-06-09T21:25:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.06696v3",
    "title": "Social Media Should Feel Like Minecraft, Not Instagram: 3D Gamer Youth Visions for Meaningful Social Connections through Fictional Inquiry",
    "authors": [
      "Kim",
      "Cho",
      "Liu"
    ],
    "summary": "We investigate youth visions for ideal remote social interactions, drawing on co-design interviews with 23 participants (aged 15-24) experienced with 3D gaming environments. Using a Fictional Inquiry (FI) method set in the Harry Potter universe, this research reveals that young people desire social media that functions more like immersive, navigable shared social spaces. Across these interviews, participants identified six key priorities for meaningful social connection over social media: intuitive social navigation, shared collaborative experiences, communal environments fostering close relationships, flexible self-presentation, intentional engagement, and playful social mechanics. We introduce the \\textit{spatial integrity} framework, a set of four interrelated design principles: spatial presence, spatial composition, spatial configuration, and spatial depth. Together, these principles outline how online spaces can be designed to feel more like meaningful environments, spaces where relationships can grow through shared presence, movement, and intentional interaction. Participants also described the FI process itself as meaningful, not only for generating new ideas but for empowering them to imagine and shape the future of social media.",
    "pdf_url": "https://arxiv.org/pdf/2502.06696v3",
    "github_url": null,
    "published": "2025-02-10T17:19:45+00:00",
    "updated": "2025-09-17T07:21:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.02664v2",
    "title": "Differentiable Composite Neural Signed Distance Fields for Robot Navigation in Dynamic Indoor Environments",
    "authors": [
      "Bukhari",
      "Lawson",
      "Qureshi"
    ],
    "summary": "Neural Signed Distance Fields (SDFs) provide a differentiable environment representation to readily obtain collision checks and well-defined gradients for robot navigation tasks. However, updating neural SDFs as the scene evolves entails re-training, which is tedious, time consuming, and inefficient, making it unsuitable for robot navigation with limited field-of-view in dynamic environments. Towards this objective, we propose a compositional framework of neural SDFs to solve robot navigation in indoor environments using only an onboard RGB-D sensor. Our framework embodies a dual mode procedure for trajectory optimization, with different modes using complementary methods of modeling collision costs and collision avoidance gradients. The primary stage queries the robot body's SDF, swept along the route to goal, at the obstacle point cloud, enabling swift local optimization of trajectories. The secondary stage infers the visible scene's SDF by aligning and composing the SDF representations of its constituents, providing better informed costs and gradients for trajectory optimization. The dual mode procedure combines the best of both stages, achieving a success rate of 98%, 14.4% higher than baseline with comparable amortized plan time on iGibson 2.0. We also demonstrate its effectiveness in adapting to real-world indoor scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2502.02664v2",
    "github_url": null,
    "published": "2025-02-04T19:07:29+00:00",
    "updated": "2025-03-06T18:31:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.01857v2",
    "title": "IG-MCTS: Human-in-the-Loop Cooperative Navigation under Incomplete Information",
    "authors": [
      "Chen",
      "Zhao",
      "Chinchali"
    ],
    "summary": "Human-robot cooperative navigation is challenging under incomplete information. We introduce CoNav-Maze, a simulated environment where a robot navigates with local perception while a human operator provides guidance based on an inaccurate map. The robot can share its onboard camera views to help the operator refine their understanding of the environment. To enable efficient cooperation, we propose Information Gain Monte Carlo Tree Search (IG-MCTS), an online planning algorithm that jointly optimizes autonomous movement and informative communication. IG-MCTS leverages a learned Neural Human Perception Model (NHPM) -- trained on a crowdsourced mapping dataset -- to predict how the human's internal map evolves as new observations are shared. User studies show that IG-MCTS significantly reduces communication demands and yields eye-tracking metrics indicative of lower cognitive load, while maintaining task performance comparable to teleoperation and instruction-following baselines. Finally, we illustrate generalization beyond discrete mazes through a continuous-space waterway navigation setting, in which NHPM benefits from deeper encoder-decoder architectures and IG-MCTS leverages a dynamically constructed Voronoi-partitioned traversability graph.",
    "pdf_url": "https://arxiv.org/pdf/2502.01857v2",
    "github_url": null,
    "published": "2025-02-03T22:08:04+00:00",
    "updated": "2025-10-09T18:20:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.01536v3",
    "title": "VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion",
    "authors": [
      "Zhu",
      "Mou",
      "Li"
    ],
    "summary": "Recent success in legged robot locomotion is attributed to the integration of reinforcement learning and physical simulators. However, these policies often encounter challenges when deployed in real-world environments due to sim-to-real gaps, as simulators typically fail to replicate visual realism and complex real-world geometry. Moreover, the lack of realistic visual rendering limits the ability of these policies to support high-level tasks requiring RGB-based perception like ego-centric navigation. This paper presents a Real-to-Sim-to-Real framework that generates photorealistic and physically interactive \"digital twin\" simulation environments for visual navigation and locomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based scene reconstruction from multi-view images and integrates these environments into simulations that support ego-centric visual perception and mesh-based physical interactions. To demonstrate its effectiveness, we train a reinforcement learning policy within the simulator to perform a visual goal-tracking task. Extensive experiments show that our framework achieves RGB-only sim-to-real policy transfer. Additionally, our framework facilitates the rapid adaptation of robot policies with effective exploration capability in complex new environments, highlighting its potential for applications in households and factories.",
    "pdf_url": "https://arxiv.org/pdf/2502.01536v3",
    "github_url": null,
    "published": "2025-02-03T17:15:05+00:00",
    "updated": "2025-06-03T05:45:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.00543v1",
    "title": "VertiFormer: A Data-Efficient Multi-Task Transformer for Off-Road Robot Mobility",
    "authors": [
      "Nazeri",
      "Pokhrel",
      "Card"
    ],
    "summary": "Sophisticated learning architectures, e.g., Transformers, present a unique opportunity for robots to understand complex vehicle-terrain kinodynamic interactions for off-road mobility. While internet-scale data are available for Natural Language Processing (NLP) and Computer Vision (CV) tasks to train Transformers, real-world mobility data are difficult to acquire with physical robots navigating off-road terrain. Furthermore, training techniques specifically designed to process text and image data in NLP and CV may not apply to robot mobility. In this paper, we propose VertiFormer, a novel data-efficient multi-task Transformer model trained with only one hour of data to address such challenges of applying Transformer architectures for robot mobility on extremely rugged, vertically challenging, off-road terrain. Specifically, VertiFormer employs a new learnable masked modeling and next token prediction paradigm to predict the next pose, action, and terrain patch to enable a variety of off-road mobility tasks simultaneously, e.g., forward and inverse kinodynamics modeling. The non-autoregressive design mitigates computational bottlenecks and error propagation associated with autoregressive models. VertiFormer's unified modality representation also enhances learning of diverse temporal mappings and state representations, which, combined with multiple objective functions, further improves model generalization. Our experiments offer insights into effectively utilizing Transformers for off-road robot mobility with limited data and demonstrate our efficiently trained Transformer can facilitate multiple off-road mobility tasks onboard a physical mobile robot.",
    "pdf_url": "https://arxiv.org/pdf/2502.00543v1",
    "github_url": null,
    "published": "2025-02-01T20:21:00+00:00",
    "updated": "2025-02-01T20:21:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2502.00114v2",
    "title": "Mobile Robot Navigation Using Hand-Drawn Maps: A Vision Language Model Approach",
    "authors": [
      "Tan",
      "Fung",
      "Wang"
    ],
    "summary": "Hand-drawn maps can be used to convey navigation instructions between humans and robots in a natural and efficient manner. However, these maps can often contain inaccuracies such as scale distortions and missing landmarks which present challenges for mobile robot navigation. This paper introduces a novel Hand-drawn Map Navigation (HAM-Nav) architecture that leverages pre-trained vision language models (VLMs) for robot navigation across diverse environments, hand-drawing styles, and robot embodiments, even in the presence of map inaccuracies. HAM-Nav integrates a unique Selective Visual Association Prompting approach for topological map-based position estimation and navigation planning as well as a Predictive Navigation Plan Parser to infer missing landmarks. Extensive experiments were conducted in photorealistic simulated environments, using both wheeled and legged robots, demonstrating the effectiveness of HAM-Nav in terms of navigation success rates and Success weighted by Path Length. Furthermore, a user study in real-world environments highlighted the practical utility of hand-drawn maps for robot navigation as well as successful navigation outcomes compared against a non-hand-drawn map approach.",
    "pdf_url": "https://arxiv.org/pdf/2502.00114v2",
    "github_url": null,
    "published": "2025-01-31T19:03:33+00:00",
    "updated": "2025-04-28T18:14:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.17403v1",
    "title": "General Scene Adaptation for Vision-and-Language Navigation",
    "authors": [
      "Hong",
      "Qiao",
      "Wang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) tasks mainly evaluate agents based on one-time execution of individual instructions across multiple environments, aiming to develop agents capable of functioning in any environment in a zero-shot manner. However, real-world navigation robots often operate in persistent environments with relatively consistent physical layouts, visual observations, and language styles from instructors. Such a gap in the task setting presents an opportunity to improve VLN agents by incorporating continuous adaptation to specific environments. To better reflect these real-world conditions, we introduce GSA-VLN, a novel task requiring agents to execute navigation instructions within a specific scene and simultaneously adapt to it for improved performance over time. To evaluate the proposed task, one has to address two challenges in existing VLN datasets: the lack of OOD data, and the limited number and style diversity of instructions for each scene. Therefore, we propose a new dataset, GSA-R2R, which significantly expands the diversity and quantity of environments and instructions for the R2R dataset to evaluate agent adaptability in both ID and OOD contexts. Furthermore, we design a three-stage instruction orchestration pipeline that leverages LLMs to refine speaker-generated instructions and apply role-playing techniques to rephrase instructions into different speaking styles. This is motivated by the observation that each individual user often has consistent signatures or preferences in their instructions. We conducted extensive experiments on GSA-R2R to thoroughly evaluate our dataset and benchmark various methods. Based on our findings, we propose a novel method, GR-DUET, which incorporates memory-based navigation graphs with an environment-specific training strategy, achieving state-of-the-art results on all GSA-R2R splits.",
    "pdf_url": "https://arxiv.org/pdf/2501.17403v1",
    "github_url": null,
    "published": "2025-01-29T03:57:56+00:00",
    "updated": "2025-01-29T03:57:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.11858v2",
    "title": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents",
    "authors": [
      "Cheng",
      "Tu",
      "Li"
    ],
    "summary": "Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.",
    "pdf_url": "https://arxiv.org/pdf/2501.11858v2",
    "github_url": "https://github.com/thunlp/EmbodiedEval",
    "published": "2025-01-21T03:22:10+00:00",
    "updated": "2025-04-11T04:26:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.10074v3",
    "title": "SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and Chain-of-Thought for Embodied Task Planning",
    "authors": [
      "Liu",
      "Chi",
      "Wu"
    ],
    "summary": "Spatial reasoning is an essential problem in embodied AI research. Efforts to enhance spatial reasoning abilities through supplementary spatial data and fine-tuning have proven limited and ineffective when addressing complex embodied tasks, largely due to their dependence on language-based outputs. While some approaches have introduced a point-based action space to mitigate this issue, they fall short in managing more intricate tasks within complex environments. This deficiency arises from their failure to fully exploit the inherent thinking and reasoning capabilities that are fundamental strengths of Vision-Language Models (VLMs). To address these limitations, we propose a novel approach named SpatialCoT, specifically designed to bolster the spatial reasoning capabilities of VLMs. Our approach comprises two stages: spatial coordinate bi-directional alignment, which aligns vision-language inputs with spatial coordinates, and chain-of-thought spatial grounding, which harnesses the reasoning capabilities of language models for advanced spatial reasoning. We evaluate SpatialCoT on challenging navigation and manipulation tasks, both in simulation and real-world settings. Experimental results demonstrate that our method significantly outperforms previous state-of-the-art approaches in both tasks.",
    "pdf_url": "https://arxiv.org/pdf/2501.10074v3",
    "github_url": null,
    "published": "2025-01-17T09:46:27+00:00",
    "updated": "2025-01-23T02:31:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.06946v1",
    "title": "Learning Implicit Social Navigation Behavior using Deep Inverse Reinforcement Learning",
    "authors": [
      "Kathuria",
      "Liu",
      "Jang"
    ],
    "summary": "This paper reports on learning a reward map for social navigation in dynamic environments where the robot can reason about its path at any time, given agents' trajectories and scene geometry. Humans navigating in dense and dynamic indoor environments often work with several implied social rules. A rule-based approach fails to model all possible interactions between humans, robots, and scenes. We propose a novel Smooth Maximum Entropy Deep Inverse Reinforcement Learning (S-MEDIRL) algorithm that can extrapolate beyond expert demos to better encode scene navigability from few-shot demonstrations. The agent learns to predict the cost maps reasoning on trajectory data and scene geometry. The agent samples a trajectory that is then executed using a local crowd navigation controller. We present results in a photo-realistic simulation environment, with a robot and a human navigating a narrow crossing scenario. The robot implicitly learns to exhibit social behaviors such as yielding to oncoming traffic and avoiding deadlocks. We compare the proposed approach to the popular model-based crowd navigation algorithm ORCA and a rule-based agent that exhibits yielding.",
    "pdf_url": "https://arxiv.org/pdf/2501.06946v1",
    "github_url": null,
    "published": "2025-01-12T21:46:57+00:00",
    "updated": "2025-01-12T21:46:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.05750v3",
    "title": "Semantic Mapping in Indoor Embodied AI -- A Survey on Advances, Challenges, and Future Directions",
    "authors": [
      "Raychaudhuri",
      "Chang"
    ],
    "summary": "Intelligent embodied agents (e.g. robots) need to perform complex semantic tasks in unfamiliar environments. Among many skills that the agents need to possess, building and maintaining a semantic map of the environment is most crucial in long-horizon tasks. A semantic map captures information about the environment in a structured way, allowing the agent to reference it for advanced reasoning throughout the task. While existing surveys in embodied AI focus on general advancements or specific tasks like navigation and manipulation, this paper provides a comprehensive review of semantic map-building approaches in embodied AI, specifically for indoor navigation. We categorize these approaches based on their structural representation (spatial grids, topological graphs, dense point-clouds or hybrid maps) and the type of information they encode (implicit features or explicit environmental data). We also explore the strengths and limitations of the map building techniques, highlight current challenges, and propose future research directions. We identify that the field is moving towards developing open-vocabulary, queryable, task-agnostic map representations, while high memory demands and computational inefficiency still remaining to be open challenges. This survey aims to guide current and future researchers in advancing semantic mapping techniques for embodied AI systems.",
    "pdf_url": "https://arxiv.org/pdf/2501.05750v3",
    "github_url": null,
    "published": "2025-01-10T06:58:14+00:00",
    "updated": "2025-08-10T18:26:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.04947v1",
    "title": "Seeing with Partial Certainty: Conformal Prediction for Robotic Scene Recognition in Built Environments",
    "authors": [
      "Xu",
      "Kamat",
      "Menassa"
    ],
    "summary": "In assistive robotics serving people with disabilities (PWD), accurate place recognition in built environments is crucial to ensure that robots navigate and interact safely within diverse indoor spaces. Language interfaces, particularly those powered by Large Language Models (LLM) and Vision Language Models (VLM), hold significant promise in this context, as they can interpret visual scenes and correlate them with semantic information. However, such interfaces are also known for their hallucinated predictions. In addition, language instructions provided by humans can also be ambiguous and lack precise details about specific locations, objects, or actions, exacerbating the hallucination issue. In this work, we introduce Seeing with Partial Certainty (SwPC) - a framework designed to measure and align uncertainty in VLM-based place recognition, enabling the model to recognize when it lacks confidence and seek assistance when necessary. This framework is built on the theory of conformal prediction to provide statistical guarantees on place recognition while minimizing requests for human help in complex indoor environment settings. Through experiments on the widely used richly-annotated scene dataset Matterport3D, we show that SwPC significantly increases the success rate and decreases the amount of human intervention required relative to the prior art. SwPC can be utilized with any VLMs directly without requiring model fine-tuning, offering a promising, lightweight approach to uncertainty modeling that complements and scales alongside the expanding capabilities of foundational models.",
    "pdf_url": "https://arxiv.org/pdf/2501.04947v1",
    "github_url": null,
    "published": "2025-01-09T03:50:00+00:00",
    "updated": "2025-01-09T03:50:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.04515v1",
    "title": "SplineFormer: An Explainable Transformer-Based Approach for Autonomous Endovascular Navigation",
    "authors": [
      "Jianu",
      "Doust",
      "Li"
    ],
    "summary": "Endovascular navigation is a crucial aspect of minimally invasive procedures, where precise control of curvilinear instruments like guidewires is critical for successful interventions. A key challenge in this task is accurately predicting the evolving shape of the guidewire as it navigates through the vasculature, which presents complex deformations due to interactions with the vessel walls. Traditional segmentation methods often fail to provide accurate real-time shape predictions, limiting their effectiveness in highly dynamic environments. To address this, we propose SplineFormer, a new transformer-based architecture, designed specifically to predict the continuous, smooth shape of the guidewire in an explainable way. By leveraging the transformer's ability, our network effectively captures the intricate bending and twisting of the guidewire, representing it as a spline for greater accuracy and smoothness. We integrate our SplineFormer into an end-to-end robot navigation system by leveraging the condensed information. The experimental results demonstrate that our SplineFormer is able to perform endovascular navigation autonomously and achieves a 50% success rate when cannulating the brachiocephalic artery on the real robot.",
    "pdf_url": "https://arxiv.org/pdf/2501.04515v1",
    "github_url": null,
    "published": "2025-01-08T14:05:24+00:00",
    "updated": "2025-01-08T14:05:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.04279v1",
    "title": "OpenIN: Open-Vocabulary Instance-Oriented Navigation in Dynamic Domestic Environments",
    "authors": [
      "Tang",
      "Wang",
      "Deng"
    ],
    "summary": "In daily domestic settings, frequently used objects like cups often have unfixed positions and multiple instances within the same category, and their carriers frequently change as well. As a result, it becomes challenging for a robot to efficiently navigate to a specific instance. To tackle this challenge, the robot must capture and update scene changes and plans continuously. However, current object navigation approaches primarily focus on the semantic level and lack the ability to dynamically update scene representation. In contrast, this paper captures the relationships between frequently used objects and their static carriers. It constructs an open-vocabulary Carrier-Relationship Scene Graph (CRSG) and updates the carrying status during robot navigation to reflect the dynamic changes of the scene. Based on the CRSG, we further propose an instance navigation strategy that models the navigation process as a Markov Decision Process. At each step, decisions are informed by the Large Language Model's commonsense knowledge and visual-language feature similarity. We designed a series of long-sequence navigation tasks for frequently used everyday items in the Habitat simulator. The results demonstrate that by updating the CRSG, the robot can efficiently navigate to moved targets. Additionally, we deployed our algorithm on a real robot and validated its practical effectiveness. The project page can be found here: https://OpenIN-nav.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2501.04279v1",
    "github_url": null,
    "published": "2025-01-08T05:01:59+00:00",
    "updated": "2025-01-08T05:01:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.05478v2",
    "title": "Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models",
    "authors": [
      "Mansour",
      "Aly",
      "Tharwat"
    ],
    "summary": "Large Language Models (LLMs) such as GPT-4, trained on huge amount of datasets spanning multiple domains, exhibit significant reasoning, understanding, and planning capabilities across various tasks. This study presents the first-ever work in Arabic language integration within the Vision-and-Language Navigation (VLN) domain in robotics, an area that has been notably underexplored in existing research. We perform a comprehensive evaluation of state-of-the-art multi-lingual Small Language Models (SLMs), including GPT-4o mini, Llama 3 8B, and Phi-3 medium 14B, alongside the Arabic-centric LLM, Jais. Our approach utilizes the NavGPT framework, a pure LLM-based instruction-following navigation agent, to assess the impact of language on navigation reasoning through zero-shot sequential action prediction using the R2R dataset. Through comprehensive experiments, we demonstrate that our framework is capable of high-level planning for navigation tasks when provided with instructions in both English and Arabic. However, certain models struggled with reasoning and planning in the Arabic language due to inherent limitations in their capabilities, sub-optimal performance, and parsing issues. These findings highlight the importance of enhancing planning and reasoning capabilities in language models for effective navigation, emphasizing this as a key area for further development while also unlocking the potential of Arabic-language models for impactful real-world applications.",
    "pdf_url": "https://arxiv.org/pdf/2501.05478v2",
    "github_url": null,
    "published": "2025-01-07T16:01:25+00:00",
    "updated": "2025-06-17T16:28:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.01539v1",
    "title": "In Search of a Lost Metric: Human Empowerment as a Pillar of Socially Conscious Navigation",
    "authors": [
      "Baddam",
      "Chalaki",
      "Tadiparthi"
    ],
    "summary": "In social robot navigation, traditional metrics like proxemics and behavior naturalness emphasize human comfort and adherence to social norms but often fail to capture an agent's autonomy and adaptability in dynamic environments. This paper introduces human empowerment, an information-theoretic concept that measures a human's ability to influence their future states and observe those changes, as a complementary metric for evaluating social compliance. This metric reveals how robot navigation policies can indirectly impact human empowerment. We present a framework that integrates human empowerment into the evaluation of social performance in navigation tasks. Through numerical simulations, we demonstrate that human empowerment as a metric not only aligns with intuitive social behavior, but also shows statistically significant differences across various robot navigation policies. These results provide a deeper understanding of how different policies affect social compliance, highlighting the potential of human empowerment as a complementary metric for future research in social navigation.",
    "pdf_url": "https://arxiv.org/pdf/2501.01539v1",
    "github_url": null,
    "published": "2025-01-02T21:13:46+00:00",
    "updated": "2025-01-02T21:13:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2501.09024v1",
    "title": "Social-LLaVA: Enhancing Robot Navigation through Human-Language Reasoning in Social Spaces",
    "authors": [
      "Payandeh",
      "Song",
      "Nazeri"
    ],
    "summary": "Most existing social robot navigation techniques either leverage hand-crafted rules or human demonstrations to connect robot perception to socially compliant actions. However, there remains a significant gap in effectively translating perception into socially compliant actions, much like how human reasoning naturally occurs in dynamic environments. Considering the recent success of Vision-Language Models (VLMs), we propose using language to bridge the gap in human-like reasoning between perception and socially aware robot actions. We create a vision-language dataset, Social robot Navigation via Explainable Interactions (SNEI), featuring 40K human-annotated Visual Question Answers (VQAs) based on 2K human-robot social interactions in unstructured, crowded public spaces, spanning perception, prediction, chain-of-thought reasoning, action, and explanation. We fine-tune a VLM, Social-LLaVA, using SNEI to demonstrate the practical application of our dataset. Social-LLaVA outperforms state-of-the-art models like GPT-4V and Gemini, based on the average of fifteen different human-judge scores across 50 VQA. Deployed onboard a mobile robot, Social-LLaVA enables human-like reasoning, marking a promising step toward socially compliant robot navigation in dynamic public spaces through language reasoning.",
    "pdf_url": "https://arxiv.org/pdf/2501.09024v1",
    "github_url": null,
    "published": "2024-12-30T23:59:30+00:00",
    "updated": "2024-12-30T23:59:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.20977v2",
    "title": "UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI",
    "authors": [
      "Zhong",
      "Wu",
      "Wang"
    ],
    "summary": "We introduce UnrealZoo, a collection of over 100 photo-realistic 3D virtual worlds built on Unreal Engine, designed to reflect the complexity and variability of open-world environments. We also provide a rich variety of playable entities, including humans, animals, robots, and vehicles for embodied AI research. We extend UnrealCV with optimized APIs and tools for data collection, environment augmentation, distributed training, and benchmarking. These improvements achieve significant improvements in the efficiency of rendering and communication, enabling advanced applications such as multi-agent interactions. Our experimental evaluation across visual navigation and tracking tasks reveals two key insights: 1) environmental diversity provides substantial benefits for developing generalizable reinforcement learning (RL) agents, and 2) current embodied agents face persistent challenges in open-world scenarios, including navigation in unstructured terrain, adaptation to unseen morphologies, and managing latency in the close-loop control systems for interacting in highly dynamic objects. UnrealZoo thus serves as both a comprehensive testing ground and a pathway toward developing more capable embodied AI systems for real-world deployment.",
    "pdf_url": "https://arxiv.org/pdf/2412.20977v2",
    "github_url": null,
    "published": "2024-12-30T14:31:01+00:00",
    "updated": "2025-08-12T11:56:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.19595v1",
    "title": "SocRATES: Towards Automated Scenario-based Testing of Social Navigation Algorithms",
    "authors": [
      "Marpally",
      "Goyal",
      "Soh"
    ],
    "summary": "Current social navigation methods and benchmarks primarily focus on proxemics and task efficiency. While these factors are important, qualitative aspects such as perceptions of a robot's social competence are equally crucial for successful adoption and integration into human environments. We propose a more comprehensive evaluation of social navigation through scenario-based testing, where specific human-robot interaction scenarios can reveal key robot behaviors. However, creating such scenarios is often labor-intensive and complex. In this work, we address this challenge by introducing a pipeline that automates the generation of context-, and location-appropriate social navigation scenarios, ready for simulation. Our pipeline transforms simple scenario metadata into detailed textual scenarios, infers pedestrian and robot trajectories, and simulates pedestrian behaviors, which enables more controlled evaluation. We leverage the social reasoning and code-generation capabilities of Large Language Models (LLMs) to streamline scenario generation and translation. Our experiments show that our pipeline produces realistic scenarios and significantly improves scenario translation over naive LLM prompting. Additionally, we present initial feedback from a usability study with social navigation experts and a case-study demonstrating a scenario-based evaluation of three navigation algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2412.19595v1",
    "github_url": null,
    "published": "2024-12-27T11:33:19+00:00",
    "updated": "2024-12-27T11:33:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.19159v1",
    "title": "Mobile Robots through Task-Based Human Instructions using Incremental Curriculum Learning",
    "authors": [
      "Muttaqien",
      "Yorozu",
      "Ohya"
    ],
    "summary": "This paper explores the integration of incremental curriculum learning (ICL) with deep reinforcement learning (DRL) techniques to facilitate mobile robot navigation through task-based human instruction. By adopting a curriculum that mirrors the progressive complexity encountered in human learning, our approach systematically enhances robots' ability to interpret and execute complex instructions over time. We explore the principles of DRL and its synergy with ICL, demonstrating how this combination not only improves training efficiency but also equips mobile robots with the generalization capability required for navigating through dynamic indoor environments. Empirical results indicate that robots trained with our ICL-enhanced DRL framework outperform those trained without curriculum learning, highlighting the benefits of structured learning progressions in robotic training.",
    "pdf_url": "https://arxiv.org/pdf/2412.19159v1",
    "github_url": null,
    "published": "2024-12-26T10:38:40+00:00",
    "updated": "2024-12-26T10:38:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.18865v1",
    "title": "Autonomous Navigation of 4WIS4WID Agricultural Field Mobile Robot using Deep Reinforcement Learning",
    "authors": [
      "Baby",
      "Gohil",
      "Bhattacharya"
    ],
    "summary": "In the futuristic agricultural fields compatible with Agriculture 4.0, robots are envisaged to navigate through crops to perform functions like pesticide spraying and fruit harvesting, which are complex tasks due to factors such as non-geometric internal obstacles, space constraints, and outdoor conditions. In this paper, we attempt to employ Deep Reinforcement Learning (DRL) to solve the problem of 4WIS4WID mobile robot navigation in a structured, automated agricultural field. This paper consists of three sections: parameterization of four-wheel steering configurations, crop row tracking using DRL, and autonomous navigation of 4WIS4WID mobile robot using DRL through multiple crop rows. We show how to parametrize various configurations of four-wheel steering to two variables. This includes symmetric four-wheel steering, zero-turn, and an additional steering configuration that allows the 4WIS4WID mobile robot to move laterally. Using DRL, we also followed an irregularly shaped crop row with symmetric four-wheel steering. In the multiple crop row simulation environment, with the help of waypoints, we effectively performed point-to-point navigation. Finally, a comparative analysis of various DRL algorithms that use continuous actions was carried out.",
    "pdf_url": "https://arxiv.org/pdf/2412.18865v1",
    "github_url": null,
    "published": "2024-12-25T10:33:33+00:00",
    "updated": "2024-12-25T10:33:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.18292v5",
    "title": "Enhancing Multi-Robot Semantic Navigation Through Multimodal Chain-of-Thought Score Collaboration",
    "authors": [
      "Shen",
      "Luo",
      "Chen"
    ],
    "summary": "Understanding how humans cooperatively utilize semantic knowledge to explore unfamiliar environments and decide on navigation directions is critical for house service multi-robot systems. Previous methods primarily focused on single-robot centralized planning strategies, which severely limited exploration efficiency. Recent research has considered decentralized planning strategies for multiple robots, assigning separate planning models to each robot, but these approaches often overlook communication costs. In this work, we propose Multimodal Chain-of-Thought Co-Navigation (MCoCoNav), a modular approach that utilizes multimodal Chain-of-Thought to plan collaborative semantic navigation for multiple robots. MCoCoNav combines visual perception with Vision Language Models (VLMs) to evaluate exploration value through probabilistic scoring, thus reducing time costs and achieving stable outputs. Additionally, a global semantic map is used as a communication bridge, minimizing communication overhead while integrating observational results. Guided by scores that reflect exploration trends, robots utilize this map to assess whether to explore new frontier points or revisit history nodes. Experiments on HM3D_v0.2 and MP3D demonstrate the effectiveness of our approach. Our code is available at https://github.com/FrankZxShen/MCoCoNav.git.",
    "pdf_url": "https://arxiv.org/pdf/2412.18292v5",
    "github_url": "https://github.com/FrankZxShen/MCoCoNav",
    "published": "2024-12-24T09:00:31+00:00",
    "updated": "2025-08-26T05:01:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.17282v1",
    "title": "LMD-PGN: Cross-Modal Knowledge Distillation from First-Person-View Images to Third-Person-View BEV Maps for Universal Point Goal Navigation",
    "authors": [
      "Uemura",
      "Tanaka",
      "Tsukahara"
    ],
    "summary": "Point goal navigation (PGN) is a mapless navigation approach that trains robots to visually navigate to goal points without relying on pre-built maps. Despite significant progress in handling complex environments using deep reinforcement learning, current PGN methods are designed for single-robot systems, limiting their generalizability to multi-robot scenarios with diverse platforms. This paper addresses this limitation by proposing a knowledge transfer framework for PGN, allowing a teacher robot to transfer its learned navigation model to student robots, including those with unknown or black-box platforms. We introduce a novel knowledge distillation (KD) framework that transfers first-person-view (FPV) representations (view images, turning/forward actions) to universally applicable third-person-view (TPV) representations (local maps, subgoals). The state is redefined as reconstructed local maps using SLAM, while actions are mapped to subgoals on a predefined grid. To enhance training efficiency, we propose a sampling-efficient KD approach that aligns training episodes via a noise-robust local map descriptor (LMD). Although validated on 2D wheeled robots, this method can be extended to 3D action spaces, such as drones. Experiments conducted in Habitat-Sim demonstrate the feasibility of the proposed framework, requiring minimal implementation effort. This study highlights the potential for scalable and cross-platform PGN solutions, expanding the applicability of embodied AI systems in multi-robot scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2412.17282v1",
    "github_url": null,
    "published": "2024-12-23T05:05:44+00:00",
    "updated": "2024-12-23T05:05:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.15664v1",
    "title": "SCENIC: Scene-aware Semantic Navigation with Instruction-guided Control",
    "authors": [
      "Zhang",
      "Starke",
      "Guzov"
    ],
    "summary": "Synthesizing natural human motion that adapts to complex environments while allowing creative control remains a fundamental challenge in motion synthesis. Existing models often fall short, either by assuming flat terrain or lacking the ability to control motion semantics through text. To address these limitations, we introduce SCENIC, a diffusion model designed to generate human motion that adapts to dynamic terrains within virtual scenes while enabling semantic control through natural language. The key technical challenge lies in simultaneously reasoning about complex scene geometry while maintaining text control. This requires understanding both high-level navigation goals and fine-grained environmental constraints. The model must ensure physical plausibility and precise navigation across varied terrain, while also preserving user-specified text control, such as ``carefully stepping over obstacles\" or ``walking upstairs like a zombie.\" Our solution introduces a hierarchical scene reasoning approach. At its core is a novel scene-dependent, goal-centric canonicalization that handles high-level goal constraint, and is complemented by an ego-centric distance field that captures local geometric details. This dual representation enables our model to generate physically plausible motion across diverse 3D scenes. By implementing frame-wise text alignment, our system achieves seamless transitions between different motion styles while maintaining scene constraints. Experiments demonstrate our novel diffusion model generates arbitrarily long human motions that both adapt to complex scenes with varying terrain surfaces and respond to textual prompts. Additionally, we show SCENIC can generalize to four real-scene datasets. Our code, dataset, and models will be released at \\url{https://virtualhumans.mpi-inf.mpg.de/scenic/}.",
    "pdf_url": "https://arxiv.org/pdf/2412.15664v1",
    "github_url": null,
    "published": "2024-12-20T08:25:15+00:00",
    "updated": "2024-12-20T08:25:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.14401v2",
    "title": "The One RING: a Robotic Indoor Navigation Generalist",
    "authors": [
      "Eftekhar",
      "Hendrix",
      "Weihs"
    ],
    "summary": "Modern robots vary significantly in shape, size, and sensor configurations used to perceive and interact with their environments. However, most navigation policies are embodiment-specific--a policy trained on one robot typically fails to generalize to another, even with minor changes in body size or camera viewpoint. As custom hardware becomes increasingly common, there is a growing need for a single policy that generalizes across embodiments, eliminating the need to retrain for each specific robot. In this paper, we introduce RING (Robotic Indoor Navigation Generalist), an embodiment-agnostic policy that turns any mobile robot into an effective indoor semantic navigator. Trained entirely in simulation, RING leverages large-scale randomization over robot embodiments to enable robust generalization to many real-world platforms. To support this, we augment the AI2-THOR simulator to instantiate robots with controllable configurations, varying in body size, rotation pivot point, and camera parameters. On the visual object-goal navigation task, RING achieves strong cross-embodiment (XE) generalization--72.1% average success rate across five simulated embodiments (a 16.7% absolute improvement on the Chores-S benchmark) and 78.9% across four real-world platforms, including Stretch RE-1, LoCoBot, and Unitree Go1--matching or even surpassing embodiment-specific policies. We further deploy RING on the RB-Y1 wheeled humanoid in a real-world kitchen environment, showcasing its out-of-the-box potential for mobile manipulation platforms. (Project website: https://one-ring-policy.allen.ai)",
    "pdf_url": "https://arxiv.org/pdf/2412.14401v2",
    "github_url": null,
    "published": "2024-12-18T23:15:41+00:00",
    "updated": "2025-05-23T21:41:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.13729v2",
    "title": "THÖR-MAGNI Act: Actions for Human Motion Modeling in Robot-Shared Industrial Spaces",
    "authors": [
      "Almeida",
      "Schreiter",
      "Rudenko"
    ],
    "summary": "Accurate human activity and trajectory prediction are crucial for ensuring safe and reliable human-robot interactions in dynamic environments, such as industrial settings, with mobile robots. Datasets with fine-grained action labels for moving people in industrial environments with mobile robots are scarce, as most existing datasets focus on social navigation in public spaces. This paper introduces the THÖR-MAGNI Act dataset, a substantial extension of the THÖR-MAGNI dataset, which captures participant movements alongside robots in diverse semantic and spatial contexts. THÖR-MAGNI Act provides 8.3 hours of manually labeled participant actions derived from egocentric videos recorded via eye-tracking glasses. These actions, aligned with the provided THÖR-MAGNI motion cues, follow a long-tailed distribution with diversified acceleration, velocity, and navigation distance profiles. We demonstrate the utility of THÖR-MAGNI Act for two tasks: action-conditioned trajectory prediction and joint action and trajectory prediction. We propose two efficient transformer-based models that outperform the baselines to address these tasks. These results underscore the potential of THÖR-MAGNI Act to develop predictive models for enhanced human-robot interaction in complex environments.",
    "pdf_url": "https://arxiv.org/pdf/2412.13729v2",
    "github_url": null,
    "published": "2024-12-18T11:08:25+00:00",
    "updated": "2024-12-23T22:40:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.13026v2",
    "title": "NAVCON: A Cognitively Inspired and Linguistically Grounded Corpus for Vision and Language Navigation",
    "authors": [
      "Wanchoo",
      "Zuo",
      "Gonzalez"
    ],
    "summary": "We present NAVCON, a large-scale annotated Vision-Language Navigation (VLN) corpus built on top of two popular datasets (R2R and RxR). The paper introduces four core, cognitively motivated and linguistically grounded, navigation concepts and an algorithm for generating large-scale silver annotations of naturally occurring linguistic realizations of these concepts in navigation instructions. We pair the annotated instructions with video clips of an agent acting on these instructions. NAVCON contains 236, 316 concept annotations for approximately 30, 0000 instructions and 2.7 million aligned images (from approximately 19, 000 instructions) showing what the agent sees when executing an instruction. To our knowledge, this is the first comprehensive resource of navigation concepts. We evaluated the quality of the silver annotations by conducting human evaluation studies on NAVCON samples. As further validation of the quality and usefulness of the resource, we trained a model for detecting navigation concepts and their linguistic realizations in unseen instructions. Additionally, we show that few-shot learning with GPT-4o performs well on this task using large-scale silver annotations of NAVCON.",
    "pdf_url": "https://arxiv.org/pdf/2412.13026v2",
    "github_url": null,
    "published": "2024-12-17T15:48:25+00:00",
    "updated": "2024-12-18T03:05:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.11523v2",
    "title": "ON as ALC: Active Loop Closing Object Goal Navigation",
    "authors": [
      "Iwata",
      "Tanaka",
      "Miyazaki"
    ],
    "summary": "In simultaneous localization and mapping, active loop closing (ALC) is an active vision problem that aims to visually guide a robot to maximize the chances of revisiting previously visited points, thereby resetting the drift errors accumulated in the incrementally built map during travel. However, current mainstream navigation strategies that leverage such incomplete maps as workspace prior knowledge often fail in modern long-term autonomy long-distance travel scenarios where map accumulation errors become significant. To address these limitations of map-based navigation, this paper is the first to explore mapless navigation in the embodied AI field, in particular, to utilize object-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques that efficiently explore target objects without using such a prior map. Specifically, in this work, we start from an off-the-shelf mapless ON planner, extend it to utilize a prior map, and further show that the performance in long-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss\" and ``ON loss\". This study highlights a simple and effective approach, called ALC-ON (ALCON), to accelerate the progress of challenging long-distance ALC technology by leveraging the growing frontier-guided, data-driven, and LLM-guided ON technologies.",
    "pdf_url": "https://arxiv.org/pdf/2412.11523v2",
    "github_url": null,
    "published": "2024-12-16T07:59:23+00:00",
    "updated": "2025-05-14T15:19:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.10137v4",
    "title": "Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous Environments",
    "authors": [
      "Chen",
      "An",
      "Huang"
    ],
    "summary": "We address the task of Vision-Language Navigation in Continuous Environments (VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly challenging due to the absence of expert demonstrations for training and minimal environment structural prior to guide navigation. To confront these challenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes zero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion process. CA-Nav continuously translates sub-instructions into navigation plans using two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and the Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria for decomposed sub-instructions as constraints and tracks navigation progress by switching sub-instructions in a constraint-aware manner. CVM, guided by CSM's constraints, generates a value map on the fly and refines it using superpixel clustering to improve navigation stability. CA-Nav achieves the state-of-the-art performance on two VLN-CE benchmarks, surpassing the previous best method by 12 percent and 13 percent in Success Rate on the validation unseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates its effectiveness in real-world robot deployments across various indoor scenes and instructions.",
    "pdf_url": "https://arxiv.org/pdf/2412.10137v4",
    "github_url": null,
    "published": "2024-12-13T13:38:41+00:00",
    "updated": "2025-04-15T02:20:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.09878v1",
    "title": "SonicBoom: Contact Localization Using Array of Microphones",
    "authors": [
      "Lee",
      "Yoo",
      "Oh"
    ],
    "summary": "In cluttered environments where visual sensors encounter heavy occlusion, such as in agricultural settings, tactile signals can provide crucial spatial information for the robot to locate rigid objects and maneuver around them. We introduce SonicBoom, a holistic hardware and learning pipeline that enables contact localization through an array of contact microphones. While conventional sound source localization methods effectively triangulate sources in air, localization through solid media with irregular geometry and structure presents challenges that are difficult to model analytically. We address this challenge through a feature engineering and learning based approach, autonomously collecting 18,000 robot interaction sound pairs to learn a mapping between acoustic signals and collision locations on the robot end effector link. By leveraging relative features between microphones, SonicBoom achieves localization errors of 0.42cm for in distribution interactions and maintains robust performance of 2.22cm error even with novel objects and contact conditions. We demonstrate the system's practical utility through haptic mapping of occluded branches in mock canopy settings, showing that acoustic based sensing can enable reliable robot navigation in visually challenging environments.",
    "pdf_url": "https://arxiv.org/pdf/2412.09878v1",
    "github_url": null,
    "published": "2024-12-13T05:50:13+00:00",
    "updated": "2024-12-13T05:50:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.09624v4",
    "title": "GenEx: Generating an Explorable World",
    "authors": [
      "Lu",
      "Shu",
      "Xiao"
    ],
    "summary": "Understanding, navigating, and exploring the 3D physical real world has long been a central challenge in the development of artificial intelligence. In this work, we take a step toward this goal by introducing GenEx, a system capable of planning complex embodied world exploration, guided by its generative imagination that forms priors (expectations) about the surrounding environments. GenEx generates an entire 3D-consistent imaginative environment from as little as a single RGB image, bringing it to life through panoramic video streams. Leveraging scalable 3D world data curated from Unreal Engine, our generative model is rounded in the physical world. It captures a continuous 360-degree environment with little effort, offering a boundless landscape for AI agents to explore and interact with. GenEx achieves high-quality world generation, robust loop consistency over long trajectories, and demonstrates strong 3D capabilities such as consistency and active 3D mapping. Powered by generative imagination of the world, GPT-assisted agents are equipped to perform complex embodied tasks, including both goal-agnostic exploration and goal-driven navigation. These agents utilize predictive expectation regarding unseen parts of the physical world to refine their beliefs, simulate different outcomes based on potential decisions, and make more informed choices. In summary, we demonstrate that GenEx provides a transformative platform for advancing embodied AI in imaginative spaces and brings potential for extending these capabilities to real-world exploration.",
    "pdf_url": "https://arxiv.org/pdf/2412.09624v4",
    "github_url": null,
    "published": "2024-12-12T18:59:57+00:00",
    "updated": "2025-01-20T16:51:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.09082v3",
    "title": "Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method",
    "authors": [
      "Song",
      "Chen",
      "Liu"
    ],
    "summary": "Existing Vision-Language Navigation (VLN) methods primarily focus on single-stage navigation, limiting their effectiveness in multi-stage and long-horizon tasks within complex and dynamic environments. To address these limitations, we propose a novel VLN task, named Long-Horizon Vision-Language Navigation (LH-VLN), which emphasizes long-term planning and decision consistency across consecutive subtasks. Furthermore, to support LH-VLN, we develop an automated data generation platform NavGen, which constructs datasets with complex task structures and improves data utility through a bidirectional, multi-granularity generation approach. To accurately evaluate complex tasks, we construct the Long-Horizon Planning and Reasoning in VLN (LHPR-VLN) benchmark consisting of 3,260 tasks with an average of 150 task steps, serving as the first dataset specifically designed for the long-horizon vision-language navigation task. Furthermore, we propose Independent Success Rate (ISR), Conditional Success Rate (CSR), and CSR weight by Ground Truth (CGT) metrics, to provide fine-grained assessments of task completion. To improve model adaptability in complex tasks, we propose a novel Multi-Granularity Dynamic Memory (MGDM) module that integrates short-term memory blurring with long-term memory retrieval to enable flexible navigation in dynamic environments. Our platform, benchmark and method supply LH-VLN with a robust data generation pipeline, comprehensive model evaluation dataset, reasonable metrics, and a novel VLN model, establishing a foundational framework for advancing LH-VLN.",
    "pdf_url": "https://arxiv.org/pdf/2412.09082v3",
    "github_url": null,
    "published": "2024-12-12T09:08:13+00:00",
    "updated": "2025-03-19T13:31:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.08591v2",
    "title": "RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation",
    "authors": [
      "Han",
      "Ma",
      "Zhumakhanova"
    ],
    "summary": "Vision-and-Language Navigation (VLN) suffers from the limited diversity and scale of training data, primarily constrained by the manual curation of existing simulators. To address this, we introduce RoomTour3D, a video-instruction dataset derived from web-based room tour videos that capture real-world indoor spaces and human walking demonstrations. Unlike existing VLN datasets, RoomTour3D leverages the scale and diversity of online videos to generate open-ended human walking trajectories and open-world navigable instructions. To compensate for the lack of navigation data in online videos, we perform 3D reconstruction and obtain 3D trajectories of walking paths augmented with additional information on the room types, object locations and 3D shape of surrounding scenes. Our dataset includes $\\sim$100K open-ended description-enriched trajectories with $\\sim$200K instructions, and 17K action-enriched trajectories from 1847 room tour environments. We demonstrate experimentally that RoomTour3D enables significant improvements across multiple VLN tasks including CVDN, SOON, R2R, and REVERIE. Moreover, RoomTour3D facilitates the development of trainable zero-shot VLN agents, showcasing the potential and challenges of advancing towards open-world navigation.",
    "pdf_url": "https://arxiv.org/pdf/2412.08591v2",
    "github_url": null,
    "published": "2024-12-11T18:10:21+00:00",
    "updated": "2025-03-19T10:05:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.08467v2",
    "title": "Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel",
    "authors": [
      "Wang",
      "Li",
      "Hong"
    ],
    "summary": "Creating high-quality data for training robust language-instructed agents is a long-lasting challenge in embodied AI. In this paper, we introduce a Self-Refining Data Flywheel (SRDF) that generates high-quality and large-scale navigational instruction-trajectory pairs by iteratively refining the data pool through the collaboration between two models, the instruction generator and the navigator, without any human-in-the-loop annotation. Specifically, SRDF starts with using a base generator to create an initial data pool for training a base navigator, followed by applying the trained navigator to filter the data pool. This leads to higher-fidelity data to train a better generator, which can, in turn, produce higher-quality data for training the next-round navigator. Such a flywheel establishes a data self-refining process, yielding a continuously improved and highly effective dataset for large-scale language-guided navigation learning. Our experiments demonstrate that after several flywheel rounds, the navigator elevates the performance boundary from 70% to 78% SPL on the classic R2R test set, surpassing human performance (76%) for the first time. Meanwhile, this process results in a superior generator, evidenced by a SPICE increase from 23.5 to 26.2, better than all previous VLN instruction generation methods. Finally, we demonstrate the scalability of our method through increasing environment and instruction diversity, and the generalization ability of our pre-trained navigator across various downstream navigation tasks, surpassing state-of-the-art methods by a large margin in all cases.",
    "pdf_url": "https://arxiv.org/pdf/2412.08467v2",
    "github_url": null,
    "published": "2024-12-11T15:32:24+00:00",
    "updated": "2025-02-28T08:06:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.10439v3",
    "title": "CogNav: Cognitive Process Modeling for Object Goal Navigation with LLMs",
    "authors": [
      "Cao",
      "Zhang",
      "Yu"
    ],
    "summary": "Object goal navigation (ObjectNav) is a fundamental task in embodied AI, requiring an agent to locate a target object in previously unseen environments. This task is particularly challenging because it requires both perceptual and cognitive processes, including object recognition and decision-making. While substantial advancements in perception have been driven by the rapid development of visual foundation models, progress on the cognitive aspect remains constrained, primarily limited to either implicit learning through simulator rollouts or explicit reliance on predefined heuristic rules. Inspired by neuroscientific findings demonstrating that humans maintain and dynamically update fine-grained cognitive states during object search tasks in novel environments, we propose CogNav, a framework designed to mimic this cognitive process using large language models. Specifically, we model the cognitive process using a finite state machine comprising fine-grained cognitive states, ranging from exploration to identification. Transitions between states are determined by a large language model based on a dynamically constructed heterogeneous cognitive map, which contains spatial and semantic information about the scene being explored. Extensive evaluations on the HM3D, MP3D, and RoboTHOR benchmarks demonstrate that our cognitive process modeling significantly improves the success rate of ObjectNav at least by relative 14% over the state-of-the-arts.",
    "pdf_url": "https://arxiv.org/pdf/2412.10439v3",
    "github_url": null,
    "published": "2024-12-11T09:50:35+00:00",
    "updated": "2025-08-28T04:36:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.06465v5",
    "title": "Agent Journey Beyond RGB: Hierarchical Semantic-Spatial Representation Enrichment for Vision-and-Language Navigation",
    "authors": [
      "Zhang",
      "Xu",
      "Li"
    ],
    "summary": "Navigating unseen environments from natural language instructions remains challenging for egocentric agents in Vision-and-Language Navigation (VLN). Humans naturally ground concrete semantic knowledge within spatial layouts during indoor navigation. Although prior work has introduced diverse environment representations to improve reasoning, auxiliary modalities are often naively concatenated with RGB features, which underutilizes each modality's distinct contribution. We propose a hierarchical Semantic Understanding and Spatial Awareness (SUSA) architecture to enable agents to perceive and ground environments at multiple scales. Specifically, the Textual Semantic Understanding (TSU) module supports local action prediction by generating view-level descriptions, capturing fine-grained semantics and narrowing the modality gap between instructions and environments. Complementarily, the Depth Enhanced Spatial Perception (DSP) module incrementally builds a trajectory-level depth exploration map, providing a coarse-grained representation of global spatial layout. Extensive experiments show that the hierarchical representation enrichment of SUSA significantly improves navigation performance over the baseline on discrete VLN benchmarks (REVERIE, R2R, and SOON) and generalizes better to the continuous R2R-CE benchmark.",
    "pdf_url": "https://arxiv.org/pdf/2412.06465v5",
    "github_url": null,
    "published": "2024-12-09T13:10:28+00:00",
    "updated": "2025-11-13T08:51:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.06413v2",
    "title": "World-Consistent Data Generation for Vision-and-Language Navigation",
    "authors": [
      "Zhong",
      "Zhang",
      "Zhang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is a challenging task that requires an agent to navigate through photorealistic environments following natural-language instructions. One main obstacle existing in VLN is data scarcity, leading to poor generalization performance over unseen environments. Though data argumentation is a promising way for scaling up the dataset, how to generate VLN data both diverse and world-consistent remains problematic. To cope with this issue, we propose the world-consistent data generation (WCGEN), an efficacious data-augmentation framework satisfying both diversity and world-consistency, aimed at enhancing the generalization of agents to novel environments. Roughly, our framework consists of two stages, the trajectory stage which leverages a point-cloud based technique to ensure spatial coherency among viewpoints, and the viewpoint stage which adopts a novel angle synthesis method to guarantee spatial and wraparound consistency within the entire observation. By accurately predicting viewpoint changes with 3D knowledge, our approach maintains the world-consistency during the generation procedure. Experiments on a wide range of datasets verify the effectiveness of our method, demonstrating that our data augmentation strategy enables agents to achieve new state-of-the-art results on all navigation tasks, and is capable of enhancing the VLN agents' generalization ability to unseen environments.",
    "pdf_url": "https://arxiv.org/pdf/2412.06413v2",
    "github_url": null,
    "published": "2024-12-09T11:40:54+00:00",
    "updated": "2025-06-25T10:03:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.06224v2",
    "title": "Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks",
    "authors": [
      "Zhang",
      "Wang",
      "Wang"
    ],
    "summary": "A practical navigation agent must be capable of handling a wide range of interaction demands, such as following instructions, searching objects, answering questions, tracking people, and more. Existing models for embodied navigation fall short of serving as practical generalists in the real world, as they are often constrained by specific task configurations or pre-defined maps with discretized waypoints. In this work, we present Uni-NaVid, the first video-based vision-language-action (VLA) model designed to unify diverse embodied navigation tasks and enable seamless navigation for mixed long-horizon tasks in unseen real-world environments. Uni-NaVid achieves this by harmonizing the input and output data configurations for all commonly used embodied navigation tasks and thereby integrating all tasks in one model. For training Uni-NaVid, we collect 3.6 million navigation data samples in total from four essential navigation sub-tasks and foster synergy in learning across them. Extensive experiments on comprehensive navigation benchmarks clearly demonstrate the advantages of unification modeling in Uni-NaVid and show it achieves state-of-the-art performance. Additionally, real-world experiments confirm the model's effectiveness and efficiency, shedding light on its strong generalizability.",
    "pdf_url": "https://arxiv.org/pdf/2412.06224v2",
    "github_url": null,
    "published": "2024-12-09T05:55:55+00:00",
    "updated": "2025-02-06T10:14:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.13211v3",
    "title": "ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home Rearrangement Tasks",
    "authors": [
      "Shukla",
      "Tao",
      "Su"
    ],
    "summary": "High-quality benchmarks are the foundation for embodied AI research, enabling significant advancements in long-horizon navigation, manipulation and rearrangement tasks. However, as frontier tasks in robotics get more advanced, they require faster simulation speed, more intricate test environments, and larger demonstration datasets. To this end, we present MS-HAB, a holistic benchmark for low-level manipulation and in-home object rearrangement. First, we provide a GPU-accelerated implementation of the Home Assistant Benchmark (HAB). We support realistic low-level control and achieve over 3x the speed of prior magical grasp implementations at a fraction of the GPU memory usage. Second, we train extensive reinforcement learning (RL) and imitation learning (IL) baselines for future work to compare against. Finally, we develop a rule-based trajectory filtering system to sample specific demonstrations from our RL policies which match predefined criteria for robot behavior and safety. Combining demonstration filtering with our fast environments enables efficient, controlled data generation at scale.",
    "pdf_url": "https://arxiv.org/pdf/2412.13211v3",
    "github_url": null,
    "published": "2024-12-09T01:29:24+00:00",
    "updated": "2025-02-28T10:10:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.10402v1",
    "title": "TANGO: Training-free Embodied AI Agents for Open-world Tasks",
    "authors": [
      "Ziliotto",
      "Campari",
      "Serafini"
    ],
    "summary": "Large Language Models (LLMs) have demonstrated excellent capabilities in composing various modules together to create programs that can perform complex reasoning tasks on images. In this paper, we propose TANGO, an approach that extends the program composition via LLMs already observed for images, aiming to integrate those capabilities into embodied agents capable of observing and acting in the world. Specifically, by employing a simple PointGoal Navigation model combined with a memory-based exploration policy as a foundational primitive for guiding an agent through the world, we show how a single model can address diverse tasks without additional training. We task an LLM with composing the provided primitives to solve a specific task, using only a few in-context examples in the prompt. We evaluate our approach on three key Embodied AI tasks: Open-Set ObjectGoal Navigation, Multi-Modal Lifelong Navigation, and Open Embodied Question Answering, achieving state-of-the-art results without any specific fine-tuning in challenging zero-shot scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2412.10402v1",
    "github_url": null,
    "published": "2024-12-05T21:52:20+00:00",
    "updated": "2024-12-05T21:52:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.04453v2",
    "title": "NaVILA: Legged Robot Vision-Language-Action Model for Navigation",
    "authors": [
      "Cheng",
      "Ji",
      "Yang"
    ],
    "summary": "This paper proposes to solve the problem of Vision-and-Language Navigation with legged robots, which not only provides a flexible way for humans to command but also allows the robot to navigate through more challenging and cluttered scenes. However, it is non-trivial to translate human language instructions all the way to low-level leg joint actions. We propose NaVILA, a 2-level framework that unifies a Vision-Language-Action model (VLA) with locomotion skills. Instead of directly predicting low-level actions from VLA, NaVILA first generates mid-level actions with spatial information in the form of language, (e.g., \"moving forward 75cm\"), which serves as an input for a visual locomotion RL policy for execution. NaVILA substantially improves previous approaches on existing benchmarks. The same advantages are demonstrated in our newly developed benchmarks with IsaacLab, featuring more realistic scenes, low-level controls, and real-world robot experiments. We show more results at https://navila-bot.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2412.04453v2",
    "github_url": null,
    "published": "2024-12-05T18:58:17+00:00",
    "updated": "2025-02-17T18:27:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.02795v1",
    "title": "Hijacking Vision-and-Language Navigation Agents with Adversarial Environmental Attacks",
    "authors": [
      "Yang",
      "Shi",
      "Slyman"
    ],
    "summary": "Assistive embodied agents that can be instructed in natural language to perform tasks in open-world environments have the potential to significantly impact labor tasks like manufacturing or in-home care -- benefiting the lives of those who come to depend on them. In this work, we consider how this benefit might be hijacked by local modifications in the appearance of the agent's operating environment. Specifically, we take the popular Vision-and-Language Navigation (VLN) task as a representative setting and develop a whitebox adversarial attack that optimizes a 3D attack object's appearance to induce desired behaviors in pretrained VLN agents that observe it in the environment. We demonstrate that the proposed attack can cause VLN agents to ignore their instructions and execute alternative actions after encountering the attack object -- even for instructions and agent paths not considered when optimizing the attack. For these novel settings, we find our attacks can induce early-termination behaviors or divert an agent along an attacker-defined multi-step trajectory. Under both conditions, environmental attacks significantly reduce agent capabilities to successfully follow user instructions.",
    "pdf_url": "https://arxiv.org/pdf/2412.02795v1",
    "github_url": null,
    "published": "2024-12-03T19:54:32+00:00",
    "updated": "2024-12-03T19:54:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.01292v2",
    "title": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences",
    "authors": [
      "Zhi",
      "Chen",
      "Li"
    ],
    "summary": "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement.",
    "pdf_url": "https://arxiv.org/pdf/2412.01292v2",
    "github_url": null,
    "published": "2024-12-02T09:07:57+00:00",
    "updated": "2025-02-02T11:49:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.01250v3",
    "title": "Collaborative Instance Object Navigation: Leveraging Uncertainty-Awareness to Minimize Human-Agent Dialogues",
    "authors": [
      "Taioli",
      "Zorzi",
      "Franchi"
    ],
    "summary": "Language-driven instance object navigation assumes that human users initiate the task by providing a detailed description of the target instance to the embodied agent. While this description is crucial for distinguishing the target from visually similar instances in a scene, providing it prior to navigation can be demanding for human. To bridge this gap, we introduce Collaborative Instance object Navigation (CoIN), a new task setting where the agent actively resolve uncertainties about the target instance during navigation in natural, template-free, open-ended dialogues with human. We propose a novel training-free method, Agent-user Interaction with UncerTainty Awareness (AIUTA), which operates independently from the navigation policy, and focuses on the human-agent interaction reasoning with Vision-Language Models (VLMs) and Large Language Models (LLMs). First, upon object detection, a Self-Questioner model initiates a self-dialogue within the agent to obtain a complete and accurate observation description with a novel uncertainty estimation technique. Then, an Interaction Trigger module determines whether to ask a question to the human, continue or halt navigation, minimizing user input. For evaluation, we introduce CoIN-Bench, with a curated dataset designed for challenging multi-instance scenarios. CoIN-Bench supports both online evaluation with humans and reproducible experiments with simulated user-agent interactions. On CoIN-Bench, we show that AIUTA serves as a competitive baseline, while existing language-driven instance navigation methods struggle in complex multi-instance scenes. Code and benchmark will be available upon acceptance at https://intelligolabs.github.io/CoIN/",
    "pdf_url": "https://arxiv.org/pdf/2412.01250v3",
    "github_url": null,
    "published": "2024-12-02T08:16:38+00:00",
    "updated": "2025-03-18T16:09:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.01857v2",
    "title": "Planning from Imagination: Episodic Simulation and Episodic Memory for Vision-and-Language Navigation",
    "authors": [
      "Pan",
      "Xu",
      "Liu"
    ],
    "summary": "Humans navigate unfamiliar environments using episodic simulation and episodic memory, which facilitate a deeper understanding of the complex relationships between environments and objects. Developing an imaginative memory system inspired by human mechanisms can enhance the navigation performance of embodied agents in unseen environments. However, existing Vision-and-Language Navigation (VLN) agents lack a memory mechanism of this kind. To address this, we propose a novel architecture that equips agents with a reality-imagination hybrid memory system. This system enables agents to maintain and expand their memory through both imaginative mechanisms and navigation actions. Additionally, we design tailored pre-training tasks to develop the agent's imaginative capabilities. Our agent can imagine high-fidelity RGB images for future scenes, achieving state-of-the-art result in Success rate weighted by Path Length (SPL).",
    "pdf_url": "https://arxiv.org/pdf/2412.01857v2",
    "github_url": null,
    "published": "2024-11-30T16:49:14+00:00",
    "updated": "2024-12-25T08:59:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2412.00291v1",
    "title": "Real-Time Metric-Semantic Mapping for Autonomous Navigation in Outdoor Environments",
    "authors": [
      "Jiao",
      "Geng",
      "Li"
    ],
    "summary": "The creation of a metric-semantic map, which encodes human-prior knowledge, represents a high-level abstraction of environments. However, constructing such a map poses challenges related to the fusion of multi-modal sensor data, the attainment of real-time mapping performance, and the preservation of structural and semantic information consistency. In this paper, we introduce an online metric-semantic mapping system that utilizes LiDAR-Visual-Inertial sensing to generate a global metric-semantic mesh map of large-scale outdoor environments. Leveraging GPU acceleration, our mapping process achieves exceptional speed, with frame processing taking less than 7ms, regardless of scenario scale. Furthermore, we seamlessly integrate the resultant map into a real-world navigation system, enabling metric-semantic-based terrain assessment and autonomous point-to-point navigation within a campus environment. Through extensive experiments conducted on both publicly available and self-collected datasets comprising 24 sequences, we demonstrate the effectiveness of our mapping and navigation methodologies. Code has been publicly released: https://github.com/gogojjh/cobra",
    "pdf_url": "https://arxiv.org/pdf/2412.00291v1",
    "github_url": "https://github.com/gogojjh/cobra",
    "published": "2024-11-30T00:05:10+00:00",
    "updated": "2024-11-30T00:05:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.18539v1",
    "title": "AdaVLN: Towards Visual Language Navigation in Continuous Indoor Environments with Moving Humans",
    "authors": [
      "Loh",
      "Bednarz",
      "Xia"
    ],
    "summary": "Visual Language Navigation is a task that challenges robots to navigate in realistic environments based on natural language instructions. While previous research has largely focused on static settings, real-world navigation must often contend with dynamic human obstacles. Hence, we propose an extension to the task, termed Adaptive Visual Language Navigation (AdaVLN), which seeks to narrow this gap. AdaVLN requires robots to navigate complex 3D indoor environments populated with dynamically moving human obstacles, adding a layer of complexity to navigation tasks that mimic the real-world. To support exploration of this task, we also present AdaVLN simulator and AdaR2R datasets. The AdaVLN simulator enables easy inclusion of fully animated human models directly into common datasets like Matterport3D. We also introduce a \"freeze-time\" mechanism for both the navigation task and simulator, which pauses world state updates during agent inference, enabling fair comparisons and experimental reproducibility across different hardware. We evaluate several baseline models on this task, analyze the unique challenges introduced by AdaVLN, and demonstrate its potential to bridge the sim-to-real gap in VLN research.",
    "pdf_url": "https://arxiv.org/pdf/2411.18539v1",
    "github_url": null,
    "published": "2024-11-27T17:36:08+00:00",
    "updated": "2024-11-27T17:36:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.17030v1",
    "title": "g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks",
    "authors": [
      "Wang",
      "Lee"
    ],
    "summary": "We introduce Generalizable 3D-Language Feature Fields (g3D-LF), a 3D representation model pre-trained on large-scale 3D-language dataset for embodied tasks. Our g3D-LF processes posed RGB-D images from agents to encode feature fields for: 1) Novel view representation predictions from any position in the 3D scene; 2) Generations of BEV maps centered on the agent; 3) Querying targets using multi-granularity language within the above-mentioned representations. Our representation can be generalized to unseen environments, enabling real-time construction and dynamic updates. By volume rendering latent features along sampled rays and integrating semantic and spatial relationships through multiscale encoders, our g3D-LF produces representations at different scales and perspectives, aligned with multi-granularity language, via multi-level contrastive learning. Furthermore, we prepare a large-scale 3D-language dataset to align the representations of the feature fields with language. Extensive experiments on Vision-and-Language Navigation under both Panorama and Monocular settings, Zero-shot Object Navigation, and Situated Question Answering tasks highlight the significant advantages and effectiveness of our g3D-LF for embodied tasks.",
    "pdf_url": "https://arxiv.org/pdf/2411.17030v1",
    "github_url": null,
    "published": "2024-11-26T01:54:52+00:00",
    "updated": "2024-11-26T01:54:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.16425v2",
    "title": "TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for Zero-shot Object Navigation",
    "authors": [
      "Zhong",
      "Gao",
      "Ding"
    ],
    "summary": "The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find a previously unseen object by navigating in unfamiliar environments. Such a goal-oriented exploration heavily relies on the ability to perceive, understand, and reason based on the spatial information of the environment. However, current LLM-based approaches convert visual observations to language descriptions and reason in the linguistic space, leading to the loss of spatial information. In this paper, we introduce TopV-Nav, an MLLM-based method that directly reasons on the top-view map with sufficient spatial information. To fully unlock the MLLM's spatial reasoning potential in top-view perspective, we propose the Adaptive Visual Prompt Generation (AVPG) method to adaptively construct semantically-rich top-view map. It enables the agent to directly utilize spatial information contained in the top-view map to conduct thorough reasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to dynamically zoom top-view map at preferred scales, enhancing local fine-grained reasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to predict and to utilize target locations, facilitating global and human-like exploration. Experiments on MP3D and HM3D datasets demonstrate the superiority of our TopV-Nav.",
    "pdf_url": "https://arxiv.org/pdf/2411.16425v2",
    "github_url": null,
    "published": "2024-11-25T14:27:55+00:00",
    "updated": "2025-03-26T07:26:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.16053v2",
    "title": "UnitedVLN: Generalizable Gaussian Splatting for Continuous Vision-Language Navigation",
    "authors": [
      "Dai",
      "Zhao",
      "Chen"
    ],
    "summary": "Vision-and-Language Navigation (VLN), where an agent follows instructions to reach a target destination, has recently seen significant advancements. In contrast to navigation in discrete environments with predefined trajectories, VLN in Continuous Environments (VLN-CE) presents greater challenges, as the agent is free to navigate any unobstructed location and is more vulnerable to visual occlusions or blind spots. Recent approaches have attempted to address this by imagining future environments, either through predicted future visual images or semantic features, rather than relying solely on current observations. However, these RGB-based and feature-based methods lack intuitive appearance-level information or high-level semantic complexity crucial for effective navigation. To overcome these limitations, we introduce a novel, generalizable 3DGS-based pre-training paradigm, called UnitedVLN, which enables agents to better explore future environments by unitedly rendering high-fidelity 360 visual images and semantic features. UnitedVLN employs two key schemes: search-then-query sampling and separate-then-united rendering, which facilitate efficient exploitation of neural primitives, helping to integrate both appearance and semantic information for more robust navigation. Extensive experiments demonstrate that UnitedVLN outperforms state-of-the-art methods on existing VLN-CE benchmarks.",
    "pdf_url": "https://arxiv.org/pdf/2411.16053v2",
    "github_url": null,
    "published": "2024-11-25T02:44:59+00:00",
    "updated": "2025-03-16T10:43:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.14811v2",
    "title": "Fine-Grained Alignment in Vision-and-Language Navigation through Bayesian Optimization",
    "authors": [
      "Song",
      "Gianni",
      "Yang"
    ],
    "summary": "This paper addresses the challenge of fine-grained alignment in Vision-and-Language Navigation (VLN) tasks, where robots navigate realistic 3D environments based on natural language instructions. Current approaches use contrastive learning to align language with visual trajectory sequences. Nevertheless, they encounter difficulties with fine-grained vision negatives. To enhance cross-modal embeddings, we introduce a novel Bayesian Optimization-based adversarial optimization framework for creating fine-grained contrastive vision samples. To validate the proposed methodology, we conduct a series of experiments to assess the effectiveness of the enriched embeddings on fine-grained vision negatives. We conduct experiments on two common VLN benchmarks R2R and REVERIE, experiments on the them demonstrate that these embeddings benefit navigation, and can lead to a promising performance enhancement. Our source code and trained models are available at: https://anonymous.4open.science/r/FGVLN.",
    "pdf_url": "https://arxiv.org/pdf/2411.14811v2",
    "github_url": null,
    "published": "2024-11-22T09:12:02+00:00",
    "updated": "2024-11-30T08:47:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.13262v1",
    "title": "FASTNav: Fine-tuned Adaptive Small-language-models Trained for Multi-point Robot Navigation",
    "authors": [
      "Chen",
      "Han",
      "Li"
    ],
    "summary": "With the rapid development of large language models (LLM), robots are starting to enjoy the benefits of new interaction methods that large language models bring. Because edge computing fulfills the needs for rapid response, privacy, and network autonomy, we believe it facilitates the extensive deployment of large models for robot navigation across various industries. To enable local deployment of language models on edge devices, we adopt some model boosting methods. In this paper, we propose FASTNav - a method for boosting lightweight LLMs, also known as small language models (SLMs), for robot navigation. The proposed method contains three modules: fine-tuning, teacher-student iteration, and language-based multi-point robot navigation. We train and evaluate models with FASTNav in both simulation and real robots, proving that we can deploy them with low cost, high accuracy and low response time. Compared to other model compression methods, FASTNav shows potential in the local deployment of language models and tends to be a promising solution for language-guided robot navigation on edge devices.",
    "pdf_url": "https://arxiv.org/pdf/2411.13262v1",
    "github_url": null,
    "published": "2024-11-20T12:28:13+00:00",
    "updated": "2024-11-20T12:28:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.12150v2",
    "title": "HEIGHT: Heterogeneous Interaction Graph Transformer for Robot Navigation in Crowded and Constrained Environments",
    "authors": [
      "Liu",
      "Xia",
      "Pouria"
    ],
    "summary": "We study the problem of robot navigation in dense and interactive crowds with environmental constraints such as corridors and furniture. Previous methods fail to consider all types of interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-based representation of crowded and constrained scenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different components in the environment and propose a heterogeneous spatio-temporal (st) graph to model distinct interactions among humans, robots, and obstacles. Based on the heterogeneous st-graph, we propose HEIGHT, a novel navigation policy network architecture with different components to capture heterogeneous interactions among entities through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robot to avoid collisions adaptively. Through extensive simulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success and efficiency in challenging navigation scenarios. Furthermore, we demonstrate that our pipeline achieves better zero-shot generalization capability than previous works when the densities of humans and obstacles change. More videos are available at https://sites.google.com/view/crowdnav-height/home.",
    "pdf_url": "https://arxiv.org/pdf/2411.12150v2",
    "github_url": null,
    "published": "2024-11-19T00:56:35+00:00",
    "updated": "2025-05-01T20:03:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.11609v1",
    "title": "VLN-Game: Vision-Language Equilibrium Search for Zero-Shot Semantic Navigation",
    "authors": [
      "Yu",
      "Liu",
      "Han"
    ],
    "summary": "Following human instructions to explore and search for a specified target in an unfamiliar environment is a crucial skill for mobile service robots. Most of the previous works on object goal navigation have typically focused on a single input modality as the target, which may lead to limited consideration of language descriptions containing detailed attributes and spatial relationships. To address this limitation, we propose VLN-Game, a novel zero-shot framework for visual target navigation that can process object names and descriptive language targets effectively. To be more precise, our approach constructs a 3D object-centric spatial map by integrating pre-trained visual-language features with a 3D reconstruction of the physical environment. Then, the framework identifies the most promising areas to explore in search of potential target candidates. A game-theoretic vision language model is employed to determine which target best matches the given language description. Experiments conducted on the Habitat-Matterport 3D (HM3D) dataset demonstrate that the proposed framework achieves state-of-the-art performance in both object goal navigation and language-based navigation tasks. Moreover, we show that VLN-Game can be easily deployed on real-world robots. The success of VLN-Game highlights the promising potential of using game-theoretic methods with compact vision-language models to advance decision-making capabilities in robotic systems. The supplementary video and code can be accessed via the following link: https://sites.google.com/view/vln-game.",
    "pdf_url": "https://arxiv.org/pdf/2411.11609v1",
    "github_url": null,
    "published": "2024-11-18T14:30:46+00:00",
    "updated": "2024-11-18T14:30:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.11394v1",
    "title": "InstruGen: Automatic Instruction Generation for Vision-and-Language Navigation Via Large Multimodal Models",
    "authors": [
      "Yan",
      "Xu",
      "Zhang"
    ],
    "summary": "Recent research on Vision-and-Language Navigation (VLN) indicates that agents suffer from poor generalization in unseen environments due to the lack of realistic training environments and high-quality path-instruction pairs. Most existing methods for constructing realistic navigation scenes have high costs, and the extension of instructions mainly relies on predefined templates or rules, lacking adaptability. To alleviate the issue, we propose InstruGen, a VLN path-instruction pairs generation paradigm. Specifically, we use YouTube house tour videos as realistic navigation scenes and leverage the powerful visual understanding and generation abilities of large multimodal models (LMMs) to automatically generate diverse and high-quality VLN path-instruction pairs. Our method generates navigation instructions with different granularities and achieves fine-grained alignment between instructions and visual observations, which was difficult to achieve with previous methods. Additionally, we design a multi-stage verification mechanism to reduce hallucinations and inconsistency of LMMs. Experimental results demonstrate that agents trained with path-instruction pairs generated by InstruGen achieves state-of-the-art performance on the R2R and RxR benchmarks, particularly in unseen environments. Code is available at https://github.com/yanyu0526/InstruGen.",
    "pdf_url": "https://arxiv.org/pdf/2411.11394v1",
    "github_url": "https://github.com/yanyu0526/InstruGen",
    "published": "2024-11-18T09:11:48+00:00",
    "updated": "2024-11-18T09:11:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.08579v1",
    "title": "NavAgent: Multi-scale Urban Street View Fusion For UAV Embodied Vision-and-Language Navigation",
    "authors": [
      "Liu",
      "Yao",
      "Yue"
    ],
    "summary": "Vision-and-Language Navigation (VLN), as a widely discussed research direction in embodied intelligence, aims to enable embodied agents to navigate in complicated visual environments through natural language commands. Most existing VLN methods focus on indoor ground robot scenarios. However, when applied to UAV VLN in outdoor urban scenes, it faces two significant challenges. First, urban scenes contain numerous objects, which makes it challenging to match fine-grained landmarks in images with complex textual descriptions of these landmarks. Second, overall environmental information encompasses multiple modal dimensions, and the diversity of representations significantly increases the complexity of the encoding process. To address these challenges, we propose NavAgent, the first urban UAV embodied navigation model driven by a large Vision-Language Model. NavAgent undertakes navigation tasks by synthesizing multi-scale environmental information, including topological maps (global), panoramas (medium), and fine-grained landmarks (local). Specifically, we utilize GLIP to build a visual recognizer for landmark capable of identifying and linguisticizing fine-grained landmarks. Subsequently, we develop dynamically growing scene topology map that integrate environmental information and employ Graph Convolutional Networks to encode global environmental data. In addition, to train the visual recognizer for landmark, we develop NavAgent-Landmark2K, the first fine-grained landmark dataset for real urban street scenes. In experiments conducted on the Touchdown and Map2seq datasets, NavAgent outperforms strong baseline models. The code and dataset will be released to the community to facilitate the exploration and development of outdoor VLN.",
    "pdf_url": "https://arxiv.org/pdf/2411.08579v1",
    "github_url": null,
    "published": "2024-11-13T12:51:49+00:00",
    "updated": "2024-11-13T12:51:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.07848v3",
    "title": "Zero-shot Object-Centric Instruction Following: Integrating Foundation Models with Traditional Navigation",
    "authors": [
      "Raychaudhuri",
      "Ta",
      "Ashton"
    ],
    "summary": "Large scale scenes such as multifloor homes can be robustly and efficiently mapped with a 3D graph of landmarks estimated jointly with robot poses in a factor graph, a technique commonly used in commercial robots such as drones and robot vacuums. In this work, we propose Language-Inferred Factor Graph for Instruction Following (LIFGIF), a zero-shot method to ground natural language instructions in such a map. LIFGIF also includes a policy for following natural language navigation instructions in a novel environment while the map is constructed, enabling robust navigation performance in the physical world. To evaluate LIFGIF, we present a new dataset, Object-Centric VLN (OC-VLN), in order to evaluate grounding of object-centric natural language navigation instructions. We compare to two state-of-the-art zero-shot baselines from related tasks, Object Goal Navigation and Vision Language Navigation, to demonstrate that LIFGIF outperforms them across all our evaluation metrics on OCVLN. Finally, we successfully demonstrate the effectiveness of LIFGIF for performing zero-shot object-centric instruction following in the real world on a Boston Dynamics Spot robot.",
    "pdf_url": "https://arxiv.org/pdf/2411.07848v3",
    "github_url": null,
    "published": "2024-11-12T15:01:40+00:00",
    "updated": "2025-05-07T22:19:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.06736v5",
    "title": "MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory",
    "authors": [
      "Park",
      "Cho",
      "Ahn"
    ],
    "summary": "Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers frequently become performance bottlenecks due to repeated failures. In this paper, we argue that the primary cause of failure in many low-level controllers is the absence of an episodic memory system. To address this, we introduce MrSteve (Memory Recall Steve), a novel low-level controller equipped with Place Event Memory (PEM), a form of episodic memory that captures what, where, and when information from episodes. This directly addresses the main limitation of the popular low-level controller, Steve-1. Unlike previous models that rely on short-term memory, PEM organizes spatial and event-based data, enabling efficient recall and navigation in long-horizon tasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing agents to alternate between exploration and task-solving based on recalled events. Our approach significantly improves task-solving and exploration efficiency compared to existing methods. We will release our code and demos on the project page: https://sites.google.com/view/mr-steve.",
    "pdf_url": "https://arxiv.org/pdf/2411.06736v5",
    "github_url": null,
    "published": "2024-11-11T06:04:53+00:00",
    "updated": "2025-04-11T01:35:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.04796v1",
    "title": "MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation",
    "authors": [
      "Paul",
      "Roychoudhury",
      "Bhowmick"
    ],
    "summary": "Visual odometry (VO) is essential for enabling accurate point-goal navigation of embodied agents in indoor environments where GPS and compass sensors are unreliable and inaccurate. However, traditional VO methods face challenges in wide-baseline scenarios, where fast robot motions and low frames per second (FPS) during inference hinder their performance, leading to drift and catastrophic failures in point-goal navigation. Recent deep-learned VO methods show robust performance but suffer from sample inefficiency during training; hence, they require huge datasets and compute resources. So, we propose a robust and sample-efficient VO pipeline based on motion priors available while an agent is navigating an environment. It consists of a training-free action-prior based geometric VO module that estimates a coarse relative pose which is further consumed as a motion prior by a deep-learned VO model, which finally produces a fine relative pose to be used by the navigation policy. This strategy helps our pipeline achieve up to 2x sample efficiency during training and demonstrates superior accuracy and robustness in point-goal navigation tasks compared to state-of-the-art VO method(s). Realistic indoor environments of the Gibson dataset is used in the AI-Habitat simulator to evaluate the proposed approach using navigation metrics (like success/SPL) and pose metrics (like RPE/ATE). We hope this method further opens a direction of work where motion priors from various sources can be utilized to improve VO estimates and achieve better results in embodied navigation tasks.",
    "pdf_url": "https://arxiv.org/pdf/2411.04796v1",
    "github_url": null,
    "published": "2024-11-07T15:36:49+00:00",
    "updated": "2024-11-07T15:36:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.04408v1",
    "title": "Repairing Neural Networks for Safety in Robotic Systems using Predictive Models",
    "authors": [
      "Majd",
      "Clark",
      "Fainekos"
    ],
    "summary": "This paper introduces a new method for safety-aware robot learning, focusing on repairing policies using predictive models. Our method combines behavioral cloning with neural network repair in a two-step supervised learning framework. It first learns a policy from expert demonstrations and then applies repair subject to predictive models to enforce safety constraints. The predictive models can encompass various aspects relevant to robot learning applications, such as proprioceptive states and collision likelihood. Our experimental results demonstrate that the learned policy successfully adheres to a predefined set of safety constraints on two applications: mobile robot navigation, and real-world lower-leg prostheses. Additionally, we have shown that our method effectively reduces repeated interaction with the robot, leading to substantial time savings during the learning process.",
    "pdf_url": "https://arxiv.org/pdf/2411.04408v1",
    "github_url": null,
    "published": "2024-11-07T03:57:37+00:00",
    "updated": "2024-11-07T03:57:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.03873v2",
    "title": "Biomechanics-Aware Trajectory Optimization for Online Navigation during Robotic Physiotherapy",
    "authors": [
      "Belli",
      "Melis",
      "Prendergast"
    ],
    "summary": "Robotic devices provide a great opportunity to assist in delivering physical therapy and rehabilitation movements, yet current robot-assisted methods struggle to incorporate biomechanical metrics essential for safe and effective therapy. We introduce BATON, a Biomechanics-Aware Trajectory Optimization approach to online robotic Navigation of human musculoskeletal loads for rotator cuff rehabilitation. BATON embeds a high-fidelity OpenSim model of the human shoulder into an optimal control framework, generating strain-minimizing trajectories for real-time control of therapeutic movements. \\addedText{Its core strength lies in the ability to adapt biomechanics-informed trajectories online to unpredictable volitional human actions or reflexive reactions during physical human-robot interaction based on robot-sensed motion and forces. BATON's adaptability is enabled by a real-time, model-based estimator that infers changes in muscle activity via a rapid redundancy solver driven by robot pose and force/torque sensor data. We validated BATON through physical human-robot interaction experiments, assessing response speed, motion smoothness, and interaction forces.",
    "pdf_url": "https://arxiv.org/pdf/2411.03873v2",
    "github_url": null,
    "published": "2024-11-06T12:40:59+00:00",
    "updated": "2025-07-11T12:27:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.03669v1",
    "title": "Imagined Potential Games: A Framework for Simulating, Learning and Evaluating Interactive Behaviors",
    "authors": [
      "Sun",
      "Wang",
      "Hung"
    ],
    "summary": "Interacting with human agents in complex scenarios presents a significant challenge for robotic navigation, particularly in environments that necessitate both collision avoidance and collaborative interaction, such as indoor spaces. Unlike static or predictably moving obstacles, human behavior is inherently complex and unpredictable, stemming from dynamic interactions with other agents. Existing simulation tools frequently fail to adequately model such reactive and collaborative behaviors, impeding the development and evaluation of robust social navigation strategies. This paper introduces a novel framework utilizing distributed potential games to simulate human-like interactions in highly interactive scenarios. Within this framework, each agent imagines a virtual cooperative game with others based on its estimation. We demonstrate this formulation can facilitate the generation of diverse and realistic interaction patterns in a configurable manner across various scenarios. Additionally, we have developed a gym-like environment leveraging our interactive agent model to facilitate the learning and evaluation of interactive navigation algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2411.03669v1",
    "github_url": null,
    "published": "2024-11-06T05:08:49+00:00",
    "updated": "2024-11-06T05:08:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.05831v1",
    "title": "To Ask or Not to Ask? Detecting Absence of Information in Vision and Language Navigation",
    "authors": [
      "Abraham",
      "Garg",
      "Dayoub"
    ],
    "summary": "Recent research in Vision Language Navigation (VLN) has overlooked the development of agents' inquisitive abilities, which allow them to ask clarifying questions when instructions are incomplete. This paper addresses how agents can recognize \"when\" they lack sufficient information, without focusing on \"what\" is missing, particularly in VLN tasks with vague instructions. Equipping agents with this ability enhances efficiency by reducing potential digressions and seeking timely assistance. The challenge in identifying such uncertain points is balancing between being overly cautious (high recall) and overly confident (high precision). We propose an attention-based instruction-vagueness estimation module that learns associations between instructions and the agent's trajectory. By leveraging instruction-to-path alignment information during training, the module's vagueness estimation performance improves by around 52% in terms of precision-recall balance. In our ablative experiments, we also demonstrate the effectiveness of incorporating this additional instruction-to-path attention network alongside the cross-modal attention networks within the navigator module. Our results show that the attention scores from the instruction-to-path attention network serve as better indicators for estimating vagueness.",
    "pdf_url": "https://arxiv.org/pdf/2411.05831v1",
    "github_url": null,
    "published": "2024-11-06T04:21:15+00:00",
    "updated": "2024-11-06T04:21:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.03575v1",
    "title": "Semantic Navigation for AI-assisted Ideation",
    "authors": [
      "Sandholm",
      "Dong",
      "Mukherjee"
    ],
    "summary": "We present a novel AI-based ideation assistant and evaluate it in a user study with a group of innovators. The key contribution of our work is twofold: we propose a method of idea exploration in a constrained domain by means of LLM-supported semantic navigation of problem and solution spaces, and employ novel automated data input filtering to improve generations. We found that semantic exploration is preferred to the traditional prompt-output interactions, measured both in explicit survey rankings, and in terms of innovation assistant engagement, where 2.1x more generations were performed using semantic exploration. We also show that filtering input data with metrics such as relevancy, coherence and human alignment leads to improved generations in the same metrics as well as enhanced quality of experience among innovators.",
    "pdf_url": "https://arxiv.org/pdf/2411.03575v1",
    "github_url": null,
    "published": "2024-11-06T00:32:21+00:00",
    "updated": "2024-11-06T00:32:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.01814v1",
    "title": "Enhancing Social Robot Navigation with Integrated Motion Prediction and Trajectory Planning in Dynamic Human Environments",
    "authors": [
      "Canh",
      "HoangVan",
      "Chong"
    ],
    "summary": "Navigating safely in dynamic human environments is crucial for mobile service robots, and social navigation is a key aspect of this process. In this paper, we proposed an integrative approach that combines motion prediction and trajectory planning to enable safe and socially-aware robot navigation. The main idea of the proposed method is to leverage the advantages of Socially Acceptable trajectory prediction and Timed Elastic Band (TEB) by incorporating human interactive information including position, orientation, and motion into the objective function of the TEB algorithms. In addition, we designed social constraints to ensure the safety of robot navigation. The proposed system is evaluated through physical simulation using both quantitative and qualitative metrics, demonstrating its superior performance in avoiding human and dynamic obstacles, thereby ensuring safe navigation. The implementations are open source at: \\url{https://github.com/thanhnguyencanh/SGan-TEB.git}",
    "pdf_url": "https://arxiv.org/pdf/2411.01814v1",
    "github_url": "https://github.com/thanhnguyencanh/SGan-TEB",
    "published": "2024-11-04T05:34:30+00:00",
    "updated": "2024-11-04T05:34:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.22087v1",
    "title": "4D-based Robot Navigation Using Relativistic Image Processing",
    "authors": [
      "Müller",
      "Kranzlmüller"
    ],
    "summary": "Machine perception is an important prerequisite for safe interaction and locomotion in dynamic environments. This requires not only the timely perception of surrounding geometries and distances but also the ability to react to changing situations through predefined, learned but also reusable skill endings of a robot so that physical damage or bodily harm can be avoided. In this context, 4D perception offers the possibility of predicting one's own position and changes in the environment over time. In this paper, we present a 4D-based approach to robot navigation using relativistic image processing. Relativistic image processing handles the temporal-related sensor information in a tensor model within a constructive 4D space. 4D-based navigation expands the causal understanding and the resulting interaction radius of a robot through the use of visual and sensory 4D information.",
    "pdf_url": "https://arxiv.org/pdf/2410.22087v1",
    "github_url": null,
    "published": "2024-10-29T14:42:19+00:00",
    "updated": "2024-10-29T14:42:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.21842v2",
    "title": "Diffusion as Reasoning: Enhancing Object Navigation via Diffusion Model Conditioned on LLM-based Object-Room Knowledge",
    "authors": [
      "Ji",
      "Yun",
      "Liu"
    ],
    "summary": "The Object Navigation (ObjectNav) task aims to guide an agent to locate target objects in unseen environments using partial observations. Prior approaches have employed location prediction paradigms to achieve long-term goal reasoning, yet these methods often struggle to effectively integrate contextual relation reasoning. Alternatively, map completion-based paradigms predict long-term goals by generating semantic maps of unexplored areas. However, existing methods in this category fail to fully leverage known environmental information, resulting in suboptimal map quality that requires further improvement. In this work, we propose a novel approach to enhancing the ObjectNav task, by training a diffusion model to learn the statistical distribution patterns of objects in semantic maps, and using the map of the explored regions during navigation as the condition to generate the map of the unknown regions, thereby realizing the long-term goal reasoning of the target object, i.e., diffusion as reasoning (DAR). Meanwhile, we propose the Room Guidance method, which leverages commonsense knowledge derived from large language models (LLMs) to guide the diffusion model in generating room-aware object distributions. Based on the generated map in the unknown region, the agent sets the predicted location of the target as the goal and moves towards it. Experiments on Gibson and MP3D show the effectiveness of our method.",
    "pdf_url": "https://arxiv.org/pdf/2410.21842v2",
    "github_url": null,
    "published": "2024-10-29T08:10:06+00:00",
    "updated": "2025-06-06T02:18:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.20545v1",
    "title": "ChartA11y: Designing Accessible Touch Experiences of Visualizations with Blind Smartphone Users",
    "authors": [
      "Zhang",
      "Thompson",
      "Shah"
    ],
    "summary": "We introduce ChartA11y, an app developed to enable accessible 2-D visualizations on smartphones for blind users through a participatory and iterative design process involving 13 sessions with two blind partners. We also present a design journey for making accessible touch experiences that go beyond simple auditory feedback, incorporating multimodal interactions and multisensory data representations. Together, ChartA11y aimed at providing direct chart accessing and comprehensive chart understanding by applying a two-mode setting: a semantic navigation framework mode and a direct touch mapping mode. By re-designing traditional touch-to-audio interactions, ChartA11y also extends to accessible scatter plots, addressing the under-explored challenges posed by their non-linear data distribution. Our main contributions encompass the detailed participatory design process and the resulting system, ChartA11y, offering a novel approach for blind users to access visualizations on their smartphones.",
    "pdf_url": "https://arxiv.org/pdf/2410.20545v1",
    "github_url": null,
    "published": "2024-10-27T18:24:05+00:00",
    "updated": "2024-10-27T18:24:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2411.05022v1",
    "title": "Towards Probabilistic Planning of Explanations for Robot Navigation",
    "authors": [
      "Halilovic",
      "Krivic"
    ],
    "summary": "In robotics, ensuring that autonomous systems are comprehensible and accountable to users is essential for effective human-robot interaction. This paper introduces a novel approach that integrates user-centered design principles directly into the core of robot path planning processes. We propose a probabilistic framework for automated planning of explanations for robot navigation, where the preferences of different users regarding explanations are probabilistically modeled to tailor the stochasticity of the real-world human-robot interaction and the communication of decisions of the robot and its actions towards humans. This approach aims to enhance the transparency of robot path planning and adapt to diverse user explanation needs by anticipating the types of explanations that will satisfy individual users.",
    "pdf_url": "https://arxiv.org/pdf/2411.05022v1",
    "github_url": null,
    "published": "2024-10-26T09:52:14+00:00",
    "updated": "2024-10-26T09:52:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.19072v2",
    "title": "Analyzing Human Perceptions of a MEDEVAC Robot in a Simulated Evacuation Scenario",
    "authors": [
      "Jordan",
      "Pandey",
      "Doshi"
    ],
    "summary": "The use of autonomous systems in medical evacuation (MEDEVAC) scenarios is promising, but existing implementations overlook key insights from human-robot interaction (HRI) research. Studies on human-machine teams demonstrate that human perceptions of a machine teammate are critical in governing the machine's performance. Here, we present a mixed factorial design to assess human perceptions of a MEDEVAC robot in a simulated evacuation scenario. Participants were assigned to the role of casualty (CAS) or bystander (BYS) and subjected to three within-subjects conditions based on the MEDEVAC robot's operating mode: autonomous-slow (AS), autonomous-fast (AF), and teleoperation (TO). During each trial, a MEDEVAC robot navigated an 11-meter path, acquiring a casualty and transporting them to an ambulance exchange point while avoiding an idle bystander. Following each trial, subjects completed a questionnaire measuring their emotional states, perceived safety, and social compatibility with the robot. Results indicate a consistent main effect of operating mode on reported emotional states and perceived safety. Pairwise analyses suggest that the employment of the AF operating mode negatively impacted perceptions along these dimensions. There were no persistent differences between casualty and bystander responses.",
    "pdf_url": "https://arxiv.org/pdf/2410.19072v2",
    "github_url": null,
    "published": "2024-10-24T18:28:06+00:00",
    "updated": "2024-10-29T15:44:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.18570v1",
    "title": "Zero-shot Object Navigation with Vision-Language Models Reasoning",
    "authors": [
      "Wen",
      "Huang",
      "Huang"
    ],
    "summary": "Object navigation is crucial for robots, but traditional methods require substantial training data and cannot be generalized to unknown environments. Zero-shot object navigation (ZSON) aims to address this challenge, allowing robots to interact with unknown objects without specific training data. Language-driven zero-shot object navigation (L-ZSON) is an extension of ZSON that incorporates natural language instructions to guide robot navigation and interaction with objects. In this paper, we propose a novel Vision Language model with a Tree-of-thought Network (VLTNet) for L-ZSON. VLTNet comprises four main modules: vision language model understanding, semantic mapping, tree-of-thought reasoning and exploration, and goal identification. Among these modules, Tree-of-Thought (ToT) reasoning and exploration module serves as a core component, innovatively using the ToT reasoning framework for navigation frontier selection during robot exploration. Compared to conventional frontier selection without reasoning, navigation using ToT reasoning involves multi-path reasoning processes and backtracking when necessary, enabling globally informed decision-making with higher accuracy. Experimental results on PASTURE and RoboTHOR benchmarks demonstrate the outstanding performance of our model in LZSON, particularly in scenarios involving complex natural language as target instructions.",
    "pdf_url": "https://arxiv.org/pdf/2410.18570v1",
    "github_url": null,
    "published": "2024-10-24T09:24:07+00:00",
    "updated": "2024-10-24T09:24:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.15044v2",
    "title": "Adanonymizer: Interactively Navigating and Balancing the Duality of Privacy and Output Performance in Human-LLM Interaction",
    "authors": [
      "Zhang",
      "Yi",
      "Xing"
    ],
    "summary": "Current Large Language Models (LLMs) cannot support users to precisely balance privacy protection and output performance during individual consultations. We introduce Adanonymizer, an anonymization plug-in that allows users to control this balance by navigating a trade-off curve. A survey (N=221) revealed a privacy paradox, where users frequently disclosed sensitive information despite acknowledging privacy risks. The study further demonstrated that privacy risks were not significantly correlated with model output performance, highlighting the potential to navigate this trade-off. Adanonymizer normalizes privacy and utility ratings by type and automates the pseudonymization of sensitive terms based on user preferences, significantly reducing user effort. Its 2D color palette interface visualizes the privacy-utility trade-off, allowing users to adjust the balance by manipulating a point. An evaluation (N=36) compared Adanonymizer with ablation methods and differential privacy techniques, where Adanonymizer significantly reduced modification time, achieved better perceived model performance and overall user preference.",
    "pdf_url": "https://arxiv.org/pdf/2410.15044v2",
    "github_url": null,
    "published": "2024-10-19T09:04:01+00:00",
    "updated": "2025-01-27T12:47:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.14851v1",
    "title": "IntelliMove: Enhancing Robotic Planning with Semantic Mapping",
    "authors": [
      "Ngom",
      "Zhang",
      "Zhang"
    ],
    "summary": "Semantic navigation enables robots to understand their environments beyond basic geometry, allowing them to reason about objects, their functions, and their interrelationships. In semantic robotic navigation, creating accurate and semantically enriched maps is fundamental. Planning based on semantic maps not only enhances the robot's planning efficiency and computational speed but also makes the planning more meaningful, supporting a broader range of semantic tasks. In this paper, we introduce two core modules of IntelliMove: IntelliMap, a generic hierarchical semantic topometric map framework developed through an analysis of current technologies strengths and weaknesses, and Semantic Planning, which utilizes the semantic maps from IntelliMap. We showcase use cases that highlight IntelliMove's adaptability and effectiveness. Through experiments in simulated environments, we further demonstrate IntelliMove's capability in semantic navigation.",
    "pdf_url": "https://arxiv.org/pdf/2410.14851v1",
    "github_url": null,
    "published": "2024-10-18T20:32:43+00:00",
    "updated": "2024-10-18T20:32:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.14250v1",
    "title": "Vision-Language Navigation with Energy-Based Policy",
    "authors": [
      "Liu",
      "Wang",
      "Yang"
    ],
    "summary": "Vision-language navigation (VLN) requires an agent to execute actions following human instructions. Existing VLN models are optimized through expert demonstrations by supervised behavioural cloning or incorporating manual reward engineering. While straightforward, these efforts overlook the accumulation of errors in the Markov decision process, and struggle to match the distribution of the expert policy. Going beyond this, we propose an Energy-based Navigation Policy (ENP) to model the joint state-action distribution using an energy-based model. At each step, low energy values correspond to the state-action pairs that the expert is most likely to perform, and vice versa. Theoretically, the optimization objective is equivalent to minimizing the forward divergence between the occupancy measure of the expert and ours. Consequently, ENP learns to globally align with the expert policy by maximizing the likelihood of the actions and modeling the dynamics of the navigation states in a collaborative manner. With a variety of VLN architectures, ENP achieves promising performances on R2R, REVERIE, RxR, and R2R-CE, unleashing the power of existing VLN models.",
    "pdf_url": "https://arxiv.org/pdf/2410.14250v1",
    "github_url": null,
    "published": "2024-10-18T08:01:36+00:00",
    "updated": "2024-10-18T08:01:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.13825v2",
    "title": "AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents",
    "authors": [
      "Yang",
      "Liu",
      "Chaudhary"
    ],
    "summary": "Autonomy via agents using large language models (LLMs) for personalized, standardized tasks boosts human efficiency. Automating web tasks (like booking hotels within a budget) is increasingly sought after. Fulfilling practical needs, the web agent also serves as an important proof-of-concept example for various agent grounding scenarios, with its success promising advancements in many future applications. Prior research often handcrafts web agent strategies (e.g., prompting templates, multi-agent systems, search methods, etc.) and the corresponding in-context examples, which may not generalize well across all real-world scenarios. On the other hand, there has been limited study on the misalignment between a web agent's observation/action representation and the pre-training data of the LLM it's based on. This discrepancy is especially notable when LLMs are primarily trained for language completion rather than tasks involving embodied navigation actions and symbolic web elements. Our study enhances an LLM-based web agent by simply refining its observation and action space to better align with the LLM's capabilities. This approach enables our base agent to significantly outperform previous methods on a wide variety of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose web interaction tasks, our agent AgentOccam surpasses the previous state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute points respectively, and boosts the success rate by 26.6 points (+161%) over similar plain web agents with its observation and action space alignment. We achieve this without using in-context examples, new agent roles, online feedback or search strategies. AgentOccam's simple design highlights LLMs' impressive zero-shot performance on web tasks, and underlines the critical role of carefully tuning observation and action spaces for LLM-based agents.",
    "pdf_url": "https://arxiv.org/pdf/2410.13825v2",
    "github_url": null,
    "published": "2024-10-17T17:50:38+00:00",
    "updated": "2025-05-24T03:55:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.13418v1",
    "title": "Interactive Navigation with Adaptive Non-prehensile Mobile Manipulation",
    "authors": [
      "Dai",
      "Liu",
      "Sreenath"
    ],
    "summary": "This paper introduces a framework for interactive navigation through adaptive non-prehensile mobile manipulation. A key challenge in this process is handling objects with unknown dynamics, which are difficult to infer from visual observation. To address this, we propose an adaptive dynamics model for common movable indoor objects via learned SE(2) dynamics representations. This model is integrated into Model Predictive Path Integral (MPPI) control to guide the robot's interactions. Additionally, the learned dynamics help inform decision-making when navigating around objects that cannot be manipulated.Our approach is validated in both simulation and real-world scenarios, demonstrating its ability to accurately represent object dynamics and effectively manipulate various objects. We further highlight its success in the Navigation Among Movable Objects (NAMO) task by deploying the proposed framework on a dynamically balancing mobile robot, Shmoobot. Project website: https://cmushmoobot.github.io/AdaptivePushing/.",
    "pdf_url": "https://arxiv.org/pdf/2410.13418v1",
    "github_url": null,
    "published": "2024-10-17T10:40:31+00:00",
    "updated": "2024-10-17T10:40:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.12407v2",
    "title": "Beyond Coarse-Grained Matching in Video-Text Retrieval",
    "authors": [
      "Chen",
      "Doughty",
      "Li"
    ],
    "summary": "Video-text retrieval has seen significant advancements, yet the ability of models to discern subtle differences in captions still requires verification. In this paper, we introduce a new approach for fine-grained evaluation. Our approach can be applied to existing datasets by automatically generating hard negative test captions with subtle single-word variations across nouns, verbs, adjectives, adverbs, and prepositions. We perform comprehensive experiments using four state-of-the-art models across two standard benchmarks (MSR-VTT and VATEX) and two specially curated datasets enriched with detailed descriptions (VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our analyses show that the current evaluation benchmarks fall short in detecting a model's ability to perceive subtle single-word differences, 2) our fine-grained evaluation highlights the difficulty models face in distinguishing such subtle variations. To enhance fine-grained understanding, we propose a new baseline that can be easily combined with current methods. Experiments on our fine-grained evaluations demonstrate that this approach enhances a model's ability to understand fine-grained differences.",
    "pdf_url": "https://arxiv.org/pdf/2410.12407v2",
    "github_url": null,
    "published": "2024-10-16T09:42:29+00:00",
    "updated": "2024-10-17T15:59:34+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.11402v1",
    "title": "M2Diffuser: Diffusion-based Trajectory Optimization for Mobile Manipulation in 3D Scenes",
    "authors": [
      "Yan",
      "Zhang",
      "Han"
    ],
    "summary": "Recent advances in diffusion models have opened new avenues for research into embodied AI agents and robotics. Despite significant achievements in complex robotic locomotion and skills, mobile manipulation-a capability that requires the coordination of navigation and manipulation-remains a challenge for generative AI techniques. This is primarily due to the high-dimensional action space, extended motion trajectories, and interactions with the surrounding environment. In this paper, we introduce M2Diffuser, a diffusion-based, scene-conditioned generative model that directly generates coordinated and efficient whole-body motion trajectories for mobile manipulation based on robot-centric 3D scans. M2Diffuser first learns trajectory-level distributions from mobile manipulation trajectories provided by an expert planner. Crucially, it incorporates an optimization module that can flexibly accommodate physical constraints and task objectives, modeled as cost and energy functions, during the inference process. This enables the reduction of physical violations and execution errors at each denoising step in a fully differentiable manner. Through benchmarking on three types of mobile manipulation tasks across over 20 scenes, we demonstrate that M2Diffuser outperforms state-of-the-art neural planners and successfully transfers the generated trajectories to a real-world robot. Our evaluations underscore the potential of generative AI to enhance the generalization of traditional planning and learning-based robotic methods, while also highlighting the critical role of enforcing physical constraints for safe and robust execution.",
    "pdf_url": "https://arxiv.org/pdf/2410.11402v1",
    "github_url": null,
    "published": "2024-10-15T08:49:35+00:00",
    "updated": "2024-10-15T08:49:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.10646v2",
    "title": "DR-MPC: Deep Residual Model Predictive Control for Real-world Social Navigation",
    "authors": [
      "Han",
      "Thomas",
      "Zhang"
    ],
    "summary": "How can a robot safely navigate around people with complex motion patterns? Deep Reinforcement Learning (DRL) in simulation holds some promise, but much prior work relies on simulators that fail to capture the nuances of real human motion. Thus, we propose Deep Residual Model Predictive Control (DR-MPC) to enable robots to quickly and safely perform DRL from real-world crowd navigation data. By blending MPC with model-free DRL, DR-MPC overcomes the DRL challenges of large data requirements and unsafe initial behavior. DR-MPC is initialized with MPC-based path tracking, and gradually learns to interact more effectively with humans. To further accelerate learning, a safety component estimates out-of-distribution states to guide the robot away from likely collisions. In simulation, we show that DR-MPC substantially outperforms prior work, including traditional DRL and residual DRL models. Hardware experiments show our approach successfully enables a robot to navigate a variety of crowded situations with few errors using less than 4 hours of training data.",
    "pdf_url": "https://arxiv.org/pdf/2410.10646v2",
    "github_url": null,
    "published": "2024-10-14T15:56:43+00:00",
    "updated": "2025-02-14T02:14:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.09874v1",
    "title": "ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination",
    "authors": [
      "Zhao",
      "Cai",
      "Tang"
    ],
    "summary": "Visual navigation is an essential skill for home-assistance robots, providing the object-searching ability to accomplish long-horizon daily tasks. Many recent approaches use Large Language Models (LLMs) for commonsense inference to improve exploration efficiency. However, the planning process of LLMs is limited within texts and it is difficult to represent the spatial occupancy and geometry layout only by texts. Both are important for making rational navigation decisions. In this work, we seek to unleash the spatial perception and planning ability of Vision-Language Models (VLMs), and explore whether the VLM, with only on-board camera captured RGB/RGB-D stream inputs, can efficiently finish the visual navigation tasks in a mapless manner. We achieve this by developing the imagination-powered navigation framework ImagineNav, which imagines the future observation images at valuable robot views and translates the complex navigation planning process into a rather simple best-view image selection problem for VLM. To generate appropriate candidate robot views for imagination, we introduce the Where2Imagine module, which is distilled to align with human navigation habits. Finally, to reach the VLM preferred views, an off-the-shelf point-goal navigation policy is utilized. Empirical experiments on the challenging open-vocabulary object navigation benchmarks demonstrates the superiority of our proposed system.",
    "pdf_url": "https://arxiv.org/pdf/2410.09874v1",
    "github_url": null,
    "published": "2024-10-13T15:31:31+00:00",
    "updated": "2024-10-13T15:31:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.08649v1",
    "title": "E-Motion: Future Motion Simulation via Event Sequence Diffusion",
    "authors": [
      "Wu",
      "Zhu",
      "Hou"
    ],
    "summary": "Forecasting a typical object's future motion is a critical task for interpreting and interacting with dynamic environments in computer vision. Event-based sensors, which could capture changes in the scene with exceptional temporal granularity, may potentially offer a unique opportunity to predict future motion with a level of detail and precision previously unachievable. Inspired by that, we propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion simulation framework. Specifically, we initially employ pre-trained stable video diffusion models to adapt the event sequence dataset. This process facilitates the transfer of extensive knowledge from RGB videos to an event-centric domain. Moreover, we introduce an alignment mechanism that utilizes reinforcement learning techniques to enhance the reverse generation trajectory of the diffusion model, ensuring improved performance and accuracy. Through extensive testing and validation, we demonstrate the effectiveness of our method in various complex scenarios, showcasing its potential to revolutionize motion flow prediction in computer vision applications such as autonomous vehicle guidance, robotic navigation, and interactive media. Our findings suggest a promising direction for future research in enhancing the interpretative power and predictive accuracy of computer vision systems.",
    "pdf_url": "https://arxiv.org/pdf/2410.08649v1",
    "github_url": null,
    "published": "2024-10-11T09:19:23+00:00",
    "updated": "2024-10-11T09:19:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.08500v3",
    "title": "Exploring Spatial Representation to Enhance LLM Reasoning in Aerial Vision-Language Navigation",
    "authors": [
      "Gao",
      "Wang",
      "Han"
    ],
    "summary": "Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned Aerial Vehicles (UAVs) to navigate in outdoor environments through natural language instructions and visual cues. However, it remains challenging due to the complex spatial relationships in aerial scenes.In this paper, we propose a training-free, zero-shot framework for aerial VLN tasks, where the large language model (LLM) is leveraged as the agent for action prediction. Specifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to enhance the spatial reasoning capabilities of LLMs. This is achieved by extracting and projecting instruction-related semantic masks onto a top-down map, which presents spatial and topological information about surrounding landmarks and grows during the navigation process. At each step, a local map centered at the UAV is extracted from the growing top-down map, and transformed into a ma trix representation with distance metrics, serving as the text prompt to LLM for action prediction in response to the given instruction. Experiments conducted in real and simulation environments have proved the effectiveness and robustness of our method, achieving absolute success rate improvements of 26.8% and 5.8% over current state-of-the-art methods on simple and complex navigation tasks, respectively. The dataset and code will be released soon.",
    "pdf_url": "https://arxiv.org/pdf/2410.08500v3",
    "github_url": null,
    "published": "2024-10-11T03:54:48+00:00",
    "updated": "2025-08-11T03:42:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.08189v1",
    "title": "SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation",
    "authors": [
      "Yin",
      "Xu",
      "Wu"
    ],
    "summary": "In this paper, we propose a new framework for zero-shot object navigation. Existing zero-shot object navigation methods prompt LLM with the text of spatially closed objects, which lacks enough scene context for in-depth reasoning. To better preserve the information of environment and fully exploit the reasoning ability of LLM, we propose to represent the observed scene with 3D scene graph. The scene graph encodes the relationships between objects, groups and rooms with a LLM-friendly structure, for which we design a hierarchical chain-of-thought prompt to help LLM reason the goal location according to scene context by traversing the nodes and edges. Moreover, benefit from the scene graph representation, we further design a re-perception mechanism to empower the object navigation framework with the ability to correct perception error. We conduct extensive experiments on MP3D, HM3D and RoboTHOR environments, where SG-Nav surpasses previous state-of-the-art zero-shot methods by more than 10% SR on all benchmarks, while the decision process is explainable. To the best of our knowledge, SG-Nav is the first zero-shot method that achieves even higher performance than supervised object navigation methods on the challenging MP3D benchmark.",
    "pdf_url": "https://arxiv.org/pdf/2410.08189v1",
    "github_url": null,
    "published": "2024-10-10T17:57:19+00:00",
    "updated": "2024-10-10T17:57:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.07087v2",
    "title": "Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology",
    "authors": [
      "Wang",
      "Yang",
      "Wang"
    ],
    "summary": "Developing agents capable of navigating to a target location based on language instructions and visual information, known as vision-language navigation (VLN), has attracted widespread interest. Most research has focused on ground-based agents, while UAV-based VLN remains relatively underexplored. Recent efforts in UAV vision-language navigation predominantly adopt ground-based VLN settings, relying on predefined discrete action spaces and neglecting the inherent disparities in agent movement dynamics and the complexity of navigation tasks between ground and aerial environments. To address these disparities and challenges, we propose solutions from three perspectives: platform, benchmark, and methodology. To enable realistic UAV trajectory simulation in VLN tasks, we propose the OpenUAV platform, which features diverse environments, realistic flight control, and extensive algorithmic support. We further construct a target-oriented VLN dataset consisting of approximately 12k trajectories on this platform, serving as the first dataset specifically designed for realistic UAV VLN tasks. To tackle the challenges posed by complex aerial environments, we propose an assistant-guided UAV object search benchmark called UAV-Need-Help, which provides varying levels of guidance information to help UAVs better accomplish realistic VLN tasks. We also propose a UAV navigation LLM that, given multi-view images, task descriptions, and assistant instructions, leverages the multimodal understanding capabilities of the MLLM to jointly process visual and textual information, and performs hierarchical trajectory generation. The evaluation results of our method significantly outperform the baseline models, while there remains a considerable gap between our results and those achieved by human operators, underscoring the challenge presented by the UAV-Need-Help task.",
    "pdf_url": "https://arxiv.org/pdf/2410.07087v2",
    "github_url": null,
    "published": "2024-10-09T17:29:01+00:00",
    "updated": "2024-10-10T05:02:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.06613v2",
    "title": "ES-Gaussian: Gaussian Splatting Mapping via Error Space-Based Gaussian Completion",
    "authors": [
      "Chen",
      "Zeng",
      "Li"
    ],
    "summary": "Accurate and affordable indoor 3D reconstruction is critical for effective robot navigation and interaction. Traditional LiDAR-based mapping provides high precision but is costly, heavy, and power-intensive, with limited ability for novel view rendering. Vision-based mapping, while cost-effective and capable of capturing visual data, often struggles with high-quality 3D reconstruction due to sparse point clouds. We propose ES-Gaussian, an end-to-end system using a low-altitude camera and single-line LiDAR for high-quality 3D indoor reconstruction. Our system features Visual Error Construction (VEC) to enhance sparse point clouds by identifying and correcting areas with insufficient geometric detail from 2D error maps. Additionally, we introduce a novel 3DGS initialization method guided by single-line LiDAR, overcoming the limitations of traditional multi-view setups and enabling effective reconstruction in resource-constrained environments. Extensive experimental results on our new Dreame-SR dataset and a publicly available dataset demonstrate that ES-Gaussian outperforms existing methods, particularly in challenging scenarios. The project page is available at https://chenlu-china.github.io/ES-Gaussian/.",
    "pdf_url": "https://arxiv.org/pdf/2410.06613v2",
    "github_url": null,
    "published": "2024-10-09T07:09:29+00:00",
    "updated": "2024-10-30T10:21:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.06437v2",
    "title": "LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality",
    "authors": [
      "Takeyama",
      "Liu",
      "Sra"
    ],
    "summary": "Understanding human locomotion is crucial for AI agents such as robots, particularly in complex indoor home environments. Modeling human trajectories in these spaces requires insight into how individuals maneuver around physical obstacles and manage social navigation dynamics. These dynamics include subtle behaviors influenced by proxemics - the social use of space, such as stepping aside to allow others to pass or choosing longer routes to avoid collisions. Previous research has developed datasets of human motion in indoor scenes, but these are often limited in scale and lack the nuanced social navigation dynamics common in home environments. To address this, we present LocoVR, a dataset of 7000+ two-person trajectories captured in virtual reality from over 130 different indoor home environments. LocoVR provides accurate trajectory data and precise spatial information, along with rich examples of socially-motivated movement behaviors. For example, the dataset captures instances of individuals navigating around each other in narrow spaces, adjusting paths to respect personal boundaries in living areas, and coordinating movements in high-traffic zones like entryways and kitchens. Our evaluation shows that LocoVR significantly enhances model performance in three practical indoor tasks utilizing human trajectories, and demonstrates predicting socially-aware navigation patterns in home environments.",
    "pdf_url": "https://arxiv.org/pdf/2410.06437v2",
    "github_url": null,
    "published": "2024-10-09T00:45:02+00:00",
    "updated": "2025-03-04T23:49:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.17267v1",
    "title": "Zero-Shot Vision-and-Language Navigation with Collision Mitigation in Continuous Environment",
    "authors": [
      "Jeong",
      "Kang",
      "Kim"
    ],
    "summary": "We propose the zero-shot Vision-and-Language Navigation with Collision Mitigation (VLN-CM), which takes these considerations. VLN-CM is composed of four modules and predicts the direction and distance of the next movement at each step. We utilize large foundation models for each modules. To select the direction, we use the Attention Spot Predictor (ASP), View Selector (VS), and Progress Monitor (PM). The ASP employs a Large Language Model (e.g. ChatGPT) to split navigation instructions into attention spots, which are objects or scenes at the location to move to (e.g. a yellow door). The VS selects from panorama images provided at 30-degree intervals the one that includes the attention spot, using CLIP similarity. We then choose the angle of the selected image as the direction to move in. The PM uses a rule-based approach to decide which attention spot to focus on next, among multiple spots derived from the instructions. If the similarity between the current attention spot and the visual observations decreases consecutively at each step, the PM determines that the agent has passed the current spot and moves on to the next one. For selecting the distance to move, we employed the Open Map Predictor (OMP). The OMP uses panorama depth information to predict an occupancy mask. We then selected a collision-free distance in the predicted direction based on the occupancy mask. We evaluated our method using the validation data of VLN-CE. Our approach showed better performance than several baseline methods, and the OPM was effective in mitigating collisions for the agent.",
    "pdf_url": "https://arxiv.org/pdf/2410.17267v1",
    "github_url": null,
    "published": "2024-10-07T11:59:01+00:00",
    "updated": "2024-10-07T11:59:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.04302v1",
    "title": "PANav: Toward Privacy-Aware Robot Navigation via Vision-Language Models",
    "authors": [
      "Yu",
      "Kasaei",
      "Cao"
    ],
    "summary": "Navigating robots discreetly in human work environments while considering the possible privacy implications of robotic tasks presents significant challenges. Such scenarios are increasingly common, for instance, when robots transport sensitive objects that demand high levels of privacy in spaces crowded with human activities. While extensive research has been conducted on robotic path planning and social awareness, current robotic systems still lack the functionality of privacy-aware navigation in public environments. To address this, we propose a new framework for mobile robot navigation that leverages vision-language models to incorporate privacy awareness into adaptive path planning. Specifically, all potential paths from the starting point to the destination are generated using the A* algorithm. Concurrently, the vision-language model is used to infer the optimal path for privacy-awareness, given the environmental layout and the navigational instruction. This approach aims to minimize the robot's exposure to human activities and preserve the privacy of the robot and its surroundings. Experimental results on the S3DIS dataset demonstrate that our framework significantly enhances mobile robots' privacy awareness of navigation in human-shared public environments. Furthermore, we demonstrate the practical applicability of our framework by successfully navigating a robotic platform through real-world office environments. The supplementary video and code can be accessed via the following link: https://sites.google.com/view/privacy-aware-nav.",
    "pdf_url": "https://arxiv.org/pdf/2410.04302v1",
    "github_url": null,
    "published": "2024-10-05T22:54:31+00:00",
    "updated": "2024-10-05T22:54:31+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.03603v1",
    "title": "LeLaN: Learning A Language-Conditioned Navigation Policy from In-the-Wild Videos",
    "authors": [
      "Hirose",
      "Glossop",
      "Sridhar"
    ],
    "summary": "The world is filled with a wide variety of objects. For robots to be useful, they need the ability to find arbitrary objects described by people. In this paper, we present LeLaN(Learning Language-conditioned Navigation policy), a novel approach that consumes unlabeled, action-free egocentric data to learn scalable, language-conditioned object navigation. Our framework, LeLaN leverages the semantic knowledge of large vision-language models, as well as robotic foundation models, to label in-the-wild data from a variety of indoor and outdoor environments. We label over 130 hours of data collected in real-world indoor and outdoor environments, including robot observations, YouTube video tours, and human walking data. Extensive experiments with over 1000 real-world trials show that our approach enables training a policy from unlabeled action-free videos that outperforms state-of-the-art robot navigation methods, while being capable of inference at 4 times their speed on edge compute. We open-source our models, datasets and provide supplementary videos on our project page (https://learning-language-navigation.github.io/).",
    "pdf_url": "https://arxiv.org/pdf/2410.03603v1",
    "github_url": null,
    "published": "2024-10-04T17:03:14+00:00",
    "updated": "2024-10-04T17:03:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.03488v1",
    "title": "MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-object Demand-driven Navigation",
    "authors": [
      "Wang",
      "Liu",
      "Cai"
    ],
    "summary": "The process of satisfying daily demands is a fundamental aspect of humans' daily lives. With the advancement of embodied AI, robots are increasingly capable of satisfying human demands. Demand-driven navigation (DDN) is a task in which an agent must locate an object to satisfy a specified demand instruction, such as ``I am thirsty.'' The previous study typically assumes that each demand instruction requires only one object to be fulfilled and does not consider individual preferences. However, the realistic human demand may involve multiple objects. In this paper, we introduce the Multi-object Demand-driven Navigation (MO-DDN) benchmark, which addresses these nuanced aspects, including multi-object search and personal preferences, thus making the MO-DDN task more reflective of real-life scenarios compared to DDN. Building upon previous work, we employ the concept of ``attribute'' to tackle this new task. However, instead of solely relying on attribute features in an end-to-end manner like DDN, we propose a modular method that involves constructing a coarse-to-fine attribute-based exploration agent (C2FAgent). Our experimental results illustrate that this coarse-to-fine exploration strategy capitalizes on the advantages of attributes at various decision-making levels, resulting in superior performance compared to baseline methods. Code and video can be found at https://sites.google.com/view/moddn.",
    "pdf_url": "https://arxiv.org/pdf/2410.03488v1",
    "github_url": null,
    "published": "2024-10-04T14:59:20+00:00",
    "updated": "2024-10-04T14:59:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.02751v1",
    "title": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for Embodied AI",
    "authors": [
      "Elawady",
      "Chhablani",
      "Ramrakhya"
    ],
    "summary": "Intelligent embodied agents need to quickly adapt to new scenarios by integrating long histories of experience into decision-making. For instance, a robot in an unfamiliar house initially wouldn't know the locations of objects needed for tasks and might perform inefficiently. However, as it gathers more experience, it should learn the layout of its environment and remember where objects are, allowing it to complete new tasks more efficiently. To enable such rapid adaptation to new tasks, we present ReLIC, a new approach for in-context reinforcement learning (RL) for embodied agents. With ReLIC, agents are capable of adapting to new environments using 64,000 steps of in-context experience with full attention while being trained through self-generated experience via RL. We achieve this by proposing a novel policy update scheme for on-policy RL called \"partial updates'' as well as a Sink-KV mechanism that enables effective utilization of a long observation history for embodied agents. Our method outperforms a variety of meta-RL baselines in adapting to unseen houses in an embodied multi-object navigation task. In addition, we find that ReLIC is capable of few-shot imitation learning despite never being trained with expert demonstrations. We also provide a comprehensive analysis of ReLIC, highlighting that the combination of large-scale RL training, the proposed partial updates scheme, and the Sink-KV are essential for effective in-context learning. The code for ReLIC and all our experiments is at https://github.com/aielawady/relic",
    "pdf_url": "https://arxiv.org/pdf/2410.02751v1",
    "github_url": "https://github.com/aielawady/relic",
    "published": "2024-10-03T17:58:11+00:00",
    "updated": "2024-10-03T17:58:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.02730v3",
    "title": "DivScene: Towards Open-Vocabulary Object Navigation with Large Vision Language Models in Diverse Scenes",
    "authors": [
      "Wang",
      "Zhang",
      "Fang"
    ],
    "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in tasks like visual question answering and document understanding. However, their potential to comprehend embodied environments and navigate within them remains underexplored. In this work, we first study the challenge of open-vocabulary object navigation by introducing DivScene, a large-scale dataset with 4,614 houses across 81 scene types and 5,707 kinds of target objects. Our dataset provides a much greater diversity of target objects and scene types than existing datasets, enabling a comprehensive task evaluation. We evaluated various methods with LVLMs and LLMs on our dataset and found that current models still fall short of open-vocab object navigation ability. Then, we fine-tuned LVLMs to predict the next action with CoT explanations. We observe that LVLM's navigation ability can be improved substantially with only BFS-generated shortest paths without any human supervision, surpassing GPT-4o by over 20% in success rates.",
    "pdf_url": "https://arxiv.org/pdf/2410.02730v3",
    "github_url": null,
    "published": "2024-10-03T17:49:28+00:00",
    "updated": "2025-09-01T03:33:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.02389v1",
    "title": "Diffusion Meets Options: Hierarchical Generative Skill Composition for Temporally-Extended Tasks",
    "authors": [
      "Feng",
      "Luan",
      "Ma"
    ],
    "summary": "Safe and successful deployment of robots requires not only the ability to generate complex plans but also the capacity to frequently replan and correct execution errors. This paper addresses the challenge of long-horizon trajectory planning under temporally extended objectives in a receding horizon manner. To this end, we propose DOPPLER, a data-driven hierarchical framework that generates and updates plans based on instruction specified by linear temporal logic (LTL). Our method decomposes temporal tasks into chain of options with hierarchical reinforcement learning from offline non-expert datasets. It leverages diffusion models to generate options with low-level actions. We devise a determinantal-guided posterior sampling technique during batch generation, which improves the speed and diversity of diffusion generated options, leading to more efficient querying. Experiments on robot navigation and manipulation tasks demonstrate that DOPPLER can generate sequences of trajectories that progressively satisfy the specified formulae for obstacle avoidance and sequential visitation. Demonstration videos are available online at: https://philiptheother.github.io/doppler/.",
    "pdf_url": "https://arxiv.org/pdf/2410.02389v1",
    "github_url": null,
    "published": "2024-10-03T11:10:37+00:00",
    "updated": "2024-10-03T11:10:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.01273v3",
    "title": "CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction",
    "authors": [
      "Choi",
      "Cho",
      "Kim"
    ],
    "summary": "Real-life robot navigation involves more than just reaching a destination; it requires optimizing movements while addressing scenario-specific goals. An intuitive way for humans to express these goals is through abstract cues like verbal commands or rough sketches. Such human guidance may lack details or be noisy. Nonetheless, we expect robots to navigate as intended. For robots to interpret and execute these abstract instructions in line with human expectations, they must share a common understanding of basic navigation concepts with humans. To this end, we introduce CANVAS, a novel framework that combines visual and linguistic instructions for commonsense-aware navigation. Its success is driven by imitation learning, enabling the robot to learn from human navigation behavior. We present COMMAND, a comprehensive dataset with human-annotated navigation results, spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems in simulated environments. Our experiments show that CANVAS outperforms the strong rule-based system ROS NavStack across all environments, demonstrating superior performance with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also closely aligns with human demonstrations and commonsense constraints, even in unseen environments. Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a total success rate of 69%, highlighting the potential of learning from human demonstrations in simulated environments for real-world applications.",
    "pdf_url": "https://arxiv.org/pdf/2410.01273v3",
    "github_url": null,
    "published": "2024-10-02T06:34:45+00:00",
    "updated": "2025-08-08T04:20:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.00649v1",
    "title": "LASMP: Language Aided Subset Sampling Based Motion Planner",
    "authors": [
      "Bhattacharjee",
      "Sinha",
      "Ekenna"
    ],
    "summary": "This paper presents the Language Aided Subset Sampling Based Motion Planner (LASMP), a system that helps mobile robots plan their movements by using natural language instructions. LASMP uses a modified version of the Rapidly Exploring Random Tree (RRT) method, which is guided by user-provided commands processed through a language model (RoBERTa). The system improves efficiency by focusing on specific areas of the robot's workspace based on these instructions, making it faster and less resource-intensive. Compared to traditional RRT methods, LASMP reduces the number of nodes needed by 55% and cuts random sample queries by 80%, while still generating safe, collision-free paths. Tested in both simulated and real-world environments, LASMP has shown better performance in handling complex indoor scenarios. The results highlight the potential of combining language processing with motion planning to make robot navigation more efficient.",
    "pdf_url": "https://arxiv.org/pdf/2410.00649v1",
    "github_url": null,
    "published": "2024-10-01T13:03:15+00:00",
    "updated": "2024-10-01T13:03:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.20445v1",
    "title": "Robot Navigation Using Physically Grounded Vision-Language Models in Outdoor Environments",
    "authors": [
      "Elnoor",
      "Weerakoon",
      "Seneviratne"
    ],
    "summary": "We present a novel autonomous robot navigation algorithm for outdoor environments that is capable of handling diverse terrain traversability conditions. Our approach, VLM-GroNav, uses vision-language models (VLMs) and integrates them with physical grounding that is used to assess intrinsic terrain properties such as deformability and slipperiness. We use proprioceptive-based sensing, which provides direct measurements of these physical properties, and enhances the overall semantic understanding of the terrains. Our formulation uses in-context learning to ground the VLM's semantic understanding with proprioceptive data to allow dynamic updates of traversability estimates based on the robot's real-time physical interactions with the environment. We use the updated traversability estimations to inform both the local and global planners for real-time trajectory replanning. We validate our method on a legged robot (Ghost Vision 60) and a wheeled robot (Clearpath Husky), in diverse real-world outdoor environments with different deformable and slippery terrains. In practice, we observe significant improvements over state-of-the-art methods by up to 50% increase in navigation success rate.",
    "pdf_url": "https://arxiv.org/pdf/2409.20445v1",
    "github_url": null,
    "published": "2024-09-30T16:03:44+00:00",
    "updated": "2024-09-30T16:03:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.19459v1",
    "title": "Language-guided Robust Navigation for Mobile Robots in Dynamically-changing Environments",
    "authors": [
      "Simons",
      "Liu",
      "Marcus"
    ],
    "summary": "In this paper, we develop an embodied AI system for human-in-the-loop navigation with a wheeled mobile robot. We propose a direct yet effective method of monitoring the robot's current plan to detect changes in the environment that impact the intended trajectory of the robot significantly and then query a human for feedback. We also develop a means to parse human feedback expressed in natural language into local navigation waypoints and integrate it into a global planning system, by leveraging a map of semantic features and an aligned obstacle map. Extensive testing in simulation and physical hardware experiments with a resource-constrained wheeled robot tasked to navigate in a real-world environment validate the efficacy and robustness of our method. This work can support applications like precision agriculture and construction, where persistent monitoring of the environment provides a human with information about the environment state.",
    "pdf_url": "https://arxiv.org/pdf/2409.19459v1",
    "github_url": null,
    "published": "2024-09-28T21:30:23+00:00",
    "updated": "2024-09-28T21:30:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.18800v1",
    "title": "MiniVLN: Efficient Vision-and-Language Navigation by Progressive Knowledge Distillation",
    "authors": [
      "Zhu",
      "Qiao",
      "Zhang"
    ],
    "summary": "In recent years, Embodied Artificial Intelligence (Embodied AI) has advanced rapidly, yet the increasing size of models conflicts with the limited computational capabilities of Embodied AI platforms. To address this challenge, we aim to achieve both high model performance and practical deployability. Specifically, we focus on Vision-and-Language Navigation (VLN), a core task in Embodied AI. This paper introduces a two-stage knowledge distillation framework, producing a student model, MiniVLN, and showcasing the significant potential of distillation techniques in developing lightweight models. The proposed method aims to capture fine-grained knowledge during the pretraining phase and navigation-specific knowledge during the fine-tuning phase. Our findings indicate that the two-stage distillation approach is more effective in narrowing the performance gap between the teacher model and the student model compared to single-stage distillation. On the public R2R and REVERIE benchmarks, MiniVLN achieves performance on par with the teacher model while having only about 12% of the teacher model's parameter count.",
    "pdf_url": "https://arxiv.org/pdf/2409.18800v1",
    "github_url": null,
    "published": "2024-09-27T14:54:54+00:00",
    "updated": "2024-09-27T14:54:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.18794v2",
    "title": "Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in Continuous Environment with Open-Source LLMs",
    "authors": [
      "Qiao",
      "Lyu",
      "Wang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) tasks require an agent to follow textual instructions to navigate through 3D environments. Traditional approaches use supervised learning methods, relying heavily on domain-specific datasets to train VLN models. Recent methods try to utilize closed-source large language models (LLMs) like GPT-4 to solve VLN tasks in zero-shot manners, but face challenges related to expensive token costs and potential data breaches in real-world applications. In this work, we introduce Open-Nav, a novel study that explores open-source LLMs for zero-shot VLN in the continuous environment. Open-Nav employs a spatial-temporal chain-of-thought (CoT) reasoning approach to break down tasks into instruction comprehension, progress estimation, and decision-making. It enhances scene perceptions with fine-grained object and spatial knowledge to improve LLM's reasoning in navigation. Our extensive experiments in both simulated and real-world environments demonstrate that Open-Nav achieves competitive performance compared to using closed-source LLMs.",
    "pdf_url": "https://arxiv.org/pdf/2409.18794v2",
    "github_url": null,
    "published": "2024-09-27T14:47:18+00:00",
    "updated": "2025-02-11T00:55:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.18743v1",
    "title": "OpenObject-NAV: Open-Vocabulary Object-Oriented Navigation Based on Dynamic Carrier-Relationship Scene Graph",
    "authors": [
      "Tang",
      "Wang",
      "Deng"
    ],
    "summary": "In everyday life, frequently used objects like cups often have unfixed positions and multiple instances within the same category, and their carriers frequently change as well. As a result, it becomes challenging for a robot to efficiently navigate to a specific instance. To tackle this challenge, the robot must capture and update scene changes and plans continuously. However, current object navigation approaches primarily focus on semantic-level and lack the ability to dynamically update scene representation. This paper captures the relationships between frequently used objects and their static carriers. It constructs an open-vocabulary Carrier-Relationship Scene Graph (CRSG) and updates the carrying status during robot navigation to reflect the dynamic changes of the scene. Based on the CRSG, we further propose an instance navigation strategy that models the navigation process as a Markov Decision Process. At each step, decisions are informed by Large Language Model's commonsense knowledge and visual-language feature similarity. We designed a series of long-sequence navigation tasks for frequently used everyday items in the Habitat simulator. The results demonstrate that by updating the CRSG, the robot can efficiently navigate to moved targets. Additionally, we deployed our algorithm on a real robot and validated its practical effectiveness.",
    "pdf_url": "https://arxiv.org/pdf/2409.18743v1",
    "github_url": null,
    "published": "2024-09-27T13:33:52+00:00",
    "updated": "2024-09-27T13:33:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.18084v3",
    "title": "GSON: A Group-based Social Navigation Framework with Large Multimodal Model",
    "authors": [
      "Luo",
      "Sun",
      "Zhu"
    ],
    "summary": "With the increasing presence of service robots and autonomous vehicles in human environments, navigation systems need to evolve beyond simple destination reach to incorporate social awareness. This paper introduces GSON, a novel group-based social navigation framework that leverages Large Multimodal Models (LMMs) to enhance robots' social perception capabilities. Our approach uses visual prompting to enable zero-shot extraction of social relationships among pedestrians and integrates these results with robust pedestrian detection and tracking pipelines to overcome the inherent inference speed limitations of LMMs. The planning system incorporates a mid-level planner that sits between global path planning and local motion planning, effectively preserving both global context and reactive responsiveness while avoiding disruption of the predicted social group. We validate GSON through extensive real-world mobile robot navigation experiments involving complex social scenarios such as queuing, conversations, and photo sessions. Comparative results show that our system significantly outperforms existing navigation approaches in minimizing social perturbations while maintaining comparable performance on traditional navigation metrics.",
    "pdf_url": "https://arxiv.org/pdf/2409.18084v3",
    "github_url": null,
    "published": "2024-09-26T17:27:15+00:00",
    "updated": "2025-07-29T13:17:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2410.09054v1",
    "title": "Circuits and Systems for Embodied AI: Exploring uJ Multi-Modal Perception for Nano-UAVs on the Kraken Shield",
    "authors": [
      "Potocnik",
      "Mauro",
      "Lamberti"
    ],
    "summary": "Embodied artificial intelligence (AI) requires pushing complex multi-modal models to the extreme edge for time-constrained tasks such as autonomous navigation of robots and vehicles. On small form-factor devices, e.g., nano-sized unmanned aerial vehicles (UAVs), such challenges are exacerbated by stringent constraints on energy efficiency and weight. In this paper, we explore embodied multi-modal AI-based perception for Nano-UAVs with the Kraken shield, a 7g multi-sensor (frame-based and event-based imagers) board based on Kraken, a 22 nm SoC featuring multiple acceleration engines for multi-modal event and frame-based inference based on spiking (SNN) and ternary (TNN) neural networks, respectively. Kraken can execute SNN real-time inference for depth estimation at 1.02k inf/s, 18 μJ/inf, TNN real-time inference for object classification at 10k inf/s, 6 μJ/inf, and real-time inference for obstacle avoidance at 221 frame/s, 750 μJ/inf.",
    "pdf_url": "https://arxiv.org/pdf/2410.09054v1",
    "github_url": null,
    "published": "2024-09-26T12:59:03+00:00",
    "updated": "2024-09-26T12:59:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.17313v1",
    "title": "Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation",
    "authors": [
      "Wang",
      "Wu",
      "Cao"
    ],
    "summary": "This study presents a novel evaluation framework for the Vision-Language Navigation (VLN) task. It aims to diagnose current models for various instruction categories at a finer-grained level. The framework is structured around the context-free grammar (CFG) of the task. The CFG serves as the basis for the problem decomposition and the core premise of the instruction categories design. We propose a semi-automatic method for CFG construction with the help of Large-Language Models (LLMs). Then, we induct and generate data spanning five principal instruction categories (i.e. direction change, landmark recognition, region recognition, vertical movement, and numerical comprehension). Our analysis of different models reveals notable performance discrepancies and recurrent issues. The stagnation of numerical comprehension, heavy selective biases over directional concepts, and other interesting findings contribute to the development of future language-guided navigation systems.",
    "pdf_url": "https://arxiv.org/pdf/2409.17313v1",
    "github_url": null,
    "published": "2024-09-25T19:49:39+00:00",
    "updated": "2024-09-25T19:49:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.16680v2",
    "title": "Online 6DoF Global Localisation in Forests using Semantically-Guided Re-Localisation and Cross-View Factor-Graph Optimisation",
    "authors": [
      "Lima",
      "Griffiths",
      "Haghighat"
    ],
    "summary": "This paper presents FGLoc6D, a novel approach for robust global localisation and online 6DoF pose estimation of ground robots in forest environments by leveraging deep semantically-guided re-localisation and cross-view factor graph optimisation. The proposed method addresses the challenges of aligning aerial and ground data for pose estimation, which is crucial for accurate point-to-point navigation in GPS-degraded environments. By integrating information from both perspectives into a factor graph framework, our approach effectively estimates the robot's global position and orientation. Additionally, we enhance the repeatability of deep-learned keypoints for metric localisation in forests by incorporating a semantically-guided regression loss. This loss encourages greater attention to wooden structures, e.g., tree trunks, which serve as stable and distinguishable features, thereby improving the consistency of keypoints and increasing the success rate of global registration, a process we refer to as re-localisation. The re-localisation module along with the factor-graph structure, populated by odometry and ground-to-aerial factors over time, allows global localisation under dense canopies. We validate the performance of our method through extensive experiments in three forest scenarios, demonstrating its global localisation capability and superiority over alternative state-of-the-art in terms of accuracy and robustness in these challenging environments. Experimental results show that our proposed method can achieve drift-free localisation with bounded positioning errors, ensuring reliable and safe robot navigation through dense forests.",
    "pdf_url": "https://arxiv.org/pdf/2409.16680v2",
    "github_url": null,
    "published": "2024-09-25T07:11:00+00:00",
    "updated": "2025-03-11T10:06:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.16484v2",
    "title": "BehAV: Behavioral Rule Guided Autonomy Using VLMs for Robot Navigation in Outdoor Scenes",
    "authors": [
      "Weerakoon",
      "Elnoor",
      "Seneviratne"
    ],
    "summary": "We present BehAV, a novel approach for autonomous robot navigation in outdoor scenes guided by human instructions and leveraging Vision Language Models (VLMs). Our method interprets human commands using a Large Language Model (LLM) and categorizes the instructions into navigation and behavioral guidelines. Navigation guidelines consist of directional commands (e.g., \"move forward until\") and associated landmarks (e.g., \"the building with blue windows\"), while behavioral guidelines encompass regulatory actions (e.g., \"stay on\") and their corresponding objects (e.g., \"pavements\"). We use VLMs for their zero-shot scene understanding capabilities to estimate landmark locations from RGB images for robot navigation. Further, we introduce a novel scene representation that utilizes VLMs to ground behavioral rules into a behavioral cost map. This cost map encodes the presence of behavioral objects within the scene and assigns costs based on their regulatory actions. The behavioral cost map is integrated with a LiDAR-based occupancy map for navigation. To navigate outdoor scenes while adhering to the instructed behaviors, we present an unconstrained Model Predictive Control (MPC)-based planner that prioritizes both reaching landmarks and following behavioral guidelines. We evaluate the performance of BehAV on a quadruped robot across diverse real-world scenarios, demonstrating a 22.49% improvement in alignment with human-teleoperated actions, as measured by Frechet distance, and achieving a 40% higher navigation success rate compared to state-of-the-art methods.",
    "pdf_url": "https://arxiv.org/pdf/2409.16484v2",
    "github_url": null,
    "published": "2024-09-24T22:15:24+00:00",
    "updated": "2024-10-02T19:50:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.15922v5",
    "title": "The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards",
    "authors": [
      "Huang",
      "Liu",
      "Lipovetzky"
    ],
    "summary": "While Vision-Language Models (VLMs) are increasingly used to generate reward signals for training embodied agents to follow instructions, our research reveals that agents guided by VLM rewards often underperform compared to those employing only intrinsic (exploration-driven) rewards, contradicting expectations set by recent work. We hypothesize that false positive rewards -- instances where unintended trajectories are incorrectly rewarded -- are more detrimental than false negatives. Our analysis confirms this hypothesis, revealing that the widely used cosine similarity metric is prone to false positive reward estimates. To address this, we introduce BiMI ({Bi}nary {M}utual {I}nformation), a novel reward function designed to mitigate noise. BiMI significantly enhances learning efficiency across diverse and challenging embodied navigation environments. Our findings offer a nuanced understanding of how different types of reward noise impact agent learning and highlight the importance of addressing multimodal reward signal noise when training embodied agents",
    "pdf_url": "https://arxiv.org/pdf/2409.15922v5",
    "github_url": null,
    "published": "2024-09-24T09:45:20+00:00",
    "updated": "2025-11-08T04:37:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.14899v1",
    "title": "CON: Continual Object Navigation via Data-Free Inter-Agent Knowledge Transfer in Unseen and Unfamiliar Places",
    "authors": [
      "Terashima",
      "Iwata",
      "Tanaka"
    ],
    "summary": "This work explores the potential of brief inter-agent knowledge transfer (KT) to enhance the robotic object goal navigation (ON) in unseen and unfamiliar environments. Drawing on the analogy of human travelers acquiring local knowledge, we propose a framework in which a traveler robot (student) communicates with local robots (teachers) to obtain ON knowledge through minimal interactions. We frame this process as a data-free continual learning (CL) challenge, aiming to transfer knowledge from a black-box model (teacher) to a new model (student). In contrast to approaches like zero-shot ON using large language models (LLMs), which utilize inherently communication-friendly natural language for knowledge representation, the other two major ON approaches -- frontier-driven methods using object feature maps and learning-based ON using neural state-action maps -- present complex challenges where data-free KT remains largely uncharted. To address this gap, we propose a lightweight, plug-and-play KT module targeting non-cooperative black-box teachers in open-world settings. Using the universal assumption that every teacher robot has vision and mobility capabilities, we define state-action history as the primary knowledge base. Our formulation leads to the development of a query-based occupancy map that dynamically represents target object locations, serving as an effective and communication-friendly knowledge representation. We validate the effectiveness of our method through experiments conducted in the Habitat environment.",
    "pdf_url": "https://arxiv.org/pdf/2409.14899v1",
    "github_url": null,
    "published": "2024-09-23T10:50:11+00:00",
    "updated": "2024-09-23T10:50:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.14296v1",
    "title": "HM3D-OVON: A Dataset and Benchmark for Open-Vocabulary Object Goal Navigation",
    "authors": [
      "Yokoyama",
      "Ramrakhya",
      "Das"
    ],
    "summary": "We present the Habitat-Matterport 3D Open Vocabulary Object Goal Navigation dataset (HM3D-OVON), a large-scale benchmark that broadens the scope and semantic range of prior Object Goal Navigation (ObjectNav) benchmarks. Leveraging the HM3DSem dataset, HM3D-OVON incorporates over 15k annotated instances of household objects across 379 distinct categories, derived from photo-realistic 3D scans of real-world environments. In contrast to earlier ObjectNav datasets, which limit goal objects to a predefined set of 6-20 categories, HM3D-OVON facilitates the training and evaluation of models with an open-set of goals defined through free-form language at test-time. Through this open-vocabulary formulation, HM3D-OVON encourages progress towards learning visuo-semantic navigation behaviors that are capable of searching for any object specified by text in an open-vocabulary manner. Additionally, we systematically evaluate and compare several different types of approaches on HM3D-OVON. We find that HM3D-OVON can be used to train an open-vocabulary ObjectNav agent that achieves both higher performance and is more robust to localization and actuation noise than the state-of-the-art ObjectNav approach. We hope that our benchmark and baseline results will drive interest in developing embodied agents that can navigate real-world spaces to find household objects specified through free-form language, taking a step towards more flexible and human-like semantic visual navigation. Code and videos available at: naoki.io/ovon.",
    "pdf_url": "https://arxiv.org/pdf/2409.14296v1",
    "github_url": null,
    "published": "2024-09-22T02:12:29+00:00",
    "updated": "2024-09-22T02:12:29+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.13838v1",
    "title": "Key-Scan-Based Mobile Robot Navigation: Integrated Mapping, Planning, and Control using Graphs of Scan Regions",
    "authors": [
      "Latha",
      "Arslan"
    ],
    "summary": "Safe autonomous navigation in a priori unknown environments is an essential skill for mobile robots to reliably and adaptively perform diverse tasks (e.g., delivery, inspection, and interaction) in unstructured cluttered environments. Hybrid metric-topological maps, constructed as a pose graph of local submaps, offer a computationally efficient world representation for adaptive mapping, planning, and control at the regional level. In this paper, we consider a pose graph of locally sensed star-convex scan regions as a metric-topological map, with star convexity enabling simple yet effective local navigation strategies. We design a new family of safe local scan navigation policies and present a perception-driven feedback motion planning method through the sequential composition of local scan navigation policies, enabling provably correct and safe robot navigation over the union of local scan regions. We introduce a new concept of bridging and frontier scans for automated key scan selection and exploration for integrated mapping and navigation in unknown environments. We demonstrate the effectiveness of our key-scan-based navigation and mapping framework using a mobile robot equipped with a 360$^{\\circ}$ laser range scanner in 2D cluttered environments through numerical ROS-Gazebo simulations and real hardware~experiments.",
    "pdf_url": "https://arxiv.org/pdf/2409.13838v1",
    "github_url": null,
    "published": "2024-09-20T18:25:09+00:00",
    "updated": "2024-09-20T18:25:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.13682v1",
    "title": "ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation",
    "authors": [
      "Anwar",
      "Welsh",
      "Biswas"
    ],
    "summary": "Navigating and understanding complex environments over extended periods of time is a significant challenge for robots. People interacting with the robot may want to ask questions like where something happened, when it occurred, or how long ago it took place, which would require the robot to reason over a long history of their deployment. To address this problem, we introduce a Retrieval-augmented Memory for Embodied Robots, or ReMEmbR, a system designed for long-horizon video question answering for robot navigation. To evaluate ReMEmbR, we introduce the NaVQA dataset where we annotate spatial, temporal, and descriptive questions to long-horizon robot navigation videos. ReMEmbR employs a structured approach involving a memory building and a querying phase, leveraging temporal information, spatial information, and images to efficiently handle continuously growing robot histories. Our experiments demonstrate that ReMEmbR outperforms LLM and VLM baselines, allowing ReMEmbR to achieve effective long-horizon reasoning with low latency. Additionally, we deploy ReMEmbR on a robot and show that our approach can handle diverse queries. The dataset, code, videos, and other material can be found at the following link: https://nvidia-ai-iot.github.io/remembr",
    "pdf_url": "https://arxiv.org/pdf/2409.13682v1",
    "github_url": null,
    "published": "2024-09-20T17:50:07+00:00",
    "updated": "2024-09-20T17:50:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.13675v2",
    "title": "OLiVia-Nav: An Online Lifelong Vision Language Approach for Mobile Robot Social Navigation",
    "authors": [
      "Narasimhan",
      "Tan",
      "Choi"
    ],
    "summary": "Service robots in human-centered environments such as hospitals, office buildings, and long-term care homes need to navigate while adhering to social norms to ensure the safety and comfortability of the people they are sharing the space with. Furthermore, they need to adapt to new social scenarios that can arise during robot navigation. In this paper, we present a novel Online Lifelong Vision Language architecture, OLiVia- Nav, which uniquely integrates vision-language models (VLMs) with an online lifelong learning framework for robot social navigation. We introduce a unique distillation approach, Social Context Contrastive Language Image Pre-training (SC-CLIP), to transfer the social reasoning capabilities of large VLMs to a lightweight VLM, in order for OLiVia-Nav to directly encode social and environment context during robot navigation. These encoded embeddings are used to generate and select robot social compliant trajectories. The lifelong learning capabilities of SC-CLIP enable OLiVia-Nav to update the robot trajectory planning overtime as new social scenarios are encountered. We conducted extensive real-world experiments in diverse social navigation scenarios. The results showed that OLiVia-Nav outperformed existing state-of-the-art DRL and VLM methods in terms of mean squared error, Hausdorff loss, and personal space violation duration. Ablation studies also verified the design choices for OLiVia-Nav.",
    "pdf_url": "https://arxiv.org/pdf/2409.13675v2",
    "github_url": null,
    "published": "2024-09-20T17:33:36+00:00",
    "updated": "2025-03-08T15:15:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.13573v2",
    "title": "Human-Robot Cooperative Distribution Coupling for Hamiltonian-Constrained Social Navigation",
    "authors": [
      "Wang",
      "Yu",
      "Wang"
    ],
    "summary": "Navigating in human-filled public spaces is a critical challenge for deploying autonomous robots in real-world environments. This paper introduces NaviDIFF, a novel Hamiltonian-constrained socially-aware navigation framework designed to address the complexities of human-robot interaction and socially-aware path planning. NaviDIFF integrates a port-Hamiltonian framework to model dynamic physical interactions and a diffusion model to manage uncertainty in human-robot cooperation. The framework leverages a spatial-temporal transformer to capture social and temporal dependencies, enabling more accurate spatial-temporal environmental dynamics understanding and port-Hamiltonian physical interactive process construction. Additionally, reinforcement learning from human feedback is employed to fine-tune robot policies, ensuring adaptation to human preferences and social norms. Extensive experiments demonstrate that NaviDIFF outperforms state-of-the-art methods in social navigation tasks, offering improved stability, efficiency, and adaptability.",
    "pdf_url": "https://arxiv.org/pdf/2409.13573v2",
    "github_url": null,
    "published": "2024-09-20T15:17:51+00:00",
    "updated": "2025-03-07T19:49:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.13393v1",
    "title": "Hey Robot! Personalizing Robot Navigation through Model Predictive Control with a Large Language Model",
    "authors": [
      "Martinez-Baselga",
      "Groot",
      "Knoedler"
    ],
    "summary": "Robot navigation methods allow mobile robots to operate in applications such as warehouses or hospitals. While the environment in which the robot operates imposes requirements on its navigation behavior, most existing methods do not allow the end-user to configure the robot's behavior and priorities, possibly leading to undesirable behavior (e.g., fast driving in a hospital). We propose a novel approach to adapt robot motion behavior based on natural language instructions provided by the end-user. Our zero-shot method uses an existing Visual Language Model to interpret a user text query or an image of the environment. This information is used to generate the cost function and reconfigure the parameters of a Model Predictive Controller, translating the user's instruction to the robot's motion behavior. This allows our method to safely and effectively navigate in dynamic and challenging environments. We extensively evaluate our method's individual components and demonstrate the effectiveness of our method on a ground robot in simulation and real-world experiments, and across a variety of environments and user specifications.",
    "pdf_url": "https://arxiv.org/pdf/2409.13393v1",
    "github_url": null,
    "published": "2024-09-20T10:48:53+00:00",
    "updated": "2024-09-20T10:48:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.13244v2",
    "title": "From Cognition to Precognition: A Future-Aware Framework for Social Navigation",
    "authors": [
      "Gong",
      "Hu",
      "Qiu"
    ],
    "summary": "To navigate safely and efficiently in crowded spaces, robots should not only perceive the current state of the environment but also anticipate future human movements. In this paper, we propose a reinforcement learning architecture, namely Falcon, to tackle socially-aware navigation by explicitly predicting human trajectories and penalizing actions that block future human paths. To facilitate realistic evaluation, we introduce a novel SocialNav benchmark containing two new datasets, Social-HM3D and Social-MP3D. This benchmark offers large-scale photo-realistic indoor scenes populated with a reasonable amount of human agents based on scene area size, incorporating natural human movements and trajectory patterns. We conduct a detailed experimental analysis with the state-of-the-art learning-based method and two classic rule-based path-planning algorithms on the new benchmark. The results demonstrate the importance of future prediction and our method achieves the best task success rate of 55% while maintaining about 90% personal space compliance. We will release our code and datasets. Videos of demonstrations can be viewed at https://zeying-gong.github.io/projects/falcon/ .",
    "pdf_url": "https://arxiv.org/pdf/2409.13244v2",
    "github_url": null,
    "published": "2024-09-20T06:08:24+00:00",
    "updated": "2025-02-08T15:07:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.12471v1",
    "title": "Arena 4.0: A Comprehensive ROS2 Development and Benchmarking Platform for Human-centric Navigation Using Generative-Model-based Environment Generation",
    "authors": [
      "Shcherbyna1",
      "Kästner",
      "Diaz"
    ],
    "summary": "Building on the foundations of our previous work, this paper introduces Arena 4.0, a significant advancement over Arena 3.0, Arena-Bench, Arena 1.0, and Arena 2.0. Arena 4.0 offers three key novel contributions: (1) a generative-model-based world and scenario generation approach that utilizes large language models (LLMs) and diffusion models to dynamically generate complex, human-centric environments from text prompts or 2D floorplans, useful for the development and benchmarking of social navigation strategies; (2) a comprehensive 3D model database, extendable with additional 3D assets that are semantically linked and annotated for dynamic spawning and arrangement within 3D worlds; and (3) a complete migration to ROS 2, enabling compatibility with modern hardware and enhanced functionalities for improved navigation, usability, and easier deployment on real robots. We evaluated the platform's performance through a comprehensive user study, demonstrating significant improvements in usability and efficiency compared to previous versions. Arena 4.0 is openly available at https://github.com/Arena-Rosnav.",
    "pdf_url": "https://arxiv.org/pdf/2409.12471v1",
    "github_url": null,
    "published": "2024-09-19T05:20:13+00:00",
    "updated": "2024-09-19T05:20:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.12420v2",
    "title": "Spatially-invariant opinion dynamics on the circle",
    "authors": [
      "Amorim",
      "Bizyaeva",
      "Franci"
    ],
    "summary": "We propose and analyze a nonlinear opinion dynamics model for an agent making decisions about a continuous distribution of options in the presence of input. Inspired by perceptual decision-making, we develop new theory for opinion formation in response to inputs about options distributed on the circle. Options on the circle can represent, e.g., the possible directions of perceived objects and resulting heading directions in planar robotic navigation problems. Interactions among options are encoded through a spatially invariant kernel, which we design to ensure that only a small (finite) subset of options can be favored over the continuum. We leverage the spatial invariance of the model linearization to design flexible, distributed opinion-forming behaviors using spatiotemporal frequency domain and bifurcation analysis. We illustrate our model's versatility with an application to robotic navigation in crowded spaces.",
    "pdf_url": "https://arxiv.org/pdf/2409.12420v2",
    "github_url": null,
    "published": "2024-09-19T02:40:49+00:00",
    "updated": "2024-11-27T17:19:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.11764v2",
    "title": "One Map to Find Them All: Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation",
    "authors": [
      "Busch",
      "Homberger",
      "Ortega-Peimbert"
    ],
    "summary": "The capability to efficiently search for objects in complex environments is fundamental for many real-world robot applications. Recent advances in open-vocabulary vision models have resulted in semantically-informed object navigation methods that allow a robot to search for an arbitrary object without prior training. However, these zero-shot methods have so far treated the environment as unknown for each consecutive query. In this paper we introduce a new benchmark for zero-shot multi-object navigation, allowing the robot to leverage information gathered from previous searches to more efficiently find new objects. To address this problem we build a reusable open-vocabulary feature map tailored for real-time object search. We further propose a probabilistic-semantic map update that mitigates common sources of errors in semantic feature extraction and leverage this semantic uncertainty for informed multi-object exploration. We evaluate our method on a set of object navigation tasks in both simulation as well as with a real robot, running in real-time on a Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art approaches both on single and multi-object navigation tasks. Additional videos, code and the multi-object navigation benchmark will be available on https://finnbsch.github.io/OneMap.",
    "pdf_url": "https://arxiv.org/pdf/2409.11764v2",
    "github_url": null,
    "published": "2024-09-18T07:44:08+00:00",
    "updated": "2025-03-03T18:50:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.11561v2",
    "title": "Hypergraph-based Coordinated Task Allocation and Socially-aware Navigation for Multi-Robot Systems",
    "authors": [
      "Wang",
      "Bera",
      "Min"
    ],
    "summary": "A team of multiple robots seamlessly and safely working in human-filled public environments requires adaptive task allocation and socially-aware navigation that account for dynamic human behavior. Current approaches struggle with highly dynamic pedestrian movement and the need for flexible task allocation. We propose Hyper-SAMARL, a hypergraph-based system for multi-robot task allocation and socially-aware navigation, leveraging multi-agent reinforcement learning (MARL). Hyper-SAMARL models the environmental dynamics between robots, humans, and points of interest (POIs) using a hypergraph, enabling adaptive task assignment and socially-compliant navigation through a hypergraph diffusion mechanism. Our framework, trained with MARL, effectively captures interactions between robots and humans, adapting tasks based on real-time changes in human activity. Experimental results demonstrate that Hyper-SAMARL outperforms baseline models in terms of social navigation, task completion efficiency, and adaptability in various simulated scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2409.11561v2",
    "github_url": null,
    "published": "2024-09-17T21:20:17+00:00",
    "updated": "2025-03-07T19:49:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.10906v1",
    "title": "Multi-Floor Zero-Shot Object Navigation Policy",
    "authors": [
      "Zhang",
      "Wang",
      "Xiao"
    ],
    "summary": "Object navigation in multi-floor environments presents a formidable challenge in robotics, requiring sophisticated spatial reasoning and adaptive exploration strategies. Traditional approaches have primarily focused on single-floor scenarios, overlooking the complexities introduced by multi-floor structures. To address these challenges, we first propose a Multi-floor Navigation Policy (MFNP) and implement it in Zero-Shot object navigation tasks. Our framework comprises three key components: (i) Multi-floor Navigation Policy, which enables an agent to explore across multiple floors; (ii) Multi-modal Large Language Models (MLLMs) for reasoning in the navigation process; and (iii) Inter-Floor Navigation, ensuring efficient floor transitions. We evaluate MFNP on the Habitat-Matterport 3D (HM3D) and Matterport 3D (MP3D) datasets, both include multi-floor scenes. Our experiment results demonstrate that MFNP significantly outperforms all the existing methods in Zero-Shot object navigation, achieving higher success rates and improved exploration efficiency. Ablation studies further highlight the effectiveness of each component in addressing the unique challenges of multi-floor navigation. Meanwhile, we conducted real-world experiments to evaluate the feasibility of our policy. Upon deployment of MFNP, the Unitree quadruped robot demonstrated successful multi-floor navigation and found the target object in a completely unseen environment. By introducing MFNP, we offer a new paradigm for tackling complex, multi-floor environments in object navigation tasks, opening avenues for future research in visual-based navigation in realistic, multi-floor settings.",
    "pdf_url": "https://arxiv.org/pdf/2409.10906v1",
    "github_url": null,
    "published": "2024-09-17T05:53:04+00:00",
    "updated": "2024-09-17T05:53:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.10655v3",
    "title": "Disentangling Uncertainty for Safe Social Navigation using Deep Reinforcement Learning",
    "authors": [
      "Flögel",
      "Villafañe",
      "Ransiek"
    ],
    "summary": "Autonomous mobile robots are increasingly used in pedestrian-rich environments where safe navigation and appropriate human interaction are crucial. While Deep Reinforcement Learning (DRL) enables socially integrated robot behavior, challenges persist in novel or perturbed scenarios to indicate when and why the policy is uncertain. Unknown uncertainty in decision-making can lead to collisions or human discomfort and is one reason why safe and risk-aware navigation is still an open problem. This work introduces a novel approach that integrates aleatoric, epistemic, and predictive uncertainty estimation into a DRL navigation framework for policy distribution uncertainty estimates. We, therefore, incorporate Observation-Dependent Variance (ODV) and dropout into the Proximal Policy Optimization (PPO) algorithm. For different types of perturbations, we compare the ability of deep ensembles and Monte-Carlo dropout (MC-dropout) to estimate the uncertainties of the policy. In uncertain decision-making situations, we propose to change the robot's social behavior to conservative collision avoidance. The results show improved training performance with ODV and dropout in PPO and reveal that the training scenario has an impact on the generalization. In addition, MC-dropout is more sensitive to perturbations and correlates the uncertainty type to the perturbation better. With the safe action selection, the robot can navigate in perturbed environments with fewer collisions.",
    "pdf_url": "https://arxiv.org/pdf/2409.10655v3",
    "github_url": null,
    "published": "2024-09-16T18:49:38+00:00",
    "updated": "2025-07-09T09:52:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.10283v4",
    "title": "ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone Navigation via Scene-Aware Control Barrier Functions",
    "authors": [
      "Sanyal",
      "Roy"
    ],
    "summary": "In the rapidly evolving field of vision-language navigation (VLN), ensuring safety for physical agents remains an open challenge. For a human-in-the-loop language-operated drone to navigate safely, it must understand natural language commands, perceive the environment, and simultaneously avoid hazards in real time. Control Barrier Functions (CBFs) are formal methods that enforce safe operating conditions. Model Predictive Control (MPC) is an optimization framework that plans a sequence of future actions over a prediction horizon, ensuring smooth trajectory tracking while obeying constraints. In this work, we consider a VLN-operated drone platform and enhance its safety by formulating a novel scene-aware CBF that leverages ego-centric observations from a camera which has both Red-Green-Blue as well as Depth (RGB-D) channels. A CBF-less baseline system uses a Vision-Language Encoder with cross-modal attention to convert commands into an ordered sequence of landmarks. An object detection model identifies and verifies these landmarks in the captured images to generate a planned path. To further enhance safety, an Adaptive Safety Margin Algorithm (ASMA) is proposed. ASMA tracks moving objects and performs scene-aware CBF evaluation on-the-fly, which serves as an additional constraint within the MPC framework. By continuously identifying potentially risky observations, the system performs prediction in real time about unsafe conditions and proactively adjusts its control actions to maintain safe navigation throughout the trajectory. Deployed on a Parrot Bebop2 quadrotor in the Gazebo environment using the Robot Operating System (ROS), ASMA achieves 64%-67% increase in success rates with only a slight increase (1.4%-5.8%) in trajectory lengths compared to the baseline CBF-less VLN.",
    "pdf_url": "https://arxiv.org/pdf/2409.10283v4",
    "github_url": null,
    "published": "2024-09-16T13:44:50+00:00",
    "updated": "2025-07-19T18:48:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.10071v5",
    "title": "Towards Physically Realizable Adversarial Attacks in Embodied Vision Navigation",
    "authors": [
      "Chen",
      "Tu",
      "Qi"
    ],
    "summary": "The significant advancements in embodied vision navigation have raised concerns about its susceptibility to adversarial attacks exploiting deep neural networks. Investigating the adversarial robustness of embodied vision navigation is crucial, especially given the threat of 3D physical attacks that could pose risks to human safety. However, existing attack methods for embodied vision navigation often lack physical feasibility due to challenges in transferring digital perturbations into the physical world. Moreover, current physical attacks for object detection struggle to achieve both multi-view effectiveness and visual naturalness in navigation scenarios. To address this, we propose a practical attack method for embodied navigation by attaching adversarial patches to objects, where both opacity and textures are learnable. Specifically, to ensure effectiveness across varying viewpoints, we employ a multi-view optimization strategy based on object-aware sampling, which optimizes the patch's texture based on feedback from the vision-based perception model used in navigation. To make the patch inconspicuous to human observers, we introduce a two-stage opacity optimization mechanism, in which opacity is fine-tuned after texture optimization. Experimental results demonstrate that our adversarial patches decrease the navigation success rate by an average of 22.39%, outperforming previous methods in practicality, effectiveness, and naturalness. Code is available at: https://github.com/chen37058/Physical-Attacks-in-Embodied-Nav",
    "pdf_url": "https://arxiv.org/pdf/2409.10071v5",
    "github_url": "https://github.com/chen37058/Physical-Attacks-in-Embodied-Nav",
    "published": "2024-09-16T08:21:22+00:00",
    "updated": "2025-08-15T02:26:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.10027v4",
    "title": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation with Language Models",
    "authors": [
      "Kim",
      "Kim",
      "Oh"
    ],
    "summary": "Large language models (LLMs) have shown significant potential in guiding embodied agents to execute language instructions across a range of tasks, including robotic manipulation and navigation. However, existing methods are primarily designed for static environments and do not leverage the agent's own experiences to refine its initial plans. Given that real-world environments are inherently stochastic, initial plans based solely on LLMs' general knowledge may fail to achieve their objectives, unlike in static scenarios. To address this limitation, this study introduces the Experience-and-Emotion Map (E2Map), which integrates not only LLM knowledge but also the agent's real-world experiences, drawing inspiration from human emotional responses. The proposed methodology enables one-shot behavior adjustments by updating the E2Map based on the agent's experiences. Our evaluation in stochastic navigation environments, including both simulations and real-world scenarios, demonstrates that the proposed method significantly enhances performance in stochastic environments compared to existing LLM-based approaches. Code and supplementary materials are available at https://e2map.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2409.10027v4",
    "github_url": null,
    "published": "2024-09-16T06:35:18+00:00",
    "updated": "2025-02-03T01:26:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.05593v1",
    "title": "StratXplore: Strategic Novelty-seeking and Instruction-aligned Exploration for Vision and Language Navigation",
    "authors": [
      "Gopinathan",
      "Abu-Khalaf",
      "Suter"
    ],
    "summary": "Embodied navigation requires robots to understand and interact with the environment based on given tasks. Vision-Language Navigation (VLN) is an embodied navigation task, where a robot navigates within a previously seen and unseen environment, based on linguistic instruction and visual inputs. VLN agents need access to both local and global action spaces; former for immediate decision making and the latter for recovering from navigational mistakes. Prior VLN agents rely only on instruction-viewpoint alignment for local and global decision making and back-track to a previously visited viewpoint, if the instruction and its current viewpoint mismatches. These methods are prone to mistakes, due to the complexity of the instruction and partial observability of the environment. We posit that, back-tracking is sub-optimal and agent that is aware of its mistakes can recover efficiently. For optimal recovery, exploration should be extended to unexplored viewpoints (or frontiers). The optimal frontier is a recently observed but unexplored viewpoint that aligns with the instruction and is novel. We introduce a memory-based and mistake-aware path planning strategy for VLN agents, called \\textit{StratXplore}, that presents global and local action planning to select the optimal frontier for path correction. The proposed method collects all past actions and viewpoint features during navigation and then selects the optimal frontier suitable for recovery. Experimental results show this simple yet effective strategy improves the success rate on two VLN datasets with different task complexities.",
    "pdf_url": "https://arxiv.org/pdf/2409.05593v1",
    "github_url": null,
    "published": "2024-09-09T13:23:24+00:00",
    "updated": "2024-09-09T13:23:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.05583v1",
    "title": "Spatially-Aware Speaker for Vision-and-Language Navigation Instruction Generation",
    "authors": [
      "Gopinathan",
      "Masek",
      "Abu-Khalaf"
    ],
    "summary": "Embodied AI aims to develop robots that can \\textit{understand} and execute human language instructions, as well as communicate in natural languages. On this front, we study the task of generating highly detailed navigational instructions for the embodied robots to follow. Although recent studies have demonstrated significant leaps in the generation of step-by-step instructions from sequences of images, the generated instructions lack variety in terms of their referral to objects and landmarks. Existing speaker models learn strategies to evade the evaluation metrics and obtain higher scores even for low-quality sentences. In this work, we propose SAS (Spatially-Aware Speaker), an instruction generator or \\textit{Speaker} model that utilises both structural and semantic knowledge of the environment to produce richer instructions. For training, we employ a reward learning method in an adversarial setting to avoid systematic bias introduced by language evaluation metrics. Empirically, our method outperforms existing instruction generation models, evaluated using standard metrics. Our code is available at \\url{https://github.com/gmuraleekrishna/SAS}.",
    "pdf_url": "https://arxiv.org/pdf/2409.05583v1",
    "github_url": "https://github.com/gmuraleekrishna/SAS",
    "published": "2024-09-09T13:12:11+00:00",
    "updated": "2024-09-09T13:12:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.05552v2",
    "title": "Seeing is Believing? Enhancing Vision-Language Navigation using Visual Perturbations",
    "authors": [
      "Zhang",
      "Li",
      "Xu"
    ],
    "summary": "Autonomous navigation guided by natural language instructions in embodied environments remains a challenge for vision-language navigation (VLN) agents. Although recent advancements in learning diverse and fine-grained visual environmental representations have shown promise, the fragile performance improvements may not conclusively attribute to enhanced visual grounding,a limitation also observed in related vision-language tasks. In this work, we preliminarily investigate whether advanced VLN models genuinely comprehend the visual content of their environments by introducing varying levels of visual perturbations. These perturbations include ground-truth depth images, perturbed views and random noise. Surprisingly, we experimentally find that simple branch expansion, even with noisy visual inputs, paradoxically improves the navigational efficacy. Inspired by these insights, we further present a versatile Multi-Branch Architecture (MBA) designed to delve into the impact of both the branch quantity and visual quality. The proposed MBA extends a base agent into a multi-branch variant, where each branch processes a different visual input. This approach is embarrassingly simple yet agnostic to topology-based VLN agents. Extensive experiments on three VLN benchmarks (R2R, REVERIE, SOON) demonstrate that our method with optimal visual permutations matches or even surpasses state-of-the-art results. The source code is available at here.",
    "pdf_url": "https://arxiv.org/pdf/2409.05552v2",
    "github_url": null,
    "published": "2024-09-09T12:17:38+00:00",
    "updated": "2025-04-07T12:15:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.04965v2",
    "title": "Socially-Aware Robot Navigation Enhanced by Bidirectional Natural Language Conversations Using Large Language Models",
    "authors": [
      "Wen",
      "Liu",
      "Bethala"
    ],
    "summary": "Robot navigation is crucial across various domains, yet traditional methods focus on efficiency and obstacle avoidance, often overlooking human behavior in shared spaces. With the rise of service robots, socially aware navigation has gained prominence. However, existing approaches primarily predict pedestrian movements or issue alerts, lacking true human-robot interaction. We introduce Hybrid Soft Actor-Critic with Large Language Model (HSAC-LLM), a novel framework for socially aware navigation. By integrating deep reinforcement learning with large language models, HSAC-LLM enables bidirectional natural language interactions, predicting both continuous and discrete navigation actions. When potential collisions arise, the robot proactively communicates with pedestrians to determine avoidance strategies. Experiments in 2D simulation, Gazebo, and real-world environments demonstrate that HSAC-LLM outperforms state-of-the-art DRL methods in interaction, navigation, and obstacle avoidance. This paradigm advances effective human-robot interactions in dynamic settings. Videos are available at https://hsacllm.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2409.04965v2",
    "github_url": null,
    "published": "2024-09-08T04:04:21+00:00",
    "updated": "2025-03-23T19:45:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.04837v2",
    "title": "Context-Aware Replanning with Pre-explored Semantic Map for Object Navigation",
    "authors": [
      "Ko",
      "Su",
      "Chen"
    ],
    "summary": "Pre-explored Semantic Maps, constructed through prior exploration using visual language models (VLMs), have proven effective as foundational elements for training-free robotic applications. However, existing approaches assume the map's accuracy and do not provide effective mechanisms for revising decisions based on incorrect maps. To address this, we introduce Context-Aware Replanning (CARe), which estimates map uncertainty through confidence scores and multi-view consistency, enabling the agent to revise erroneous decisions stemming from inaccurate maps without requiring additional labels. We demonstrate the effectiveness of our proposed method by integrating it with two modern mapping backbones, VLMaps and OpenMask3D, and observe significant performance improvements in object navigation tasks. More details can be found on the project page: https://care-maps.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2409.04837v2",
    "github_url": null,
    "published": "2024-09-07T14:25:08+00:00",
    "updated": "2024-11-02T15:03:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.02669v2",
    "title": "Causality-Aware Transformer Networks for Robotic Navigation",
    "authors": [
      "Wang",
      "Liu",
      "Cao"
    ],
    "summary": "Current research in Visual Navigation reveals opportunities for improvement. First, the direct adoption of RNNs and Transformers often overlooks the specific differences between Embodied AI and traditional sequential data modelling, potentially limiting its performance in Embodied AI tasks. Second, the reliance on task-specific configurations, such as pre-trained modules and dataset-specific logic, compromises the generalizability of these methods. We address these constraints by initially exploring the unique differences between Navigation tasks and other sequential data tasks through the lens of Causality, presenting a causal framework to elucidate the inadequacies of conventional sequential methods for Navigation. By leveraging this causal perspective, we propose Causality-Aware Transformer (CAT) Networks for Navigation, featuring a Causal Understanding Module to enhance the models's Environmental Understanding capability. Meanwhile, our method is devoid of task-specific inductive biases and can be trained in an End-to-End manner, which enhances the method's generalizability across various contexts. Empirical evaluations demonstrate that our methodology consistently surpasses benchmark performances across a spectrum of settings, tasks and simulation environments. Extensive ablation studies reveal that the performance gains can be attributed to the Causal Understanding Module, which demonstrates effectiveness and efficiency in both Reinforcement Learning and Supervised Learning settings.",
    "pdf_url": "https://arxiv.org/pdf/2409.02669v2",
    "github_url": null,
    "published": "2024-09-04T12:53:26+00:00",
    "updated": "2024-10-05T11:34:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.02561v2",
    "title": "Vision-Language Navigation with Continual Learning",
    "authors": [
      "Li",
      "Lv",
      "Tu"
    ],
    "summary": "Vision-language navigation (VLN) is a critical domain within embedded intelligence, requiring agents to navigate 3D environments based on natural language instructions. Traditional VLN research has focused on improving environmental understanding and decision accuracy. However, these approaches often exhibit a significant performance gap when agents are deployed in novel environments, mainly due to the limited diversity of training data. Expanding datasets to cover a broader range of environments is impractical and costly. We propose the Vision-Language Navigation with Continual Learning (VLNCL) paradigm to address this challenge. In this paradigm, agents incrementally learn new environments while retaining previously acquired knowledge. VLNCL enables agents to maintain an environmental memory and extract relevant knowledge, allowing rapid adaptation to new environments while preserving existing information. We introduce a novel dual-loop scenario replay method (Dual-SR) inspired by brain memory replay mechanisms integrated with VLN agents. This method facilitates consolidating past experiences and enhances generalization across new tasks. By utilizing a multi-scenario memory buffer, the agent efficiently organizes and replays task memories, thereby bolstering its ability to adapt quickly to new environments and mitigating catastrophic forgetting. Our work pioneers continual learning in VLN agents, introducing a novel experimental setup and evaluation metrics. We demonstrate the effectiveness of our approach through extensive evaluations and establish a benchmark for the VLNCL paradigm. Comparative experiments with existing continual learning and VLN methods show significant improvements, achieving state-of-the-art performance in continual learning ability and highlighting the potential of our approach in enabling rapid adaptation while preserving prior knowledge.",
    "pdf_url": "https://arxiv.org/pdf/2409.02561v2",
    "github_url": null,
    "published": "2024-09-04T09:28:48+00:00",
    "updated": "2024-09-23T03:17:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.02522v2",
    "title": "Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments",
    "authors": [
      "Li",
      "Lu",
      "Mu"
    ],
    "summary": "Vision Language Navigation in Continuous Environments (VLN-CE) represents a frontier in embodied AI, demanding agents to navigate freely in unbounded 3D spaces solely guided by natural language instructions. This task introduces distinct challenges in multimodal comprehension, spatial reasoning, and decision-making. To address these challenges, we introduce Cog-GA, a generative agent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA employs a dual-pronged strategy to emulate human-like cognitive processes. Firstly, it constructs a cognitive map, integrating temporal, spatial, and semantic elements, thereby facilitating the development of spatial memory within LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints, strategically optimizing the exploration trajectory to maximize navigational efficiency. Each waypoint is accompanied by a dual-channel scene description, categorizing environmental cues into 'what' and 'where' streams as the brain. This segregation enhances the agent's attentional focus, enabling it to discern pertinent spatial information for navigation. A reflective mechanism complements these strategies by capturing feedback from prior navigation experiences, facilitating continual learning and adaptive replanning. Extensive evaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art performance and ability to simulate human-like navigation behaviors. This research significantly contributes to the development of strategic and interpretable VLN-CE agents.",
    "pdf_url": "https://arxiv.org/pdf/2409.02522v2",
    "github_url": null,
    "published": "2024-09-04T08:30:03+00:00",
    "updated": "2024-09-23T03:18:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.02389v2",
    "title": "Multi-modal Situated Reasoning in 3D Scenes",
    "authors": [
      "Linghu",
      "Huang",
      "Niu"
    ],
    "summary": "Situation awareness is essential for understanding and reasoning about 3D scenes in embodied AI agents. However, existing datasets and benchmarks for situated understanding are limited in data modality, diversity, scale, and task scope. To address these limitations, we propose Multi-modal Situated Question Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably collected leveraging 3D scene graphs and vision-language models (VLMs) across a diverse range of real-world 3D scenes. MSQA includes 251K situated question-answering pairs across 9 distinct question categories, covering complex scenarios within 3D scenes. We introduce a novel interleaved multi-modal input setting in our benchmark to provide text, image, and point cloud for situation and question description, resolving ambiguity in previous single-modality convention (e.g., text). Additionally, we devise the Multi-modal Situated Next-step Navigation (MSNN) benchmark to evaluate models' situated reasoning for navigation. Comprehensive evaluations on MSQA and MSNN highlight the limitations of existing vision-language models and underscore the importance of handling multi-modal interleaved inputs and situation modeling. Experiments on data scaling and cross-domain transfer further demonstrate the efficacy of leveraging MSQA as a pre-training dataset for developing more powerful situated reasoning models.",
    "pdf_url": "https://arxiv.org/pdf/2409.02389v2",
    "github_url": null,
    "published": "2024-09-04T02:37:38+00:00",
    "updated": "2024-11-18T02:32:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.01630v1",
    "title": "SafeEmbodAI: a Safety Framework for Mobile Robots in Embodied AI Systems",
    "authors": [
      "Zhang",
      "Kong",
      "Braunl"
    ],
    "summary": "Embodied AI systems, including AI-powered robots that autonomously interact with the physical world, stand to be significantly advanced by Large Language Models (LLMs), which enable robots to better understand complex language commands and perform advanced tasks with enhanced comprehension and adaptability, highlighting their potential to improve embodied AI capabilities. However, this advancement also introduces safety challenges, particularly in robotic navigation tasks. Improper safety management can lead to failures in complex environments and make the system vulnerable to malicious command injections, resulting in unsafe behaviours such as detours or collisions. To address these issues, we propose \\textit{SafeEmbodAI}, a safety framework for integrating mobile robots into embodied AI systems. \\textit{SafeEmbodAI} incorporates secure prompting, state management, and safety validation mechanisms to secure and assist LLMs in reasoning through multi-modal data and validating responses. We designed a metric to evaluate mission-oriented exploration, and evaluations in simulated environments demonstrate that our framework effectively mitigates threats from malicious commands and improves performance in various environment settings, ensuring the safety of embodied AI systems. Notably, In complex environments with mixed obstacles, our method demonstrates a significant performance increase of 267\\% compared to the baseline in attack scenarios, highlighting its robustness in challenging conditions.",
    "pdf_url": "https://arxiv.org/pdf/2409.01630v1",
    "github_url": null,
    "published": "2024-09-03T05:56:50+00:00",
    "updated": "2024-09-03T05:56:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.01581v1",
    "title": "GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing Color Point Clouds via 3D Gaussian Splatting",
    "authors": [
      "Guo",
      "Xie",
      "Xie"
    ],
    "summary": "Dense colored point clouds enhance visual perception and are of significant value in various robotic applications. However, existing learning-based point cloud upsampling methods are constrained by computational resources and batch processing strategies, which often require subdividing point clouds into smaller patches, leading to distortions that degrade perceptual quality. To address this challenge, we propose a novel 2D-3D hybrid colored point cloud upsampling framework (GaussianPU) based on 3D Gaussian Splatting (3DGS) for robotic perception. This approach leverages 3DGS to bridge 3D point clouds with their 2D rendered images in robot vision systems. A dual scale rendered image restoration network transforms sparse point cloud renderings into dense representations, which are then input into 3DGS along with precise robot camera poses and interpolated sparse point clouds to reconstruct dense 3D point clouds. We have made a series of enhancements to the vanilla 3DGS, enabling precise control over the number of points and significantly boosting the quality of the upsampled point cloud for robotic scene understanding. Our framework supports processing entire point clouds on a single consumer-grade GPU, such as the NVIDIA GeForce RTX 3090, eliminating the need for segmentation and thus producing high-quality, dense colored point clouds with millions of points for robot navigation and manipulation tasks. Extensive experimental results on generating million-level point cloud data validate the effectiveness of our method, substantially improving the quality of colored point clouds and demonstrating significant potential for applications involving large-scale point clouds in autonomous robotics and human-robot interaction scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2409.01581v1",
    "github_url": null,
    "published": "2024-09-03T03:35:04+00:00",
    "updated": "2024-09-03T03:35:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.14183v2",
    "title": "Robot Navigation with Entity-Based Collision Avoidance using Deep Reinforcement Learning",
    "authors": [
      "Kolomeytsev",
      "Golembiovsky"
    ],
    "summary": "Efficient navigation in dynamic environments is crucial for autonomous robots interacting with moving agents and static obstacles. We present a novel deep reinforcement learning approach that improves robot navigation and interaction with different types of agents and obstacles based on specific safety requirements. Our approach uses information about the entity types, improving collision avoidance and ensuring safer navigation. We introduce a new reward function that penalizes the robot for being close to or colliding with different entities such as adults, bicyclists, children, and static obstacles, while also encouraging the robot's progress toward the goal. We propose an optimized algorithm that significantly accelerates the training, validation, and testing phases, enabling efficient learning in complex environments. Comprehensive experiments demonstrate that our approach consistently outperforms state-of-the-art navigation and collision avoidance methods.",
    "pdf_url": "https://arxiv.org/pdf/2408.14183v2",
    "github_url": null,
    "published": "2024-08-26T11:16:03+00:00",
    "updated": "2025-09-28T16:25:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.11051v2",
    "title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments",
    "authors": [
      "Xu",
      "Pan",
      "Liu"
    ],
    "summary": "Large Language Models (LLMs) have demonstrated potential in Vision-and-Language Navigation (VLN) tasks, yet current applications face challenges. While LLMs excel in general conversation scenarios, they struggle with specialized navigation tasks, yielding suboptimal performance compared to specialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied Agent), a novel Multimodal LLM-based agent and architecture designed for urban VLN tasks that efficiently handles multiple observations. Our approach implements a three-phase tuning technique for effective adaptation to navigation tasks, including single perception tuning for street view description, multiple perception tuning for route summarization, and end-to-end training on VLN datasets. The augmented datasets are synthesized automatically. Experimental results demonstrate FLAME's superiority over existing methods, surpassing state-of-the-art methods by a 7.3% increase in task completion on Touchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs) in complex navigation tasks, representing an advancement towards applications of MLLMs in the field of embodied intelligence.",
    "pdf_url": "https://arxiv.org/pdf/2408.11051v2",
    "github_url": null,
    "published": "2024-08-20T17:57:46+00:00",
    "updated": "2025-01-21T04:06:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.10899v1",
    "title": "All Robots in One: A New Standard and Unified Dataset for Versatile, General-Purpose Embodied Agents",
    "authors": [
      "Wang",
      "Zheng",
      "Nie"
    ],
    "summary": "Embodied AI is transforming how AI systems interact with the physical world, yet existing datasets are inadequate for developing versatile, general-purpose agents. These limitations include a lack of standardized formats, insufficient data diversity, and inadequate data volume. To address these issues, we introduce ARIO (All Robots In One), a new data standard that enhances existing datasets by offering a unified data format, comprehensive sensory modalities, and a combination of real-world and simulated data. ARIO aims to improve the training of embodied AI agents, increasing their robustness and adaptability across various tasks and environments. Building upon the proposed new standard, we present a large-scale unified ARIO dataset, comprising approximately 3 million episodes collected from 258 series and 321,064 tasks. The ARIO standard and dataset represent a significant step towards bridging the gaps of existing data resources. By providing a cohesive framework for data collection and representation, ARIO paves the way for the development of more powerful and versatile embodied AI agents, capable of navigating and interacting with the physical world in increasingly complex and diverse ways. The project is available on https://imaei.github.io/project_pages/ario/",
    "pdf_url": "https://arxiv.org/pdf/2408.10899v1",
    "github_url": null,
    "published": "2024-08-20T14:40:20+00:00",
    "updated": "2024-08-20T14:40:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.10578v1",
    "title": "Where to Fetch: Extracting Visual Scene Representation from Large Pre-Trained Models for Robotic Goal Navigation",
    "authors": [
      "Li",
      "Li",
      "Zhao"
    ],
    "summary": "To complete a complex task where a robot navigates to a goal object and fetches it, the robot needs to have a good understanding of the instructions and the surrounding environment. Large pre-trained models have shown capabilities to interpret tasks defined via language descriptions. However, previous methods attempting to integrate large pre-trained models with daily tasks are not competent in many robotic goal navigation tasks due to poor understanding of the environment. In this work, we present a visual scene representation built with large-scale visual language models to form a feature representation of the environment capable of handling natural language queries. Combined with large language models, this method can parse language instructions into action sequences for a robot to follow, and accomplish goal navigation with querying the scene representation. Experiments demonstrate that our method enables the robot to follow a wide range of instructions and complete complex goal navigation tasks.",
    "pdf_url": "https://arxiv.org/pdf/2408.10578v1",
    "github_url": null,
    "published": "2024-08-20T06:36:40+00:00",
    "updated": "2024-08-20T06:36:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.10388v1",
    "title": "Narrowing the Gap between Vision and Action in Navigation",
    "authors": [
      "Zhang",
      "Kordjamshidi"
    ],
    "summary": "The existing methods for Vision and Language Navigation in the Continuous Environment (VLN-CE) commonly incorporate a waypoint predictor to discretize the environment. This simplifies the navigation actions into a view selection task and improves navigation performance significantly compared to direct training using low-level actions. However, the VLN-CE agents are still far from the real robots since there are gaps between their visual perception and executed actions. First, VLN-CE agents that discretize the visual environment are primarily trained with high-level view selection, which causes them to ignore crucial spatial reasoning within the low-level action movements. Second, in these models, the existing waypoint predictors neglect object semantics and their attributes related to passibility, which can be informative in indicating the feasibility of actions. To address these two issues, we introduce a low-level action decoder jointly trained with high-level action prediction, enabling the current VLN agent to learn and ground the selected visual view to the low-level controls. Moreover, we enhance the current waypoint predictor by utilizing visual representations containing rich semantic information and explicitly masking obstacles based on humans' prior knowledge about the feasibility of actions. Empirically, our agent can improve navigation performance metrics compared to the strong baselines on both high-level and low-level actions.",
    "pdf_url": "https://arxiv.org/pdf/2408.10388v1",
    "github_url": null,
    "published": "2024-08-19T20:09:56+00:00",
    "updated": "2024-08-19T20:09:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2409.00015v1",
    "title": "Navigating the sociotechnical labyrinth: Dynamic certification for responsible embodied AI",
    "authors": [
      "Bakirtzis",
      "Tubella",
      "Theodorou"
    ],
    "summary": "Sociotechnical requirements shape the governance of artificially intelligent (AI) systems. In an era where embodied AI technologies are rapidly reshaping various facets of contemporary society, their inherent dynamic adaptability presents a unique blend of opportunities and challenges. Traditional regulatory mechanisms, often designed for static -- or slower-paced -- technologies, find themselves at a crossroads when faced with the fluid and evolving nature of AI systems. Moreover, typical problems in AI, for example, the frequent opacity and unpredictability of the behaviour of the systems, add additional sociotechnical challenges.   To address these interconnected issues, we introduce the concept of dynamic certification, an adaptive regulatory framework specifically crafted to keep pace with the continuous evolution of AI systems. The complexity of these challenges requires common progress in multiple domains: technical, socio-governmental, and regulatory. Our proposed transdisciplinary approach is designed to ensure the safe, ethical, and practical deployment of AI systems, aligning them bidirectionally with the real-world contexts in which they operate. By doing so, we aim to bridge the gap between rapid technological advancement and effective regulatory oversight, ensuring that AI systems not only achieve their intended goals but also adhere to ethical standards and societal values.",
    "pdf_url": "https://arxiv.org/pdf/2409.00015v1",
    "github_url": null,
    "published": "2024-08-16T08:35:26+00:00",
    "updated": "2024-08-16T08:35:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.08301v1",
    "title": "VLPG-Nav: Object Navigation Using Visual Language Pose Graph and Object Localization Probability Maps",
    "authors": [
      "Arul",
      "Kumar",
      "Sugirtharaj"
    ],
    "summary": "We present VLPG-Nav, a visual language navigation method for guiding robots to specified objects within household scenes. Unlike existing methods primarily focused on navigating the robot toward objects, our approach considers the additional challenge of centering the object within the robot's camera view. Our method builds a visual language pose graph (VLPG) that functions as a spatial map of VL embeddings. Given an open vocabulary object query, we plan a viewpoint for object navigation using the VLPG. Despite navigating to the viewpoint, real-world challenges like object occlusion, displacement, and the robot's localization error can prevent visibility. We build an object localization probability map that leverages the robot's current observations and prior VLPG. When the object isn't visible, the probability map is updated and an alternate viewpoint is computed. In addition, we propose an object-centering formulation that locally adjusts the robot's pose to center the object in the camera view. We evaluate the effectiveness of our approach through simulations and real-world experiments, evaluating its ability to successfully view and center the object within the camera field of view. VLPG-Nav demonstrates improved performance in locating the object, navigating around occlusions, and centering the object within the robot's camera view, outperforming the selected baselines in the evaluation metrics.",
    "pdf_url": "https://arxiv.org/pdf/2408.08301v1",
    "github_url": null,
    "published": "2024-08-15T17:54:55+00:00",
    "updated": "2024-08-15T17:54:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.05877v2",
    "title": "Towards pedestrian head tracking: A benchmark dataset and a multi-source data fusion network",
    "authors": [
      "Sun",
      "Wang",
      "Liu"
    ],
    "summary": "Pedestrian detection and tracking in crowded video sequences have many applications, including autonomous driving, robot navigation and pedestrian flow analysis. However, detecting and tracking pedestrians in high-density crowds face many challenges, including intra-class occlusions, complex motions, and diverse poses. Although artificial intelligence (AI) models have achieved great progress in head detection, head tracking datasets and methods are extremely lacking. Existing head datasets have limited coverage of complex pedestrian flows and scenes (e.g., pedestrian interactions, occlusions, and object interference). It is of great importance to develop new head tracking datasets and methods. To address these challenges, we present a Chinese Large-scale Cross-scene Pedestrian Head Tracking dataset (Cchead) and a Multi-source Data Fusion Network (MDFN). The dataset has features that are of considerable interest, including 10 diverse scenes of 50,528 frames with about 2,366,249 heads and 2,358 tracks. Our dataset contains diverse pedestrian moving speeds, directions, and complex crowd pedestrian flows with collision avoidance behaviors. Existing state-of-the-art (SOTA) algorithms are tested and compared on the Cchead dataset. MDFN is the first end-to-end convolutional neural network (CNN)-based head detection and tracking network that jointly trains Red, Green, Blue (RGB) frames, pixel-level motion information, depth maps, and density maps in videos. Ablation experiments confirm the significance of multi-source data fusion. Compared with SOTA pedestrian detection and tracking methods, MDFN achieves superior performance across three datasets: Cchead, Restaurant and Crowd of Heads Dataset (CroHD). To promote further development, we share our source code and trained models for global researchers: https://github.com/kailaisun/Cchead.",
    "pdf_url": "https://arxiv.org/pdf/2408.05877v2",
    "github_url": "https://github.com/kailaisun/Cchead",
    "published": "2024-08-12T00:21:36+00:00",
    "updated": "2025-08-20T00:10:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.05090v1",
    "title": "Loc4Plan: Locating Before Planning for Outdoor Vision and Language Navigation",
    "authors": [
      "Tian",
      "Meng",
      "Zheng"
    ],
    "summary": "Vision and Language Navigation (VLN) is a challenging task that requires agents to understand instructions and navigate to the destination in a visual environment.One of the key challenges in outdoor VLN is keeping track of which part of the instruction was completed. To alleviate this problem, previous works mainly focus on grounding the natural language to the visual input, but neglecting the crucial role of the agent's spatial position information in the grounding process. In this work, we first explore the substantial effect of spatial position locating on the grounding of outdoor VLN, drawing inspiration from human navigation. In real-world navigation scenarios, before planning a path to the destination, humans typically need to figure out their current location. This observation underscores the pivotal role of spatial localization in the navigation process. In this work, we introduce a novel framework, Locating be for Planning (Loc4Plan), designed to incorporate spatial perception for action planning in outdoor VLN tasks. The main idea behind Loc4Plan is to perform the spatial localization before planning a decision action based on corresponding guidance, which comprises a block-aware spatial locating (BAL) module and a spatial-aware action planning (SAP) module. Specifically, to help the agent perceive its spatial location in the environment, we propose to learn a position predictor that measures how far the agent is from the next intersection for reflecting its position, which is achieved by the BAL module. After the locating process, we propose the SAP module to incorporate spatial information to ground the corresponding guidance and enhance the precision of action planning. Extensive experiments on the Touchdown and map2seq datasets show that the proposed Loc4Plan outperforms the SOTA methods.",
    "pdf_url": "https://arxiv.org/pdf/2408.05090v1",
    "github_url": null,
    "published": "2024-08-09T14:31:09+00:00",
    "updated": "2024-08-09T14:31:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.04423v1",
    "title": "UNMuTe: Unifying Navigation and Multimodal Dialogue-like Text Generation",
    "authors": [
      "Rawal",
      "Bigazzi",
      "Baraldi"
    ],
    "summary": "Smart autonomous agents are becoming increasingly important in various real-life applications, including robotics and autonomous vehicles. One crucial skill that these agents must possess is the ability to interact with their surrounding entities, such as other agents or humans. In this work, we aim at building an intelligent agent that can efficiently navigate in an environment while being able to interact with an oracle (or human) in natural language and ask for directions when it is unsure about its navigation performance. The interaction is started by the agent that produces a question, which is then answered by the oracle on the basis of the shortest trajectory to the goal. The process can be performed multiple times during navigation, thus enabling the agent to hold a dialogue with the oracle. To this end, we propose a novel computational model, named UNMuTe, that consists of two main components: a dialogue model and a navigator. Specifically, the dialogue model is based on a GPT-2 decoder that handles multimodal data consisting of both text and images. First, the dialogue model is trained to generate question-answer pairs: the question is generated using the current image, while the answer is produced leveraging future images on the path toward the goal. Subsequently, a VLN model is trained to follow the dialogue predicting navigation actions or triggering the dialogue model if it needs help. In our experimental analysis, we show that UNMuTe achieves state-of-the-art performance on the main navigation tasks implying dialogue, i.e. Cooperative Vision and Dialogue Navigation (CVDN) and Navigation from Dialogue History (NDH), proving that our approach is effective in generating useful questions and answers to guide navigation.",
    "pdf_url": "https://arxiv.org/pdf/2408.04423v1",
    "github_url": null,
    "published": "2024-08-08T12:47:52+00:00",
    "updated": "2024-08-08T12:47:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.03807v1",
    "title": "Navigating the Human Maze: Real-Time Robot Pathfinding with Generative Imitation Learning",
    "authors": [
      "Moder",
      "Adhisaputra",
      "Pauli"
    ],
    "summary": "This paper addresses navigation in crowded environments by integrating goal-conditioned generative models with Sampling-based Model Predictive Control (SMPC). We introduce goal-conditioned autoregressive models to generate crowd behaviors, capturing intricate interactions among individuals. The model processes potential robot trajectory samples and predicts the reactions of surrounding individuals, enabling proactive robotic navigation in complex scenarios. Extensive experiments show that this algorithm enables real-time navigation, significantly reducing collision rates and path lengths, and outperforming selected baseline methods. The practical effectiveness of this algorithm is validated on an actual robotic platform, demonstrating its capability in dynamic settings.",
    "pdf_url": "https://arxiv.org/pdf/2408.03807v1",
    "github_url": null,
    "published": "2024-08-07T14:32:41+00:00",
    "updated": "2024-08-07T14:32:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.03515v2",
    "title": "A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems",
    "authors": [
      "Zhang",
      "Kong",
      "Dewitt"
    ],
    "summary": "The integration of Large Language Models (LLMs) like GPT-4o into robotic systems represents a significant advancement in embodied artificial intelligence. These models can process multi-modal prompts, enabling them to generate more context-aware responses. However, this integration is not without challenges. One of the primary concerns is the potential security risks associated with using LLMs in robotic navigation tasks. These tasks require precise and reliable responses to ensure safe and effective operation. Multi-modal prompts, while enhancing the robot's understanding, also introduce complexities that can be exploited maliciously. For instance, adversarial inputs designed to mislead the model can lead to incorrect or dangerous navigational decisions. This study investigates the impact of prompt injections on mobile robot performance in LLM-integrated systems and explores secure prompt strategies to mitigate these risks. Our findings demonstrate a substantial overall improvement of approximately 30.8% in both attack detection and system performance with the implementation of robust defence mechanisms, highlighting their critical role in enhancing security and reliability in mission-oriented tasks.",
    "pdf_url": "https://arxiv.org/pdf/2408.03515v2",
    "github_url": null,
    "published": "2024-08-07T02:48:22+00:00",
    "updated": "2024-09-09T01:55:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.02535v1",
    "title": "Towards Coarse-grained Visual Language Navigation Task Planning Enhanced by Event Knowledge Graph",
    "authors": [
      "Kaichen",
      "Yaoxian",
      "Haiquan"
    ],
    "summary": "Visual language navigation (VLN) is one of the important research in embodied AI. It aims to enable an agent to understand the surrounding environment and complete navigation tasks. VLN instructions could be categorized into coarse-grained and fine-grained commands. Fine-grained command describes a whole task with subtasks step-by-step. In contrast, coarse-grained command gives an abstract task description, which more suites human habits. Most existing work focuses on the former kind of instruction in VLN tasks, ignoring the latter abstract instructions belonging to daily life scenarios. To overcome the above challenge in abstract instruction, we attempt to consider coarse-grained instruction in VLN by event knowledge enhancement. Specifically, we first propose a prompt-based framework to extract an event knowledge graph (named VLN-EventKG) for VLN integrally over multiple mainstream benchmark datasets. Through small and large language model collaboration, we realize knowledge-enhanced navigation planning (named EventNav) for VLN tasks with coarse-grained instruction input. Additionally, we design a novel dynamic history backtracking module to correct potential error action planning in real time. Experimental results in various public benchmarks show our knowledge-enhanced method has superiority in coarse-grained-instruction VLN using our proposed VLN-EventKG with over $5\\%$ improvement in success rate. Our project is available at https://sites.google.com/view/vln-eventkg",
    "pdf_url": "https://arxiv.org/pdf/2408.02535v1",
    "github_url": null,
    "published": "2024-08-05T15:08:26+00:00",
    "updated": "2024-08-05T15:08:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.01867v1",
    "title": "TrustNavGPT: Modeling Uncertainty to Improve Trustworthiness of Audio-Guided LLM-Based Robot Navigation",
    "authors": [
      "Sun",
      "Zhang",
      "Tang"
    ],
    "summary": "While LLMs are proficient at processing text in human conversations, they often encounter difficulties with the nuances of verbal instructions and, thus, remain prone to hallucinate trust in human command. In this work, we present TrustNavGPT, an LLM based audio guided navigation agent that uses affective cues in spoken communication elements such as tone and inflection that convey meaning beyond words, allowing it to assess the trustworthiness of human commands and make effective, safe decisions. Our approach provides a lightweight yet effective approach that extends existing LLMs to model audio vocal features embedded in the voice command and model uncertainty for safe robotic navigation.",
    "pdf_url": "https://arxiv.org/pdf/2408.01867v1",
    "github_url": null,
    "published": "2024-08-03T21:32:43+00:00",
    "updated": "2024-08-03T21:32:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2408.00343v2",
    "title": "IN-Sight: Interactive Navigation through Sight",
    "authors": [
      "Schoch",
      "Yang",
      "Ma"
    ],
    "summary": "Current visual navigation systems often treat the environment as static, lacking the ability to adaptively interact with obstacles. This limitation leads to navigation failure when encountering unavoidable obstructions. In response, we introduce IN-Sight, a novel approach to self-supervised path planning, enabling more effective navigation strategies through interaction with obstacles. Utilizing RGB-D observations, IN-Sight calculates traversability scores and incorporates them into a semantic map, facilitating long-range path planning in complex, maze-like environments. To precisely navigate around obstacles, IN-Sight employs a local planner, trained imperatively on a differentiable costmap using representation learning techniques. The entire framework undergoes end-to-end training within the state-of-the-art photorealistic Intel SPEAR Simulator. We validate the effectiveness of IN-Sight through extensive benchmarking in a variety of simulated scenarios and ablation studies. Moreover, we demonstrate the system's real-world applicability with zero-shot sim-to-real transfer, deploying our planner on the legged robot platform ANYmal, showcasing its practical potential for interactive navigation in real environments.",
    "pdf_url": "https://arxiv.org/pdf/2408.00343v2",
    "github_url": null,
    "published": "2024-08-01T07:27:54+00:00",
    "updated": "2024-08-12T10:19:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.21452v1",
    "title": "Navigating Beyond Instructions: Vision-and-Language Navigation in Obstructed Environments",
    "authors": [
      "Hong",
      "Wang",
      "Huang"
    ],
    "summary": "Real-world navigation often involves dealing with unexpected obstructions such as closed doors, moved objects, and unpredictable entities. However, mainstream Vision-and-Language Navigation (VLN) tasks typically assume instructions perfectly align with the fixed and predefined navigation graphs without any obstructions. This assumption overlooks potential discrepancies in actual navigation graphs and given instructions, which can cause major failures for both indoor and outdoor agents. To address this issue, we integrate diverse obstructions into the R2R dataset by modifying both the navigation graphs and visual observations, introducing an innovative dataset and task, R2R with UNexpected Obstructions (R2R-UNO). R2R-UNO contains various types and numbers of path obstructions to generate instruction-reality mismatches for VLN research. Experiments on R2R-UNO reveal that state-of-the-art VLN methods inevitably encounter significant challenges when facing such mismatches, indicating that they rigidly follow instructions rather than navigate adaptively. Therefore, we propose a novel method called ObVLN (Obstructed VLN), which includes a curriculum training strategy and virtual graph construction to help agents effectively adapt to obstructed environments. Empirical results show that ObVLN not only maintains robust performance in unobstructed scenarios but also achieves a substantial performance advantage with unexpected obstructions.",
    "pdf_url": "https://arxiv.org/pdf/2407.21452v1",
    "github_url": null,
    "published": "2024-07-31T08:55:57+00:00",
    "updated": "2024-07-31T08:55:57+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.20164v1",
    "title": "Language-Conditioned Offline RL for Multi-Robot Navigation",
    "authors": [
      "Morad",
      "Shankar",
      "Blumenkamp"
    ],
    "summary": "We present a method for developing navigation policies for multi-robot teams that interpret and follow natural language instructions. We condition these policies on embeddings from pretrained Large Language Models (LLMs), and train them via offline reinforcement learning with as little as 20 minutes of randomly-collected data. Experiments on a team of five real robots show that these policies generalize well to unseen commands, indicating an understanding of the LLM latent space. Our method requires no simulators or environment models, and produces low-latency control policies that can be deployed directly to real robots without finetuning. We provide videos of our experiments at https://sites.google.com/view/llm-marl.",
    "pdf_url": "https://arxiv.org/pdf/2407.20164v1",
    "github_url": null,
    "published": "2024-07-29T16:49:30+00:00",
    "updated": "2024-07-29T16:49:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.17460v2",
    "title": "SoNIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning",
    "authors": [
      "Yao",
      "Zhang",
      "Xia"
    ],
    "summary": "Reinforcement learning (RL) enables social robots to generate trajectories without relying on human-designed rules or interventions, making it generally more effective than rule-based systems in adapting to complex, dynamic real-world scenarios. However, social navigation is a safety-critical task that requires robots to avoid collisions with pedestrians, whereas existing RL-based solutions often fall short of ensuring safety in complex environments. In this paper, we propose SoNIC, which to the best of our knowledge is the first algorithm that integrates adaptive conformal inference (ACI) with constrained reinforcement learning (CRL) to enable safe policy learning for social navigation. Specifically, our method not only augments RL observations with ACI-generated nonconformity scores, which inform the agent of the quantified uncertainty but also employs these uncertainty estimates to effectively guide the behaviors of RL agents by using constrained reinforcement learning. This integration regulates the behaviors of RL agents and enables them to handle safety-critical situations. On the standard CrowdNav benchmark, our method achieves a success rate of 96.93%, which is 11.67% higher than the previous state-of-the-art RL method and results in 4.5 times fewer collisions and 2.8 times fewer intrusions to ground-truth human future trajectories as well as enhanced robustness in out-of-distribution scenarios. To further validate our approach, we deploy our algorithm on a real robot by developing a ROS2-based navigation system. Our experiments demonstrate that the system can generate robust and socially polite decision-making when interacting with both sparse and dense crowds. The video demos can be found on our project website: https://sonic-social-nav.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2407.17460v2",
    "github_url": null,
    "published": "2024-07-24T17:57:21+00:00",
    "updated": "2025-02-06T18:55:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.16771v2",
    "title": "Topology-Guided ORCA: Smooth Multi-Agent Motion Planning in Constrained Environments",
    "authors": [
      "Pouria",
      "Huang",
      "Yammanuru"
    ],
    "summary": "We present Topology-Guided ORCA as an alternative simulator to replace ORCA for planning smooth multi-agent motions in environments with static obstacles. Despite the impressive performance in simulating multi-agent crowd motion in free space, ORCA encounters a significant challenge in navigating the agents with the presence of static obstacles. ORCA ignores static obstacles until an agent gets too close to an obstacle, and the agent will get stuck if the obstacle intercepts an agent's path toward the goal. To address this challenge, Topology-Guided ORCA constructs a graph to represent the topology of the traversable region of the environment. We use a path planner to plan a path of waypoints that connects each agent's start and goal positions. The waypoints are used as a sequence of goals to guide ORCA. The experiments of crowd simulation in constrained environments show that our method outperforms ORCA in terms of generating smooth and natural motions of multiple agents in constrained environments, which indicates great potential of Topology-Guided ORCA for serving as an effective simulator for training constrained social navigation policies.",
    "pdf_url": "https://arxiv.org/pdf/2407.16771v2",
    "github_url": null,
    "published": "2024-07-23T18:08:12+00:00",
    "updated": "2024-08-20T12:22:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.14758v1",
    "title": "DISCO: Embodied Navigation and Interaction via Differentiable Scene Semantics and Dual-level Control",
    "authors": [
      "Xu",
      "Luo",
      "Yang"
    ],
    "summary": "Building a general-purpose intelligent home-assistant agent skilled in diverse tasks by human commands is a long-term blueprint of embodied AI research, which poses requirements on task planning, environment modeling, and object interaction. In this work, we study primitive mobile manipulations for embodied agents, i.e. how to navigate and interact based on an instructed verb-noun pair. We propose DISCO, which features non-trivial advancements in contextualized scene modeling and efficient controls. In particular, DISCO incorporates differentiable scene representations of rich semantics in object and affordance, which is dynamically learned on the fly and facilitates navigation planning. Besides, we propose dual-level coarse-to-fine action controls leveraging both global and local cues to accomplish mobile manipulation tasks efficiently. DISCO easily integrates into embodied tasks such as embodied instruction following. To validate our approach, we take the ALFRED benchmark of large-scale long-horizon vision-language navigation and interaction tasks as a test bed. In extensive experiments, we make comprehensive evaluations and demonstrate that DISCO outperforms the art by a sizable +8.6% success rate margin in unseen scenes, even without step-by-step instructions. Our code is publicly released at https://github.com/AllenXuuu/DISCO.",
    "pdf_url": "https://arxiv.org/pdf/2407.14758v1",
    "github_url": "https://github.com/AllenXuuu/DISCO",
    "published": "2024-07-20T05:39:28+00:00",
    "updated": "2024-07-20T05:39:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.13567v3",
    "title": "Hyp2Nav: Hyperbolic Planning and Curiosity for Crowd Navigation",
    "authors": [
      "Melendugno",
      "Flaborea",
      "Mettes"
    ],
    "summary": "Autonomous robots are increasingly becoming a strong fixture in social environments. Effective crowd navigation requires not only safe yet fast planning, but should also enable interpretability and computational efficiency for working in real-time on embedded devices. In this work, we advocate for hyperbolic learning to enable crowd navigation and we introduce Hyp2Nav. Different from conventional reinforcement learning-based crowd navigation methods, Hyp2Nav leverages the intrinsic properties of hyperbolic geometry to better encode the hierarchical nature of decision-making processes in navigation tasks. We propose a hyperbolic policy model and a hyperbolic curiosity module that results in effective social navigation, best success rates, and returns across multiple simulation settings, using up to 6 times fewer parameters than competitor state-of-the-art models. With our approach, it becomes even possible to obtain policies that work in 2-dimensional embedding spaces, opening up new possibilities for low-resource crowd navigation and model interpretability. Insightfully, the internal hyperbolic representation of Hyp2Nav correlates with how much attention the robot pays to the surrounding crowds, e.g. due to multiple people occluding its pathway or to a few of them showing colliding plans, rather than to its own planned route. The code is available at https://github.com/GDam90/hyp2nav.",
    "pdf_url": "https://arxiv.org/pdf/2407.13567v3",
    "github_url": "https://github.com/GDam90/hyp2nav",
    "published": "2024-07-18T14:40:33+00:00",
    "updated": "2024-09-06T10:16:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.12366v2",
    "title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models",
    "authors": [
      "Zhou",
      "Hong",
      "Wang"
    ],
    "summary": "Capitalizing on the remarkable advancements in Large Language Models (LLMs), there is a burgeoning initiative to harness LLMs for instruction following robotic navigation. Such a trend underscores the potential of LLMs to generalize navigational reasoning and diverse language understanding. However, a significant discrepancy in agent performance is observed when integrating LLMs in the Vision-and-Language navigation (VLN) tasks compared to previous downstream specialist models. Furthermore, the inherent capacity of language to interpret and facilitate communication in agent interactions is often underutilized in these integrations. In this work, we strive to bridge the divide between VLN-specialized models and LLM-based navigation paradigms, while maintaining the interpretative prowess of LLMs in generating linguistic navigational reasoning. By aligning visual content in a frozen LLM, we encompass visual observation comprehension for LLMs and exploit a way to incorporate LLMs and navigation policy networks for effective action predictions and navigational reasoning. We demonstrate the data efficiency of the proposed methods and eliminate the gap between LM-based agents and state-of-the-art VLN specialists.",
    "pdf_url": "https://arxiv.org/pdf/2407.12366v2",
    "github_url": null,
    "published": "2024-07-17T07:44:26+00:00",
    "updated": "2024-09-20T01:32:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.11487v1",
    "title": "PRET: Planning with Directed Fidelity Trajectory for Vision and Language Navigation",
    "authors": [
      "Lu",
      "Meng",
      "Zheng"
    ],
    "summary": "Vision and language navigation is a task that requires an agent to navigate according to a natural language instruction. Recent methods predict sub-goals on constructed topology map at each step to enable long-term action planning. However, they suffer from high computational cost when attempting to support such high-level predictions with GCN-like models. In this work, we propose an alternative method that facilitates navigation planning by considering the alignment between instructions and directed fidelity trajectories, which refers to a path from the initial node to the candidate locations on a directed graph without detours. This planning strategy leads to an efficient model while achieving strong performance. Specifically, we introduce a directed graph to illustrate the explored area of the environment, emphasizing directionality. Then, we firstly define the trajectory representation as a sequence of directed edge features, which are extracted from the panorama based on the corresponding orientation. Ultimately, we assess and compare the alignment between instruction and different trajectories during navigation to determine the next navigation target. Our method outperforms previous SOTA method BEVBert on RxR dataset and is comparable on R2R dataset while largely reducing the computational cost. Code is available: https://github.com/iSEE-Laboratory/VLN-PRET.",
    "pdf_url": "https://arxiv.org/pdf/2407.11487v1",
    "github_url": "https://github.com/iSEE-Laboratory/VLN-PRET",
    "published": "2024-07-16T08:22:18+00:00",
    "updated": "2024-07-16T08:22:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.10943v1",
    "title": "GRUtopia: Dream General Robots in a City at Scale",
    "authors": [
      "Wang",
      "Chen",
      "Huang"
    ],
    "summary": "Recent works have been exploring the scaling laws in the field of Embodied AI. Given the prohibitive costs of collecting real-world data, we believe the Simulation-to-Real (Sim2Real) paradigm is a crucial step for scaling the learning of embodied models. This paper introduces project GRUtopia, the first simulated interactive 3D society designed for various robots. It features several advancements: (a) The scene dataset, GRScenes, includes 100k interactive, finely annotated scenes, which can be freely combined into city-scale environments. In contrast to previous works mainly focusing on home, GRScenes covers 89 diverse scene categories, bridging the gap of service-oriented environments where general robots would be initially deployed. (b) GRResidents, a Large Language Model (LLM) driven Non-Player Character (NPC) system that is responsible for social interaction, task generation, and task assignment, thus simulating social scenarios for embodied AI applications. (c) The benchmark, GRBench, supports various robots but focuses on legged robots as primary agents and poses moderately challenging tasks involving Object Loco-Navigation, Social Loco-Navigation, and Loco-Manipulation. We hope that this work can alleviate the scarcity of high-quality data in this field and provide a more comprehensive assessment of Embodied AI research. The project is available at https://github.com/OpenRobotLab/GRUtopia.",
    "pdf_url": "https://arxiv.org/pdf/2407.10943v1",
    "github_url": "https://github.com/OpenRobotLab/GRUtopia",
    "published": "2024-07-15T17:40:46+00:00",
    "updated": "2024-07-15T17:40:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.10547v2",
    "title": "Learning Social Cost Functions for Human-Aware Path Planning",
    "authors": [
      "Eirale",
      "Leonetti",
      "Chiaberge"
    ],
    "summary": "Achieving social acceptance is one of the main goals of Social Robotic Navigation. Despite this topic has received increasing interest in recent years, most of the research has focused on driving the robotic agent along obstacle-free trajectories, planning around estimates of future human motion to respect personal distances and optimize navigation. However, social interactions in everyday life are also dictated by norms that do not strictly depend on movement, such as when standing at the end of a queue rather than cutting it. In this paper, we propose a novel method to recognize common social scenarios and modify a traditional planner's cost function to adapt to them. This solution enables the robot to carry out different social navigation behaviors that would not arise otherwise, maintaining the robustness of traditional navigation. Our approach allows the robot to learn different social norms with a single learned model, rather than having different modules for each task. As a proof of concept, we consider the tasks of queuing and respect interaction spaces of groups of people talking to one another, but the method can be extended to other human activities that do not involve motion.",
    "pdf_url": "https://arxiv.org/pdf/2407.10547v2",
    "github_url": null,
    "published": "2024-07-15T08:57:02+00:00",
    "updated": "2024-10-18T12:25:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.09016v1",
    "title": "OVExp: Open Vocabulary Exploration for Object-Oriented Navigation",
    "authors": [
      "Wei",
      "Wang",
      "Chen"
    ],
    "summary": "Object-oriented embodied navigation aims to locate specific objects, defined by category or depicted in images. Existing methods often struggle to generalize to open vocabulary goals without extensive training data. While recent advances in Vision-Language Models (VLMs) offer a promising solution by extending object recognition beyond predefined categories, efficient goal-oriented exploration becomes more challenging in an open vocabulary setting. We introduce OVExp, a learning-based framework that integrates VLMs for Open-Vocabulary Exploration. OVExp constructs scene representations by encoding observations with VLMs and projecting them onto top-down maps for goal-conditioned exploration. Goals are encoded in the same VLM feature space, and a lightweight transformer-based decoder predicts target locations while maintaining versatile representation abilities. To address the impracticality of fusing dense pixel embeddings with full 3D scene reconstruction for training, we propose constructing maps using low-cost semantic categories and transforming them into CLIP's embedding space via the text encoder. The simple but effective design of OVExp significantly reduces computational costs and demonstrates strong generalization abilities to various navigation settings. Experiments on established benchmarks show OVExp outperforms previous zero-shot methods, can generalize to diverse scenes, and handle different goal modalities.",
    "pdf_url": "https://arxiv.org/pdf/2407.09016v1",
    "github_url": null,
    "published": "2024-07-12T06:07:49+00:00",
    "updated": "2024-07-12T06:07:49+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.08725v2",
    "title": "MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility",
    "authors": [
      "Wu",
      "He",
      "He"
    ],
    "summary": "Public urban spaces like streetscapes and plazas serve residents and accommodate social life in all its vibrant variations. Recent advances in Robotics and Embodied AI make public urban spaces no longer exclusive to humans. Food delivery bots and electric wheelchairs have started sharing sidewalks with pedestrians, while robot dogs and humanoids have recently emerged in the street. Micromobility enabled by AI for short-distance travel in public urban spaces plays a crucial component in the future transportation system. Ensuring the generalizability and safety of AI models maneuvering mobile machines is essential. In this work, we present MetaUrban, a compositional simulation platform for the AI-driven urban micromobility research. MetaUrban can construct an infinite number of interactive urban scenes from compositional elements, covering a vast array of ground plans, object placements, pedestrians, vulnerable road users, and other mobile agents' appearances and dynamics. We design point navigation and social navigation tasks as the pilot study using MetaUrban for urban micromobility research and establish various baselines of Reinforcement Learning and Imitation Learning. We conduct extensive evaluation across mobile machines, demonstrating that heterogeneous mechanical structures significantly influence the learning and execution of AI policies. We perform a thorough ablation study, showing that the compositional nature of the simulated environments can substantially improve the generalizability and safety of the trained mobile agents. MetaUrban will be made publicly available to provide research opportunities and foster safe and trustworthy embodied AI and micromobility in cities. The code and dataset will be publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2407.08725v2",
    "github_url": null,
    "published": "2024-07-11T17:56:49+00:00",
    "updated": "2024-10-11T09:41:17+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.07392v1",
    "title": "Malicious Path Manipulations via Exploitation of Representation Vulnerabilities of Vision-Language Navigation Systems",
    "authors": [
      "Islam",
      "Salman",
      "Shams"
    ],
    "summary": "Building on the unprecedented capabilities of large language models for command understanding and zero-shot recognition of multi-modal vision-language transformers, visual language navigation (VLN) has emerged as an effective way to address multiple fundamental challenges toward a natural language interface to robot navigation. However, such vision-language models are inherently vulnerable due to the lack of semantic meaning of the underlying embedding space. Using a recently developed gradient based optimization procedure, we demonstrate that images can be modified imperceptibly to match the representation of totally different images and unrelated texts for a vision-language model. Building on this, we develop algorithms that can adversarially modify a minimal number of images so that the robot will follow a route of choice for commands that require a number of landmarks. We demonstrate that experimentally using a recently proposed VLN system; for a given navigation command, a robot can be made to follow drastically different routes. We also develop an efficient algorithm to detect such malicious modifications reliably based on the fact that the adversarially modified images have much higher sensitivity to added Gaussian noise than the original images.",
    "pdf_url": "https://arxiv.org/pdf/2407.07392v1",
    "github_url": null,
    "published": "2024-07-10T06:32:58+00:00",
    "updated": "2024-07-10T06:32:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.07035v2",
    "title": "Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models",
    "authors": [
      "Zhang",
      "Ma",
      "Li"
    ],
    "summary": "Vision-and-Language Navigation (VLN) has gained increasing attention over recent years and many approaches have emerged to advance their development. The remarkable achievements of foundation models have shaped the challenges and proposed methods for VLN research. In this survey, we provide a top-down review that adopts a principled framework for embodied planning and reasoning, and emphasizes the current methods and future opportunities leveraging foundation models to address VLN challenges. We hope our in-depth discussions could provide valuable resources and insights: on one hand, to milestone the progress and explore opportunities and potential roles for foundation models in this field, and on the other, to organize different challenges and solutions in VLN to foundation model researchers.",
    "pdf_url": "https://arxiv.org/pdf/2407.07035v2",
    "github_url": null,
    "published": "2024-07-09T16:53:36+00:00",
    "updated": "2024-12-29T23:16:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.06886v8",
    "title": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
    "authors": [
      "Liu",
      "Chen",
      "Bai"
    ],
    "summary": "Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications (e.g., intelligent mechatronics systems, smart manufacturing) that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for embodied agents. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss potential future directions. We hope this survey will serve as a foundational reference for the research community. The associated project can be found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.",
    "pdf_url": "https://arxiv.org/pdf/2407.06886v8",
    "github_url": "https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List",
    "published": "2024-07-09T14:14:47+00:00",
    "updated": "2025-08-25T02:20:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.06545v1",
    "title": "Visual-Geometry GP-based Navigable Space for Autonomous Navigation",
    "authors": [
      "Ali",
      "Pushp",
      "Chen"
    ],
    "summary": "Autonomous navigation in unknown environments is challenging and demands the consideration of both geometric and semantic information in order to parse the navigability of the environment. In this work, we propose a novel space modeling framework, Visual-Geometry Sparse Gaussian Process (VG-SGP), that simultaneously considers semantics and geometry of the scene. Our proposed approach can overcome the limitation of visual planners that fail to recognize geometry associated with the semantic and the geometric planners that completely overlook the semantic information which is very critical in real-world navigation. The proposed method leverages dual Sparse Gaussian Processes in an integrated manner; the first is trained to forecast geometrically navigable spaces while the second predicts the semantically navigable areas. This integrated model is able to pinpoint the overlapping (geometric and semantic) navigable space. The simulation and real-world experiments demonstrate that the ability of the proposed VG-SGP model, coupled with our innovative navigation strategy, outperforms models solely reliant on visual or geometric navigation algorithms, highlighting a superior adaptive behavior.",
    "pdf_url": "https://arxiv.org/pdf/2407.06545v1",
    "github_url": null,
    "published": "2024-07-09T04:51:50+00:00",
    "updated": "2024-07-09T04:51:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.05890v2",
    "title": "Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation",
    "authors": [
      "Chen",
      "Lin",
      "Liu"
    ],
    "summary": "LLM-based agents have demonstrated impressive zero-shot performance in vision-language navigation (VLN) task. However, existing LLM-based methods often focus only on solving high-level task planning by selecting nodes in predefined navigation graphs for movements, overlooking low-level control in navigation scenarios. To bridge this gap, we propose AO-Planner, a novel Affordances-Oriented Planner for continuous VLN task. Our AO-Planner integrates various foundation models to achieve affordances-oriented low-level motion planning and high-level decision-making, both performed in a zero-shot setting. Specifically, we employ a Visual Affordances Prompting (VAP) approach, where the visible ground is segmented by SAM to provide navigational affordances, based on which the LLM selects potential candidate waypoints and plans low-level paths towards selected waypoints. We further propose a high-level PathAgent which marks planned paths into the image input and reasons the most probable path by comprehending all environmental information. Finally, we convert the selected path into 3D coordinates using camera intrinsic parameters and depth information, avoiding challenging 3D predictions for LLMs. Experiments on the challenging R2R-CE and RxR-CE datasets show that AO-Planner achieves state-of-the-art zero-shot performance (8.8% improvement on SPL). Our method can also serve as a data annotator to obtain pseudo-labels, distilling its waypoint prediction ability into a learning-based predictor. This new predictor does not require any waypoint data from the simulator and achieves 47% SR competing with supervised methods. We establish an effective connection between LLM and 3D world, presenting novel prospects for employing foundation models in low-level motion control.",
    "pdf_url": "https://arxiv.org/pdf/2407.05890v2",
    "github_url": null,
    "published": "2024-07-08T12:52:46+00:00",
    "updated": "2024-08-20T14:51:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.04158v2",
    "title": "ELCC: the Emergent Language Corpus Collection",
    "authors": [
      "Boldt",
      "Mortensen"
    ],
    "summary": "We introduce the Emergent Language Corpus Collection (ELCC): a collection of corpora generated from open source implementations of emergent communication systems across the literature. These systems include a variety of signalling game environments as well as more complex environments like a social deduction game and embodied navigation. Each corpus is annotated with metadata describing the characteristics of the source system as well as a suite of analyses of the corpus (e.g., size, entropy, average message length, performance as transfer learning data). Currently, research studying emergent languages requires directly running different systems which takes time away from actual analyses of such languages, makes studies which compare diverse emergent languages rare, and presents a barrier to entry for researchers without a background in deep learning. The availability of a substantial collection of well-documented emergent language corpora, then, will enable research which can analyze a wider variety of emergent languages, which more effectively uncovers general principles in emergent communication rather than artifacts of particular environments. We provide some quantitative and qualitative analyses with ELCC to demonstrate potential use cases of the resource in this vein.",
    "pdf_url": "https://arxiv.org/pdf/2407.04158v2",
    "github_url": null,
    "published": "2024-07-04T21:23:18+00:00",
    "updated": "2024-12-04T15:23:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.02473v1",
    "title": "Open Scene Graphs for Open World Object-Goal Navigation",
    "authors": [
      "Loo",
      "Wu",
      "Hsu"
    ],
    "summary": "How can we build robots for open-world semantic navigation tasks, like searching for target objects in novel scenes? While foundation models have the rich knowledge and generalisation needed for these tasks, a suitable scene representation is needed to connect them into a complete robot system. We address this with Open Scene Graphs (OSGs), a topo-semantic representation that retains and organises open-set scene information for these models, and has a structure that can be configured for different environment types. We integrate foundation models and OSGs into the OpenSearch system for Open World Object-Goal Navigation, which is capable of searching for open-set objects specified in natural language, while generalising zero-shot across diverse environments and embodiments. Our OSGs enhance reasoning with Large Language Models (LLM), enabling robust object-goal navigation outperforming existing LLM approaches. Through simulation and real-world experiments, we validate OpenSearch's generalisation across varied environments, robots and novel instructions.",
    "pdf_url": "https://arxiv.org/pdf/2407.02473v1",
    "github_url": null,
    "published": "2024-07-02T17:52:12+00:00",
    "updated": "2024-07-02T17:52:12+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.02539v3",
    "title": "Research on Autonomous Robots Navigation based on Reinforcement Learning",
    "authors": [
      "Wang",
      "Yan",
      "Wang"
    ],
    "summary": "Reinforcement learning continuously optimizes decision-making based on real-time feedback reward signals through continuous interaction with the environment, demonstrating strong adaptive and self-learning capabilities. In recent years, it has become one of the key methods to achieve autonomous navigation of robots. In this work, an autonomous robot navigation method based on reinforcement learning is introduced. We use the Deep Q Network (DQN) and Proximal Policy Optimization (PPO) models to optimize the path planning and decision-making process through the continuous interaction between the robot and the environment, and the reward signals with real-time feedback. By combining the Q-value function with the deep neural network, deep Q network can handle high-dimensional state space, so as to realize path planning in complex environments. Proximal policy optimization is a strategy gradient-based method, which enables robots to explore and utilize environmental information more efficiently by optimizing policy functions. These methods not only improve the robot's navigation ability in the unknown environment, but also enhance its adaptive and self-learning capabilities. Through multiple training and simulation experiments, we have verified the effectiveness and robustness of these models in various complex scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2407.02539v3",
    "github_url": null,
    "published": "2024-07-02T00:44:06+00:00",
    "updated": "2024-08-14T04:49:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2407.00632v1",
    "title": "CAMON: Cooperative Agents for Multi-Object Navigation with LLM-based Conversations",
    "authors": [
      "Wu",
      "Mu",
      "Zhou"
    ],
    "summary": "Visual navigation tasks are critical for household service robots. As these tasks become increasingly complex, effective communication and collaboration among multiple robots become imperative to ensure successful completion. In recent years, large language models (LLMs) have exhibited remarkable comprehension and planning abilities in the context of embodied agents. However, their application in household scenarios, specifically in the use of multiple agents collaborating to complete complex navigation tasks through communication, remains unexplored. Therefore, this paper proposes a framework for decentralized multi-agent navigation, leveraging LLM-enabled communication and collaboration. By designing the communication-triggered dynamic leadership organization structure, we achieve faster team consensus with fewer communication instances, leading to better navigation effectiveness and collaborative exploration efficiency. With the proposed novel communication scheme, our framework promises to be conflict-free and robust in multi-object navigation tasks, even when there is a surge in team size.",
    "pdf_url": "https://arxiv.org/pdf/2407.00632v1",
    "github_url": null,
    "published": "2024-06-30T09:14:33+00:00",
    "updated": "2024-06-30T09:14:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.20083v1",
    "title": "PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators",
    "authors": [
      "Zeng",
      "Zhang",
      "Ehsani"
    ],
    "summary": "We present PoliFormer (Policy Transformer), an RGB-only indoor navigation agent trained end-to-end with reinforcement learning at scale that generalizes to the real-world without adaptation despite being trained purely in simulation. PoliFormer uses a foundational vision transformer encoder with a causal transformer decoder enabling long-term memory and reasoning. It is trained for hundreds of millions of interactions across diverse environments, leveraging parallelized, multi-machine rollouts for efficient training with high throughput. PoliFormer is a masterful navigator, producing state-of-the-art results across two distinct embodiments, the LoCoBot and Stretch RE-1 robots, and four navigation benchmarks. It breaks through the plateaus of previous work, achieving an unprecedented 85.5% success rate in object goal navigation on the CHORES-S benchmark, a 28.5% absolute improvement. PoliFormer can also be trivially extended to a variety of downstream applications such as object tracking, multi-object navigation, and open-vocabulary navigation with no finetuning.",
    "pdf_url": "https://arxiv.org/pdf/2406.20083v1",
    "github_url": null,
    "published": "2024-06-28T17:51:10+00:00",
    "updated": "2024-06-28T17:51:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.19967v1",
    "title": "Into the Unknown: Generating Geospatial Descriptions for New Environments",
    "authors": [
      "Paz-Argaman",
      "Palowitch",
      "Kulkarni"
    ],
    "summary": "Similar to vision-and-language navigation (VLN) tasks that focus on bridging the gap between vision and language for embodied navigation, the new Rendezvous (RVS) task requires reasoning over allocentric spatial relationships (independent of the observer's viewpoint) using non-sequential navigation instructions and maps. However, performance substantially drops in new environments with no training data. Using opensource descriptions paired with coordinates (e.g., Wikipedia) provides training data but suffers from limited spatially-oriented text resulting in low geolocation resolution. We propose a large-scale augmentation method for generating high-quality synthetic data for new environments using readily available geospatial data. Our method constructs a grounded knowledge-graph, capturing entity relationships. Sampled entities and relations (`shop north of school') generate navigation instructions via (i) generating numerous templates using context-free grammar (CFG) to embed specific entities and relations; (ii) feeding the entities and relation into a large language model (LLM) for instruction generation. A comprehensive evaluation on RVS, showed that our approach improves the 100-meter accuracy by 45.83% on unseen environments. Furthermore, we demonstrate that models trained with CFG-based augmentation achieve superior performance compared with those trained with LLM-based augmentation, both in unseen and seen environments. These findings suggest that the potential advantages of explicitly structuring spatial information for text-based geospatial reasoning in previously unknown, can unlock data-scarce scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2406.19967v1",
    "github_url": null,
    "published": "2024-06-28T14:56:21+00:00",
    "updated": "2024-06-28T14:56:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.19236v3",
    "title": "Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions",
    "authors": [
      "Li",
      "Li",
      "Cheng"
    ],
    "summary": "Vision-and-Language Navigation (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce Human-Aware Vision-and-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN's unique challenges, underscores the need for further research to enhance HA-VLN agents' real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and Sim2Real transfer, paving the way for more realistic and applicable VLN systems in human-populated environments.",
    "pdf_url": "https://arxiv.org/pdf/2406.19236v3",
    "github_url": null,
    "published": "2024-06-27T15:01:42+00:00",
    "updated": "2024-11-02T02:14:09+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.17960v1",
    "title": "MAGIC: Meta-Ability Guided Interactive Chain-of-Distillation for Effective-and-Efficient Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "He",
      "Shen"
    ],
    "summary": "Despite the remarkable developments of recent large models in Embodied Artificial Intelligence (E-AI), their integration into robotics is hampered by their excessive parameter sizes and computational demands. Towards the Vision-and-Language Navigation (VLN) task, a core task in E-AI, this paper reveals the great potential of using knowledge distillation for obtaining lightweight student models by proposing a Meta-Ability Guided Interactive Chain-of-distillation (MAGIC) method. Specifically, a Meta-Ability Knowledge Distillation (MAKD) framework is proposed for decoupling and refining the necessary meta-abilities of VLN agents. A Meta-Knowledge Randomization Weighting (MKRW) and a Meta-Knowledge Transferable Determination (MKTD) module are incorporated to dynamically adjust aggregation weights at the meta-ability and sample levels, respectively. Move beyond the traditional one-step unidirectional distillation, an Interactive Chain-of-Distillation (ICoD) learning strategy is proposed to allow students to give feedback to teachers, forming a new multi-step teacher-student co-evolution pipeline. Remarkably, on the R2R test unseen public leaderboard, our smallest model, MAGIC-S, with only 5% (11M) of the teacher's size, outperforms all previous methods under the same training data. Additionally, our largest model, MAGIC-L, surpasses the previous state-of-the-art by 5.84% in SPL and 3.18% in SR. Furthermore, a new dataset was collected and annotated from our living environments, where MAGIC-S demonstrated superior performance and real-time efficiency. Our code is publicly available on https://github.com/CrystalSixone/VLN-MAGIC.",
    "pdf_url": "https://arxiv.org/pdf/2406.17960v1",
    "github_url": "https://github.com/CrystalSixone/VLN-MAGIC",
    "published": "2024-06-25T22:33:41+00:00",
    "updated": "2024-06-25T22:33:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.14240v3",
    "title": "CityNav: A Large-Scale Dataset for Real-World Aerial Navigation",
    "authors": [
      "Lee",
      "Miyanishi",
      "Kurita"
    ],
    "summary": "Vision-and-language navigation (VLN) aims to develop agents capable of navigating in realistic environments. While recent cross-modal training approaches have significantly improved navigation performance in both indoor and outdoor scenarios, aerial navigation over real-world cities remains underexplored primarily due to limited datasets and the difficulty of integrating visual and geographic information. To fill this gap, we introduce CityNav, the first large-scale real-world dataset for aerial VLN. Our dataset consists of 32,637 human demonstration trajectories, each paired with a natural language description, covering 4.65 km$^2$ across two real cities: Cambridge and Birmingham. In contrast to existing datasets composed of synthetic scenes such as AerialVLN, our dataset presents a unique challenge because agents must interpret spatial relationships between real-world landmarks and the navigation destination, making CityNav an essential benchmark for advancing aerial VLN. Furthermore, as an initial step toward addressing this challenge, we provide a methodology of creating geographic semantic maps that can be used as an auxiliary modality input during navigation. In our experiments, we compare performance of three representative aerial VLN agents (Seq2seq, CMA and AerialVLN models) and demonstrate that the semantic map representation significantly improves their navigation performance.",
    "pdf_url": "https://arxiv.org/pdf/2406.14240v3",
    "github_url": null,
    "published": "2024-06-20T12:08:27+00:00",
    "updated": "2025-08-02T16:25:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.14103v2",
    "title": "Efficient Strategy Learning by Decoupling Searching and Pathfinding for Object Navigation",
    "authors": [
      "Zheng",
      "Feng",
      "Huang"
    ],
    "summary": "Inspired by human-like behaviors for navigation: first searching to explore unknown areas before discovering the target, and then the pathfinding of moving towards the discovered target, recent studies design parallel submodules to achieve different functions in the searching and pathfinding stages, while ignoring the differences in reward signals between the two stages. As a result, these models often cannot be fully trained or are overfitting on training scenes. Another bottleneck that restricts agents from learning two-stage strategies is spatial perception ability, since the studies used generic visual encoders without considering the depth information of navigation scenes. To release the potential of the model on strategy learning, we propose the Two-Stage Reward Mechanism (TSRM) for object navigation that decouples the searching and pathfinding behaviours in an episode, enabling the agent to explore larger area in searching stage and seek the optimal path in pathfinding stage. Also, we propose a pretraining method Depth Enhanced Masked Autoencoders (DE-MAE) that enables agent to determine explored and unexplored areas during the searching stage, locate target object and plan paths during the pathfinding stage more accurately. In addition, we propose a new metric of Searching Success weighted by Searching Path Length (SSSPL) that assesses agent's searching ability and exploring efficiency. Finally, we evaluated our method on AI2-Thor and RoboTHOR extensively and demonstrated it can outperform the state-of-the-art (SOTA) methods in both the success rate and the navigation efficiency.",
    "pdf_url": "https://arxiv.org/pdf/2406.14103v2",
    "github_url": null,
    "published": "2024-06-20T08:35:10+00:00",
    "updated": "2025-07-22T02:17:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.13636v2",
    "title": "Contrast Sets for Evaluating Language-Guided Robot Policies",
    "authors": [
      "Anwar",
      "Gupta",
      "Thomason"
    ],
    "summary": "Robot evaluations in language-guided, real world settings are time-consuming and often sample only a small space of potential instructions across complex scenes. In this work, we introduce contrast sets for robotics as an approach to make small, but specific, perturbations to otherwise independent, identically distributed (i.i.d.) test instances. We investigate the relationship between experimenter effort to carry out an evaluation and the resulting estimated test performance as well as the insights that can be drawn from performance on perturbed instances. We use the relative performance change of different contrast set perturbations to characterize policies at reduced experimenter effort in both a simulated manipulation task and a physical robot vision-and-language navigation task. We encourage the use of contrast set evaluations as a more informative alternative to small scale, i.i.d. demonstrations on physical robots, and as a scalable alternative to industry-scale real world evaluations.",
    "pdf_url": "https://arxiv.org/pdf/2406.13636v2",
    "github_url": null,
    "published": "2024-06-19T15:31:21+00:00",
    "updated": "2024-10-25T15:23:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.12253v2",
    "title": "Influence-Based Reward Modulation for Implicit Communication in Human-Robot Interaction",
    "authors": [
      "Jiang",
      "Croft",
      "Burke"
    ],
    "summary": "Communication is essential for successful interaction. In human-robot interaction, implicit communication holds the potential to enhance robots' understanding of human needs, emotions, and intentions. This paper introduces a method to foster implicit communication in HRI without explicitly modelling human intentions or relying on pre-existing knowledge. Leveraging Transfer Entropy, we modulate influence between agents in social interactions in scenarios involving either collaboration or competition. By integrating influence into agents' rewards within a partially observable Markov decision process, we demonstrate that boosting influence enhances collaboration, while resisting influence diminishes performance. Our findings are validated through simulations and real-world experiments with human participants in social navigation settings.",
    "pdf_url": "https://arxiv.org/pdf/2406.12253v2",
    "github_url": null,
    "published": "2024-06-18T04:04:38+00:00",
    "updated": "2025-02-20T04:05:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.11495v2",
    "title": "Online Context Learning for Socially Compliant Navigation",
    "authors": [
      "Okunevich",
      "Lombard",
      "Krajnik"
    ],
    "summary": "Robot social navigation needs to adapt to different human factors and environmental contexts. However, since these factors and contexts are difficult to predict and cannot be exhaustively enumerated, traditional learning-based methods have difficulty in ensuring the social attributes of robots in long-term and cross-environment deployments. This letter introduces an online context learning method that aims to empower robots to adapt to new social environments online. The proposed method adopts a two-layer structure. The bottom layer is built using a deep reinforcement learning-based method to ensure the output of basic robot navigation commands. The upper layer is implemented using an online robot learning-based method to socialize the control commands suggested by the bottom layer. Experiments using a community-wide simulator show that our method outperforms the state-of-the-art ones. Experimental results in the most challenging scenarios show that our method improves the performance of the state-of-the-art by 8%. The source code of the proposed method, the data used, and the tools for the per-training step are publicly available at https://github.com/Nedzhaken/SOCSARL-OL.",
    "pdf_url": "https://arxiv.org/pdf/2406.11495v2",
    "github_url": "https://github.com/Nedzhaken/SOCSARL-OL",
    "published": "2024-06-17T12:59:13+00:00",
    "updated": "2025-03-14T10:41:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.11247v1",
    "title": "STEVE Series: Step-by-Step Construction of Agent Systems in Minecraft",
    "authors": [
      "Zhao",
      "Chai",
      "Wang"
    ],
    "summary": "Building an embodied agent system with a large language model (LLM) as its core is a promising direction. Due to the significant costs and uncontrollable factors associated with deploying and training such agents in the real world, we have decided to begin our exploration within the Minecraft environment. Our STEVE Series agents can complete basic tasks in a virtual environment and more challenging tasks such as navigation and even creative tasks, with an efficiency far exceeding previous state-of-the-art methods by a factor of $2.5\\times$ to $7.3\\times$. We begin our exploration with a vanilla large language model, augmenting it with a vision encoder and an action codebase trained on our collected high-quality dataset STEVE-21K. Subsequently, we enhanced it with a Critic and memory to transform it into a complex system. Finally, we constructed a hierarchical multi-agent system. Our recent work explored how to prune the agent system through knowledge distillation. In the future, we will explore more potential applications of STEVE agents in the real world.",
    "pdf_url": "https://arxiv.org/pdf/2406.11247v1",
    "github_url": null,
    "published": "2024-06-17T06:18:08+00:00",
    "updated": "2024-06-17T06:18:08+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.11047v1",
    "title": "Enhancing Supermarket Robot Interaction: A Multi-Level LLM Conversational Interface for Handling Diverse Customer Intents",
    "authors": [
      "Nandkumar",
      "Peternel"
    ],
    "summary": "This paper presents the design and evaluation of a novel multi-level LLM interface for supermarket robots to assist customers. The proposed interface allows customers to convey their needs through both generic and specific queries. While state-of-the-art systems like OpenAI's GPTs are highly adaptable and easy to build and deploy, they still face challenges such as increased response times and limitations in strategic control of the underlying model for tailored use-case and cost optimization. Driven by the goal of developing faster and more efficient conversational agents, this paper advocates for using multiple smaller, specialized LLMs fine-tuned to handle different user queries based on their specificity and user intent. We compare this approach to a specialized GPT model powered by GPT-4 Turbo, using the Artificial Social Agent Questionnaire (ASAQ) and qualitative participant feedback in a counterbalanced within-subjects experiment. Our findings show that our multi-LLM chatbot architecture outperformed the benchmarked GPT model across all 13 measured criteria, with statistically significant improvements in four key areas: performance, user satisfaction, user-agent partnership, and self-image enhancement. The paper also presents a method for supermarket robot navigation by mapping the final chatbot response to correct shelf numbers, enabling the robot to sequentially navigate towards the respective products, after which lower-level robot perception, control, and planning can be used for automated object retrieval. We hope this work encourages more efforts into using multiple, specialized smaller models instead of relying on a single powerful, but more expensive and slower model.",
    "pdf_url": "https://arxiv.org/pdf/2406.11047v1",
    "github_url": null,
    "published": "2024-06-16T19:13:01+00:00",
    "updated": "2024-06-16T19:13:01+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.10721v1",
    "title": "RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics",
    "authors": [
      "Yuan",
      "Duan",
      "Blukis"
    ],
    "summary": "From rearranging objects on a table to putting groceries into shelves, robots must plan precise action points to perform tasks accurately and reliably. In spite of the recent adoption of vision language models (VLMs) to control robot behavior, VLMs struggle to precisely articulate robot actions using language. We introduce an automatic synthetic data generation pipeline that instruction-tunes VLMs to robotic domains and needs. Using the pipeline, we train RoboPoint, a VLM that predicts image keypoint affordances given language instructions. Compared to alternative approaches, our method requires no real-world data collection or human demonstration, making it much more scalable to diverse environments and viewpoints. In addition, RoboPoint is a general model that enables several downstream applications such as robot navigation, manipulation, and augmented reality (AR) assistance. Our experiments demonstrate that RoboPoint outperforms state-of-the-art VLMs (GPT-4o) and visual prompting techniques (PIVOT) by 21.8% in the accuracy of predicting spatial affordance and by 30.5% in the success rate of downstream tasks. Project website: https://robo-point.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2406.10721v1",
    "github_url": null,
    "published": "2024-06-15T19:22:51+00:00",
    "updated": "2024-06-15T19:22:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.09798v3",
    "title": "Sim-to-Real Transfer via 3D Feature Fields for Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "Li",
      "Yang"
    ],
    "summary": "Vision-and-language navigation (VLN) enables the agent to navigate to a remote location in 3D environments following the natural language instruction. In this field, the agent is usually trained and evaluated in the navigation simulators, lacking effective approaches for sim-to-real transfer. The VLN agents with only a monocular camera exhibit extremely limited performance, while the mainstream VLN models trained with panoramic observation, perform better but are difficult to deploy on most monocular robots. For this case, we propose a sim-to-real transfer approach to endow the monocular robots with panoramic traversability perception and panoramic semantic understanding, thus smoothly transferring the high-performance panoramic VLN models to the common monocular robots. In this work, the semantic traversable map is proposed to predict agent-centric navigable waypoints, and the novel view representations of these navigable waypoints are predicted through the 3D feature fields. These methods broaden the limited field of view of the monocular robots and significantly improve navigation performance in the real world. Our VLN system outperforms previous SOTA monocular VLN methods in R2R-CE and RxR-CE benchmarks within the simulation environments and is also validated in real-world environments, providing a practical and high-performance solution for real-world VLN.",
    "pdf_url": "https://arxiv.org/pdf/2406.09798v3",
    "github_url": null,
    "published": "2024-06-14T07:50:09+00:00",
    "updated": "2024-10-14T04:48:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.05881v6",
    "title": "LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning",
    "authors": [
      "Singh",
      "Bhattacharyya",
      "Namboodiri"
    ],
    "summary": "Large language models (LLMs) have shown remarkable abilities in logical reasoning, in-context learning, and code generation. However, translating natural language instructions into effective robotic control policies remains a significant challenge, especially for tasks requiring long-horizon planning and operating under sparse reward conditions. Hierarchical Reinforcement Learning (HRL) provides a natural framework to address this challenge in robotics; however, it typically suffers from non-stationarity caused by the changing behavior of the lower-level policy during training, destabilizing higher-level policy learning. We introduce LGR2, a novel HRL framework that leverages LLMs to generate language-guided reward functions for the higher-level policy. By decoupling high-level reward generation from low-level policy changes, LGR2 fundamentally mitigates the non-stationarity problem in off-policy HRL, enabling stable and efficient learning. To further enhance sample efficiency in sparse environments, we integrate goal-conditioned hindsight experience relabeling. Extensive experiments across simulated and real-world robotic navigation and manipulation tasks demonstrate LGR2 outperforms both hierarchical and non-hierarchical baselines, achieving over 55% success rates on challenging tasks and robust transfer to real robots, without additional fine-tuning.",
    "pdf_url": "https://arxiv.org/pdf/2406.05881v6",
    "github_url": null,
    "published": "2024-06-09T18:40:24+00:00",
    "updated": "2025-08-27T17:57:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.05080v2",
    "title": "I2EDL: Interactive Instruction Error Detection and Localization",
    "authors": [
      "Taioli",
      "Rosa",
      "Castellini"
    ],
    "summary": "In the Vision-and-Language Navigation in Continuous Environments (VLN-CE) task, the human user guides an autonomous agent to reach a target goal via a series of low-level actions following a textual instruction in natural language. However, most existing methods do not address the likely case where users may make mistakes when providing such instruction (e.g. \"turn left\" instead of \"turn right\"). In this work, we address a novel task of Interactive VLN in Continuous Environments (IVLN-CE), which allows the agent to interact with the user during the VLN-CE navigation to verify any doubts regarding the instruction errors. We propose an Interactive Instruction Error Detector and Localizer (I2EDL) that triggers the user-agent interaction upon the detection of instruction errors during the navigation. We leverage a pre-trained module to detect instruction errors and pinpoint them in the instruction by cross-referencing the textual input and past observations. In such way, the agent is able to query the user for a timely correction, without demanding the user's cognitive load, as we locate the probable errors to a precise part of the instruction. We evaluate the proposed I2EDL on a dataset of instructions containing errors, and further devise a novel metric, the Success weighted by Interaction Number (SIN), to reflect both the navigation performance and the interaction effectiveness. We show how the proposed method can ask focused requests for corrections to the user, which in turn increases the navigation success, while minimizing the interactions.",
    "pdf_url": "https://arxiv.org/pdf/2406.05080v2",
    "github_url": null,
    "published": "2024-06-07T16:52:57+00:00",
    "updated": "2024-06-23T22:58:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.02208v1",
    "title": "Why Only Text: Empowering Vision-and-Language Navigation with Multi-modal Prompts",
    "authors": [
      "Hong",
      "Wang",
      "Huang"
    ],
    "summary": "Current Vision-and-Language Navigation (VLN) tasks mainly employ textual instructions to guide agents. However, being inherently abstract, the same textual instruction can be associated with different visual signals, causing severe ambiguity and limiting the transfer of prior knowledge in the vision domain from the user to the agent. To fill this gap, we propose Vision-and-Language Navigation with Multi-modal Prompts (VLN-MP), a novel task augmenting traditional VLN by integrating both natural language and images in instructions. VLN-MP not only maintains backward compatibility by effectively handling text-only prompts but also consistently shows advantages with different quantities and relevance of visual prompts. Possible forms of visual prompts include both exact and similar object images, providing adaptability and versatility in diverse navigation scenarios. To evaluate VLN-MP under a unified framework, we implement a new benchmark that offers: (1) a training-free pipeline to transform textual instructions into multi-modal forms with landmark images; (2) diverse datasets with multi-modal instructions for different downstream tasks; (3) a novel module designed to process various image prompts for seamless integration with state-of-the-art VLN models. Extensive experiments on four VLN benchmarks (R2R, RxR, REVERIE, CVDN) show that incorporating visual prompts significantly boosts navigation performance. While maintaining efficiency with text-only prompts, VLN-MP enables agents to navigate in the pre-explore setting and outperform text-based models, showing its broader applicability.",
    "pdf_url": "https://arxiv.org/pdf/2406.02208v1",
    "github_url": null,
    "published": "2024-06-04T11:06:13+00:00",
    "updated": "2024-06-04T11:06:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.01797v1",
    "title": "The Empirical Impact of Forgetting and Transfer in Continual Visual Odometry",
    "authors": [
      "Cudrano",
      "Luo",
      "Matteucci"
    ],
    "summary": "As robotics continues to advance, the need for adaptive and continuously-learning embodied agents increases, particularly in the realm of assistance robotics. Quick adaptability and long-term information retention are essential to operate in dynamic environments typical of humans' everyday lives. A lifelong learning paradigm is thus required, but it is scarcely addressed by current robotics literature. This study empirically investigates the impact of catastrophic forgetting and the effectiveness of knowledge transfer in neural networks trained continuously in an embodied setting. We focus on the task of visual odometry, which holds primary importance for embodied agents in enabling their self-localization. We experiment on the simple continual scenario of discrete transitions between indoor locations, akin to a robot navigating different apartments. In this regime, we observe initial satisfactory performance with high transferability between environments, followed by a specialization phase where the model prioritizes current environment-specific knowledge at the expense of generalization. Conventional regularization strategies and increased model capacity prove ineffective in mitigating this phenomenon. Rehearsal is instead mildly beneficial but with the addition of a substantial memory cost. Incorporating action information, as commonly done in embodied settings, facilitates quicker convergence but exacerbates specialization, making the model overly reliant on its motion expectations and less adept at correctly interpreting visual cues. These findings emphasize the open challenges of balancing adaptation and memory retention in lifelong robotics and contribute valuable insights into the application of a lifelong paradigm on embodied agents.",
    "pdf_url": "https://arxiv.org/pdf/2406.01797v1",
    "github_url": null,
    "published": "2024-06-03T21:32:50+00:00",
    "updated": "2024-06-03T21:32:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.01256v1",
    "title": "Augmented Commonsense Knowledge for Remote Object Grounding",
    "authors": [
      "Mohammadi",
      "Hong",
      "Qi"
    ],
    "summary": "The vision-and-language navigation (VLN) task necessitates an agent to perceive the surroundings, follow natural language instructions, and act in photo-realistic unseen environments. Most of the existing methods employ the entire image or object features to represent navigable viewpoints. However, these representations are insufficient for proper action prediction, especially for the REVERIE task, which uses concise high-level instructions, such as ''Bring me the blue cushion in the master bedroom''. To address enhancing representation, we propose an augmented commonsense knowledge model (ACK) to leverage commonsense information as a spatio-temporal knowledge graph for improving agent navigation. Specifically, the proposed approach involves constructing a knowledge base by retrieving commonsense information from ConceptNet, followed by a refinement module to remove noisy and irrelevant knowledge. We further present ACK which consists of knowledge graph-aware cross-modal and concept aggregation modules to enhance visual representation and visual-textual data alignment by integrating visible objects, commonsense knowledge, and concept history, which includes object and knowledge temporal information. Moreover, we add a new pipeline for the commonsense-based decision-making process which leads to more accurate local action prediction. Experimental results demonstrate our proposed model noticeably outperforms the baseline and archives the state-of-the-art on the REVERIE benchmark.",
    "pdf_url": "https://arxiv.org/pdf/2406.01256v1",
    "github_url": null,
    "published": "2024-06-03T12:12:33+00:00",
    "updated": "2024-06-03T12:12:33+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.00968v1",
    "title": "Evaluating MEDIRL: A Replication and Ablation Study of Maximum Entropy Deep Inverse Reinforcement Learning for Human Social Navigation",
    "authors": [
      "Gupta",
      "Gunukula"
    ],
    "summary": "In this study, we enhance the Maximum Entropy Deep Inverse Reinforcement Learning (MEDIRL) framework, targeting its application in human robot interaction (HRI) for modeling pedestrian behavior in crowded environments. Our work is grounded in the pioneering research by Fahad, Chen, and Guo, and aims to elevate MEDIRL's efficacy in real world HRI settings. We replicated the original MEDIRL model and conducted detailed ablation studies, focusing on key model components like learning rates, state dimensions, and network layers. Our findings reveal the effectiveness of a two dimensional state representation over three dimensional approach, significantly improving model accuracy for pedestrian behavior prediction in HRI scenarios. These results not only demonstrate MEDIRL's enhanced performance but also offer valuable insights for future HRI system development, emphasizing the importance of model customization to specific environmental contexts. Our research contributes to advancing the field of socially intelligent navigation systems, promoting more intuitive and safer human robot interactions.",
    "pdf_url": "https://arxiv.org/pdf/2406.00968v1",
    "github_url": null,
    "published": "2024-06-03T03:45:18+00:00",
    "updated": "2024-06-03T03:45:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.00837v1",
    "title": "Arena 3.0: Advancing Social Navigation in Collaborative and Highly Dynamic Environments",
    "authors": [
      "Kästner",
      "Shcherbyna",
      "Zeng"
    ],
    "summary": "Building upon our previous contributions, this paper introduces Arena 3.0, an extension of Arena-Bench, Arena 1.0, and Arena 2.0. Arena 3.0 is a comprehensive software stack containing multiple modules and simulation environments focusing on the development, simulation, and benchmarking of social navigation approaches in collaborative environments. We significantly enhance the realism of human behavior simulation by incorporating a diverse array of new social force models and interaction patterns, encompassing both human-human and human-robot dynamics. The platform provides a comprehensive set of new task modes, designed for extensive benchmarking and testing and is capable of generating realistic and human-centric environments dynamically, catering to a broad spectrum of social navigation scenarios. In addition, the platform's functionalities have been abstracted across three widely used simulators, each tailored for specific training and testing purposes. The platform's efficacy has been validated through an extensive benchmark and user evaluations of the platform by a global community of researchers and students, which noted the substantial improvement compared to previous versions and expressed interests to utilize the platform for future research and development. Arena 3.0 is openly available at https://github.com/Arena-Rosnav.",
    "pdf_url": "https://arxiv.org/pdf/2406.00837v1",
    "github_url": null,
    "published": "2024-06-02T19:05:24+00:00",
    "updated": "2024-06-02T19:05:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2406.00375v1",
    "title": "Teledrive: An Embodied AI based Telepresence System",
    "authors": [
      "Banerjee",
      "Paul",
      "Roychoudhury"
    ],
    "summary": "This article presents Teledrive, a telepresence robotic system with embodied AI features that empowers an operator to navigate the telerobot in any unknown remote place with minimal human intervention. We conceive Teledrive in the context of democratizing remote care-giving for elderly citizens as well as for isolated patients, affected by contagious diseases. In particular, this paper focuses on the problem of navigating to a rough target area (like bedroom or kitchen) rather than pre-specified point destinations. This ushers in a unique AreaGoal based navigation feature, which has not been explored in depth in the contemporary solutions. Further, we describe an edge computing-based software system built on a WebRTC-based communication framework to realize the aforementioned scheme through an easy-to-use speech-based human-robot interaction. Moreover, to enhance the ease of operation for the remote caregiver, we incorporate a person following feature, whereby a robot follows a person on the move in its premises as directed by the operator. Moreover, the system presented is loosely coupled with specific robot hardware, unlike the existing solutions. We have evaluated the efficacy of the proposed system through baseline experiments, user study, and real-life deployment.",
    "pdf_url": "https://arxiv.org/pdf/2406.00375v1",
    "github_url": null,
    "published": "2024-06-01T09:27:42+00:00",
    "updated": "2024-06-01T09:27:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.18937v2",
    "title": "Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description",
    "authors": [
      "Ahmed",
      "Fei",
      "Ding"
    ],
    "summary": "In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a challenging task aimed at advancing 3D multimodal learning for fine-grained, part-aware segmentation grounding and detailed explanation of 3D objects. Existing 3D datasets largely focus on either vision-only part segmentation or vision-language scene segmentation, lacking the fine-grained multimodal segmentation needed for robotic navigation and interaction in real-world environments. To address this gap, we present the 3DCoMPaT Grounded Instructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich point cloud descriptions with corresponding part-level segmentation masks. This dataset encompasses extensive samples designed for both PaPGD and fine-grained single-part grounding tasks. To tackle the inherent challenges of grounding objects and generating grounded descriptions at the part level, we propose Kestrel, a part-aware 3D multimodal large language model that integrates an advanced language model for nuanced language comprehension with multi-level point feature propagation and query refinement mechanism to enhance spatial reasoning at the part level. The extensive experiments demonstrate that Kestrel effectively bridges the gap between part-aware language understanding and 3D segmentation grounding, paving the way for more robust and interpretable 3D object comprehension that meets the demands of real-world robotic applications. Project page at https://feielysia.github.io/Kestrel.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2405.18937v2",
    "github_url": null,
    "published": "2024-05-29T09:43:48+00:00",
    "updated": "2025-08-04T13:54:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.18721v2",
    "title": "Correctable Landmark Discovery via Large Models for Vision-Language Navigation",
    "authors": [
      "Lin",
      "Nie",
      "Wei"
    ],
    "summary": "Vision-Language Navigation (VLN) requires the agent to follow language instructions to reach a target position. A key factor for successful navigation is to align the landmarks implied in the instruction with diverse visual observations. However, previous VLN agents fail to perform accurate modality alignment especially in unexplored scenes, since they learn from limited navigation data and lack sufficient open-world alignment knowledge. In this work, we propose a new VLN paradigm, called COrrectable LaNdmark DiScOvery via Large ModEls (CONSOLE). In CONSOLE, we cast VLN as an open-world sequential landmark discovery problem, by introducing a novel correctable landmark discovery scheme based on two large models ChatGPT and CLIP. Specifically, we use ChatGPT to provide rich open-world landmark cooccurrence commonsense, and conduct CLIP-driven landmark discovery based on these commonsense priors. To mitigate the noise in the priors due to the lack of visual constraints, we introduce a learnable cooccurrence scoring module, which corrects the importance of each cooccurrence according to actual observations for accurate landmark discovery. We further design an observation enhancement strategy for an elegant combination of our framework with different VLN agents, where we utilize the corrected landmark features to obtain enhanced observation features for action decision. Extensive experimental results on multiple popular VLN benchmarks (R2R, REVERIE, R4R, RxR) show the significant superiority of CONSOLE over strong baselines. Especially, our CONSOLE establishes the new state-of-the-art results on R2R and R4R in unseen scenarios. Code is available at https://github.com/expectorlin/CONSOLE.",
    "pdf_url": "https://arxiv.org/pdf/2405.18721v2",
    "github_url": "https://github.com/expectorlin/CONSOLE",
    "published": "2024-05-29T03:05:59+00:00",
    "updated": "2024-06-05T09:59:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.16994v1",
    "title": "Vision-and-Language Navigation Generative Pretrained Transformer",
    "authors": [
      "Hanlin"
    ],
    "summary": "In the Vision-and-Language Navigation (VLN) field, agents are tasked with navigating real-world scenes guided by linguistic instructions. Enabling the agent to adhere to instructions throughout the process of navigation represents a significant challenge within the domain of VLN. To address this challenge, common approaches often rely on encoders to explicitly record past locations and actions, increasing model complexity and resource consumption. Our proposal, the Vision-and-Language Navigation Generative Pretrained Transformer (VLN-GPT), adopts a transformer decoder model (GPT2) to model trajectory sequence dependencies, bypassing the need for historical encoding modules. This method allows for direct historical information access through trajectory sequence, enhancing efficiency. Furthermore, our model separates the training process into offline pre-training with imitation learning and online fine-tuning with reinforcement learning. This distinction allows for more focused training objectives and improved performance. Performance assessments on the VLN dataset reveal that VLN-GPT surpasses complex state-of-the-art encoder-based models.",
    "pdf_url": "https://arxiv.org/pdf/2405.16994v1",
    "github_url": null,
    "published": "2024-05-27T09:42:04+00:00",
    "updated": "2024-05-27T09:42:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.16818v1",
    "title": "Advancing Behavior Generation in Mobile Robotics through High-Fidelity Procedural Simulations",
    "authors": [
      "Kich",
      "Bottega",
      "Steinmetz"
    ],
    "summary": "This paper introduces YamaS, a simulator integrating Unity3D Engine with Robotic Operating System for robot navigation research and aims to facilitate the development of both Deep Reinforcement Learning (Deep-RL) and Natural Language Processing (NLP). It supports single and multi-agent configurations with features like procedural environment generation, RGB vision, and dynamic obstacle navigation. Unique to YamaS is its ability to construct single and multi-agent environments, as well as generating agent's behaviour through textual descriptions. The simulator's fidelity is underscored by comparisons with the real-world Yamabiko Beego robot, demonstrating high accuracy in sensor simulations and spatial reasoning. Moreover, YamaS integrates Virtual Reality (VR) to augment Human-Robot Interaction (HRI) studies, providing an immersive platform for developers and researchers. This fusion establishes YamaS as a versatile and valuable tool for the development and testing of autonomous systems, contributing to the fields of robot simulation and AI-driven training methodologies.",
    "pdf_url": "https://arxiv.org/pdf/2405.16818v1",
    "github_url": null,
    "published": "2024-05-27T04:31:55+00:00",
    "updated": "2024-05-27T04:31:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.16439v3",
    "title": "Multi-Agent Inverse Reinforcement Learning in Real World Unstructured Pedestrian Crowds",
    "authors": [
      "Chandra",
      "Karnan",
      "Mehr"
    ],
    "summary": "Social robot navigation in crowded public spaces such as university campuses, restaurants, grocery stores, and hospitals, is an increasingly important area of research. One of the core strategies for achieving this goal is to understand humans' intent--underlying psychological factors that govern their motion--by learning their reward functions, typically via inverse reinforcement learning (IRL). Despite significant progress in IRL, learning reward functions of multiple agents simultaneously in dense unstructured pedestrian crowds has remained intractable due to the nature of the tightly coupled social interactions that occur in these scenarios \\textit{e.g.} passing, intersections, swerving, weaving, etc. In this paper, we present a new multi-agent maximum entropy inverse reinforcement learning algorithm for real world unstructured pedestrian crowds. Key to our approach is a simple, but effective, mathematical trick which we name the so-called tractability-rationality trade-off trick that achieves tractability at the cost of a slight reduction in accuracy. We compare our approach to the classical single-agent MaxEnt IRL as well as state-of-the-art trajectory prediction methods on several datasets including the ETH, UCY, SCAND, JRDB, and a new dataset, called Speedway, collected at a busy intersection on a University campus focusing on dense, complex agent interactions. Our key findings show that, on the dense Speedway dataset, our approach ranks 1st among top 7 baselines with >2X improvement over single-agent IRL, and is competitive with state-of-the-art large transformer-based encoder-decoder models on sparser datasets such as ETH/UCY (ranks 3rd among top 7 baselines).",
    "pdf_url": "https://arxiv.org/pdf/2405.16439v3",
    "github_url": null,
    "published": "2024-05-26T05:48:21+00:00",
    "updated": "2025-03-26T21:19:58+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.15222v2",
    "title": "Leveraging Unknown Objects to Construct Labeled-Unlabeled Meta-Relationships for Zero-Shot Object Navigation",
    "authors": [
      "Zheng",
      "Li",
      "Lan"
    ],
    "summary": "Zero-shot object navigation (ZSON) addresses situation where an agent navigates to an unseen object that does not present in the training set. Previous works mainly train agent using seen objects with known labels, and ignore the seen objects without labels. In this paper, we introduce seen objects without labels, herein termed as ``unknown objects'', into training procedure to enrich the agent's knowledge base with distinguishable but previously overlooked information. Furthermore, we propose the label-wise meta-correlation module (LWMCM) to harness relationships among objects with and without labels, and obtain enhanced objects information. Specially, we propose target feature generator (TFG) to generate the features representation of the unlabeled target objects. Subsequently, the unlabeled object identifier (UOI) module assesses whether the unlabeled target object appears in the current observation frame captured by the camera and produces an adapted target features representation specific to the observed context. In meta contrastive feature modifier (MCFM), the target features is modified via approaching the features of objects within the observation frame while distancing itself from features of unobserved objects. Finally, the meta object-graph learner (MOGL) module is utilized to calculate the relationships among objects based on the features. Experiments conducted on AI2THOR and RoboTHOR platforms demonstrate the effectiveness of our proposed method.",
    "pdf_url": "https://arxiv.org/pdf/2405.15222v2",
    "github_url": null,
    "published": "2024-05-24T05:26:18+00:00",
    "updated": "2024-05-27T02:39:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.14154v4",
    "title": "Skip-SCAR: Hardware-Friendly High-Quality Embodied Visual Navigation",
    "authors": [
      "Liu",
      "Cao",
      "Zhang"
    ],
    "summary": "In ObjectNav, agents must locate specific objects within unseen environments, requiring effective perception, prediction, localization and planning capabilities. This study finds that state-of-the-art embodied AI agents compete for higher navigation quality, but often compromise the computational efficiency. To address this issue, we introduce \"Skip-SCAR,\" an optimization framework that builds computationally and memory-efficient embodied AI agents to accomplish high-quality visual navigation tasks. Skip-SCAR opportunistically skips the redundant step computations during semantic segmentation and local re-planning without hurting the navigation quality. Skip-SCAR also adopts a novel hybrid sparse and dense network for object prediction, optimizing both the computation and memory footprint. Tested on the HM3D ObjectNav datasets and real-world physical hardware systems, Skip-SCAR not only minimizes hardware resources but also sets new performance benchmarks, demonstrating the benefits of optimizing both navigation quality and computational efficiency for robotics.",
    "pdf_url": "https://arxiv.org/pdf/2405.14154v4",
    "github_url": null,
    "published": "2024-05-23T04:03:39+00:00",
    "updated": "2024-12-07T05:10:19+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.11410v2",
    "title": "Characterizing the Complexity of Social Robot Navigation Scenarios",
    "authors": [
      "Stratton",
      "Hauser",
      "Mavrogiannis"
    ],
    "summary": "Social robot navigation algorithms are often demonstrated in overly simplified scenarios, prohibiting the extraction of practical insights about their relevance to real-world domains. Our key insight is that an understanding of the inherent complexity of a social robot navigation scenario could help characterize the limitations of existing navigation algorithms and provide actionable directions for improvement. Through an exploration of recent literature, we identify a series of factors contributing to the complexity of a scenario, disambiguating between contextual and robot-related ones. We then conduct a simulation study investigating how manipulations of contextual factors impact the performance of a variety of navigation algorithms. We find that dense and narrow environments correlate most strongly with performance drops, while the heterogeneity of agent policies and directionality of interactions have a less pronounced effect. Our findings motivate a shift towards developing and testing algorithms under higher-complexity settings.",
    "pdf_url": "https://arxiv.org/pdf/2405.11410v2",
    "github_url": null,
    "published": "2024-05-18T23:03:22+00:00",
    "updated": "2024-12-10T16:08:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.10620v2",
    "title": "MC-GPT: Empowering Vision-and-Language Navigation with Memory Map and Reasoning Chains",
    "authors": [
      "Zhan",
      "Yu",
      "Yu"
    ],
    "summary": "In the Vision-and-Language Navigation (VLN) task, the agent is required to navigate to a destination following a natural language instruction. While learning-based approaches have been a major solution to the task, they suffer from high training costs and lack of interpretability. Recently, Large Language Models (LLMs) have emerged as a promising tool for VLN due to their strong generalization capabilities. However, existing LLM-based methods face limitations in memory construction and diversity of navigation strategies. To address these challenges, we propose a suite of techniques. Firstly, we introduce a method to maintain a topological map that stores navigation history, retaining information about viewpoints, objects, and their spatial relationships. This map also serves as a global action space. Additionally, we present a Navigation Chain of Thoughts module, leveraging human navigation examples to enrich navigation strategy diversity. Finally, we establish a pipeline that integrates navigational memory and strategies with perception and action prediction modules. Experimental results on the REVERIE and R2R datasets show that our method effectively enhances the navigation ability of the LLM and improves the interpretability of navigation reasoning.",
    "pdf_url": "https://arxiv.org/pdf/2405.10620v2",
    "github_url": null,
    "published": "2024-05-17T08:33:27+00:00",
    "updated": "2024-08-12T14:07:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.07060v1",
    "title": "Memory-Maze: Scenario Driven Benchmark and Visual Language Navigation Model for Guiding Blind People",
    "authors": [
      "Kuribayashi",
      "Uehara",
      "Wang"
    ],
    "summary": "Visual Language Navigation (VLN) powered navigation robots have the potential to guide blind people by understanding and executing route instructions provided by sighted passersby. This capability allows robots to operate in environments that are often unknown a priori. Existing VLN models are insufficient for the scenario of navigation guidance for blind people, as they need to understand routes described from human memory, which frequently contain stutters, errors, and omission of details as opposed to those obtained by thinking out loud, such as in the Room-to-Room dataset. However, currently, there is no benchmark that simulates instructions that were obtained from human memory in environments where blind people navigate. To this end, we present our benchmark, Memory-Maze, which simulates the scenario of seeking route instructions for guiding blind people. Our benchmark contains a maze-like structured virtual environment and novel route instruction data from human memory. To collect natural language instructions, we conducted two studies from sighted passersby onsite and annotators online. Our analysis demonstrates that instructions data collected onsite were more lengthy and contained more varied wording. Alongside our benchmark, we propose a VLN model better equipped to handle the scenario. Our proposed VLN model uses Large Language Models (LLM) to parse instructions and generate Python codes for robot control. We further show that the existing state-of-the-art model performed suboptimally on our benchmark. In contrast, our proposed method outperformed the state-of-the-art model by a fair margin. We found that future research should exercise caution when considering VLN technology for practical applications, as real-world scenarios have different characteristics than ones collected in traditional settings.",
    "pdf_url": "https://arxiv.org/pdf/2405.07060v1",
    "github_url": null,
    "published": "2024-05-11T17:25:23+00:00",
    "updated": "2024-05-11T17:25:23+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.05852v1",
    "title": "Pre-trained Text-to-Image Diffusion Models Are Versatile Representation Learners for Control",
    "authors": [
      "Gupta",
      "Yadav",
      "Gal"
    ],
    "summary": "Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark.",
    "pdf_url": "https://arxiv.org/pdf/2405.05852v1",
    "github_url": null,
    "published": "2024-05-09T15:39:54+00:00",
    "updated": "2024-05-09T15:39:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.05363v1",
    "title": "LOC-ZSON: Language-driven Object-Centric Zero-Shot Object Retrieval and Navigation",
    "authors": [
      "Guan",
      "Yang",
      "Cheng"
    ],
    "summary": "In this paper, we present LOC-ZSON, a novel Language-driven Object-Centric image representation for object navigation task within complex scenes. We propose an object-centric image representation and corresponding losses for visual-language model (VLM) fine-tuning, which can handle complex object-level queries. In addition, we design a novel LLM-based augmentation and prompt templates for stability during training and zero-shot inference. We implement our method on Astro robot and deploy it in both simulated and real-world environments for zero-shot object navigation. We show that our proposed method can achieve an improvement of 1.38 - 13.38% in terms of text-to-image recall on different benchmark settings for the retrieval task. For object navigation, we show the benefit of our approach in simulation and real world, showing 5% and 16.67% improvement in terms of navigation success rate, respectively.",
    "pdf_url": "https://arxiv.org/pdf/2405.05363v1",
    "github_url": null,
    "published": "2024-05-08T18:45:37+00:00",
    "updated": "2024-05-08T18:45:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.04235v2",
    "title": "LTLDoG: Satisfying Temporally-Extended Symbolic Constraints for Safe Diffusion-based Planning",
    "authors": [
      "Feng",
      "Luan",
      "Goyal"
    ],
    "summary": "Operating effectively in complex environments while complying with specified constraints is crucial for the safe and successful deployment of robots that interact with and operate around people. In this work, we focus on generating long-horizon trajectories that adhere to novel static and temporally-extended constraints/instructions at test time. We propose a data-driven diffusion-based framework, LTLDoG, that modifies the inference steps of the reverse process given an instruction specified using finite linear temporal logic ($\\text{LTL}_f$). LTLDoG leverages a satisfaction value function on $\\text{LTL}_f$ and guides the sampling steps using its gradient field. This value function can also be trained to generalize to new instructions not observed during training, enabling flexible test-time adaptability. Experiments in robot navigation and manipulation illustrate that the method is able to generate trajectories that satisfy formulae that specify obstacle avoidance and visitation sequences. Code and supplementary material are available online at https://github.com/clear-nus/ltldog.",
    "pdf_url": "https://arxiv.org/pdf/2405.04235v2",
    "github_url": "https://github.com/clear-nus/ltldog",
    "published": "2024-05-07T11:54:22+00:00",
    "updated": "2024-09-30T08:42:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2405.00073v1",
    "title": "Loyal Wingman Assessment: Social Navigation for Human-Autonomous Collaboration in Simulated Air Combat",
    "authors": [
      "Dantas",
      "Maximo",
      "Yoneyama"
    ],
    "summary": "This study proposes social navigation metrics for autonomous agents in air combat, aiming to facilitate their smooth integration into pilot formations. The absence of such metrics poses challenges to safety and effectiveness in mixed human-autonomous teams. The proposed metrics prioritize naturalness and comfort. We suggest validating them through a user study involving military pilots in simulated air combat scenarios alongside autonomous loyal wingmen. The experiment will involve setting up simulations, designing scenarios, and evaluating performance using feedback from questionnaires and data analysis. These metrics aim to enhance the operational performance of autonomous loyal wingmen, thereby contributing to safer and more strategic air combat.",
    "pdf_url": "https://arxiv.org/pdf/2405.00073v1",
    "github_url": null,
    "published": "2024-04-30T02:07:15+00:00",
    "updated": "2024-04-30T02:07:15+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.16006v1",
    "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI",
    "authors": [
      "Ying",
      "Meng",
      "Wang"
    ],
    "summary": "Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $30$ LVLMs such as the proprietary GPT-4V, GeminiProVision, and open-sourced InternVL-Chat, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.",
    "pdf_url": "https://arxiv.org/pdf/2404.16006v1",
    "github_url": null,
    "published": "2024-04-24T17:37:05+00:00",
    "updated": "2024-04-24T17:37:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.13678v1",
    "title": "Adaptive Social Force Window Planner with Reinforcement Learning",
    "authors": [
      "Martini",
      "Pérez-Higueras",
      "Ostuni"
    ],
    "summary": "Human-aware navigation is a complex task for mobile robots, requiring an autonomous navigation system capable of achieving efficient path planning together with socially compliant behaviors. Social planners usually add costs or constraints to the objective function, leading to intricate tuning processes or tailoring the solution to the specific social scenario. Machine Learning can enhance planners' versatility and help them learn complex social behaviors from data. This work proposes an adaptive social planner, using a Deep Reinforcement Learning agent to dynamically adjust the weighting parameters of the cost function used to evaluate trajectories. The resulting planner combines the robustness of the classic Dynamic Window Approach, integrated with a social cost based on the Social Force Model, and the flexibility of learning methods to boost the overall performance on social navigation tasks. Our extensive experimentation on different environments demonstrates the general advantage of the proposed method over static cost planners.",
    "pdf_url": "https://arxiv.org/pdf/2404.13678v1",
    "github_url": null,
    "published": "2024-04-21T14:41:40+00:00",
    "updated": "2024-04-21T14:41:40+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.11327v2",
    "title": "Following the Human Thread in Social Navigation",
    "authors": [
      "Scofano",
      "Sampieri",
      "Campari"
    ],
    "summary": "The success of collaboration between humans and robots in shared environments relies on the robot's real-time adaptation to human motion. Specifically, in Social Navigation, the agent should be close enough to assist but ready to back up to let the human move freely, avoiding collisions. Human trajectories emerge as crucial cues in Social Navigation, but they are partially observable from the robot's egocentric view and computationally complex to process.   We present the first Social Dynamics Adaptation model (SDA) based on the robot's state-action history to infer the social dynamics. We propose a two-stage Reinforcement Learning framework: the first learns to encode the human trajectories into social dynamics and learns a motion policy conditioned on this encoded information, the current status, and the previous action. Here, the trajectories are fully visible, i.e., assumed as privileged information. In the second stage, the trained policy operates without direct access to trajectories. Instead, the model infers the social dynamics solely from the history of previous actions and statuses in real-time. Tested on the novel Habitat 3.0 platform, SDA sets a novel state-of-the-art (SotA) performance in finding and following humans.   The code can be found at https://github.com/L-Scofano/SDA.",
    "pdf_url": "https://arxiv.org/pdf/2404.11327v2",
    "github_url": "https://github.com/L-Scofano/SDA",
    "published": "2024-04-17T12:39:48+00:00",
    "updated": "2025-02-25T10:43:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.11246v1",
    "title": "Learning Social Navigation from Demonstrations with Deep Neural Networks",
    "authors": [
      "Yildirim",
      "Ugur"
    ],
    "summary": "Traditional path-planning techniques treat humans as obstacles. This has changed since robots started to enter human environments. On modern robots, social navigation has become an important aspect of navigation systems. To use learning-based techniques to achieve social navigation, a powerful framework that is capable of representing complex functions with as few data as possible is required. In this study, we benefited from recent advances in deep learning at both global and local planning levels to achieve human-aware navigation on a simulated robot. Two distinct deep models are trained with respective objectives: one for global planning and one for local planning. These models are then employed in the simulated robot. In the end, it has been shown that our model can successfully carry out both global and local planning tasks. We have shown that our system could generate paths that successfully reach targets while avoiding obstacles with better performance compared to feed-forward neural networks.",
    "pdf_url": "https://arxiv.org/pdf/2404.11246v1",
    "github_url": null,
    "published": "2024-04-17T10:51:36+00:00",
    "updated": "2024-04-17T10:51:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.10241v1",
    "title": "Vision-and-Language Navigation via Causal Learning",
    "authors": [
      "Wang",
      "He",
      "Dang"
    ],
    "summary": "In the pursuit of robust and generalizable environment perception and language understanding, the ubiquitous challenge of dataset bias continues to plague vision-and-language navigation (VLN) agents, hindering their performance in unseen environments. This paper introduces the generalized cross-modal causal transformer (GOAT), a pioneering solution rooted in the paradigm of causal inference. By delving into both observable and unobservable confounders within vision, language, and history, we propose the back-door and front-door adjustment causal learning (BACL and FACL) modules to promote unbiased learning by comprehensively mitigating potential spurious correlations. Additionally, to capture global confounder features, we propose a cross-modal feature pooling (CFP) module supervised by contrastive learning, which is also shown to be effective in improving cross-modal representations during pre-training. Extensive experiments across multiple VLN datasets (R2R, REVERIE, RxR, and SOON) underscore the superiority of our proposed method over previous state-of-the-art approaches. Code is available at https://github.com/CrystalSixone/VLN-GOAT.",
    "pdf_url": "https://arxiv.org/pdf/2404.10241v1",
    "github_url": "https://github.com/CrystalSixone/VLN-GOAT",
    "published": "2024-04-16T02:40:35+00:00",
    "updated": "2024-04-16T02:40:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.10220v2",
    "title": "Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V",
    "authors": [
      "Zhi",
      "Zhang",
      "Zhao"
    ],
    "summary": "Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. In this work, we present COME-robot, the first closed-loop robotic system utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios.COME-robot incorporates two key innovative modules: (i) a multi-level open-vocabulary perception and situated reasoning module that enables effective exploration of the 3D environment and target object identification using commonsense knowledge and situated information, and (ii) an iterative closed-loop feedback and restoration mechanism that verifies task feasibility, monitors execution success, and traces failure causes across different modules for robust failure recovery. Through comprehensive experiments involving 8 challenging real-world mobile and tabletop manipulation tasks, COME-robot demonstrates a significant improvement in task success rate (~35%) compared to state-of-the-art methods. We further conduct comprehensive analyses to elucidate how COME-robot's design facilitates failure recovery, free-form instruction following, and long-horizon task planning.",
    "pdf_url": "https://arxiv.org/pdf/2404.10220v2",
    "github_url": null,
    "published": "2024-04-16T02:01:56+00:00",
    "updated": "2025-03-07T05:09:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.10054v1",
    "title": "AIGeN: An Adversarial Approach for Instruction Generation in VLN",
    "authors": [
      "Rawal",
      "Bigazzi",
      "Baraldi"
    ],
    "summary": "In the last few years, the research interest in Vision-and-Language Navigation (VLN) has grown significantly. VLN is a challenging task that involves an agent following human instructions and navigating in a previously unknown environment to reach a specified goal. Recent work in literature focuses on different ways to augment the available datasets of instructions for improving navigation performance by exploiting synthetic training data. In this work, we propose AIGeN, a novel architecture inspired by Generative Adversarial Networks (GANs) that produces meaningful and well-formed synthetic instructions to improve navigation agents' performance. The model is composed of a Transformer decoder (GPT-2) and a Transformer encoder (BERT). During the training phase, the decoder generates sentences for a sequence of images describing the agent's path to a particular point while the encoder discriminates between real and fake instructions. Experimentally, we evaluate the quality of the generated instructions and perform extensive ablation studies. Additionally, we generate synthetic instructions for 217K trajectories using AIGeN on Habitat-Matterport 3D Dataset (HM3D) and show an improvement in the performance of an off-the-shelf VLN method. The validation analysis of our proposal is conducted on REVERIE and R2R and highlights the promising aspects of our proposal, achieving state-of-the-art performance.",
    "pdf_url": "https://arxiv.org/pdf/2404.10054v1",
    "github_url": null,
    "published": "2024-04-15T18:00:30+00:00",
    "updated": "2024-04-15T18:00:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.08353v2",
    "title": "TDANet: Target-Directed Attention Network For Object-Goal Visual Navigation With Zero-Shot Ability",
    "authors": [
      "Lian",
      "Zhang"
    ],
    "summary": "The generalization of the end-to-end deep reinforcement learning (DRL) for object-goal visual navigation is a long-standing challenge since object classes and placements vary in new test environments. Learning domain-independent visual representation is critical for enabling the trained DRL agent with the ability to generalize to unseen scenes and objects. In this letter, a target-directed attention network (TDANet) is proposed to learn the end-to-end object-goal visual navigation policy with zero-shot ability. TDANet features a novel target attention (TA) module that learns both the spatial and semantic relationships among objects to help TDANet focus on the most relevant observed objects to the target. With the Siamese architecture (SA) design, TDANet distinguishes the difference between the current and target states and generates the domain-independent visual representation. To evaluate the navigation performance of TDANet, extensive experiments are conducted in the AI2-THOR embodied AI environment. The simulation results demonstrate a strong generalization ability of TDANet to unseen scenes and target objects, with higher navigation success rate (SR) and success weighted by length (SPL) than other state-of-the-art models. TDANet is finally deployed on a wheeled robot in real scenes, demonstrating satisfactory generalization of TDANet to the real world.",
    "pdf_url": "https://arxiv.org/pdf/2404.08353v2",
    "github_url": null,
    "published": "2024-04-12T09:44:18+00:00",
    "updated": "2024-08-12T07:20:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.07447v1",
    "title": "Interactive-FAR:Interactive, Fast and Adaptable Routing for Navigation Among Movable Obstacles in Complex Unknown Environments",
    "authors": [
      "He",
      "Chen",
      "Wang"
    ],
    "summary": "This paper introduces a real-time algorithm for navigating complex unknown environments cluttered with movable obstacles. Our algorithm achieves fast, adaptable routing by actively attempting to manipulate obstacles during path planning and adjusting the global plan from sensor feedback. The main contributions include an improved dynamic Directed Visibility Graph (DV-graph) for rapid global path searching, a real-time interaction planning method that adapts online from new sensory perceptions, and a comprehensive framework designed for interactive navigation in complex unknown or partially known environments. Our algorithm is capable of replanning the global path in several milliseconds. It can also attempt to move obstacles, update their affordances, and adapt strategies accordingly. Extensive experiments validate that our algorithm reduces the travel time by 33%, achieves up to 49% higher path efficiency, and runs faster than traditional methods by orders of magnitude in complex environments. It has been demonstrated to be the most efficient solution in terms of speed and efficiency for interactive navigation in environments of such complexity. We also open-source our code in the docker demo to facilitate future research.",
    "pdf_url": "https://arxiv.org/pdf/2404.07447v1",
    "github_url": null,
    "published": "2024-04-11T03:04:59+00:00",
    "updated": "2024-04-11T03:04:59+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.06609v1",
    "title": "GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation",
    "authors": [
      "Khanna",
      "Ramrakhya",
      "Chhablani"
    ],
    "summary": "The Embodied AI community has made significant strides in visual navigation tasks, exploring targets from 3D coordinates, objects, language descriptions, and images. However, these navigation models often handle only a single input modality as the target. With the progress achieved so far, it is time to move towards universal navigation models capable of handling various goal types, enabling more effective user interaction with robots. To facilitate this goal, we propose GOAT-Bench, a benchmark for the universal navigation task referred to as GO to AnyThing (GOAT). In this task, the agent is directed to navigate to a sequence of targets specified by the category name, language description, or image in an open-vocabulary fashion. We benchmark monolithic RL and modular methods on the GOAT task, analyzing their performance across modalities, the role of explicit and implicit scene memories, their robustness to noise in goal specifications, and the impact of memory in lifelong scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2404.06609v1",
    "github_url": null,
    "published": "2024-04-09T20:40:00+00:00",
    "updated": "2024-04-09T20:40:00+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.06591v2",
    "title": "Milgram's experiment in the knowledge space: Individual navigation strategies",
    "authors": [
      "Zhu",
      "Kertész"
    ],
    "summary": "Data deluge characteristic for our times has led to information overload, posing a significant challenge to effectively finding our way through the digital landscape. Addressing this issue requires an in-depth understanding of how we navigate through the abundance of information. Previous research has discovered multiple patterns in how individuals navigate in the geographic, social, and information spaces, yet individual differences in strategies for navigation in the knowledge space has remained largely unexplored. To bridge the gap, we conducted an online experiment where participants played a navigation game on Wikipedia and completed questionnaires about their personal information. Utilizing the hierarchical structure of the English Wikipedia and a graph embedding trained on it, we identified two navigation strategies and found that there are significant individual differences in the choices of them. Older, white and female participants tend to adopt a proximity-driven strategy, while younger participants prefer a hub-driven strategy. Our study connects social navigation to knowledge navigation: individuals' differing tendencies to use geographical and occupational information about the target person to navigate in the social space can be understood as different choices between the hub-driven and proximity-driven strategies in the knowledge space.",
    "pdf_url": "https://arxiv.org/pdf/2404.06591v2",
    "github_url": null,
    "published": "2024-04-09T19:39:27+00:00",
    "updated": "2025-06-11T15:02:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.05203v1",
    "title": "MeSA-DRL: Memory-Enhanced Deep Reinforcement Learning for Advanced Socially Aware Robot Navigation in Crowded Environments",
    "authors": [
      "Muhammad",
      "Montero"
    ],
    "summary": "Autonomous navigation capabilities play a critical role in service robots operating in environments where human interactions are pivotal, due to the dynamic and unpredictable nature of these environments. However, the variability in human behavior presents a substantial challenge for robots in predicting and anticipating movements, particularly in crowded scenarios. To address this issue, a memory-enabled deep reinforcement learning framework is proposed for autonomous robot navigation in diverse pedestrian scenarios. The proposed framework leverages long-term memory to retain essential information about the surroundings and model sequential dependencies effectively. The importance of human-robot interactions is also encoded to assign higher attention to these interactions. A global planning mechanism is incorporated into the memory-enabled architecture. Additionally, a multi-term reward system is designed to prioritize and encourage long-sighted robot behaviors by incorporating dynamic warning zones. Simultaneously, it promotes smooth trajectories and minimizes the time taken to reach the robot's desired goal. Extensive simulation experiments show that the suggested approach outperforms representative state-of-the-art methods, showcasing its ability to a navigation efficiency and safety in real-world scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2404.05203v1",
    "github_url": null,
    "published": "2024-04-08T05:10:35+00:00",
    "updated": "2024-04-08T05:10:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.04069v2",
    "title": "Bidirectional Human Interactive AI Framework for Social Robot Navigation",
    "authors": [
      "Girgin",
      "Girgin",
      "Yildirim"
    ],
    "summary": "Trustworthiness is a crucial concept in the context of human-robot interaction. Cooperative robots must be transparent regarding their decision-making process, especially when operating in a human-oriented environment. This paper presents a comprehensive end-to-end framework aimed at fostering trustworthy bidirectional human-robot interaction in collaborative environments for the social navigation of mobile robots. In this framework, the robot communicates verbally while the human guides with gestures. Our method enables a mobile robot to predict the trajectory of people and adjust its route in a socially-aware manner. In case of conflict between human and robot decisions, detected through visual examination, the route is dynamically modified based on human preference while verbal communication is maintained. We present our pipeline, framework design, and preliminary experiments that form the foundation of our proposition.",
    "pdf_url": "https://arxiv.org/pdf/2404.04069v2",
    "github_url": null,
    "published": "2024-04-05T12:52:17+00:00",
    "updated": "2024-05-04T14:14:21+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.03049v1",
    "title": "Language, Environment, and Robotic Navigation",
    "authors": [
      "Avery"
    ],
    "summary": "This paper explores the integration of linguistic inputs within robotic navigation systems, drawing upon the symbol interdependency hypothesis to bridge the divide between symbolic and embodied cognition. It examines previous work incorporating language and semantics into Neural Network (NN) and Simultaneous Localization and Mapping (SLAM) approaches, highlighting how these integrations have advanced the field. By contrasting abstract symbol manipulation with sensory-motor grounding, we propose a unified framework where language functions both as an abstract communicative system and as a grounded representation of perceptual experiences. Our review of cognitive models of distributional semantics and their application to autonomous agents underscores the transformative potential of language-integrated systems.",
    "pdf_url": "https://arxiv.org/pdf/2404.03049v1",
    "github_url": null,
    "published": "2024-04-03T20:30:38+00:00",
    "updated": "2024-04-03T20:30:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.02294v1",
    "title": "Constrained Robotic Navigation on Preferred Terrains Using LLMs and Speech Instruction: Exploiting the Power of Adverbs",
    "authors": [
      "Lotfi",
      "Faraji",
      "Kakodkar"
    ],
    "summary": "This paper explores leveraging large language models for map-free off-road navigation using generative AI, reducing the need for traditional data collection and annotation. We propose a method where a robot receives verbal instructions, converted to text through Whisper, and a large language model (LLM) model extracts landmarks, preferred terrains, and crucial adverbs translated into speed settings for constrained navigation. A language-driven semantic segmentation model generates text-based masks for identifying landmarks and terrain types in images. By translating 2D image points to the vehicle's motion plane using camera parameters, an MPC controller can guides the vehicle towards the desired terrain. This approach enhances adaptation to diverse environments and facilitates the use of high-level instructions for navigating complex and challenging terrains.",
    "pdf_url": "https://arxiv.org/pdf/2404.02294v1",
    "github_url": null,
    "published": "2024-04-02T20:46:13+00:00",
    "updated": "2024-04-02T20:46:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.01994v1",
    "title": "DELAN: Dual-Level Alignment for Vision-and-Language Navigation by Cross-Modal Contrastive Learning",
    "authors": [
      "Du",
      "Wu",
      "Zhang"
    ],
    "summary": "Vision-and-Language navigation (VLN) requires an agent to navigate in unseen environment by following natural language instruction. For task completion, the agent needs to align and integrate various navigation modalities, including instruction, observation and navigation history. Existing works primarily concentrate on cross-modal attention at the fusion stage to achieve this objective. Nevertheless, modality features generated by disparate uni-encoders reside in their own spaces, leading to a decline in the quality of cross-modal fusion and decision. To address this problem, we propose a Dual-levEL AligNment (DELAN) framework by cross-modal contrastive learning. This framework is designed to align various navigation-related modalities before fusion, thereby enhancing cross-modal interaction and action decision-making. Specifically, we divide the pre-fusion alignment into dual levels: instruction-history level and landmark-observation level according to their semantic correlations. We also reconstruct a dual-level instruction for adaptation to the dual-level alignment. As the training signals for pre-fusion alignment are extremely limited, self-supervised contrastive learning strategies are employed to enforce the matching between different modalities. Our approach seamlessly integrates with the majority of existing models, resulting in improved navigation performance on various VLN benchmarks, including R2R, R4R, RxR and CVDN.",
    "pdf_url": "https://arxiv.org/pdf/2404.01994v1",
    "github_url": null,
    "published": "2024-04-02T14:40:04+00:00",
    "updated": "2024-04-02T14:40:04+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.01943v1",
    "title": "Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation",
    "authors": [
      "Wang",
      "Li",
      "Yang"
    ],
    "summary": "Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. At each navigation step, the agent selects from possible candidate locations and then makes the move. For better navigation planning, the lookahead exploration strategy aims to effectively evaluate the agent's next action by accurately anticipating the future environment of candidate locations. To this end, some existing works predict RGB images for future environments, while this strategy suffers from image distortion and high computational cost. To address these issues, we propose the pre-trained hierarchical neural radiance representation model (HNR) to produce multi-level semantic features for future environments, which are more robust and efficient than pixel-wise RGB reconstruction. Furthermore, with the predicted future environmental representations, our lookahead VLN model is able to construct the navigable future path tree and select the optimal path via efficient parallel evaluation. Extensive experiments on the VLN-CE datasets confirm the effectiveness of our method.",
    "pdf_url": "https://arxiv.org/pdf/2404.01943v1",
    "github_url": null,
    "published": "2024-04-02T13:36:03+00:00",
    "updated": "2024-04-02T13:36:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.01686v1",
    "title": "JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments",
    "authors": [
      "Le",
      "Gou",
      "Datta"
    ],
    "summary": "Autonomous robot systems have attracted increasing research attention in recent years, where environment understanding is a crucial step for robot navigation, human-robot interaction, and decision. Real-world robot systems usually collect visual data from multiple sensors and are required to recognize numerous objects and their movements in complex human-crowded settings. Traditional benchmarks, with their reliance on single sensors and limited object classes and scenarios, fail to provide the comprehensive environmental understanding robots need for accurate navigation, interaction, and decision-making. As an extension of JRDB dataset, we unveil JRDB-PanoTrack, a novel open-world panoptic segmentation and tracking benchmark, towards more comprehensive environmental perception. JRDB-PanoTrack includes (1) various data involving indoor and outdoor crowded scenes, as well as comprehensive 2D and 3D synchronized data modalities; (2) high-quality 2D spatial panoptic segmentation and temporal tracking annotations, with additional 3D label projections for further spatial understanding; (3) diverse object classes for closed- and open-world recognition benchmarks, with OSPA-based metrics for evaluation. Extensive evaluation of leading methods shows significant challenges posed by our dataset.",
    "pdf_url": "https://arxiv.org/pdf/2404.01686v1",
    "github_url": null,
    "published": "2024-04-02T06:43:22+00:00",
    "updated": "2024-04-02T06:43:22+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.00354v2",
    "title": "Follow me: an architecture for user identification and social navigation with a mobile robot",
    "authors": [
      "Ruo",
      "Sabattini",
      "Villani"
    ],
    "summary": "Over the past decade, a multitude of service robots have been developed to fulfill a wide range of practical purposes. Notably, roles such as reception and robotic guidance have garnered extensive popularity. In these positions, robots are progressively assuming the responsibilities traditionally held by human staff in assisting customers. Ensuring the safe and socially acceptable operation of robots in such environments poses a fundamental challenge within the context of Socially Responsible Navigation (SRN). This article presents an architecture for user identification and social navigation with a mobile robot that employs computer vision, machine learning, and artificial intelligence algorithms to identify and guide users in a social navigation context, thereby providing an intuitive and user-friendly experience with the robot.",
    "pdf_url": "https://arxiv.org/pdf/2404.00354v2",
    "github_url": null,
    "published": "2024-03-30T13:10:04+00:00",
    "updated": "2024-05-02T07:42:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.00353v1",
    "title": "CBF-Based STL Motion Planning for Social Navigation in Crowded Environment",
    "authors": [
      "Ruo",
      "Sabattini",
      "Villani"
    ],
    "summary": "A motion planning methodology based on the combination of Control Barrier Functions (CBF) and Signal Temporal Logic (STL) is employed in this paper. This methodology allows task completion at any point within a specified time interval, considering a dynamic system subject to velocity constraints. In this work, we apply this approach into the context of Socially Responsible Navigation (SRN), introducing a rotation constraint. This constraint is designed to maintain the user within the robot's field of view (FOV), enhancing human-robot interaction with the concept of side-by-side human-robot companion. This angular constraint offers the possibility to customize social navigation to specific needs, thereby enabling safe SRN. Its validation is carried out through simulations demonstrating the system's effectiveness in adhering to spatio-temporal constraints, including those related to robot velocity, rotation, and the presence of static and dynamic obstacles.",
    "pdf_url": "https://arxiv.org/pdf/2404.00353v1",
    "github_url": null,
    "published": "2024-03-30T13:06:56+00:00",
    "updated": "2024-03-30T13:06:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2404.00210v3",
    "title": "VLM-Social-Nav: Socially Aware Robot Navigation through Scoring using Vision-Language Models",
    "authors": [
      "Song",
      "Liang",
      "Payandeh"
    ],
    "summary": "We propose VLM-Social-Nav, a novel Vision-Language Model (VLM) based navigation approach to compute a robot's motion in human-centered environments. Our goal is to make real-time decisions on robot actions that are socially compliant with human expectations. We utilize a perception model to detect important social entities and prompt a VLM to generate guidance for socially compliant robot behavior. VLM-Social-Nav uses a VLM-based scoring module that computes a cost term that ensures socially appropriate and effective robot actions generated by the underlying planner. Our overall approach reduces reliance on large training datasets and enhances adaptability in decision-making. In practice, it results in improved socially compliant navigation in human-shared environments. We demonstrate and evaluate our system in four different real-world social navigation scenarios with a Turtlebot robot. We observe at least 27.38% improvement in the average success rate and 19.05% improvement in the average collision rate in the four social navigation scenarios. Our user study score shows that VLM-Social-Nav generates the most socially compliant navigation behavior.",
    "pdf_url": "https://arxiv.org/pdf/2404.00210v3",
    "github_url": null,
    "published": "2024-03-30T01:17:40+00:00",
    "updated": "2024-11-25T21:05:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.19603v1",
    "title": "Semantic Map-based Generation of Navigation Instructions",
    "authors": [
      "Li",
      "Zhang",
      "Teufel"
    ],
    "summary": "We are interested in the generation of navigation instructions, either in their own right or as training material for robotic navigation task. In this paper, we propose a new approach to navigation instruction generation by framing the problem as an image captioning task using semantic maps as visual input. Conventional approaches employ a sequence of panorama images to generate navigation instructions. Semantic maps abstract away from visual details and fuse the information in multiple panorama images into a single top-down representation, thereby reducing computational complexity to process the input. We present a benchmark dataset for instruction generation using semantic maps, propose an initial model and ask human subjects to manually assess the quality of generated instructions. Our initial investigations show promise in using semantic maps for instruction generation instead of a sequence of panorama images, but there is vast scope for improvement. We release the code for data preparation and model training at https://github.com/chengzu-li/VLGen.",
    "pdf_url": "https://arxiv.org/pdf/2403.19603v1",
    "github_url": "https://github.com/chengzu-li/VLGen",
    "published": "2024-03-28T17:27:44+00:00",
    "updated": "2024-03-28T17:27:44+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.19336v1",
    "title": "IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation",
    "authors": [
      "Huang",
      "Zhang",
      "Zhao"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is a challenging task that requires a robot to navigate in photo-realistic environments with human natural language promptings. Recent studies aim to handle this task by constructing the semantic spatial map representation of the environment, and then leveraging the strong ability of reasoning in large language models for generalizing code for guiding the robot navigation. However, these methods face limitations in instance-level and attribute-level navigation tasks as they cannot distinguish different instances of the same object. To address this challenge, we propose a new method, namely, Instance-aware Visual Language Map (IVLMap), to empower the robot with instance-level and attribute-level semantic mapping, where it is autonomously constructed by fusing the RGBD video data collected from the robot agent with special-designed natural language map indexing in the bird's-in-eye view. Such indexing is instance-level and attribute-level. In particular, when integrated with a large language model, IVLMap demonstrates the capability to i) transform natural language into navigation targets with instance and attribute information, enabling precise localization, and ii) accomplish zero-shot end-to-end navigation tasks based on natural language commands. Extensive navigation experiments are conducted. Simulation results illustrate that our method can achieve an average improvement of 14.4\\% in navigation accuracy. Code and demo are released at https://ivlmap.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2403.19336v1",
    "github_url": null,
    "published": "2024-03-28T11:52:42+00:00",
    "updated": "2024-03-28T11:52:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.18778v1",
    "title": "3P-LLM: Probabilistic Path Planning using Large Language Model for Autonomous Robot Navigation",
    "authors": [
      "Latif"
    ],
    "summary": "Much worldly semantic knowledge can be encoded in large language models (LLMs). Such information could be of great use to robots that want to carry out high-level, temporally extended commands stated in natural language. However, the lack of real-world experience that language models have is a key limitation that makes it challenging to use them for decision-making inside a particular embodiment. This research assesses the feasibility of using LLM (GPT-3.5-turbo chatbot by OpenAI) for robotic path planning. The shortcomings of conventional approaches to managing complex environments and developing trustworthy plans for shifting environmental conditions serve as the driving force behind the research. Due to the sophisticated natural language processing abilities of LLM, the capacity to provide effective and adaptive path-planning algorithms in real-time, great accuracy, and few-shot learning capabilities, GPT-3.5-turbo is well suited for path planning in robotics. In numerous simulated scenarios, the research compares the performance of GPT-3.5-turbo with that of state-of-the-art path planners like Rapidly Exploring Random Tree (RRT) and A*. We observed that GPT-3.5-turbo is able to provide real-time path planning feedback to the robot and outperforms its counterparts. This paper establishes the foundation for LLM-powered path planning for robotic systems.",
    "pdf_url": "https://arxiv.org/pdf/2403.18778v1",
    "github_url": null,
    "published": "2024-03-27T17:26:42+00:00",
    "updated": "2024-03-27T17:26:42+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.18454v1",
    "title": "Scaling Vision-and-Language Navigation With Offline RL",
    "authors": [
      "Bundele",
      "Bhupati",
      "Banerjee"
    ],
    "summary": "The study of vision-and-language navigation (VLN) has typically relied on expert trajectories, which may not always be available in real-world situations due to the significant effort required to collect them. On the other hand, existing approaches to training VLN agents that go beyond available expert data involve data augmentations or online exploration which can be tedious and risky. In contrast, it is easy to access large repositories of suboptimal offline trajectories. Inspired by research in offline reinforcement learning (ORL), we introduce a new problem setup of VLN-ORL which studies VLN using suboptimal demonstration data. We introduce a simple and effective reward-conditioned approach that can account for dataset suboptimality for training VLN agents, as well as benchmarks to evaluate progress and promote research in this area. We empirically study various noise models for characterizing dataset suboptimality among other unique challenges in VLN-ORL and instantiate it for the VLN$\\circlearrowright$BERT and MTVM architectures in the R2R and RxR environments. Our experiments demonstrate that the proposed reward-conditioned approach leads to significant performance improvements, even in complex and intricate environments.",
    "pdf_url": "https://arxiv.org/pdf/2403.18454v1",
    "github_url": null,
    "published": "2024-03-27T11:13:20+00:00",
    "updated": "2024-03-27T11:13:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.18114v1",
    "title": "Segment Any Medical Model Extended",
    "authors": [
      "Liu",
      "Zhang",
      "Diaz-Pinto"
    ],
    "summary": "The Segment Anything Model (SAM) has drawn significant attention from researchers who work on medical image segmentation because of its generalizability. However, researchers have found that SAM may have limited performance on medical images compared to state-of-the-art non-foundation models. Regardless, the community sees potential in extending, fine-tuning, modifying, and evaluating SAM for analysis of medical imaging. An increasing number of works have been published focusing on the mentioned four directions, where variants of SAM are proposed. To this end, a unified platform helps push the boundary of the foundation model for medical images, facilitating the use, modification, and validation of SAM and its variants in medical image segmentation. In this work, we introduce SAMM Extended (SAMME), a platform that integrates new SAM variant models, adopts faster communication protocols, accommodates new interactive modes, and allows for fine-tuning of subcomponents of the models. These features can expand the potential of foundation models like SAM, and the results can be translated to applications such as image-guided therapy, mixed reality interaction, robotic navigation, and data augmentation.",
    "pdf_url": "https://arxiv.org/pdf/2403.18114v1",
    "github_url": null,
    "published": "2024-03-26T21:37:25+00:00",
    "updated": "2024-03-26T21:37:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.17334v1",
    "title": "OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation",
    "authors": [
      "Zhao",
      "Li",
      "Chen"
    ],
    "summary": "Recent advances in Iterative Vision-and-Language Navigation (IVLN) introduce a more meaningful and practical paradigm of VLN by maintaining the agent's memory across tours of scenes. Although the long-term memory aligns better with the persistent nature of the VLN task, it poses more challenges on how to utilize the highly unstructured navigation memory with extremely sparse supervision. Towards this end, we propose OVER-NAV, which aims to go over and beyond the current arts of IVLN techniques. In particular, we propose to incorporate LLMs and open-vocabulary detectors to distill key information and establish correspondence between multi-modal signals. Such a mechanism introduces reliable cross-modal supervision and enables on-the-fly generalization to unseen scenes without the need of extra annotation and re-training. To fully exploit the interpreted navigation data, we further introduce a structured representation, coded Omnigraph, to effectively integrate multi-modal information along the tour. Accompanied with a novel omnigraph fusion mechanism, OVER-NAV is able to extract the most relevant knowledge from omnigraph for a more accurate navigating action. In addition, OVER-NAV seamlessly supports both discrete and continuous environments under a unified framework. We demonstrate the superiority of OVER-NAV in extensive experiments.",
    "pdf_url": "https://arxiv.org/pdf/2403.17334v1",
    "github_url": null,
    "published": "2024-03-26T02:34:48+00:00",
    "updated": "2024-03-26T02:34:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.17124v2",
    "title": "Grounding Language Plans in Demonstrations Through Counterfactual Perturbations",
    "authors": [
      "Wang",
      "Wang",
      "Mao"
    ],
    "summary": "Grounding the common-sense reasoning of Large Language Models (LLMs) in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://yanweiw.github.io/glide",
    "pdf_url": "https://arxiv.org/pdf/2403.17124v2",
    "github_url": null,
    "published": "2024-03-25T19:04:59+00:00",
    "updated": "2024-04-29T04:34:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.16875v1",
    "title": "TAIL: A Terrain-Aware Multi-Modal SLAM Dataset for Robot Locomotion in Deformable Granular Environments",
    "authors": [
      "Yao",
      "Ge",
      "Shi"
    ],
    "summary": "Terrain-aware perception holds the potential to improve the robustness and accuracy of autonomous robot navigation in the wilds, thereby facilitating effective off-road traversals. However, the lack of multi-modal perception across various motion patterns hinders the solutions of Simultaneous Localization And Mapping (SLAM), especially when confronting non-geometric hazards in demanding landscapes. In this paper, we first propose a Terrain-Aware multI-modaL (TAIL) dataset tailored to deformable and sandy terrains. It incorporates various types of robotic proprioception and distinct ground interactions for the unique challenges and benchmark of multi-sensor fusion SLAM. The versatile sensor suite comprises stereo frame cameras, multiple ground-pointing RGB-D cameras, a rotating 3D LiDAR, an IMU, and an RTK device. This ensemble is hardware-synchronized, well-calibrated, and self-contained. Utilizing both wheeled and quadrupedal locomotion, we efficiently collect comprehensive sequences to capture rich unstructured scenarios. It spans the spectrum of scope, terrain interactions, scene changes, ground-level properties, and dynamic robot characteristics. We benchmark several state-of-the-art SLAM methods against ground truth and provide performance validations. Corresponding challenges and limitations are also reported. All associated resources are accessible upon request at \\url{https://tailrobot.github.io/}.",
    "pdf_url": "https://arxiv.org/pdf/2403.16875v1",
    "github_url": null,
    "published": "2024-03-25T15:39:46+00:00",
    "updated": "2024-03-25T15:39:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.16485v1",
    "title": "Real-time Model Predictive Control with Zonotope-Based Neural Networks for Bipedal Social Navigation",
    "authors": [
      "Shamsah",
      "Agarwal",
      "Kousik"
    ],
    "summary": "This study addresses the challenge of bipedal navigation in a dynamic human-crowded environment, a research area that remains largely underexplored in the field of legged navigation. We propose two cascaded zonotope-based neural networks: a Pedestrian Prediction Network (PPN) for pedestrians' future trajectory prediction and an Ego-agent Social Network (ESN) for ego-agent social path planning. Representing future paths as zonotopes allows for efficient reachability-based planning and collision checking. The ESN is then integrated with a Model Predictive Controller (ESN-MPC) for footstep planning for our bipedal robot Digit designed by Agility Robotics. ESN-MPC solves for a collision-free optimal trajectory by optimizing through the gradients of ESN. ESN-MPC optimal trajectory is sent to the low-level controller for full-order simulation of Digit. The overall proposed framework is validated with extensive simulations on randomly generated initial settings with varying human crowd densities.",
    "pdf_url": "https://arxiv.org/pdf/2403.16485v1",
    "github_url": null,
    "published": "2024-03-25T07:12:51+00:00",
    "updated": "2024-03-25T07:12:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.16425v2",
    "title": "Enhancing Visual Place Recognition via Fast and Slow Adaptive Biasing in Event Cameras",
    "authors": [
      "Nair",
      "Milford",
      "Fischer"
    ],
    "summary": "Event cameras are increasingly popular in robotics due to beneficial features such as low latency, energy efficiency, and high dynamic range. Nevertheless, their downstream task performance is greatly influenced by the optimization of bias parameters. These parameters, for instance, regulate the necessary change in light intensity to trigger an event, which in turn depends on factors such as the environment lighting and camera motion. This paper introduces feedback control algorithms that automatically tune the bias parameters through two interacting methods: 1) An immediate, on-the-fly \\textit{fast} adaptation of the refractory period, which sets the minimum interval between consecutive events, and 2) if the event rate exceeds the specified bounds even after changing the refractory period repeatedly, the controller adapts the pixel bandwidth and event thresholds, which stabilizes after a short period of noise events across all pixels (\\textit{slow} adaptation). Our evaluation focuses on the visual place recognition task, where incoming query images are compared to a given reference database. We conducted comprehensive evaluations of our algorithms' adaptive feedback control in real-time. To do so, we collected the QCR-Fast-and-Slow dataset that contains DAVIS346 event camera streams from 366 repeated traversals of a Scout Mini robot navigating through a 100 meter long indoor lab setting (totaling over 35km distance traveled) in varying brightness conditions with ground truth location information. Our proposed feedback controllers result in superior performance when compared to the standard bias settings and prior feedback control methods. Our findings also detail the impact of bias adjustments on task performance and feature ablation studies on the fast and slow adaptation mechanisms.",
    "pdf_url": "https://arxiv.org/pdf/2403.16425v2",
    "github_url": null,
    "published": "2024-03-25T05:10:34+00:00",
    "updated": "2024-08-13T04:16:48+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.15813v2",
    "title": "Learning Early Social Maneuvers for Enhanced Social Navigation",
    "authors": [
      "Yildirim",
      "Suzer",
      "Ugur"
    ],
    "summary": "Socially compliant navigation is an integral part of safety features in Human-Robot Interaction. Traditional approaches to mobile navigation prioritize physical aspects, such as efficiency, but social behaviors gain traction as robots appear more in daily life. Recent techniques to improve the social compliance of navigation often rely on predefined features or reward functions, introducing assumptions about social human behavior. To address this limitation, we propose a novel Learning from Demonstration (LfD) framework for social navigation that exclusively utilizes raw sensory data. Additionally, the proposed system contains mechanisms to consider the future paths of the surrounding pedestrians, acknowledging the temporal aspect of the problem. The final product is expected to reduce the anxiety of people sharing their environment with a mobile robot, helping them trust that the robot is aware of their presence and will not harm them. As the framework is currently being developed, we outline its components, present experimental results, and discuss future work towards realizing this framework.",
    "pdf_url": "https://arxiv.org/pdf/2403.15813v2",
    "github_url": null,
    "published": "2024-03-23T12:00:00+00:00",
    "updated": "2024-05-02T08:03:26+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.15691v2",
    "title": "Temporal-Spatial Object Relations Modeling for Vision-and-Language Navigation",
    "authors": [
      "Huang",
      "Zheng",
      "Lan"
    ],
    "summary": "Vision-and-Language Navigation (VLN) is a challenging task where an agent is required to navigate to a natural language described location via vision observations. The navigation abilities of the agent can be enhanced by the relations between objects, which are usually learned using internal objects or external datasets. The relationships between internal objects are modeled employing graph convolutional network (GCN) in traditional studies. However, GCN tends to be shallow, limiting its modeling ability. To address this issue, we utilize a cross attention mechanism to learn the connections between objects over a trajectory, which takes temporal continuity into account, termed as Temporal Object Relations (TOR). The external datasets have a gap with the navigation environment, leading to inaccurate modeling of relations. To avoid this problem, we construct object connections based on observations from all viewpoints in the navigational environment, which ensures complete spatial coverage and eliminates the gap, called Spatial Object Relations (SOR). Additionally, we observe that agents may repeatedly visit the same location during navigation, significantly hindering their performance. For resolving this matter, we introduce the Turning Back Penalty (TBP) loss function, which penalizes the agent's repetitive visiting behavior, substantially reducing the navigational distance. Experimental results on the REVERIE, SOON, and R2R datasets demonstrate the effectiveness of the proposed method.",
    "pdf_url": "https://arxiv.org/pdf/2403.15691v2",
    "github_url": null,
    "published": "2024-03-23T02:44:43+00:00",
    "updated": "2024-05-16T07:30:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.15648v3",
    "title": "Unifying Large Language Model and Deep Reinforcement Learning for Human-in-Loop Interactive Socially-aware Navigation",
    "authors": [
      "Wang",
      "Obi",
      "Bera"
    ],
    "summary": "Navigating human-filled spaces is crucial for the interactive social robots to support advanced services, such as cooperative carrying, which enables service provision in complex and crowded environments while adapting behavior based on real-time human language commands or feedback. However, existing social robot navigation planners face two major challenges: managing real-time user inputs and ensuring socially compliant behaviors in unfamiliar, zero-shot environments. In response, we introduce SALM, an interactive, human-in-loop Socially-Aware navigation Large Language Model framework that dynamically integrates deep reinforcement learning (DRL) with large language model (LLM) capabilities. SALM leverages contextual semantic understanding from real-time human-robot interactions to convert high-level user commands into precise, low-level control actions. A high-level LLM module parses user input, guiding the simultaneous generation of navigation commands by both a large language navigation model (LNM) and a DRL-based navigation model (RLNM). A memory mechanism archives temporal data for continuous refinement, while a multi-step graph-of-thoughts inference-based large language feedback model adaptively fuses the strengths of both planning approaches. Experimental evaluations demonstrate that SALM not only enhances navigational precision in crowded, dynamic environments but also significantly improves system adaptability, offering tailored behaviors that align with individual user preferences and real-time feedback. More details and videos about this work are available at: https://sites.google.com/view/navi-salm.",
    "pdf_url": "https://arxiv.org/pdf/2403.15648v3",
    "github_url": null,
    "published": "2024-03-22T23:12:28+00:00",
    "updated": "2025-03-07T20:03:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.15223v1",
    "title": "TriHelper: Zero-Shot Object Navigation with Dynamic Assistance",
    "authors": [
      "Zhang",
      "Zhang",
      "Wang"
    ],
    "summary": "Navigating toward specific objects in unknown environments without additional training, known as Zero-Shot object navigation, poses a significant challenge in the field of robotics, which demands high levels of auxiliary information and strategic planning. Traditional works have focused on holistic solutions, overlooking the specific challenges agents encounter during navigation such as collision, low exploration efficiency, and misidentification of targets. To address these challenges, our work proposes TriHelper, a novel framework designed to assist agents dynamically through three primary navigation challenges: collision, exploration, and detection. Specifically, our framework consists of three innovative components: (i) Collision Helper, (ii) Exploration Helper, and (iii) Detection Helper. These components work collaboratively to solve these challenges throughout the navigation process. Experiments on the Habitat-Matterport 3D (HM3D) and Gibson datasets demonstrate that TriHelper significantly outperforms all existing baseline methods in Zero-Shot object navigation, showcasing superior success rates and exploration efficiency. Our ablation studies further underscore the effectiveness of each helper in addressing their respective challenges, notably enhancing the agent's navigation capabilities. By proposing TriHelper, we offer a fresh perspective on advancing the object navigation task, paving the way for future research in the domain of Embodied AI and visual-based navigation.",
    "pdf_url": "https://arxiv.org/pdf/2403.15223v1",
    "github_url": null,
    "published": "2024-03-22T14:15:27+00:00",
    "updated": "2024-03-22T14:15:27+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.15049v3",
    "title": "Continual Vision-and-Language Navigation",
    "authors": [
      "Jeong",
      "Kang",
      "Choi"
    ],
    "summary": "Developing Vision-and-Language Navigation (VLN) agents typically assumes a \\textit{train-once-deploy-once} strategy, which is unrealistic as deployed agents continually encounter novel environments. To address this, we propose the Continual Vision-and-Language Navigation (CVLN) paradigm, where agents learn and adapt incrementally across multiple \\textit{scene domains}. CVLN includes two setups: Initial-instruction based CVLN for instruction-following, and Dialogue-based CVLN for dialogue-guided navigation. We also introduce two simple yet effective baselines for sequential decision-making: Perplexity Replay (PerpR), which replays difficult episodes, and Episodic Self-Replay (ESR), which stores and revisits action logits during training. Experiments show that existing continual learning methods fall short for CVLN, while PerpR and ESR achieve better performance by efficiently utilizing replay memory.",
    "pdf_url": "https://arxiv.org/pdf/2403.15049v3",
    "github_url": null,
    "published": "2024-03-22T09:15:36+00:00",
    "updated": "2025-10-31T01:59:02+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.14163v1",
    "title": "Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation",
    "authors": [
      "Sun",
      "Kanezaki",
      "Caron"
    ],
    "summary": "Object-goal navigation is a crucial engineering task for the community of embodied navigation; it involves navigating to an instance of a specified object category within unseen environments. Although extensive investigations have been conducted on both end-to-end and modular-based, data-driven approaches, fully enabling an agent to comprehend the environment through perceptual knowledge and perform object-goal navigation as efficiently as humans remains a significant challenge. Recently, large language models have shown potential in this task, thanks to their powerful capabilities for knowledge extraction and integration. In this study, we propose a data-driven, modular-based approach, trained on a dataset that incorporates common-sense knowledge of object-to-room relationships extracted from a large language model. We utilize the multi-channel Swin-Unet architecture to conduct multi-task learning incorporating with multimodal inputs. The results in the Habitat simulator demonstrate that our framework outperforms the baseline by an average of 10.6% in the efficiency metric, Success weighted by Path Length (SPL). The real-world demonstration shows that the proposed approach can efficiently conduct this task by traversing several rooms. For more details and real-world demonstrations, please check our project webpage (https://sunleyuan.github.io/ObjectNav).",
    "pdf_url": "https://arxiv.org/pdf/2403.14163v1",
    "github_url": null,
    "published": "2024-03-21T06:32:36+00:00",
    "updated": "2024-03-21T06:32:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.14158v1",
    "title": "Volumetric Environment Representation for Vision-Language Navigation",
    "authors": [
      "Liu",
      "Wang",
      "Yang"
    ],
    "summary": "Vision-language navigation (VLN) requires an agent to navigate through an 3D environment based on visual observations and natural language instructions. It is clear that the pivotal factor for successful navigation lies in the comprehensive scene understanding. Previous VLN agents employ monocular frameworks to extract 2D features of perspective views directly. Though straightforward, they struggle for capturing 3D geometry and semantics, leading to a partial and incomplete environment representation. To achieve a comprehensive 3D representation with fine-grained details, we introduce a Volumetric Environment Representation (VER), which voxelizes the physical world into structured 3D cells. For each cell, VER aggregates multi-view 2D features into such a unified 3D space via 2D-3D sampling. Through coarse-to-fine feature extraction and multi-task learning for VER, our agent predicts 3D occupancy, 3D room layout, and 3D bounding boxes jointly. Based on online collected VERs, our agent performs volume state estimation and builds episodic memory for predicting the next step. Experimental results show our environment representations from multi-task learning lead to evident performance gains on VLN. Our model achieves state-of-the-art performance across VLN benchmarks (R2R, REVERIE, and R4R).",
    "pdf_url": "https://arxiv.org/pdf/2403.14158v1",
    "github_url": null,
    "published": "2024-03-21T06:14:46+00:00",
    "updated": "2024-03-21T06:14:46+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.11650v2",
    "title": "Prioritized Semantic Learning for Zero-shot Instance Navigation",
    "authors": [
      "Sun",
      "Liu",
      "Zhi"
    ],
    "summary": "We study zero-shot instance navigation, in which the agent navigates to a specific object without using object annotations for training. Previous object navigation approaches apply the image-goal navigation (ImageNav) task (go to the location of an image) for pretraining, and transfer the agent to achieve object goals using a vision-language model. However, these approaches lead to issues of semantic neglect, where the model fails to learn meaningful semantic alignments. In this paper, we propose a Prioritized Semantic Learning (PSL) method to improve the semantic understanding ability of navigation agents. Specifically, a semantic-enhanced PSL agent is proposed and a prioritized semantic training strategy is introduced to select goal images that exhibit clear semantic supervision and relax the reward function from strict exact view matching. At inference time, a semantic expansion inference scheme is designed to preserve the same granularity level of the goal semantic as training. Furthermore, for the popular HM3D environment, we present an Instance Navigation (InstanceNav) task that requires going to a specific object instance with detailed descriptions, as opposed to the Object Navigation (ObjectNav) task where the goal is defined merely by the object category. Our PSL agent outperforms the previous state-of-the-art by 66% on zero-shot ObjectNav in terms of success rate and is also superior on the new InstanceNav task. Code will be released at https://github.com/XinyuSun/PSL-InstanceNav.",
    "pdf_url": "https://arxiv.org/pdf/2403.11650v2",
    "github_url": "https://github.com/XinyuSun/PSL-InstanceNav",
    "published": "2024-03-18T10:45:50+00:00",
    "updated": "2024-07-16T18:13:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.11541v3",
    "title": "Hierarchical Spatial Proximity Reasoning for Vision-and-Language Navigation",
    "authors": [
      "Xu",
      "Xie"
    ],
    "summary": "Most Vision-and-Language Navigation (VLN) algorithms are prone to making inaccurate decisions due to their lack of visual common sense and limited reasoning capabilities. To address this issue, we propose a Hierarchical Spatial Proximity Reasoning (HSPR) method. First, we introduce a scene understanding auxiliary task to help the agent build a knowledge base of hierarchical spatial proximity. This task utilizes panoramic views and object features to identify types of nodes and uncover the adjacency relationships between nodes, objects, and between nodes and objects. Second, we propose a multi-step reasoning navigation algorithm based on the hierarchical spatial proximity knowledge base, which continuously plans feasible paths to enhance exploration efficiency. Third, we introduce a residual fusion method to improve navigation decision accuracy. Finally, we validate our approach with experiments on publicly available datasets including REVERIE, SOON, R2R, and R4R. Our code is available at https://github.com/iCityLab/HSPR",
    "pdf_url": "https://arxiv.org/pdf/2403.11541v3",
    "github_url": "https://github.com/iCityLab/HSPR",
    "published": "2024-03-18T07:51:22+00:00",
    "updated": "2024-10-06T04:35:30+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.11487v3",
    "title": "Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis",
    "authors": [
      "Dorbala",
      "Chowdhury",
      "Manocha"
    ],
    "summary": "We present a novel approach to automatically synthesize \"wayfinding instructions\" for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we conduct zero-shot navigation with multiple approaches on the REVERIE dataset using the generated instructions, and observe very close correlation with the baseline on standard success metrics (< 1% change in SR), quantifying the viability of generated instructions in replacing human-annotated data. We finally discuss the applicability of our approach in enabling a generalizable evaluation of embodied navigation policies. To the best of our knowledge, ours is the first LLM-driven approach capable of generating \"human-like\" instructions in a platform-agnostic manner, without training.",
    "pdf_url": "https://arxiv.org/pdf/2403.11487v3",
    "github_url": null,
    "published": "2024-03-18T05:38:07+00:00",
    "updated": "2024-04-02T04:27:55+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.10700v2",
    "title": "Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation",
    "authors": [
      "Taioli",
      "Rosa",
      "Castellini"
    ],
    "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable performance drop (up to -25%) in Success Rate when evaluating the state-of-the-art VLN-CE methods on our benchmark. Moreover, we formally define the task of Instruction Error Detection and Localization, and establish an evaluation protocol on top of our benchmark dataset. We also propose an effective method, based on a cross-modal transformer architecture, that achieves the best performance in error detection and localization, compared to baselines. Surprisingly, our proposed method has revealed errors in the validation set of the two commonly used datasets for VLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in other tasks. Code and dataset available at https://intelligolabs.github.io/R2RIE-CE",
    "pdf_url": "https://arxiv.org/pdf/2403.10700v2",
    "github_url": null,
    "published": "2024-03-15T21:36:15+00:00",
    "updated": "2025-01-15T12:45:24+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.10105v1",
    "title": "Belief Aided Navigation using Bayesian Reinforcement Learning for Avoiding Humans in Blind Spots",
    "authors": [
      "Kim",
      "Kwak",
      "Rim"
    ],
    "summary": "Recent research on mobile robot navigation has focused on socially aware navigation in crowded environments. However, existing methods do not adequately account for human robot interactions and demand accurate location information from omnidirectional sensors, rendering them unsuitable for practical applications. In response to this need, this study introduces a novel algorithm, BNBRL+, predicated on the partially observable Markov decision process framework to assess risks in unobservable areas and formulate movement strategies under uncertainty. BNBRL+ consolidates belief algorithms with Bayesian neural networks to probabilistically infer beliefs based on the positional data of humans. It further integrates the dynamics between the robot, humans, and inferred beliefs to determine the navigation paths and embeds social norms within the reward function, thereby facilitating socially aware navigation. Through experiments in various risk laden scenarios, this study validates the effectiveness of BNBRL+ in navigating crowded environments with blind spots. The model's ability to navigate effectively in spaces with limited visibility and avoid obstacles dynamically can significantly improve the safety and reliability of autonomous vehicles.",
    "pdf_url": "https://arxiv.org/pdf/2403.10105v1",
    "github_url": null,
    "published": "2024-03-15T08:50:39+00:00",
    "updated": "2024-03-15T08:50:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.10008v1",
    "title": "Language to Map: Topological map generation from natural language path instructions",
    "authors": [
      "Deguchi",
      "Shibata",
      "Taguchi"
    ],
    "summary": "In this paper, a method for generating a map from path information described using natural language (textual path) is proposed. In recent years, robotics research mainly focus on vision-and-language navigation (VLN), a navigation task based on images and textual paths. Although VLN is expected to facilitate user instructions to robots, its current implementation requires users to explain the details of the path for each navigation session, which results in high explanation costs for users. To solve this problem, we proposed a method that creates a map as a topological map from a textual path and automatically creates a new path using this map. We believe that large language models (LLMs) can be used to understand textual path. Therefore, we propose and evaluate two methods, one for storing implicit maps in LLMs, and the other for generating explicit maps using LLMs. The implicit map is in the LLM's memory. It is created using prompts. In the explicit map, a topological map composed of nodes and edges is constructed and the actions at each node are stored. This makes it possible to estimate the path and actions at waypoints on an undescribed path, if enough information is available. Experimental results on path instructions generated in a real environment demonstrate that generating explicit maps achieves significantly higher accuracy than storing implicit maps in the LLMs.",
    "pdf_url": "https://arxiv.org/pdf/2403.10008v1",
    "github_url": null,
    "published": "2024-03-15T04:22:14+00:00",
    "updated": "2024-03-15T04:22:14+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.09905v4",
    "title": "TAS: A Transit-Aware Strategy for Embodied Navigation with Non-Stationary Targets",
    "authors": [
      "Dorbala",
      "Patel",
      "Bedi"
    ],
    "summary": "Embodied navigation methods commonly operate in static environments with stationary targets. In this work, we present a new algorithm for navigation in dynamic scenarios with non-stationary targets. Our novel Transit-Aware Strategy (TAS) enriches embodied navigation policies with object path information. TAS improves performance in non-stationary environments by rewarding agents for synchronizing their routes with target routes. To evaluate TAS, we further introduce Dynamic Object Maps (DOMs), a dynamic variant of node-attributed topological graphs with structured object transitions. DOMs are inspired by human habits to simulate realistic object routes on a graph. Our experiments show that on average, TAS improves agent Success Rate (SR) by 21.1 in non-stationary environments, while also generalizing better from static environments by 44.5% when measured by Relative Change in Success (RCS). We qualitatively investigate TAS-agent performance on DOMs and draw various inferences to help better model generalist navigation policies. To the best of our knowledge, ours is the first work that quantifies the adaptability of embodied navigation methods in non-stationary environments. Code and data for our benchmark will be made publicly available.",
    "pdf_url": "https://arxiv.org/pdf/2403.09905v4",
    "github_url": null,
    "published": "2024-03-14T22:33:22+00:00",
    "updated": "2025-10-16T18:41:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.09793v3",
    "title": "Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning",
    "authors": [
      "Flögel",
      "Fischer",
      "Rudolf"
    ],
    "summary": "Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep Reinforcement Learning (DRL) approaches are recently used to learn a robot's navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot's exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot's social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, which states that social acting is oriented toward the acting of others. The DRL policy is trained in an environment where other agents interact socially integrated and reward the robot's behavior individually. The simulation results indicate that the proposed socially integrated navigation approach outperforms a socially aware approach in terms of ego navigation performance while significantly reducing the negative impact on all agents within the environment.",
    "pdf_url": "https://arxiv.org/pdf/2403.09793v3",
    "github_url": null,
    "published": "2024-03-14T18:25:40+00:00",
    "updated": "2024-07-26T06:41:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.09285v1",
    "title": "THÖR-MAGNI: A Large-scale Indoor Motion Capture Recording of Human Movement and Robot Interaction",
    "authors": [
      "Schreiter",
      "Almeida",
      "Zhu"
    ],
    "summary": "We present a new large dataset of indoor human and robot navigation and interaction, called THÖR-MAGNI, that is designed to facilitate research on social navigation: e.g., modelling and predicting human motion, analyzing goal-oriented interactions between humans and robots, and investigating visual attention in a social interaction context. THÖR-MAGNI was created to fill a gap in available datasets for human motion analysis and HRI. This gap is characterized by a lack of comprehensive inclusion of exogenous factors and essential target agent cues, which hinders the development of robust models capable of capturing the relationship between contextual cues and human behavior in different scenarios. Unlike existing datasets, THÖR-MAGNI includes a broader set of contextual features and offers multiple scenario variations to facilitate factor isolation. The dataset includes many social human-human and human-robot interaction scenarios, rich context annotations, and multi-modal data, such as walking trajectories, gaze tracking data, and lidar and camera streams recorded from a mobile robot. We also provide a set of tools for visualization and processing of the recorded data. THÖR-MAGNI is, to the best of our knowledge, unique in the amount and diversity of sensor data collected in a contextualized and socially dynamic environment, capturing natural human-robot interactions.",
    "pdf_url": "https://arxiv.org/pdf/2403.09285v1",
    "github_url": null,
    "published": "2024-03-14T11:12:16+00:00",
    "updated": "2024-03-14T11:12:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.08984v1",
    "title": "Safe Road-Crossing by Autonomous Wheelchairs: a Novel Dataset and its Experimental Evaluation",
    "authors": [
      "Grigioni",
      "Corradini",
      "Antonucci"
    ],
    "summary": "Safe road-crossing by self-driving vehicles is a crucial problem to address in smart-cities. In this paper, we introduce a multi-sensor fusion approach to support road-crossing decisions in a system composed by an autonomous wheelchair and a flying drone featuring a robust sensory system made of diverse and redundant components. To that aim, we designed an analytical danger function based on explainable physical conditions evaluated by single sensors, including those using machine learning and artificial vision. As a proof-of-concept, we provide an experimental evaluation in a laboratory environment, showing the advantages of using multiple sensors, which can improve decision accuracy and effectively support safety assessment. We made the dataset available to the scientific community for further experimentation. The work has been developed in the context of an European project named REXASI-PRO, which aims to develop trustworthy artificial intelligence for social navigation of people with reduced mobility.",
    "pdf_url": "https://arxiv.org/pdf/2403.08984v1",
    "github_url": null,
    "published": "2024-03-13T22:19:06+00:00",
    "updated": "2024-03-13T22:19:06+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.08955v4",
    "title": "Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis",
    "authors": [
      "Liu",
      "Gupta",
      "Noorani"
    ],
    "summary": "Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration efficiency and safety. Risk-sensitive policy gradient methods, which incorporate both expected return and risk measures, have been explored for their ability to yield safe policies, yet their iteration complexity remains largely underexplored. In this work, we conduct a rigorous iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm with an exponential utility function. We establish an iteration complexity of $\\mathcal{O}(ε^{-2})$ to reach an $ε$-approximate first-order stationary point (FOSP). Furthermore, we investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutral counterparts. Our analysis indicates that risk-sensitive REINFORCE can potentially converge faster. To validate our analysis, we empirically evaluate the learning performance and convergence efficiency of the risk-neutral and risk-sensitive REINFORCE algorithms in multiple environments: CartPole, MiniGrid, and Robot Navigation. Empirical results confirm that risk-sensitive cases can converge and stabilize faster compared to their risk-neutral counterparts. More details can be found on our website https://anonymous.4open.science/w/riskrl.",
    "pdf_url": "https://arxiv.org/pdf/2403.08955v4",
    "github_url": null,
    "published": "2024-03-13T20:50:49+00:00",
    "updated": "2025-08-29T18:44:10+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.08282v2",
    "title": "Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation",
    "authors": [
      "Zhao",
      "Chen",
      "Guo"
    ],
    "summary": "Due to the dynamic and unpredictable open-world setting, navigating complex environments in Minecraft poses significant challenges for multi-agent systems. Agents must interact with the environment and coordinate their actions with other agents to achieve common objectives. However, traditional approaches often struggle to efficiently manage inter-agent communication and task distribution, crucial for effective multi-agent navigation. Furthermore, processing and integrating multi-modal information (such as visual, textual, and auditory data) is essential for agents to comprehend their goals and navigate the environment successfully and fully. To address this issue, we design the HAS framework to auto-organize groups of LLM-based agents to complete navigation tasks. In our approach, we devise a hierarchical auto-organizing navigation system, which is characterized by 1) a hierarchical system for multi-agent organization, ensuring centralized planning and decentralized execution; 2) an auto-organizing and intra-communication mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal information platform, facilitating multi-modal perception to perform the three navigation tasks with one system. To assess organizational behavior, we design a series of navigation tasks in the Minecraft environment, which includes searching and exploring. We aim to develop embodied organizations that push the boundaries of embodied AI, moving it towards a more human-like organizational structure.",
    "pdf_url": "https://arxiv.org/pdf/2403.08282v2",
    "github_url": null,
    "published": "2024-03-13T06:22:17+00:00",
    "updated": "2024-03-18T05:03:53+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.08833v1",
    "title": "TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation",
    "authors": [
      "Li",
      "Chen",
      "Lin"
    ],
    "summary": "Zero-shot navigation is a critical challenge in Vision-Language Navigation (VLN) tasks, where the ability to adapt to unfamiliar instructions and to act in unknown environments is essential. Existing supervised learning-based models, trained using annotated data through reinforcement learning, exhibit limitations in generalization capabilities. Large Language Models (LLMs), with their extensive knowledge and emergent reasoning abilities, present a potential pathway for achieving zero-shot navigation. This paper presents a VLN agent based on LLMs, exploring approaches to the zero-shot navigation problem. To compensate for the shortcomings of LLMs in environmental perception, we propose the Thinking, Interacting, and Action (TINA) framework. TINA enables the agent to scrutinize perceptual information and autonomously query key clues within the environment through an introduced question-answering module, thereby aligning instructions with specific perceptual data. The navigation agent's perceptual abilities are enhanced through the TINA framework, while the explicit thought and query processes also improve the navigational procedure's explainability and transparency. We evaluate the performance of our method on the Room-to-Room dataset. The experiment results indicate that our approach improves the navigation performance of LLM-based agents. Our approach also outperformed some supervised learning-based methods, highlighting its efficacy in zero-shot navigation.",
    "pdf_url": "https://arxiv.org/pdf/2403.08833v1",
    "github_url": null,
    "published": "2024-03-13T05:22:39+00:00",
    "updated": "2024-03-13T05:22:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.08144v1",
    "title": "Prosody for Intuitive Robotic Interface Design: It's Not What You Said, It's How You Said It",
    "authors": [
      "Sanoubari",
      "Iscen",
      "Takayama"
    ],
    "summary": "In this paper, we investigate the use of 'prosody' (the musical elements of speech) as a communicative signal for intuitive human-robot interaction interfaces. Our approach, rooted in Research through Design (RtD), examines the application of prosody in directing a quadruped robot navigation. We involved ten team members in an experiment to command a robot through an obstacle course using natural interaction. A human operator, serving as the robot's sensory and processing proxy, translated human communication into a basic set of navigation commands, effectively simulating an intuitive interface. During our analysis of interaction videos, when lexical and visual cues proved insufficient for accurate command interpretation, we turned to non-verbal auditory cues. Qualitative evidence suggests that participants intuitively relied on prosody to control robot navigation. We highlight specific distinct prosodic constructs that emerged from this preliminary exploration and discuss their pragmatic functions. This work contributes a discussion on the broader potential of prosody as a multifunctional communicative signal for designing future intuitive robotic interfaces, enabling lifelong learning and personalization in human-robot interaction.",
    "pdf_url": "https://arxiv.org/pdf/2403.08144v1",
    "github_url": null,
    "published": "2024-03-13T00:10:18+00:00",
    "updated": "2024-03-13T00:10:18+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.07376v2",
    "title": "NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning",
    "authors": [
      "Lin",
      "Nie",
      "Wei"
    ],
    "summary": "Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the reasoning from the prior steps. Through constructing formalized labels for training, the LLM can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision. Experimental results across various training settings and popular VLN benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants. Through simple parameter-efficient finetuning, our NavCoT outperforms a recent GPT4-based approach with ~7% relative improvement on the R2R dataset. We believe that NavCoT will help unlock more task-adaptive and scalable LLM-based embodied agents, which are helpful for developing real-world robotics applications. Code is available at https://github.com/expectorlin/NavCoT.",
    "pdf_url": "https://arxiv.org/pdf/2403.07376v2",
    "github_url": "https://github.com/expectorlin/NavCoT",
    "published": "2024-03-12T07:27:02+00:00",
    "updated": "2025-03-22T11:04:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.07076v1",
    "title": "Mapping High-level Semantic Regions in Indoor Environments without Object Recognition",
    "authors": [
      "Bigazzi",
      "Baraldi",
      "Kousik"
    ],
    "summary": "Robots require a semantic understanding of their surroundings to operate in an efficient and explainable way in human environments. In the literature, there has been an extensive focus on object labeling and exhaustive scene graph generation; less effort has been focused on the task of purely identifying and mapping large semantic regions. The present work proposes a method for semantic region mapping via embodied navigation in indoor environments, generating a high-level representation of the knowledge of the agent. To enable region identification, the method uses a vision-to-language model to provide scene information for mapping. By projecting egocentric scene understanding into the global frame, the proposed method generates a semantic map as a distribution over possible region labels at each location. This mapping procedure is paired with a trained navigation policy to enable autonomous map generation. The proposed method significantly outperforms a variety of baselines, including an object-based system and a pretrained scene classifier, in experiments in a photorealistic simulator.",
    "pdf_url": "https://arxiv.org/pdf/2403.07076v1",
    "github_url": null,
    "published": "2024-03-11T18:09:50+00:00",
    "updated": "2024-03-11T18:09:50+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.05770v1",
    "title": "Towards Deviation-Robust Agent Navigation via Perturbation-Aware Contrastive Learning",
    "authors": [
      "Lin",
      "Long",
      "Zhu"
    ],
    "summary": "Vision-and-language navigation (VLN) asks an agent to follow a given language instruction to navigate through a real 3D environment. Despite significant advances, conventional VLN agents are trained typically under disturbance-free environments and may easily fail in real-world scenarios, since they are unaware of how to deal with various possible disturbances, such as sudden obstacles or human interruptions, which widely exist and may usually cause an unexpected route deviation. In this paper, we present a model-agnostic training paradigm, called Progressive Perturbation-aware Contrastive Learning (PROPER) to enhance the generalization ability of existing VLN agents, by requiring them to learn towards deviation-robust navigation. Specifically, a simple yet effective path perturbation scheme is introduced to implement the route deviation, with which the agent is required to still navigate successfully following the original instruction. Since directly enforcing the agent to learn perturbed trajectories may lead to inefficient training, a progressively perturbed trajectory augmentation strategy is designed, where the agent can self-adaptively learn to navigate under perturbation with the improvement of its navigation performance for each specific trajectory. For encouraging the agent to well capture the difference brought by perturbation, a perturbation-aware contrastive learning mechanism is further developed by contrasting perturbation-free trajectory encodings and perturbation-based counterparts. Extensive experiments on R2R show that PROPER can benefit multiple VLN baselines in perturbation-free scenarios. We further collect the perturbed path data to construct an introspection subset based on the R2R, called Path-Perturbed R2R (PP-R2R). The results on PP-R2R show unsatisfying robustness of popular VLN agents and the capability of PROPER in improving the navigation robustness.",
    "pdf_url": "https://arxiv.org/pdf/2403.05770v1",
    "github_url": null,
    "published": "2024-03-09T02:34:13+00:00",
    "updated": "2024-03-09T02:34:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.04745v4",
    "title": "Not All Errors Are Made Equal: A Regret Metric for Detecting System-level Trajectory Prediction Failures",
    "authors": [
      "Nakamura",
      "Tian",
      "Bajcsy"
    ],
    "summary": "Robot decision-making increasingly relies on data-driven human prediction models when operating around people. While these models are known to mispredict in out-of-distribution interactions, only a subset of prediction errors impact downstream robot performance. We propose characterizing such \"system-level\" prediction failures via the mathematical notion of regret: high-regret interactions are precisely those in which mispredictions degraded closed-loop robot performance. We further introduce a probabilistic generalization of regret that calibrates failure detection across disparate deployment contexts and renders regret compatible with reward-based and reward-free (e.g., generative) planners. In simulated autonomous driving interactions and social navigation interactions deployed on hardware, we showcase that our system-level failure metric can be used offline to automatically extract closed-loop human-robot interactions that state-of-the-art generative human predictors and robot planners previously struggled with. We further find that the very presence of high-regret data during human predictor fine-tuning is highly predictive of robot re-deployment performance improvements. Fine-tuning with the informative but significantly smaller high-regret data (23% of deployment data) is competitive with fine-tuning on the full deployment dataset, indicating a promising avenue for efficiently mitigating system-level human-robot interaction failures. Project website: https://cmu-intentlab.github.io/not-all-errors/",
    "pdf_url": "https://arxiv.org/pdf/2403.04745v4",
    "github_url": null,
    "published": "2024-03-07T18:49:36+00:00",
    "updated": "2024-11-09T23:55:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.03405v1",
    "title": "Causality-based Cross-Modal Representation Learning for Vision-and-Language Navigation",
    "authors": [
      "Wang",
      "He",
      "Dang"
    ],
    "summary": "Vision-and-Language Navigation (VLN) has gained significant research interest in recent years due to its potential applications in real-world scenarios. However, existing VLN methods struggle with the issue of spurious associations, resulting in poor generalization with a significant performance gap between seen and unseen environments. In this paper, we tackle this challenge by proposing a unified framework CausalVLN based on the causal learning paradigm to train a robust navigator capable of learning unbiased feature representations. Specifically, we establish reasonable assumptions about confounders for vision and language in VLN using the structured causal model (SCM). Building upon this, we propose an iterative backdoor-based representation learning (IBRL) method that allows for the adaptive and effective intervention on confounders. Furthermore, we introduce the visual and linguistic backdoor causal encoders to enable unbiased feature expression for multi-modalities during training and validation, enhancing the agent's capability to generalize across different environments. Experiments on three VLN datasets (R2R, RxR, and REVERIE) showcase the superiority of our proposed method over previous state-of-the-art approaches. Moreover, detailed visualization analysis demonstrates the effectiveness of CausalVLN in significantly narrowing down the performance gap between seen and unseen environments, underscoring its strong generalization capability.",
    "pdf_url": "https://arxiv.org/pdf/2403.03405v1",
    "github_url": null,
    "published": "2024-03-06T02:01:38+00:00",
    "updated": "2024-03-06T02:01:38+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.01542v1",
    "title": "Human Robot Pacing Mismatch",
    "authors": [
      "Sun",
      "Trautman",
      "Murphey"
    ],
    "summary": "A widely accepted explanation for robots planning overcautious or overaggressive trajectories alongside human is that the crowd density exceeds a threshold such that all feasible trajectories are considered unsafe -- the freezing robot problem. However, even with low crowd density, the robot's navigation performance could still drop drastically when in close proximity to human. In this work, we argue that a broader cause of suboptimal navigation performance near human is due to the robot's misjudgement for the human's willingness (flexibility) to share space with others, particularly when the robot assumes the human's flexibility holds constant during interaction, a phenomenon of what we call human robot pacing mismatch. We show that the necessary condition for solving pacing mismatch is to model the evolution of both the robot and the human's flexibility during decision making, a strategy called distribution space modeling. We demonstrate the advantage of distribution space coupling through an anecdotal case study and discuss the future directions of solving human robot pacing mismatch.",
    "pdf_url": "https://arxiv.org/pdf/2403.01542v1",
    "github_url": null,
    "published": "2024-03-03T15:42:56+00:00",
    "updated": "2024-03-03T15:42:56+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.01450v3",
    "title": "Collision-Free Robot Navigation in Crowded Environments using Learning based Convex Model Predictive Control",
    "authors": [
      "Wen",
      "Dong",
      "Chen"
    ],
    "summary": "Navigating robots safely and efficiently in crowded and complex environments remains a significant challenge. However, due to the dynamic and intricate nature of these settings, planning efficient and collision-free paths for robots to track is particularly difficult. In this paper, we uniquely bridge the robot's perception, decision-making and control processes by utilizing the convex obstacle-free region computed from 2D LiDAR data. The overall pipeline is threefold: (1) We proposes a robot navigation framework that utilizes deep reinforcement learning (DRL), conceptualizing the observation as the convex obstacle-free region, a departure from general reliance on raw sensor inputs. (2) We design the action space, derived from the intersection of the robot's kinematic limits and the convex region, to enable efficient sampling of inherently collision-free reference points. These actions assists in guiding the robot to move towards the goal and interact with other obstacles during navigation. (3) We employ model predictive control (MPC) to track the trajectory formed by the reference points while satisfying constraints imposed by the convex obstacle-free region and the robot's kinodynamic limits. The effectiveness of proposed improvements has been validated through two sets of ablation studies and a comparative experiment against the Timed Elastic Band (TEB), demonstrating improved navigation performance in crowded and complex environments.",
    "pdf_url": "https://arxiv.org/pdf/2403.01450v3",
    "github_url": null,
    "published": "2024-03-03T09:08:07+00:00",
    "updated": "2024-10-19T12:04:03+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2403.00991v2",
    "title": "SELFI: Autonomous Self-Improvement with Reinforcement Learning for Social Navigation",
    "authors": [
      "Hirose",
      "Shah",
      "Stachowicz"
    ],
    "summary": "Autonomous self-improving robots that interact and improve with experience are key to the real-world deployment of robotic systems. In this paper, we propose an online learning method, SELFI, that leverages online robot experience to rapidly fine-tune pre-trained control policies efficiently. SELFI applies online model-free reinforcement learning on top of offline model-based learning to bring out the best parts of both learning paradigms. Specifically, SELFI stabilizes the online learning process by incorporating the same model-based learning objective from offline pre-training into the Q-values learned with online model-free reinforcement learning. We evaluate SELFI in multiple real-world environments and report improvements in terms of collision avoidance, as well as more socially compliant behavior, measured by a human user study. SELFI enables us to quickly learn useful robotic behaviors with less human interventions such as pre-emptive behavior for the pedestrians, collision avoidance for small and transparent objects, and avoiding travel on uneven floor surfaces. We provide supplementary videos to demonstrate the performance of our fine-tuned policy on our project page.",
    "pdf_url": "https://arxiv.org/pdf/2403.00991v2",
    "github_url": null,
    "published": "2024-03-01T21:27:03+00:00",
    "updated": "2024-10-05T00:12:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.19432v1",
    "title": "Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation",
    "authors": [
      "Yang",
      "Glossop",
      "Bhorkar"
    ],
    "summary": "Recent years in robotics and imitation learning have shown remarkable progress in training large-scale foundation models by leveraging data across a multitude of embodiments. The success of such policies might lead us to wonder: just how diverse can the robots in the training set be while still facilitating positive transfer? In this work, we study this question in the context of heterogeneous embodiments, examining how even seemingly very different domains, such as robotic navigation and manipulation, can provide benefits when included in the training data for the same model. We train a single goal-conditioned policy that is capable of controlling robotic arms, quadcopters, quadrupeds, and mobile bases. We then investigate the extent to which transfer can occur across navigation and manipulation on these embodiments by framing them as a single goal-reaching task. We find that co-training with navigation data can enhance robustness and performance in goal-conditioned manipulation with a wrist-mounted camera. We then deploy our policy trained only from navigation-only and static manipulation-only data on a mobile manipulator, showing that it can control a novel embodiment in a zero-shot manner. These results provide evidence that large-scale robotic policies can benefit from data collected across various embodiments. Further information and robot videos can be found on our project website http://extreme-cross-embodiment.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2402.19432v1",
    "github_url": null,
    "published": "2024-02-29T18:30:32+00:00",
    "updated": "2024-02-29T18:30:32+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.19007v2",
    "title": "DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments",
    "authors": [
      "Ma",
      "Dai",
      "Mu"
    ],
    "summary": "Zero-Shot Object Navigation (ZSON) requires agents to autonomously locate and approach unseen objects in unfamiliar environments and has emerged as a particularly challenging task within the domain of Embodied AI. Existing datasets for developing ZSON algorithms lack consideration of dynamic obstacles, object attribute diversity, and scene texts, thus exhibiting noticeable discrepancies from real-world situations. To address these issues, we propose a Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments (DOZE) that comprises ten high-fidelity 3D scenes with over 18k tasks, aiming to mimic complex, dynamic real-world scenarios. Specifically, DOZE scenes feature multiple moving humanoid obstacles, a wide array of open-vocabulary objects, diverse distinct-attribute objects, and valuable textual hints. Besides, different from existing datasets that only provide collision checking between the agent and static obstacles, we enhance DOZE by integrating capabilities for detecting collisions between the agent and moving obstacles. This novel functionality enables the evaluation of the agents' collision avoidance abilities in dynamic environments. We test four representative ZSON methods on DOZE, revealing substantial room for improvement in existing approaches concerning navigation efficiency, safety, and object recognition accuracy. Our dataset can be found at https://DOZE-Dataset.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2402.19007v2",
    "github_url": null,
    "published": "2024-02-29T10:03:57+00:00",
    "updated": "2024-07-08T07:58:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.18065v2",
    "title": "A Probabilistic Motion Model for Skid-Steer Wheeled Mobile Robot Navigation on Off-Road Terrains",
    "authors": [
      "Trivedi",
      "Zolotas",
      "Abbas"
    ],
    "summary": "Skid-Steer Wheeled Mobile Robots (SSWMRs) are increasingly being used for off-road autonomy applications. When turning at high speeds, these robots tend to undergo significant skidding and slipping. In this work, using Gaussian Process Regression (GPR) and Sigma-Point Transforms, we estimate the non-linear effects of tire-terrain interaction on robot velocities in a probabilistic fashion. Using the mean estimates from GPR, we propose a data-driven dynamic motion model that is more accurate at predicting future robot poses than conventional kinematic motion models. By efficiently solving a convex optimization problem based on the history of past robot motion, the GPR augmented motion model generalizes to previously unseen terrain conditions. The output distribution from the proposed motion model can be used for local motion planning approaches, such as stochastic model predictive control, leveraging model uncertainty to make safe decisions. We validate our work on a benchmark real-world multi-terrain SSWMR dataset. Our results show that the model generalizes to three different terrains while significantly reducing errors in linear and angular motion predictions. As shown in the attached video, we perform a separate set of experiments on a physical robot to demonstrate the robustness of the proposed algorithm.",
    "pdf_url": "https://arxiv.org/pdf/2402.18065v2",
    "github_url": null,
    "published": "2024-02-28T05:50:18+00:00",
    "updated": "2024-02-29T15:02:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.17511v1",
    "title": "Rethinking Mutual Information for Language Conditioned Skill Discovery on Imitation Learning",
    "authors": [
      "Ju",
      "Yang",
      "Wang"
    ],
    "summary": "Language-conditioned robot behavior plays a vital role in executing complex tasks by associating human commands or instructions with perception and actions. The ability to compose long-horizon tasks based on unconstrained language instructions necessitates the acquisition of a diverse set of general-purpose skills. However, acquiring inherent primitive skills in a coupled and long-horizon environment without external rewards or human supervision presents significant challenges. In this paper, we evaluate the relationship between skills and language instructions from a mathematical perspective, employing two forms of mutual information within the framework of language-conditioned policy learning. To maximize the mutual information between language and skills in an unsupervised manner, we propose an end-to-end imitation learning approach known as Language Conditioned Skill Discovery (LCSD). Specifically, we utilize vector quantization to learn discrete latent skills and leverage skill sequences of trajectories to reconstruct high-level semantic instructions. Through extensive experiments on language-conditioned robotic navigation and manipulation tasks, encompassing BabyAI, LORel, and CALVIN, we demonstrate the superiority of our method over prior works. Our approach exhibits enhanced generalization capabilities towards unseen tasks, improved skill interpretability, and notably higher rates of task completion success.",
    "pdf_url": "https://arxiv.org/pdf/2402.17511v1",
    "github_url": null,
    "published": "2024-02-27T13:53:52+00:00",
    "updated": "2024-02-27T13:53:52+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.16482v1",
    "title": "On Languaging a Simulation Engine",
    "authors": [
      "Liu",
      "Li"
    ],
    "summary": "Language model intelligence is revolutionizing the way we program materials simulations. However, the diversity of simulation scenarios renders it challenging to precisely transform human language into a tailored simulator. Here, using three functionalized types of language model, we propose a language-to-simulation (Lang2Sim) framework that enables interactive navigation on languaging a simulation engine, by taking a scenario instance of water sorption in porous matrices. Unlike line-by-line coding of a target simulator, the language models interpret each simulator as an assembly of invariant tool function and its variant input-output pair. Lang2Sim enables the precise transform of textual description by functionalizing and sequentializing the language models of, respectively, rationalizing the tool categorization, customizing its input-output combinations, and distilling the simulator input into executable format. Importantly, depending on its functionalized type, each language model features a distinct processing of chat history to best balance its memory limit and information completeness, thus leveraging the model intelligence to unstructured nature of human request. Overall, this work establishes language model as an intelligent platform to unlock the era of languaging a simulation engine.",
    "pdf_url": "https://arxiv.org/pdf/2402.16482v1",
    "github_url": null,
    "published": "2024-02-26T11:01:54+00:00",
    "updated": "2024-02-26T11:01:54+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.16117v1",
    "title": "RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis",
    "authors": [
      "Mu",
      "Chen",
      "Zhang"
    ],
    "summary": "Robotic behavior synthesis, the problem of understanding multimodal inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying multimodal large language models for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured multimodal code generation framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies code generation to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized multimodal reasoning dataset is collected for pre-training and an iterative self-updating methodology is introduced for supervised fine-tuning. Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one navigation task.",
    "pdf_url": "https://arxiv.org/pdf/2402.16117v1",
    "github_url": null,
    "published": "2024-02-25T15:31:43+00:00",
    "updated": "2024-02-25T15:31:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.15852v7",
    "title": "NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation",
    "authors": [
      "Zhang",
      "Wang",
      "Xu"
    ],
    "summary": "Vision-and-language navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic instructions. In this field, generalization is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavor to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometers, or depth inputs. Following human instruction, NaVid only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision making and instruction following. We train NaVid with 510k navigation samples collected from continuous environments, including action-planning and instruction-reasoning samples, along with 763k large-scale web data. Extensive experiments show that NaVid achieves state-of-the-art performance in simulation environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.",
    "pdf_url": "https://arxiv.org/pdf/2402.15852v7",
    "github_url": null,
    "published": "2024-02-24T16:39:16+00:00",
    "updated": "2024-06-30T11:14:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.15420v1",
    "title": "PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning",
    "authors": [
      "Holk",
      "Marta",
      "Leite"
    ],
    "summary": "Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting. To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights -- state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM. In both a simulated scenario and a user study, we reveal the effectiveness of our work by analyzing the feedback and its implications. Additionally, the collective feedback collected serves to train a robot on socially compliant trajectories in a simulated social navigation landscape. We provide video examples of the trained policies at https://sites.google.com/view/rl-predilect",
    "pdf_url": "https://arxiv.org/pdf/2402.15420v1",
    "github_url": null,
    "published": "2024-02-23T16:30:05+00:00",
    "updated": "2024-02-23T16:30:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.14304v2",
    "title": "Vision-Language Navigation with Embodied Intelligence: A Survey",
    "authors": [
      "Gao",
      "Wang",
      "Gao"
    ],
    "summary": "As a long-term vision in the field of artificial intelligence, the core goal of embodied intelligence is to improve the perception, understanding, and interaction capabilities of agents and the environment. Vision-language navigation (VLN), as a critical research path to achieve embodied intelligence, focuses on exploring how agents use natural language to communicate effectively with humans, receive and understand instructions, and ultimately rely on visual information to achieve accurate navigation. VLN integrates artificial intelligence, natural language processing, computer vision, and robotics. This field faces technical challenges but shows potential for application such as human-computer interaction. However, due to the complex process involved from language understanding to action execution, VLN faces the problem of aligning visual information and language instructions, improving generalization ability, and many other challenges. This survey systematically reviews the research progress of VLN and details the research direction of VLN with embodied intelligence. After a detailed summary of its system architecture and research based on methods and commonly used benchmark datasets, we comprehensively analyze the problems and challenges faced by current research and explore the future development direction of this field, aiming to provide a practical reference for researchers.",
    "pdf_url": "https://arxiv.org/pdf/2402.14304v2",
    "github_url": null,
    "published": "2024-02-22T05:45:17+00:00",
    "updated": "2024-03-15T12:31:35+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.11799v2",
    "title": "Decentralized Multi-Robot Navigation for Autonomous Surface Vehicles with Distributional Reinforcement Learning",
    "authors": [
      "Lin",
      "Huang",
      "Chen"
    ],
    "summary": "Collision avoidance algorithms for Autonomous Surface Vehicles (ASV) that follow the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) have been proposed in recent years. However, it may be difficult and unsafe to follow COLREGs in congested waters, where multiple ASVs are navigating in the presence of static obstacles and strong currents, due to the complex interactions. To address this problem, we propose a decentralized multi-ASV collision avoidance policy based on Distributional Reinforcement Learning, which considers the interactions among ASVs as well as with static obstacles and current flows. We evaluate the performance of the proposed Distributional RL based policy against a traditional RL-based policy and two classical methods, Artificial Potential Fields (APF) and Reciprocal Velocity Obstacles (RVO), in simulation experiments, which show that the proposed policy achieves superior performance in navigation safety, while requiring minimal travel time and energy. A variant of our framework that automatically adapts its risk sensitivity is also demonstrated to improve ASV safety in highly congested environments.",
    "pdf_url": "https://arxiv.org/pdf/2402.11799v2",
    "github_url": null,
    "published": "2024-02-19T03:06:43+00:00",
    "updated": "2024-03-06T19:06:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.10670v2",
    "title": "OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models",
    "authors": [
      "Kuang",
      "Lin",
      "Jiang"
    ],
    "summary": "Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting common sense reasoning on VSSM, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments. Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method's effectiveness. Furthermore, we perform real robot demonstrations to validate our method's open-set-ness and generalizability to real-world environments.",
    "pdf_url": "https://arxiv.org/pdf/2402.10670v2",
    "github_url": null,
    "published": "2024-02-16T13:21:33+00:00",
    "updated": "2024-03-25T02:52:43+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.08221v1",
    "title": "MetaTra: Meta-Learning for Generalized Trajectory Prediction in Unseen Domain",
    "authors": [
      "Li",
      "Huang",
      "Fan"
    ],
    "summary": "Trajectory prediction has garnered widespread attention in different fields, such as autonomous driving and robotic navigation. However, due to the significant variations in trajectory patterns across different scenarios, models trained in known environments often falter in unseen ones. To learn a generalized model that can directly handle unseen domains without requiring any model updating, we propose a novel meta-learning-based trajectory prediction method called MetaTra. This approach incorporates a Dual Trajectory Transformer (Dual-TT), which enables a thorough exploration of the individual intention and the interactions within group motion patterns in diverse scenarios. Building on this, we propose a meta-learning framework to simulate the generalization process between source and target domains. Furthermore, to enhance the stability of our prediction outcomes, we propose a Serial and Parallel Training (SPT) strategy along with a feature augmentation method named MetaMix. Experimental results on several real-world datasets confirm that MetaTra not only surpasses other state-of-the-art methods but also exhibits plug-and-play capabilities, particularly in the realm of domain generalization.",
    "pdf_url": "https://arxiv.org/pdf/2402.08221v1",
    "github_url": null,
    "published": "2024-02-13T05:25:37+00:00",
    "updated": "2024-02-13T05:25:37+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.07872v1",
    "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
    "authors": [
      "Nasiriany",
      "Xia",
      "Yu"
    ],
    "summary": "Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data?   In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",
    "pdf_url": "https://arxiv.org/pdf/2402.07872v1",
    "github_url": null,
    "published": "2024-02-12T18:33:47+00:00",
    "updated": "2024-02-12T18:33:47+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.07049v1",
    "title": "A Factor Graph Model of Trust for a Collaborative Multi-Agent System",
    "authors": [
      "Akbari",
      "Yuan",
      "Wang"
    ],
    "summary": "In the field of Multi-Agent Systems (MAS), known for their openness, dynamism, and cooperative nature, the ability to trust the resources and services of other agents is crucial. Trust, in this setting, is the reliance and confidence an agent has in the information, behaviors, intentions, truthfulness, and capabilities of others within the system. Our paper introduces a new graphical approach that utilizes factor graphs to represent the interdependent behaviors and trustworthiness among agents. This includes modeling the behavior of robots as a trajectory of actions using a Gaussian process factor graph, which accounts for smoothness, obstacle avoidance, and trust-related factors. Our method for evaluating trust is decentralized and considers key interdependent sub-factors such as proximity safety, consistency, and cooperation. The overall system comprises a network of factor graphs that interact through trust-related factors and employs a Bayesian inference method to dynamically assess trust-based decisions with informed consent. The effectiveness of this method is validated via simulations and empirical tests with autonomous robots navigating unsignalized intersections.",
    "pdf_url": "https://arxiv.org/pdf/2402.07049v1",
    "github_url": null,
    "published": "2024-02-10T21:44:28+00:00",
    "updated": "2024-02-10T21:44:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.03561v2",
    "title": "VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation",
    "authors": [
      "Li",
      "Padmakumar",
      "Sukhatme"
    ],
    "summary": "Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with three proxy tasks: Masked Language Modeling, Instruction and Trajectory Matching, and Next Action Prediction, so as to learn temporally-aware and visually-aligned instruction representations. The learned instruction representation is adapted to the state-of-the-art navigator when fine-tuning on the Touchdown dataset. Empirical results demonstrate that VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in task completion rate, achieving a new state-of-the-art on the Touchdown dataset.",
    "pdf_url": "https://arxiv.org/pdf/2402.03561v2",
    "github_url": null,
    "published": "2024-02-05T22:20:19+00:00",
    "updated": "2024-02-07T18:02:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.03494v3",
    "title": "Beyond Text: Utilizing Vocal Cues to Improve Decision Making in LLMs for Robot Navigation Tasks",
    "authors": [
      "Sun",
      "Meng",
      "Chakraborty"
    ],
    "summary": "While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present Beyond Text: an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations.This approach not only achieves a 70.26% winning rate, outperforming existing LLMs by 22.16% to 48.30% (gemini-1.5-pro and gpt-3.5 respectively), but also enhances robustness against token manipulation adversarial attacks, highlighted by a 22.44% less decrease ratio than the text-only language model in winning rate. Beyond Text' marks an advancement in social robot navigation and broader Human-Robot interactions, seamlessly integrating text-based guidance with human-audio-informed language models.",
    "pdf_url": "https://arxiv.org/pdf/2402.03494v3",
    "github_url": null,
    "published": "2024-02-05T20:11:56+00:00",
    "updated": "2024-11-11T04:03:28+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.02651v3",
    "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning",
    "authors": [
      "Chen",
      "Mees",
      "Kumar"
    ],
    "summary": "Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that encode semantic features of visual observations based on the VLM's internal knowledge and reasoning capabilities, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings from off-the-shelf, general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings. Finally, we show that our approach can use chain-of-thought prompting to produce representations of common-sense semantic reasoning, improving policy performance in novel scenes by 1.5 times.",
    "pdf_url": "https://arxiv.org/pdf/2402.02651v3",
    "github_url": null,
    "published": "2024-02-05T00:48:56+00:00",
    "updated": "2024-05-23T01:04:11+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.02559v1",
    "title": "NavHint: Vision and Language Navigation Agent with a Hint Generator",
    "authors": [
      "Zhang",
      "Guo",
      "Kordjamshidi"
    ],
    "summary": "Existing work on vision and language navigation mainly relies on navigation-related losses to establish the connection between vision and language modalities, neglecting aspects of helping the navigation agent build a deep understanding of the visual environment. In our work, we provide indirect supervision to the navigation agent through a hint generator that provides detailed visual descriptions. The hint generator assists the navigation agent in developing a global understanding of the visual environment. It directs the agent's attention toward related navigation details, including the relevant sub-instruction, potential challenges in recognition and ambiguities in grounding, and the targeted viewpoint description. To train the hint generator, we construct a synthetic dataset based on landmarks in the instructions and visible and distinctive objects in the visual environment. We evaluate our method on the R2R and R4R datasets and achieve state-of-the-art on several metrics. The experimental results demonstrate that generating hints not only enhances the navigation performance but also helps improve the interpretability of the agent's actions.",
    "pdf_url": "https://arxiv.org/pdf/2402.02559v1",
    "github_url": null,
    "published": "2024-02-04T16:23:16+00:00",
    "updated": "2024-02-04T16:23:16+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.02405v1",
    "title": "Angle Robustness Unmanned Aerial Vehicle Navigation in GNSS-Denied Scenarios",
    "authors": [
      "Wang",
      "Feng",
      "Zhang"
    ],
    "summary": "Due to the inability to receive signals from the Global Navigation Satellite System (GNSS) in extreme conditions, achieving accurate and robust navigation for Unmanned Aerial Vehicles (UAVs) is a challenging task. Recently emerged, vision-based navigation has been a promising and feasible alternative to GNSS-based navigation. However, existing vision-based techniques are inadequate in addressing flight deviation caused by environmental disturbances and inaccurate position predictions in practical settings. In this paper, we present a novel angle robustness navigation paradigm to deal with flight deviation in point-to-point navigation tasks. Additionally, we propose a model that includes the Adaptive Feature Enhance Module, Cross-knowledge Attention-guided Module and Robust Task-oriented Head Module to accurately predict direction angles for high-precision navigation. To evaluate the vision-based navigation methods, we collect a new dataset termed as UAV_AR368. Furthermore, we design the Simulation Flight Testing Instrument (SFTI) using Google Earth to simulate different flight environments, thereby reducing the expenses associated with real flight testing. Experiment results demonstrate that the proposed model outperforms the state-of-the-art by achieving improvements of 26.0% and 45.6% in the success rate of arrival under ideal and disturbed circumstances, respectively.",
    "pdf_url": "https://arxiv.org/pdf/2402.02405v1",
    "github_url": null,
    "published": "2024-02-04T08:41:20+00:00",
    "updated": "2024-02-04T08:41:20+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.01183v1",
    "title": "LINGO-Space: Language-Conditioned Incremental Grounding for Space",
    "authors": [
      "Kim",
      "Oh",
      "Hwang"
    ],
    "summary": "We aim to solve the problem of spatially localizing composite instructions referring to space: space grounding. Compared to current instance grounding, space grounding is challenging due to the ill-posedness of identifying locations referred to by discrete expressions and the compositional ambiguity of referring expressions. Therefore, we propose a novel probabilistic space-grounding methodology (LINGO-Space) that accurately identifies a probabilistic distribution of space being referred to and incrementally updates it, given subsequent referring expressions leveraging configurable polar distributions. Our evaluations show that the estimation using polar distributions enables a robot to ground locations successfully through $20$ table-top manipulation benchmark tests. We also show that updating the distribution helps the grounding method accurately narrow the referring space. We finally demonstrate the robustness of the space grounding with simulated manipulation and real quadruped robot navigation tasks. Code and videos are available at https://lingo-space.github.io.",
    "pdf_url": "https://arxiv.org/pdf/2402.01183v1",
    "github_url": null,
    "published": "2024-02-02T06:58:39+00:00",
    "updated": "2024-02-02T06:58:39+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2402.00537v2",
    "title": "Robust Path Planning via Learning from Demonstrations for Robotic Catheters in Deformable Environments",
    "authors": [
      "Li",
      "Lambranzi",
      "Wu"
    ],
    "summary": "Objective: Navigation through tortuous and deformable vessels using catheters with limited steering capability underscores the need for reliable path planning. State-of-the-art path planners do not fully account for the deformable nature of the environment. Methods: This work proposes a robust path planner via a learning from demonstrations method, named Curriculum Generative Adversarial Imitation Learning (C-GAIL). This path planning framework takes into account the interaction between steerable catheters and vessel walls and the deformable property of vessels. Results: In-silico comparative experiments show that the proposed network achieves a 38% higher success rate in static environments and 17% higher in dynamic environments compared to a state-of-the-art approach based on GAIL. In-vitro validation experiments indicate that the path generated by the proposed C-GAIL path planner achieves a targeting error of 1.26$\\pm$0.55mm and a tracking error of 5.18$\\pm$3.48mm. These results represent improvements of 41% and 40% over the conventional centerline-following technique for targeting error and tracking error, respectively. Conclusion: The proposed C-GAIL path planner outperforms the state-of-the-art GAIL approach. The in-vitro validation experiments demonstrate that the path generated by the proposed C-GAIL path planner aligns better with the actual steering capability of the pneumatic artificial muscle-driven catheter utilized in this study. Therefore, the proposed approach can provide enhanced support to the user in navigating the catheter towards the target with greater accuracy, effectively meeting clinical accuracy requirements. Significance: The proposed path planning framework exhibits superior performance in managing uncertainty associated with vessel deformation, thereby resulting in lower tracking errors.",
    "pdf_url": "https://arxiv.org/pdf/2402.00537v2",
    "github_url": null,
    "published": "2024-02-01T12:05:53+00:00",
    "updated": "2024-08-31T13:51:51+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.17914v1",
    "title": "Attention Graph for Multi-Robot Social Navigation with Deep Reinforcement Learning",
    "authors": [
      "Escudie",
      "Matignon",
      "Saraydaryan"
    ],
    "summary": "Learning robot navigation strategies among pedestrian is crucial for domain based applications. Combining perception, planning and prediction allows us to model the interactions between robots and pedestrians, resulting in impressive outcomes especially with recent approaches based on deep reinforcement learning (RL). However, these works do not consider multi-robot scenarios. In this paper, we present MultiSoc, a new method for learning multi-agent socially aware navigation strategies using RL. Inspired by recent works on multi-agent deep RL, our method leverages graph-based representation of agent interactions, combining the positions and fields of view of entities (pedestrians and agents). Each agent uses a model based on two Graph Neural Network combined with attention mechanisms. First an edge-selector produces a sparse graph, then a crowd coordinator applies node attention to produce a graph representing the influence of each entity on the others. This is incorporated into a model-free RL framework to learn multi-agent policies. We evaluate our approach on simulation and provide a series of experiments in a set of various conditions (number of agents / pedestrians). Empirical results show that our method learns faster than social navigation deep RL mono-agent techniques, and enables efficient multi-agent implicit coordination in challenging crowd navigation with multiple heterogeneous humans. Furthermore, by incorporating customizable meta-parameters, we can adjust the neighborhood density to take into account in our navigation strategy.",
    "pdf_url": "https://arxiv.org/pdf/2401.17914v1",
    "github_url": null,
    "published": "2024-01-31T15:24:13+00:00",
    "updated": "2024-01-31T15:24:13+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.17663v2",
    "title": "Social Robot Navigation with Adaptive Proxemics Based on Emotions",
    "authors": [
      "Bilen",
      "Kivrak",
      "Uluer"
    ],
    "summary": "The primary aim of this paper is to investigate the integration of emotions into the social navigation framework to analyse its effect on both navigation and human physiological safety and comfort. The proposed framework uses leg detection to find the whereabouts of people and computes adaptive proxemic zones based on their emotional state. We designed several case studies in a simulated environment and examined 3 different emotions; positive (happy), neutral and negative (angry). A survey study was conducted with 70 participants to explore their impressions about the navigation of the robot and compare the human safety and comfort measurements results. Both survey and simulation results showed that integrating emotions into proxemic zones has a significant effect on the physical safety of a human. The results revealed that when a person is angry, the robot is expected to navigate further than the standard distance to support his/her physiological comfort and safety. The results also showed that reducing the navigation distance is not preferred when a person is happy.",
    "pdf_url": "https://arxiv.org/pdf/2401.17663v2",
    "github_url": null,
    "published": "2024-01-31T08:32:33+00:00",
    "updated": "2024-02-02T07:05:36+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.14349v1",
    "title": "Learning to navigate efficiently and precisely in real environments",
    "authors": [
      "Bono",
      "Poirier",
      "Antsfeld"
    ],
    "summary": "In the context of autonomous navigation of terrestrial robots, the creation of realistic models for agent dynamics and sensing is a widespread habit in the robotics literature and in commercial applications, where they are used for model based control and/or for localization and mapping. The more recent Embodied AI literature, on the other hand, focuses on modular or end-to-end agents trained in simulators like Habitat or AI-Thor, where the emphasis is put on photo-realistic rendering and scene diversity, but high-fidelity robot motion is assigned a less privileged role. The resulting sim2real gap significantly impacts transfer of the trained models to real robotic platforms. In this work we explore end-to-end training of agents in simulation in settings which minimize the sim2real gap both, in sensing and in actuation. Our agent directly predicts (discretized) velocity commands, which are maintained through closed-loop control in the real robot. The behavior of the real robot (including the underlying low-level controller) is identified and simulated in a modified Habitat simulator. Noise models for odometry and localization further contribute in lowering the sim2real gap. We evaluate on real navigation scenarios, explore different localization and point goal calculation methods and report significant gains in performance and robustness compared to prior work.",
    "pdf_url": "https://arxiv.org/pdf/2401.14349v1",
    "github_url": null,
    "published": "2024-01-25T17:50:05+00:00",
    "updated": "2024-01-25T17:50:05+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.13800v1",
    "title": "Multi-Object Navigation in real environments using hybrid policies",
    "authors": [
      "Sadek",
      "Bono",
      "Chidlovskii"
    ],
    "summary": "Navigation has been classically solved in robotics through the combination of SLAM and planning. More recently, beyond waypoint planning, problems involving significant components of (visual) high-level reasoning have been explored in simulated environments, mostly addressed with large-scale machine learning, in particular RL, offline-RL or imitation learning. These methods require the agent to learn various skills like local planning, mapping objects and querying the learned spatial representations. In contrast to simpler tasks like waypoint planning (PointGoal), for these more complex tasks the current state-of-the-art models have been thoroughly evaluated in simulation but, to our best knowledge, not yet in real environments.   In this work we focus on sim2real transfer. We target the challenging Multi-Object Navigation (Multi-ON) task and port it to a physical environment containing real replicas of the originally virtual Multi-ON objects. We introduce a hybrid navigation method, which decomposes the problem into two different skills: (1) waypoint navigation is addressed with classical SLAM combined with a symbolic planner, whereas (2) exploration, semantic mapping and goal retrieval are dealt with deep neural networks trained with a combination of supervised learning and RL. We show the advantages of this approach compared to end-to-end methods both in simulation and a real environment and outperform the SOTA for this task.",
    "pdf_url": "https://arxiv.org/pdf/2401.13800v1",
    "github_url": null,
    "published": "2024-01-24T20:41:25+00:00",
    "updated": "2024-01-24T20:41:25+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.12275v2",
    "title": "Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation",
    "authors": [
      "Li",
      "Hua",
      "Yao"
    ],
    "summary": "Social robot navigation can be helpful in various contexts of daily life but requires safe human-robot interactions and efficient trajectory planning. While modeling pairwise relations has been widely studied in multi-agent interacting systems, the ability to capture larger-scale group-wise activities is limited. In this paper, we propose a systematic relational reasoning approach with explicit inference of the underlying dynamically evolving relational structures, and we demonstrate its effectiveness for multi-agent trajectory prediction and social robot navigation. In addition to the edges between pairs of nodes (i.e., agents), we propose to infer hyperedges that adaptively connect multiple nodes to enable group-wise reasoning in an unsupervised manner. Our approach infers dynamically evolving relation graphs and hypergraphs to capture the evolution of relations, which the trajectory predictor employs to generate future states. Meanwhile, we propose to regularize the sharpness and sparsity of the learned relations and the smoothness of the relation evolution, which proves to enhance training stability and model performance. The proposed approach is validated on synthetic crowd simulations and real-world benchmark datasets. Experiments demonstrate that the approach infers reasonable relations and achieves state-of-the-art prediction performance. In addition, we present a deep reinforcement learning (DRL) framework for social robot navigation, which incorporates relational reasoning and trajectory prediction systematically. In a group-based crowd simulation, our method outperforms the strongest baseline by a significant margin in terms of safety, efficiency, and social compliance in dense, interactive scenarios. We also demonstrate the practical applicability of our method with real-world robot experiments. The code and videos can be found at https://relational-reasoning-nav.github.io/.",
    "pdf_url": "https://arxiv.org/pdf/2401.12275v2",
    "github_url": null,
    "published": "2024-01-22T18:58:22+00:00",
    "updated": "2024-11-11T18:59:07+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.07314v3",
    "title": "MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation",
    "authors": [
      "Chen",
      "Lin",
      "Xu"
    ],
    "summary": "Embodied agents equipped with GPT as their brains have exhibited extraordinary decision-making and generalization abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt GPT-4 to select potential locations within localized environments, without constructing an effective \"global-view\" for the agent to understand the overall environment. In this work, we present a novel map-guided GPT-based agent, dubbed MapGPT, which introduces an online linguistic-formed map to encourage global exploration. Specifically, we build an online map and incorporate it into the prompts that include node information and topological relationships, to help GPT understand the spatial environment. Benefiting from this design, we further propose an adaptive planning mechanism to assist the agent in performing multi-step path planning based on a map, systematically exploring multiple candidate nodes or sub-goals step by step. Extensive experiments demonstrate that our MapGPT is applicable to both GPT-4 and GPT-4V, achieving state-of-the-art zero-shot performance on R2R and REVERIE simultaneously (~10% and ~12% improvements in SR), and showcasing the newly emergent global thinking and path planning abilities of the GPT.",
    "pdf_url": "https://arxiv.org/pdf/2401.07314v3",
    "github_url": null,
    "published": "2024-01-14T15:34:48+00:00",
    "updated": "2024-06-20T07:23:45+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.06226v1",
    "title": "Learning Crowd Behaviors in Navigation with Attention-based Spatial-Temporal Graphs",
    "authors": [
      "Zhou",
      "Garcke"
    ],
    "summary": "Safe and efficient navigation in dynamic environments shared with humans remains an open and challenging task for mobile robots. Previous works have shown the efficacy of using reinforcement learning frameworks to train policies for efficient navigation. However, their performance deteriorates when crowd configurations change, i.e. become larger or more complex. Thus, it is crucial to fully understand the complex, dynamic, and sophisticated interactions of the crowd resulting in proactive and foresighted behaviors for robot navigation. In this paper, a novel deep graph learning architecture based on attention mechanisms is proposed, which leverages the spatial-temporal graph to enhance robot navigation. We employ spatial graphs to capture the current spatial interactions, and through the integration with RNN, the temporal graphs utilize past trajectory information to infer the future intentions of each agent. The spatial-temporal graph reasoning ability allows the robot to better understand and interpret the relationships between agents over time and space, thereby making more informed decisions. Compared to previous state-of-the-art methods, our method demonstrates superior robustness in terms of safety, efficiency, and generalization in various challenging scenarios.",
    "pdf_url": "https://arxiv.org/pdf/2401.06226v1",
    "github_url": null,
    "published": "2024-01-11T19:09:41+00:00",
    "updated": "2024-01-11T19:09:41+00:00"
  },
  {
    "id": "http://arxiv.org/abs/2401.02695v2",
    "title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
    "authors": [
      "Wu",
      "Mu",
      "Wu"
    ],
    "summary": "In the realm of household robotics, the Zero-Shot Object Navigation (ZSON) task empowers agents to adeptly traverse unfamiliar environments and locate objects from novel categories without prior explicit training. This paper introduces VoroNav, a novel semantic exploration framework that proposes the Reduced Voronoi Graph to extract exploratory paths and planning nodes from a semantic map constructed in real time. By harnessing topological and semantic information, VoroNav designs text-based descriptions of paths and images that are readily interpretable by a large language model (LLM). In particular, our approach presents a synergy of path and farsight descriptions to represent the environmental context, enabling LLM to apply commonsense reasoning to ascertain waypoints for navigation. Extensive evaluation on HM3D and HSSD validates VoroNav surpasses existing benchmarks in both success rate and exploration efficiency (absolute improvement: +2.8% Success and +3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally introduced metrics that evaluate obstacle avoidance proficiency and perceptual efficiency further corroborate the enhancements achieved by our method in ZSON planning. Project page: https://voro-nav.github.io",
    "pdf_url": "https://arxiv.org/pdf/2401.02695v2",
    "github_url": null,
    "published": "2024-01-05T08:05:07+00:00",
    "updated": "2024-02-06T05:15:20+00:00"
  }
]